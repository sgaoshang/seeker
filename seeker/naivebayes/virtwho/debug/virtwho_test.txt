spam	[[<description>able to register, unable to subscribe in satellite</description>], <text>[root@crlrnwasp001 ~]# subscription-manager register --org="SIC" --activationkey="PROD_RHEL5_I386_PROD_AK"\nThe system has been registered with ID: 9a9a8288-1160-401a-9a33-9fcc2b9e461c\nInstalled Product Current Status:\nProduct Name: Red Hat Enterprise Linux Server\nStatus:       Not Subscribed\n\nUnable to find available subscriptions for all your installed products.\n==============\n[root@crlrnwasp001 ~]# subscription-manager repos\nThis system has no repositories available through subscriptions.</text>, <text>Hello,\n \nThank you for contacting Red Hat Technical Support. My name is Ganesh and I will assist you with this service request.\n\nLooking at the current scenario it seems like you have not added the appropriate subscription to the activation key.\n\nTo dig deeper into the issue we need to look at the activation key details also, confirm the machine which is being registered is the virtual machine or the physical one. \nPlease provide me the screen-shot of the activation key \n\nLogin to Satellite webUI --&gt; Content --&gt; Activation key --&gt; Select the activation key --&gt; provide the screen-shot for Details tab , subscription tab , Product content\n\nAlso, from the client machine provide the below command output\n\n# cd /etc/pki/product\n\n# rct cat-cert &lt;certname&gt;\n\n\n\nIn the meanwhile feel free to revert back if you have any further query related to same reported concern.\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>[root@crlrnwasp001 product]# rct cat-cert 69.pem\n\n+-------------------------------------------+\n        Product Certificate\n+-------------------------------------------+\n\nCertificate:\n        Path: 69.pem\n        Version: 1.0\n        Serial: 12750047592154746050\n        Start Date: 2014-02-20 10:27:58+00:00\n        End Date: 2034-02-15 10:27:58+00:00\n\nSubject:\n        CN: Red Hat Product ID [617cad16-787d-4849-99ac-cb7d6c6883e4]\n\nIssuer:\n        C: US\n        CN: Red Hat Entitlement Product Authority\n        O: Red Hat, Inc.\n        OU: Red Hat Network\n        ST: North Carolina\n        emailAddress: ca-support@redhat.com\n\nProduct:\n        ID: 69\n        Name: Red Hat Enterprise Linux Server\n        Version: 5.11\n        Arch: i386\n        Tags: rhel-5,rhel-5-server\n        Brand Type:\n        Brand Name:</text>, <text>Hello,\n\nThanks for providing the requested details.\n\nBased on the output you provided it seems like you attached the VDC ( Virtual Datacentre subscription) subscription directly to the client machine.\nHowever, the standard procedure is to attach the VDC subscription to the Hyper-visor and remove the subscription from the activation key.\n\nPlease try above steps and let us know the status. If you are looking for the virt-who mechanism it will take some time to reflect the changes. Where as you can try directly clicking on run auto-attach subscription in the -- subscription tab under the content host.\n\nMake sure you run auto-attach after following the procedure which i mentioned above.\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>Ganesh,\n\ncan you setup a remote session, i am quite confused with the subscriptions.\n\nPlease send me a link , i will be joining in once i get a link.</text>, <text>Remote Support Invitation\n\nClick on the link below and follow the directions.\n\nhttps://remotesupport.redhat.com/?ak=465d4ea9e730342151628ff4e5fb7bfb\n\nBomgar(TM) enables a support representative to view your screen in order to assist you.\nSession traffic is fully encrypted to protect your system's data.\nOnce a session has begun, you will be able to end it at any time</text>, <text>Hello,\n\nPlease find the remote session link below \n\nBomgar Session Key:1699955\nBomgar Key Url:https://remotesupport.redhat.com/?ak=465d4ea9e730342151628ff4e5fb7bfb\n \n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>Hello,\nThanks for your time over the call and remote session.\n\nThe issue is like there are two physical hosts which are not getting subscribed because there is no available physical subscriptions.\nAfter digging into the issue we found out there are virtual machines which are registered and subscribed to the Red hat smart management subscription.\n\nWe also tried removing the subscriptions of one virtual host and tried attaching it to the Physical server but the subscription is calculated as below \n\n1 Physical = 2 virtual hosts and that is socket based subscriptions and we tried find out the physical host socket those were 8 so as per the calculation we need 4 subscriptions in the bucket.\n\nFinally, to overcome this scenario i have suggested you to remove the smart management subscriptions from the virtual host and attache VDC to the virtual machine. But there are few virtual hosts which hyper-visors are not properly entitled with the Virtual Datacentre subscription. \n\nThen we tried to find out the prudent hyper-visor for that particular virtual host but those were not reported in the satellite webui.\n\nHere comes the virt-who configuration in picture, to analyze this further we need sosreport of the satellite host.\n\nPlease help us out with the sosreport of the satellite server so that we can guide you further.\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>Ganesh,\n\ni am unable to run sosreport on satellite server, below are the errors.\n\n[root@pdxlrhssp002 tmp]# sosreport --skip-plugin ipmitool\n\nsosreport (version 3.4)\n\nThis command will collect diagnostic and configuration information from\nthis Red Hat Enterprise Linux system and installed applications.\n\nAn archive containing the collected information will be generated in\n/var/tmp/sos.Q_dllh and may be provided to a Red Hat support\nrepresentative.\n\nAny information provided to Red Hat will be treated in accordance with\nthe published support policies at:\n\n  https://access.redhat.com/support/\n\nThe generated archive may contain data considered sensitive and its\ncontent should be reviewed by the originating organization before being\npassed to any third party.\n\nNo changes will be made to system configuration.\n==========\ni have attached the /var/tmp/sar file</text>, <text>Hello,\n\nI have checked the provided logs but that does not contain the required logs for analyzing the issue.\n\nWould you please provide me the below files \n\n1. /etc/virt-who.d/* all files under this directory\n\n2. /etc/sysonconfig/virt-who\n\n3. /var/log/* all log files under this directory \n\nAlso, the below command output \n\n# virt-who -d -o --- Run this command on the satellite server and provide the output.\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>Ganesh,\n\nattached the outputs, unable to send the /var/log/files, since itss comes around 6 GB, let me know if exact file need under /var/log ?</text>, <text>Hello,\n\nThanks for providing the virt-who file.\n\nLooking at the virt file i understood that there is filter applied in the configuration file and there are few hosts . I guess the host which we were looking for that might be filtered out in the file.\n\nWould you please check all the virtual machines -  hyper-visors name and just make sure those are not filtered in the configuration file.\nPlease make changes accordingly and try to remove the physical subscription form the virtual machines and attach the virtual datacentre subscription to the hypservisors associated to the specific virtual machine and then restart the virt-who service.\n\nFeel free to revert back with more comments or queries.\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>I have cleared up the some of the physical licenses\n\nRed Hat Enterprise Linux Server with Smart Management, Premium (Physical or Virtual Nodes)\n8 out of 18\tPhysical  1/30/17\t1/30/18\tPremium\t11277092\t1039931\n\n===========\ni have another 10 physical licenses free, let me know how to attache these subscription to this physical server.\n[root@crlrnwasp001 ~]# dmidecode -t 1\n# dmidecode 2.12\nSMBIOS 2.5 present.\n\nHandle 0x002C, DMI type 1, 27 bytes\nSystem Information\n        Manufacturer: IBM\n        Product Name: System x3550 M3 -[7944KAG]-\n        Version: 00\n        Serial Number: KD14XAT\n        UUID: FDF1BD7D-DF7A-3CBA-BBD1-E5E1BAF39B51\n        Wake-up Type: Power Switch\n        SKU Number: XxXxXxX\n        Family: System x</text>, <text>Hello,\n\nGlad to hear that there are free physical subscriptions available now.\n\nPlease register the machine using the activation key or using standard method and then subscribe the machine if you are registered through the standard method i.e. # subscription-manager register \n\n# subscription-manager list --available\n\n# subscription-manager attach --pool=&lt;enter the Smart management pool ID over here&gt;\n\nIf you are using the activation key then add the subscription to the activation key and then register using the activation key.\n\nAnd share the observation.\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>Ganesh,\n\nable to add the subscriptions, yum repolist are giving repos, but in satellite GUI, Subscription Status is still showing (red color), not sure why\n\n# subscription-manager status\n+-------------------------------------------+\n   System Status Details\n+-------------------------------------------+\nOverall Status: Current</text>, <text>Hello,\n\nWould you please confirm which subscription you attached to the client machine.\n\nI believe that is the physical host?\n\nPlease provide the below command details from the client machine \n\n# subscription-manager status\n\n# subscription-manager list --consumed\n\n# subscription-manager identity\n\n# subscription-manager repos --list\n\n# yum repolist\n\nAlso, provide the screen-shot from the satellite webui where you are seeing subscription status in red color.\n\nBest Regards,\nGanesh Patil, \nGSS Red Hat.</text>, <text>[root@crlrnwasp002 ~]# subscription-manager status\n+-------------------------------------------+\n   System Status Details\n+-------------------------------------------+\nOverall Status: Current\n\n[root@crlrnwasp002 ~]# subscription-manager list --consumed\n+-------------------------------------------+\n   Consumed Subscriptions\n+-------------------------------------------+\nSubscription Name: Red Hat Enterprise Linux Server with Smart Management, Premium (Physical or Virtual Nodes)\nProvides:          Oracle Java (for RHEL Server)\n                   Red Hat Enterprise Linux High Performance Networking (for RHEL Server) - Extended Update Support\n                   Red Hat Enterprise Linux Server - Extended Update Support\n                   Red Hat Developer Tools Beta (for RHEL Server)\n                   Red Hat Enterprise Linux Server\n                   Red Hat Enterprise Linux Atomic Host\n                   Red Hat EUCJP Support (for RHEL Server) - Extended Update Support\n                   Oracle Java (for RHEL Server) - Extended Update Support\n                   Red Hat Enterprise Linux Resilient Storage (for RHEL Server) - Extended Update Support\n                   dotNET on RHEL Beta (for RHEL Server)\n                   Red Hat Beta\n                   Red Hat Developer Tools (for RHEL Server)\n                   Red Hat Software Collections (for RHEL Server)\n                   dotNET on RHEL (for RHEL Server)\n                   Red Hat Developer Toolset (for RHEL Server)\n                   Red Hat Enterprise Linux Atomic Host Beta\n                   Red Hat Enterprise Linux High Availability (for RHEL Server) - Extended Update Support\n                   Red Hat Enterprise Linux Scalable File System (for RHEL Server) - Extended Update Support\n                   Red Hat Software Collections Beta (for RHEL Server)\n                   Red Hat Enterprise Linux Load Balancer (for RHEL Server) - Extended Update Support\n                   Red Hat Container Images\n                   Red Hat Container Images Beta\n                   Red Hat S-JIS Support (for RHEL Server) - Extended Update Support\nSKU:               RH00008S\nContract:          11277092\nAccount:           1039931\nSerial:            7637778076736262455\nPool ID:           402848ef59f6e64b0159f6f3a01301d3\nActive:            True\nQuantity Used:     2\nService Level:     Premium\nService Type:      L1-L3\nStatus Details:\nSubscription Type: Instance Based\nStarts:            01/30/2017\nEnds:              01/30/2018\nSystem Type:       Physical\n\n[root@crlrnwasp002 ~]# subscription-manager identity\nsystem identity: 516e1a8c-46e8-4ece-a08c-f645e446c238\nname: crlrnwasp002\norg name: SIC\norg ID: SIC\nenvironment name: PROD/A_RHEL5_i386_CCV\n[root@crlrnwasp002 ~]# subscription-manager repos --list\n+----------------------------------------------------------+\n    Available Repositories in /etc/yum.repos.d/redhat.repo\n+----------------------------------------------------------+\nRepo ID:   rhel-5-server-satellite-tools-6.2-rpms\nRepo Name: Red Hat Satellite Tools 6.2 (for RHEL 5 Server) (RPMs)\nRepo URL:  https://pdxlrhssp002.standard.com/pulp/repos/SIC/PROD/A_RHEL5_i386_CCV/content/dist/rhel/server/5/5Server/$basearch/sat-tools/6.2/os\nEnabled:   1\n\nRepo ID:   rhel-5-server-rpms\nRepo Name: Red Hat Enterprise Linux 5 Server (RPMs)\nRepo URL:  https://pdxlrhssp002.standard.com/pulp/repos/SIC/PROD/A_RHEL5_i386_CCV/content/dist/rhel/server/5/$releasever/$basearch/os\nEnabled:   1\n\nRepo ID:   rhel-5-server-thirdparty-oracle-java-rpms\nRepo Name: Red Hat Enterprise Linux 5 Server - Oracle Java (RPMs)\nRepo URL:  https://pdxlrhssp002.standard.com/pulp/repos/SIC/PROD/A_RHEL5_i386_CCV/content/dist/rhel/server/5/$releasever/$basearch/oracle-java/os\nEnabled:   0\n\n[root@crlrnwasp002 ~]# yum repolist\nLoaded plugins: enabled_repos_upload, package_upload, product-id, security, subscription-manager\nrhel-5-server-rpms                                                                                                                                                  | 2.0 kB     00:00\nrhel-5-server-satellite-tools-6.2-rpms                                                                                                                              | 1.8 kB     00:00\nrepo id                                                                           repo name                                                                                          status\npe_repo                                                                           Puppet Labs PE Packages 5Server - i386                                                                  1\nrhel-5-server-rpms                                                                Red Hat Enterprise Linux 5 Server (RPMs)                                                           13,514\nrhel-5-server-satellite-tools-6.2-rpms                                            Red Hat Satellite Tools 6.2 (for RHEL 5 Server) (RPMs)                                                 71\nrepolist: 13,586\nUploading Enabled Repositories Report\nLoaded plugins: product-id</text>, <text>Hello,\n\nThanks for your patience.\n\nIt seems like you are hitting the below mentioned bug, Please find the  Bugzilla details below \n\n#####################################################################################################\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1446706\n\n#####################################################################################################\n\nThe issue which you are facing is already reported in the Bugzilla. The raised Bugzilla is the Public one so you can directly add yourself in the CC list.\n\nFeel free to revert back with more comments or queries.\n\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>Ganesh,\n\nlet us know the status on the bugzilla case. we are still seeing the clients are showing red even though its subscribed.</text>, <text>Hello,\n\nAs of now there is no update from our Engineering team.\n\nAs soon as we get an update will share you same. Highly appreciated your patience.\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>Hello,\n\nCu is looking for a workaround on this issue for Satellite 6.2.\n\nCan we request for the same?\n\nRegards,\nRajan</text>, <text>Re: comment 8.  This is a clone of bug 1371605, which appears to have been delivered in 6.2.10.</text>, <text>Issue has re-occurred in 6.2.12. \n\nCould you please check and confirm again if this still needs fix?\n\nThanks,\nRajan</text>, <text>EMT Note: \nHi Team,\n\nCustomer escalated this case to management:\n\nCu comments: "I now URGENTLY need a means to identify the used entitlements in our disconnected satellites."\n\nRegards,\n\nAmit Tripathi\nEscalation Manager\nRed Hat - Customer Experience &amp; Engagement.</text>, <text>Hi, \n\nMy Case#01954616 is also hitting the similar issue. Any suggestion for this?\n\n\n===Environment\n\nsatellite-6.2.10 with rhel6\n\n===\n\n===Problem description\n\n8 rhel7u4 VM clients could register the satellite6 server and run `yum update` successfully. The `subscription-manager list --consumed` status(#3) of these client is normal also.\n\nBut all these 8 VMs status shows as "Unentitled" in satellite web UI.\n\n===Already Done\n\n1. It tried to unregister/register the client. But this status are still the same in satellite webUI.\n\n2. Cu also tried the registration with a freshed installed rhel7u3 system. But the status also show as "Unentitled" in satellite webUI.\n====\n\n\n\nBest Regards\nVansen Ng</text>, <text>Could we have update on this bug please</text>, <text>Hello,\n\nAs of now there is no update from our Engineering team.\n\nAs soon as we get an update will share you same. Highly appreciated your patience.\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>Hello,\n\nThanks for your patience .\n\nStill we don't have any update, we will keep you posted .\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat</text>, <text>Since the problem described in this bug report should be resolved in a recent advisory, it has been closed with a resolution of ERRATA.\n&gt; \n&gt; For information on the advisory, and where to find the updated files, follow the link below.\n&gt; \n&gt; If the solution does not work for you, open a new bug report.\n&gt; \n&gt; https://access.redhat.com/errata/RHSA-2018:0336</text>, <text>Hello,\n\nThanks for your patience with us.\n\nAs per the recent Bugzilla status, i glad to inform you that the issue has been fixed in the recent advisory.\n\nPlease find the advisory link below and apply it to resolve the issue.\n\n\n===============================================================\nhttps://access.redhat.com/errata/RHSA-2018:0336\n==============================================================\n\nAlso, We are pleased to inform that the Red Hat Satellite 6.3 has been released. This version has resolved the Bugzilla ticket which is linked to this case.\n\nRequest you to please update the Red Hat Satellite version to have the fix in place, if you observe the issue still exist, please open a new case. Will be happy to assist you.\n\nHave a good time ahead.\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>, <text>Ganesh,\n\nPlease share me the procedure to upgrade the satellite server from 6.2.13 to 6.3</text>, <text>Hello,\n\nPlease find the below document which will give you the clear idea about the up-gradation from the satellite version 6.2 to Satellite version 6.3\n\n=============================================================================================================\nhttps://access.redhat.com/documentation/en-us/red_hat_satellite/6.3/html/upgrading_and_updating_red_hat_satellite/upgrading_red_hat_satellite\n==============================================================================================================\n\nNote: Please do make sure that the current satellite server snapshot/backup is there. Without the snapshot/backup of the satellite please do not go ahead to avoid any future consequences.\n\nThere are different scenarios to upgrade the satellite server to 6.3 . Hence, please make sure that you have read the document very carefully before going further.\n\nBest Regards,\nGanesh Patil,\nGSS Red Hat.</text>]
spam	[[<description>Was bzw. welches Verhalten bereitet Ihnen Probleme? Was ist dabei unerwartet?\n\nAfter installation, configuration and starting virt-who following error-message appears:\nManagerError: Communication with subscription manager failed with code 404: Organization with id FiduciaGAD could not be found.\nChanging ORG_LABEL to ORG_ID will not solve the problem:\nManagerError: Communication with subscription manager failed with code 404: Organization with id 1 could not be found.\nAdding the four parameters rhsm_* to virt-who configuration will generate following error message:\n2017-10-05 15:54:27,838 [ERROR] @virt.py:389 - Thread 'destination_2407043753446613147' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n\nGenerating a sosreport is not possible, it stops at "Setting up plugins..." and did not continue.</description>], <text>This comment has been generated via an automated case assistant.\n\n \n\nThe following resources may be helpful for working with virt-who and virtual Subscriptions (including Virtual Data Center):\n\n \n\n    https://access.redhat.com/labs/virtwhoconfig/ Red Hat Virtualization Agent (virt-who) Configuration Helper - virt-who is required when working with any hypervisor if your subscription name includes the words 'Virtual Datacenters', '4 Guests', or 'Unlimited Guests'. This app helps users configure virt-who. When started, the app requests information about your virt-who usage and generates a configuration file based on he answers. It also provides instructions for updating your virt-who configuration.\n    https://access.redhat.com/blogs/1169563/posts/2190731 Subscription-manager for the former Red Hat Network User: Part 3 - Understanding virt-who \n    https://access.redhat.com/blogs/1169563/posts/2630111 Subscription-manager for the former Red Hat Network user - part 5 - Working with subscriptions that require virt-who\n    https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/virtual_instances_guide/ - Virtual Instances Guide for Satellite 6\n    https://access.redhat.com/documentation/en-us/red_hat_subscription_management/1/html/virtual_instances_guide/ - Virtual Instances Guide for use with Red Hat Subscription Management (RHSM) but no Satellite</text>, <text>*PRIVATE UPDATE, DO NOT MAKE PUBLIC*\n\n    The previous public update is part of a pilot to improve auto-solve and self-solve, by commenting on cases that match certain rules upon creation, to provide answers to common questions/issues.  You can find more information about this project[1], the rule match for this case[2], and how to provide feedback (whether the rule is helpful, suggestions for improvement) here:\n\n    [1] https://mojo.redhat.com/groups/support-delivery/projects/diagnostics-20\n    [2] https://projects.engineering.redhat.com/browse/CPSEARCH-1503\n    [3] https://mojo.redhat.com/thread/942968</text>, <text>Hello Klaus,\n\nlet's first focus on the sosreport, as we really should make this working.\n\nCould you kindly capture the full output as well as the commands used for the following :\n\n# rpm -qa sos\n# abrt-cli list\n# sosreport -vvv\n# df -hP\n\n\n\nAfter we have solved the sosreport issue, I suggest we focus on the virt-who meeting,\nif you agree.\n\nThanks,\nDaniel</text>, <text>Hello Daniel,\n\nhere the desired informations:\n\n[root@rhcaps6-r-01 ~]# df -h\nFilesystem                           Size  Used Avail Use% Mounted on\n/dev/mapper/system-lvroot            4.0G   76M  4.0G   2% /\ndevtmpfs                             7.8G     0  7.8G   0% /dev\ntmpfs                                7.8G   12K  7.8G   1% /dev/shm\ntmpfs                                7.8G  720K  7.8G   1% /run\ntmpfs                                7.8G     0  7.8G   0% /sys/fs/cgroup\n/dev/mapper/system-lvusr             4.0G  1.9G  2.2G  46% /usr\n/dev/sda1                            477M  202M  246M  46% /boot\n/dev/mapper/system-lvtmp             6.0G   34M  6.0G   1% /tmp\n/dev/mapper/system-lvopt             8.0G  840M  7.2G  11% /opt\n/dev/mapper/system-lvhome            2.0G   33M  2.0G   2% /home\n/dev/mapper/system-lvvar             4.0G  2.3G  1.8G  57% /var\n/dev/mapper/vg_rhsat6-lv_cache_pulp   10G   33M   10G   1% /var/cache/pulp\n/dev/mapper/vg_rhsat6db-lv_mongodb    25G   11G   15G  41% /var/lib/mongodb\n/dev/mapper/vg_rhsat6-lv_pulp        340G  199G  142G  59% /var/lib/pulp\ntmpfs                                1.6G     0  1.6G   0% /run/user/8863\n[root@rhcaps6-r-01 ~]# rpm -qa sos\nsos-3.4-6.el7.noarch\n[root@rhcaps6-r-01 ~]# abrt-cli list\nid effb8cfd4601079940e10e1ce035274fd64b2ecd\nreason:         python2.7 killed by SIGBUS\ntime:           Thu 14 Sep 2017 11:13:20 AM CEST\ncmdline:        /usr/bin/python /usr/bin/celery worker -n reserved_resource_worker-1@%h -A pulp.server.async.app -c 1 --events --umask 18 --pidfile=/var/run/pulp/reserved_resource_worker-1.pid --heartbeat-interval=30\npackage:        python-celery-3.1.11-1.el7sat\nuid:            48 (apache)\ncount:          1\nDirectory:      /var/spool/abrt/ccpp-2017-09-14-11:13:20-15833\nRun 'abrt-cli report /var/spool/abrt/ccpp-2017-09-14-11:13:20-15833' for creating a case in Red Hat Customer Portal\n\nThe Autoreporting feature is disabled. Please consider enabling it by issuing\n'abrt-auto-reporting enabled' as a user with root privileges\n[root@rhcaps6-r-01 ~]# sosreport -vvv\nset sysroot to '/' (default)\n\nsosreport (version 3.4)\n\nThis command will collect diagnostic and configuration information from\nthis Red Hat Enterprise Linux system and installed applications.\n\nAn archive containing the collected information will be generated in\n/var/tmp/sos.v0fKY9 and may be provided to a Red Hat support\nrepresentative.\n\nAny information provided to Red Hat will be treated in accordance with\nthe published support policies at:\n\n  https://access.redhat.com/support/\n\nThe generated archive may contain data considered sensitive and its\ncontent should be reviewed by the originating organization before being\npassed to any third party.\n\nNo changes will be made to system configuration.\n\nPress ENTER to continue, or CTRL-C to quit.\n\nPlease enter your first initial and last name [rhcaps6-r-01]:\nPlease enter the case id that you are generating this report for []:\n\n Setting up archive ...\n[archive:TarFileArchive] initialised empty FileCacheArchive at '/var/tmp/sos.v0fKY9/sosreport-rhcaps6-r-01-20171006064801'\n[sos.sosreport:setup] executing 'sosreport -vvv'\n Setting up plugins ...\n[plugin:abrt] packed command tuple: ('abrt-cli status', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:abrt] added cmd output 'abrt-cli status'\n[plugin:abrt] collected output of 'abrt-cli' in 0.0268261432648\n[plugin:apache] packed command tuple: ('apachectl -M', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:apache] added cmd output 'apachectl -M'\n[plugin:ata] packed command tuple: ('hdparm /dev/sda', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ata] added cmd output 'hdparm /dev/sda'\n[plugin:ata] packed command tuple: ('smartctl -a /dev/sda', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ata] added cmd output 'smartctl -a /dev/sda'\n[plugin:ata] packed command tuple: ('hdparm /dev/sdb', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ata] added cmd output 'hdparm /dev/sdb'\n[plugin:ata] packed command tuple: ('smartctl -a /dev/sdb', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ata] added cmd output 'smartctl -a /dev/sdb'\n[plugin:ata] packed command tuple: ('hdparm /dev/sdc', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ata] added cmd output 'hdparm /dev/sdc'\n[plugin:ata] packed command tuple: ('smartctl -a /dev/sdc', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ata] added cmd output 'smartctl -a /dev/sdc'\n[plugin:ata] packed command tuple: ('hdparm /dev/sdd', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ata] added cmd output 'hdparm /dev/sdd'\n[plugin:ata] packed command tuple: ('smartctl -a /dev/sdd', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ata] added cmd output 'smartctl -a /dev/sdd'\n[plugin:auditd] packed command tuple: ('ausearch --input-logs -m avc,user_avc -ts today', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:auditd] added cmd output 'ausearch --input-logs -m avc,user_avc -ts today'\n[plugin:auditd] packed command tuple: ('auditctl -s', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:auditd] added cmd output 'auditctl -s'\n[plugin:block] packed command tuple: ('lsblk', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'lsblk'\n[plugin:block] packed command tuple: ('blkid -c /dev/null', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'blkid -c /dev/null'\n[plugin:block] packed command tuple: ('blockdev --report', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'blockdev --report'\n[plugin:block] packed command tuple: ('ls -lanR /dev', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'ls -lanR /dev'\n[plugin:block] packed command tuple: ('ls -lanR /sys/block', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'ls -lanR /sys/block'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/sda', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/sda'\n[plugin:block] packed command tuple: ('parted -s /dev/sda unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/sda unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/sda', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/sda'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/sdb', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/sdb'\n[plugin:block] packed command tuple: ('parted -s /dev/sdb unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/sdb unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/sdb', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/sdb'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/sdc', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/sdc'\n[plugin:block] packed command tuple: ('parted -s /dev/sdc unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/sdc unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/sdc', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/sdc'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/sdd', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/sdd'\n[plugin:block] packed command tuple: ('parted -s /dev/sdd unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/sdd unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/sdd', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/sdd'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/sr0', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/sr0'\n[plugin:block] packed command tuple: ('parted -s /dev/sr0 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/sr0 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/sr0', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/sr0'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/dm-0', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/dm-0'\n[plugin:block] packed command tuple: ('parted -s /dev/dm-0 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/dm-0 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/dm-0', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/dm-0'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/dm-1', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/dm-1'\n[plugin:block] packed command tuple: ('parted -s /dev/dm-1 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/dm-1 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/dm-1', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/dm-1'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/dm-2', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/dm-2'\n[plugin:block] packed command tuple: ('parted -s /dev/dm-2 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/dm-2 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/dm-2', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/dm-2'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/dm-3', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/dm-3'\n[plugin:block] packed command tuple: ('parted -s /dev/dm-3 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/dm-3 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/dm-3', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/dm-3'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/dm-4', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/dm-4'\n[plugin:block] packed command tuple: ('parted -s /dev/dm-4 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/dm-4 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/dm-4', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/dm-4'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/dm-5', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/dm-5'\n[plugin:block] packed command tuple: ('parted -s /dev/dm-5 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/dm-5 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/dm-5', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/dm-5'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/dm-6', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/dm-6'\n[plugin:block] packed command tuple: ('parted -s /dev/dm-6 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/dm-6 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/dm-6', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/dm-6'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/dm-7', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/dm-7'\n[plugin:block] packed command tuple: ('parted -s /dev/dm-7 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/dm-7 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/dm-7', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/dm-7'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/dm-8', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/dm-8'\n[plugin:block] packed command tuple: ('parted -s /dev/dm-8 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/dm-8 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/dm-8', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/dm-8'\n[plugin:block] packed command tuple: ('udevadm info -ap /sys/block/dm-9', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'udevadm info -ap /sys/block/dm-9'\n[plugin:block] packed command tuple: ('parted -s /dev/dm-9 unit s print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'parted -s /dev/dm-9 unit s print'\n[plugin:block] packed command tuple: ('fdisk -l /dev/dm-9', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:block] added cmd output 'fdisk -l /dev/dm-9'\n[plugin:boot] packed command tuple: ('ls -lanR /boot', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:boot] added cmd output 'ls -lanR /boot'\n[plugin:boot] packed command tuple: ('lsinitrd', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:boot] added cmd output 'lsinitrd'\n[plugin:boot] packed command tuple: ('efibootmgr', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:boot] added cmd output 'efibootmgr'\n[plugin:cgroups] packed command tuple: ('systemd-cgls', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:cgroups] added cmd output 'systemd-cgls'\n[plugin:chrony] packed command tuple: ('chronyc tracking', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:chrony] added cmd output 'chronyc tracking'\n[plugin:chrony] packed command tuple: ('chronyc sources', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:chrony] added cmd output 'chronyc sources'\n[plugin:chrony] packed command tuple: ('chronyc sourcestats', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:chrony] added cmd output 'chronyc sourcestats'\n[plugin:chrony] packed command tuple: ('chronyc clients', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:chrony] added cmd output 'chronyc clients'\n[plugin:chrony] collecting journal: journalctl --no-pager  --unit chronyd\n[plugin:chrony] packed command tuple: ('journalctl --no-pager  --unit chronyd', 'None', 'None', None, 'True', 'True', 'None', 'None')\n[plugin:chrony] added cmd output 'journalctl --no-pager  --unit chronyd'\n[plugin:cron] packed command tuple: ('crontab -l -u root', 'root_crontab', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:cron] added cmd output 'crontab -l -u root'\n[plugin:devicemapper] packed command tuple: ('dmsetup info -c', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:devicemapper] added cmd output 'dmsetup info -c'\n[plugin:devicemapper] packed command tuple: ('dmsetup table', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:devicemapper] added cmd output 'dmsetup table'\n[plugin:devicemapper] packed command tuple: ('dmsetup status', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:devicemapper] added cmd output 'dmsetup status'\n[plugin:devicemapper] packed command tuple: ('dmsetup ls --tree', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:devicemapper] added cmd output 'dmsetup ls --tree'\n[plugin:devices] packed command tuple: ('udevadm info --export-db', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:devices] added cmd output 'udevadm info --export-db'\n[plugin:dmraid] packed command tuple: ('dmraid -V', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:dmraid] added cmd output 'dmraid -V'\n[plugin:dmraid] packed command tuple: ('dmraid -b', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:dmraid] added cmd output 'dmraid -b'\n[plugin:dmraid] packed command tuple: ('dmraid -r', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:dmraid] added cmd output 'dmraid -r'\n[plugin:dmraid] packed command tuple: ('dmraid -s', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:dmraid] added cmd output 'dmraid -s'\n[plugin:dmraid] packed command tuple: ('dmraid -tay', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:dmraid] added cmd output 'dmraid -tay'\n[plugin:dracut] packed command tuple: ('dracut --list-modules', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:dracut] added cmd output 'dracut --list-modules'\n[plugin:dracut] packed command tuple: ('dracut --print-cmdline', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:dracut] added cmd output 'dracut --print-cmdline'\n[plugin:etcd] packed command tuple: ('curl http://localhost:4001/version', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:etcd] added cmd output 'curl http://localhost:4001/version'\n[plugin:etcd] packed command tuple: ('curl http://localhost:4001/v2/members', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:etcd] added cmd output 'curl http://localhost:4001/v2/members'\n[plugin:etcd] packed command tuple: ('curl http://localhost:4001/v2/stats/leader', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:etcd] added cmd output 'curl http://localhost:4001/v2/stats/leader'\n[plugin:etcd] packed command tuple: ('curl http://localhost:4001/v2/stats/self', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:etcd] added cmd output 'curl http://localhost:4001/v2/stats/self'\n[plugin:etcd] packed command tuple: ('curl http://localhost:4001/v2/stats/store', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:etcd] added cmd output 'curl http://localhost:4001/v2/stats/store'\n[plugin:etcd] packed command tuple: ('ls -lR /var/lib/etcd/', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:etcd] added cmd output 'ls -lR /var/lib/etcd/'\n[plugin:filesys] packed command tuple: ('mount -l', 'None', 'mount', 300, 'True', 'True', 'None', 'None')\n[plugin:filesys] added cmd output 'mount -l'\n[plugin:filesys] packed command tuple: ('df -al', 'None', 'df', 300, 'True', 'True', 'None', 'None')\n[plugin:filesys] added cmd output 'df -al'\n[plugin:filesys] packed command tuple: ('df -ali', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:filesys] added cmd output 'df -ali'\n[plugin:filesys] packed command tuple: ('findmnt', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:filesys] added cmd output 'findmnt'\n[plugin:filesys] packed command tuple: ('dumpe2fs -h /dev/sda1 /boot', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:filesys] added cmd output 'dumpe2fs -h /dev/sda1 /boot'\n[plugin:firewalld] packed command tuple: ('firewall-cmd --list-all-zones', 'None', 'None', 10, 'True', 'True', 'None', 'None')\n[plugin:firewalld] added cmd output 'firewall-cmd --list-all-zones'\n[plugin:firewalld] packed command tuple: ('firewall-cmd --permanent --list-all-zones', 'None', 'None', 10, 'True', 'True', 'None', 'None')\n[plugin:firewalld] added cmd output 'firewall-cmd --permanent --list-all-zones'\n[plugin:foreman] packed command tuple: ('foreman-debug -g -q -a -d /var/tmp/sos.v0fKY9/sosreport-rhcaps6-r-01-20171006064801/sos_commands/foreman/foreman-debug', 'None', 'None', 900, 'True', 'True', 'None', 'None')\n[plugin:foreman] added cmd output 'foreman-debug -g -q -a -d /var/tmp/sos.v0fKY9/sosreport-rhcaps6-r-01-20171006064801/sos_commands/foreman/foreman-debug'\n[plugin:gdm] collecting journal: journalctl --no-pager  --unit gdm\n[plugin:gdm] packed command tuple: ('journalctl --no-pager  --unit gdm', 'None', 'None', None, 'True', 'True', 'None', 'None')\n[plugin:gdm] added cmd output 'journalctl --no-pager  --unit gdm'\n[plugin:gdm] packed command tuple: ('systemctl status gdm.service', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:gdm] added cmd output 'systemctl status gdm.service'\n[plugin:general] packed command tuple: ('hostname -f', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:general] added cmd output 'hostname -f'\n[plugin:general] packed command tuple: ('hostname', 'None', 'hostname', 300, 'True', 'True', 'None', 'None')\n[plugin:general] added cmd output 'hostname'\n[plugin:general] packed command tuple: ('date', 'None', 'date', 300, 'True', 'True', 'None', 'None')\n[plugin:general] added cmd output 'date'\n[plugin:general] packed command tuple: ('uptime', 'None', 'uptime', 300, 'True', 'True', 'None', 'None')\n[plugin:general] added cmd output 'uptime'\n[plugin:grub2] packed command tuple: ('ls -lanR /boot', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:grub2] added cmd output 'ls -lanR /boot'\n[plugin:grub2] packed command tuple: ('grub2-mkconfig', 'None', 'None', 300, 'True', 'True', 'None', '{'GRUB_DISABLE_OS_PROBER': 'true'}')\n[plugin:grub2] added cmd output 'grub2-mkconfig'\n[plugin:hardware] packed command tuple: ('dmidecode', 'None', 'dmidecode', 300, 'True', 'True', 'None', 'None')\n[plugin:hardware] added cmd output 'dmidecode'\n[plugin:i18n] packed command tuple: ('locale', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:i18n] added cmd output 'locale'\n[plugin:ipmitool] packed command tuple: ('ipmitool sel info', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ipmitool] added cmd output 'ipmitool sel info'\n[plugin:ipmitool] packed command tuple: ('ipmitool sel list', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ipmitool] added cmd output 'ipmitool sel list'\n[plugin:ipmitool] packed command tuple: ('ipmitool sensor list', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ipmitool] added cmd output 'ipmitool sensor list'\n[plugin:ipmitool] packed command tuple: ('ipmitool chassis status', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ipmitool] added cmd output 'ipmitool chassis status'\n[plugin:ipmitool] packed command tuple: ('ipmitool fru print', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ipmitool] added cmd output 'ipmitool fru print'\n[plugin:ipmitool] packed command tuple: ('ipmitool mc info', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ipmitool] added cmd output 'ipmitool mc info'\n[plugin:ipmitool] packed command tuple: ('ipmitool sdr info', 'None', 'None', 300, 'True', 'True', 'None', 'None')\n[plugin:ipmitool] added cmd output 'ipmitool sdr info'\n\nBest regards\nKlaus</text>, <text>Hello Klaus,\n\nthanks for that information,\n\nlooks like the sosreport is hanging when running ipmitool.\n\nDoes sosreport complete if we do run the following:\n\n# sosreport --skip-plugins ipmitool\n\nIn case it does, could we then let run the following:\n\n# time ipmitool sdr info\n\nand get me the output from sosreport as well as the whole # time ... comand ?\n\n\nThanks,\nDaniel</text>, <text>Hello Daniel,\n\nas we discussed on telephone now the sosreport could be created with --disable-plugin jars.\n\nBes regards\nKlaus</text>, <text>Hello Daniel,\n\nsosreport is uploaded to dropbox: /sosreport-rhsat6-r-01.noc.fiducia.de-20171006125054.tar.xz\n\nBest regards\nKlaus</text>, <text>rhsm.log\n\nManagerError: Communication with subscription manager failed with code 404: Organization with id FiduciaGAD could not be found.\n2017-10-05 15:42:49,036 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-05 15:42:49,037 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-05 15:43:08,791 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-05 15:43:09,308 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-05 15:43:09,622 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-05 15:43:11,889 [INFO] @subscriptionmanager.py:202 - Sending update in hosts-to-guests mapping for config "destination_-2265463789258014148": 183 hypervisors and 4441 guests found\n2017-10-05 15:43:14,290 [ERROR] @virt.py:625 - Error during hypervisor checkin: \nManagerError: Communication with subscription manager failed with code 404: Organization with id 1 could not be found.\n2017-10-05 15:43:34,498 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-05 15:43:34,776 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-05 15:43:51,868 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-05 15:43:52,153 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-05 15:54:07,475 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-05 15:54:07,476 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-05 15:54:26,987 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-05 15:54:27,838 [ERROR] @virt.py:389 - Thread 'destination_2407043753446613147' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-05 15:54:27,838 [INFO] @virt.py:563 - Error report received\n2017-10-05 15:54:27,838 [INFO] @virt.py:408 - Waiting 3600 seconds before performing action again 'destination_2407043753446613147'\n2017-10-05 15:57:27,538 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-05 15:57:27,539 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-05 15:57:46,815 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-05 15:57:50,203 [INFO] @subscriptionmanager.py:202 - Sending update in hosts-to-guests mapping for config "destination_-81060392286982631": 183 hypervisors and 4441 guests found\n2017-10-05 15:57:52,706 [ERROR] @virt.py:625 - Error during hypervisor checkin: \nManagerError: Communication with subscription manager failed with code 404: Organization with id 1 could not be found.\n\n\n\n\n\n[dmoessne@foobar sosreport-rhsat6-r-01.noc.fiducia.de-20171006125054]$ cat etc/virt-who.d/esx_ka_nonprod.conf \n## This is a template for virt-who configuration files. Please see\n## virt-who-config(5) manual page for detailed information.\n##\n## virt-who checks all files in /etc/virt-who.d/ if they're valid ini-like\n## files and uses them as configuration. Each file might contain more configs.\n##\n## You can uncomment and fill following template or create new file with\n## similar content.\n\n[esx_ka_nonprod]\ntype=esx\nserver=xaiimgvcs001.noc.fiducia.de\nusername=VSPHERE.LOCAL\\rh_satellite\npassword=********\n#encrypted_password=********  ; password encrypted using virt-who-password utility\nowner=1\nenv=Library\nhypervisor_id=hostname\n#rhsm_hostname=rhsat6-r-01.noc.fiducia.de\n#rhsm_username=admin\n#rhsm_password=********\n#rhsm_prefix=/rhsm \n\n\n\n--&gt; rhsm part is needed to connect to satellite in case it is subscribed to portal\n\n## For complete list of options, see virt-who-config(5) manual page.\n\n## Terse version of the config template:\n#[config name]\n#type=\n#server=\n#username=\n#password=********\n#encrypted_password=********\n#owner=\n#env=\n#hypervisor_id=\n[dmoessne@foobar sosreport-rhsat6-r-01.noc.fiducia.de-20171006125054]$ cat etc/hosts\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n172.20.128.130          rhns-z.mgmt-o.fiducia.de\n172.20.128.227          mds-o.mgmt.fiducia.de ldaphost\n172.20.128.159          loghost.mgmt-o.fiducia.de loghost\n\n192.168.163.4           rhsat6-r-01.noc.fiducia.de rhsat6-r-01\n\n172.17.128.230          rhsat6-r-01.mgt-o.fiducia.de\n172.20.128.230          rhsat6-r-01.mgmt-o.fiducia.de\n\n172.17.0.230            rhsat6-r-01.mgt-p.fiducia.de\n172.20.0.230            rhsat6-r-01.mgmt-p.fiducia.de\n\n172.17.64.230           rhsat6-r-01.mgt-g.fiducia.de\n172.20.64.230           rhsat6-r-01.mgmt-g.fiducia.de\n[dmoessne@foobar sosreport-rhsat6-r-01.noc.fiducia.de-20171006125054]$ grep rhsat6-r-01.noc.fiducia.de etc/hosts\n192.168.163.4           rhsat6-r-01.noc.fiducia.de rhsat6-r-01\n[dmoessne@foobar sosreport-rhsat6-r-01.noc.fiducia.de-20171006125054]$ cat ip_addr \n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n2: eno1    inet 172.17.128.230/17 brd 172.17.255.255 scope global eno1\\       valid_lft forever preferred_lft forever\n2: eno1    inet 172.20.128.230/17 brd 172.20.255.255 scope global eno1\\       valid_lft forever preferred_lft forever\n3: eno2    inet 192.168.163.4/23 brd 192.168.163.255 scope global eno2\\       valid_lft forever preferred_lft forever\n4: eno3    inet 172.17.0.230/18 brd 172.17.63.255 scope global eno3\\       valid_lft forever preferred_lft forever\n5: eno4    inet 172.17.64.230/18 brd 172.17.127.255 scope global eno4\\       valid_lft forever preferred_lft forever\n[dmoessne@foobar sosreport-rhsat6-r-01.noc.fiducia.de-20171006125054]$\n\n\n\n\n[dmoessne@foobar sosreport-rhsat6-r-01.noc.fiducia.de-20171006125054]$ grep -v '#' etc/sysconfig/virt-who\n\nVIRTWHO_DEBUG=0\n\n\n[dmoessne@foobar sosreport-rhsat6-r-01.noc.fiducia.de-20171006125054]$ \n\n--&gt; we miss:\n\nVIRTWHO_SATELLITE6=1\nVIRTWHO_INTERVAL=3600\n\n\ntest:\n- satellite subscribed to RHSM and with the following config:\n\n\n[root@inf3 virt-who.d]# cat dmoessne-coe.conf \n[coe-rhv-dmoessne]\ntype=rhevm\nhypervisor_id=hostname\nowner=dmoessne\nenv=Library\nserver=https://inf2.coe.muc.redhat.com:443/ovirt-engine\nusername=admin@internal\nencrypted_password=c7731b728f5ab3d8fd648450af6e3f55\n#rhsm_hostname=inf3.coe.muc.redhat.com\n#rhsm_username=admin\n#rhsm_encrypted_password=c7731b728f5ab3d8fd648450af6e3f55\n#rhsm_prefix=/rhsm\n[root@inf3 virt-who.d]# \n\n[root@inf3 virt-who.d]# virt-who --one-shot\n2017-10-09 08:40:32,091 INFO: Using configuration "coe-rhv" ("rhevm" mode)\n2017-10-09 08:40:32,091 INFO: Using configuration "coe-rhv-dmoessne" ("rhevm" mode)\n2017-10-09 08:40:32,091 INFO: Using reporter_id='inf3.coe.muc.redhat.com-8cdc33892b934dea8108d0e220bc01ec'\n2017-10-09 08:40:34,170 INFO: Report for config "coe-rhv-dmoessne" gathered, placing in datastore\n2017-10-09 08:40:34,385 INFO: Report for config "coe-rhv" gathered, placing in datastore\n2017-10-09 08:40:36,338 INFO: Sending update in hosts-to-guests mapping for config "destination_9085163018350175336": 3 hypervisors and 77 guests found\n2017-10-09 08:40:38,266 INFO: Sending update in hosts-to-guests mapping for config "destination_6212443859407541777": 3 hypervisors and 77 guests found\n2017-10-09 08:40:39,529 ERROR: Error during hypervisor checkin: \nManagerError: Communication with subscription manager failed with code 404: Organization with id dmoessne could not be found.\n[root@inf3 virt-who.d]# cat /etc/virt-who.conf \n\n\n\n\n\nsummary:\n- docu : https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/virtual_instances_guide/virt-who_configuration#virt_who_configuration_files\n- labs : https://access.redhat.com/labs/virtwhoconfig/</text>, <text>Hello Klaus,\n\nhaving a look at the configuration:\n\n~~~\n$ cat esx_ka_nonprod.conf \n....\n\n[esx_ka_nonprod]\ntype=esx\nserver=xaiimgvcs001.noc.fiducia.de\nusername=VSPHERE.LOCAL\\rh_satellite\npassword=********\n#encrypted_password=********  ; password encrypted using virt-who-password utility\nowner=1\nenv=Library\nhypervisor_id=hostname\n#rhsm_hostname=rhsat6-r-01.noc.fiducia.de\n#rhsm_username=admin\n#rhsm_password=********\n#rhsm_prefix=/rhsm \n....\n$\n~~~\n\nI can see that #rhsm part is commented out, but as the satellite is registered upstream :\n\n~~~\n$ grep hostname rhsm/rhsm.conf\n# Server hostname:\nhostname = subscription.rhn.redhat.com\nproxy_hostname = pro-int.noc.fiducia.de\n$\n~~~\n\nThis part is needed. Otherwise virt-who will try to login and report to customer portal and you get a 404 which is the last error\nI have seen in rhsm.log:\n\n~~~\n2017-10-05 15:57:52,706 [ERROR] @virt.py:625 - Error during hypervisor checkin: \nManagerError: Communication with subscription manager failed with code 404: Organization with id 1 could not be found.\n2017-10-05 15:58:25,599 [INFO] subscription-manager:46496:MainThread @managercli.py:518 - X-Correlation-ID: 2c85e9d37ba0410a9fd0ca9d0b7e73cb\n2017-10-05 15:58:25,600 [INFO] subscription-manager:46496:MainThread @managercli.py:407 - Client Versions: {'python-rhsm': '1.19.9-1.el7', 'subscription-manager': '1.19.21-1.el7'}\n2017-10-05 16:03:12,491 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n\n~~~\n\n\nFurther on I see the following missing in `/etc/sysconfig/virt-who`:\n~~~\nVIRTWHO_SATELLITE6=1\n~~~\n\n\nSo may I suggest to have a look at the documentation :\n\n- https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/virtual_instances_guide/virt-who_configuration#virt_who_configuration_files\n\nor even better make use of our lab application:\n\n- https://access.redhat.com/labs/virtwhoconfig/\n\nwhich can help you easily creating a appropriate configuration files by asking for all the necessary things and finally let you know\nwho config files should be changed.\n\n\nIf after that this does not work either, please enable debugging (VIRTWHO_DEBUG=1), run `# virt-who --one-shot` and attach the whole output of\nthis command as well as:\n- /etc/sysconfig/virt-who\n- /etc/virt-who.d/*.conf files\n- /etc/virt-who.conf\n- /var/log/rhsm/rhsm.log\n\n\nKind regards,\nDaniel</text>, <text>Hello Daniel,\n\nso if I didn't comment out the rhsm part in the configuration file I will get following error message which I told you by telephone last week:\n\n2017-10-09 09:09:33,042 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n\nI changed the configuration again.\n\n\n[root@rhsat6-r-01 ~]# virt-who --one-shot\n2017-10-09 09:20:51,462 INFO: Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-09 09:20:51,463 INFO: Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-09 09:21:11,276 INFO: Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:21:11,693 ERROR: Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-09 09:21:11,694 INFO: Error report received\n\n\nBest regards\nKlaus</text>, <text>Hello Klaus,\n\nthanks for the update and the files attached.\n\nI'll look into them and will update you as soon as I do have news, but please let me know in case you do have\nany questions meanwhile.\n\nThanks,\nDaniel</text>, <text>*/INTERNAL*/\n\n[dmoessne@foobar etc]$ grep -v '#' virt-who.conf\n\n\n[dmoessne@foobar etc]$ \n\n-------------\n\n[dmoessne@foobar etc]$ grep -v '#' sysconfig/virt-who \n\nVIRTWHO_DEBUG=1\n\n\nVIRTWHO_SATELLITE6=1\n\n[dmoessne@foobar etc]$ \n\n\n----------------\n\n[dmoessne@foobar etc]$ grep -v '#' sysconfig/virt-who \n\nVIRTWHO_DEBUG=1\n\n\n\nVIRTWHO_SATELLITE6=1\n\n\n[dmoessne@foobar etc]$ grep -v '#' virt-who.d/esx_ka_nonprod.conf \n\n[esx_ka_nonprod]\ntype=esx\nserver=xaiimgvcs001.noc.fiducia.de\nusername=VSPHERE.LOCAL\\rh_satellite\npassword=bGDS67ToaklH2NnaiioB&amp;1yF$QjKOP7cp04lnGXyUgiByUMAkE94kr5AZOdhkhJc\nowner=1\nenv=Library\nhypervisor_id=hostname\nrhsm_hostname=rhsat6-r-01.noc.fiducia.de\nrhsm_username=admin\nrhsm_password=redhat\nrhsm_prefix=/rhsm \n\n\n[dmoessne@foobar etc]$ \n\n\n\nrhsm.log:\n\n2017-10-09 08:35:38,445 [INFO] rhsmcertd-worker:8381:MainThread @dmiinfo.py:73 - Using dmidecode dump file: /dev/mem\n2017-10-09 08:35:38,479 [WARNING] rhsmcertd-worker:8381:MainThread @dmiinfo.py:88 - Error reading system DMI information: list assignment index out of range\n2017-10-09 08:37:19,706 [INFO] subscription-manager:8663:MainThread @managercli.py:518 - X-Correlation-ID: 1774725fb2404bafadcad382c2baafb2\n2017-10-09 08:37:19,706 [INFO] subscription-manager:8663:MainThread @managercli.py:407 - Client Versions: {'python-rhsm': '1.19.9-1.el7', 'subscription-manager': '1.19.21-1.el7'}\n2017-10-09 09:07:20,271 [INFO] subscription-manager:44743:MainThread @managercli.py:518 - X-Correlation-ID: 8afc5566564148548fe4506b6ab41f8e\n2017-10-09 09:07:20,271 [INFO] subscription-manager:44743:MainThread @managercli.py:407 - Client Versions: {'python-rhsm': '1.19.9-1.el7', 'subscription-manager': '1.19.21-1.el7'}\n2017-10-09 09:09:11,595 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-09 09:09:11,596 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-09 09:09:32,326 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:09:32,564 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:09:33,042 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-09 09:09:33,042 [INFO] @virt.py:563 - Error report received\n2017-10-09 09:09:33,042 [INFO] @virt.py:408 - Waiting 3600 seconds before performing action again 'destination_-3430009366329003554'\n2017-10-09 09:17:00,838 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-09 09:17:00,839 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-09 09:17:20,726 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:17:21,065 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-09 09:17:21,066 [INFO] @virt.py:563 - Error report received\n2017-10-09 09:17:21,066 [INFO] @virt.py:408 - Waiting 3600 seconds before performing action again 'destination_-3430009366329003554'\n2017-10-09 09:18:09,074 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:18:09,431 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:20:51,462 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-09 09:20:51,463 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-09 09:21:11,276 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:21:11,693 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-09 09:21:11,694 [INFO] @virt.py:563 - Error report received\n2017-10-09 09:31:45,268 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-09 09:31:45,268 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-09 09:32:05,552 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:32:06,669 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-09 09:32:06,669 [INFO] @virt.py:563 - Error report received\n[dmoessne@foobar rhsm]$</text>, <text>Hello Klaus,\n\n\nI'll engage subject matter experts to have them a look at the files provided.\n\nPlease bear with me while waiting for an update from their side.\n\n\nThanks,\nDaniel</text>, <text>*/Collab-Request*/\n\n\nHi sbr-sysmgmt,\n\nIHAC currently trying to make virt-who work (side remark, they do have a SE for running this in \na multihome setup, though not sure if this is the reason).\n\nThey do have the following configuration:\n\n~~~\n[dmoessne@foobar etc]$ grep -v '#' virt-who.conf\n\n\n[dmoessne@foobar etc]$ \n\n-------------\n\n[dmoessne@foobar etc]$ grep -v '#' sysconfig/virt-who \n\nVIRTWHO_DEBUG=1\n\n\nVIRTWHO_SATELLITE6=1\n\n[dmoessne@foobar etc]$ \n\n\n----------------\n\n[dmoessne@foobar etc]$ grep -v '#' sysconfig/virt-who \n\nVIRTWHO_DEBUG=1\n\n\n\nVIRTWHO_SATELLITE6=1\n\n\n[dmoessne@foobar etc]$ grep -v '#' virt-who.d/esx_ka_nonprod.conf \n\n[esx_ka_nonprod]\ntype=esx\nserver=xaiimgvcs001.noc.fiducia.de\nusername=VSPHERE.LOCAL\\rh_satellite\npassword=bGDS67ToaklH2NnaiioB&amp;1yF$QjKOP7cp04lnGXyUgiByUMAkE94kr5AZOdhkhJc\nowner=1\nenv=Library\nhypervisor_id=hostname\nrhsm_hostname=rhsat6-r-01.noc.fiducia.de\nrhsm_username=admin\nrhsm_password=redhat\nrhsm_prefix=/rhsm \n\n\n[dmoessne@foobar etc]$ \n\n\n~~~\n\n\nand when we run virt-who --one-shot we get:\n\n\n~~~~\n[root@rhsat6-r-01 ~]# virt-who --one-shot\n2017-10-09 09:20:51,462 INFO: Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-09 09:20:51,463 INFO: Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-09 09:21:11,276 INFO: Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:21:11,693 ERROR: Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-09 09:21:11,694 INFO: Error report received\n~~~~\n\n\nand when looking at the rhsm.lof I can see:\n\n~~~\n2017-10-09 09:07:20,271 [INFO] subscription-manager:44743:MainThread @managercli.py:407 - Client Versions: {'python-rhsm': '1.19.9-1.el7', 'subscription-manager': '1.19.21-1.el7'}\n2017-10-09 09:09:11,595 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-09 09:09:11,596 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-09 09:09:32,326 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:09:32,564 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:09:33,042 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-09 09:09:33,042 [INFO] @virt.py:563 - Error report received\n2017-10-09 09:09:33,042 [INFO] @virt.py:408 - Waiting 3600 seconds before performing action again 'destination_-3430009366329003554'\n2017-10-09 09:17:00,838 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-09 09:17:00,839 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-09 09:17:20,726 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:17:21,065 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-09 09:17:21,066 [INFO] @virt.py:563 - Error report received\n2017-10-09 09:17:21,066 [INFO] @virt.py:408 - Waiting 3600 seconds before performing action again 'destination_-3430009366329003554'\n2017-10-09 09:18:09,074 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:18:09,431 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:20:51,462 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-09 09:20:51,463 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-09 09:21:11,276 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:21:11,693 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-09 09:21:11,694 [INFO] @virt.py:563 - Error report received\n2017-10-09 09:31:45,268 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-09 09:31:45,268 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-09 09:32:05,552 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-09 09:32:06,669 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-09 09:32:06,669 [INFO] @virt.py:563 - Error report received\n[dmoessne@foobar rhsm]$  \n\n~~~\n\n\n\nCould you kindly have a look and let me know how to proceed ?\n\nThanks,\nDaniel</text>, <text>Hi,\n\nMy name is Kenny and I will be helping out with this case.\n\nWe can see a proxy is configured for rhsm:\n# grep proxy etc/rhsm/rhsm.conf | grep -v "#"\n~~~\nproxy_hostname = pro-int.noc.fiducia.de\nproxy_port = 8080\nproxy_user =\nproxy_password =\n~~~\n\n=&gt; This can interfere with virt-who communication which could be observed with:\n~~~\n017-10-09 09:09:33,042 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n~~~\n\n=&gt; To attempt to resolve this lets try to add the following parameter to /etc/sysconfig/virt-who this should tell virt-who not to use any proxy while rhsm still uses the proxy\n~~~\nNO_PROXY=*\n~~~\n\nvirt-who system config:\n# grep -v "#" sysconfig/virt-who  | grep -v "^$"\n~~~\nVIRTWHO_DEBUG=1\nVIRTWHO_SATELLITE6=1\n~~~\n\nYou can comment out # VIRTWHO_SATELLITE6=1\n\n\nThe virt-who configuration:\n# grep -v "#" esx_ka_nonprod.conf \n~~~\n[esx_ka_nonprod]\ntype=esx\nserver=xaiimgvcs001.noc.fiducia.de\nusername=VSPHERE.LOCAL\\rh_satellite\npassword=bGDS67ToaklH2NnaiioB&amp;1yF$QjKOP7cp04lnGXyUgiByUMAkE94kr5AZOdhkhJc\nowner=1\nenv=Library\nhypervisor_id=hostname\nrhsm_hostname=rhsat6-r-01.noc.fiducia.de\nrhsm_username=admin\nrhsm_password=redhat\nrhsm_prefix=/rhsm \n~~~\n\n=&gt; owner should be the name of the organization in Satellite can be verified with # hammer organization list\n\n\nAfter the changes for the proxy please restart virt-who and provide us with the new rhsm.log in case of problems:\n# service virt-who restart\n\n\nIn case of further questions or concerns, please don't hesitate to contact me.\n\nThank you\n\nKind regards,\nKenny</text>, <text>Hello Klaus,\n\nI think we have found the reason for that error you're seeing.\n\nSo, as your satellite is subscribed upstream through a proxy virt-who is reading the rhsm.conf in order \nto connect to satellite server and is trying this then through the proxy it finds in rhsm.conf.\n\nIn order to make virt-who ignore this proxy from rhsm.conf there is a NO_PROXY setting for virt-who:\n\n# man virt-who-config:\n\n~~~\nNO_PROXY\n              A  comma-separated  list  of  hostnames  or  domains  or ip addresses to ignore proxy settings for.  Optionally this may be set to '*' to bypass proxy settings for all hostnames domains or ip\n              addresses.\n~~~\n\nSo setting this to NO_PROXY=rhsat6-r-01.noc.fiducia.de in configuration file should do the trick.\n\nIn case you'd like to use that with --one-shot, you have to export this in the shell you're running the virt-who --one-shot\nas this one is not reading the option from the configuration file.\n\nCould you give that a try and let me know if this works for you ?\n\nThanks,\nDaniel</text>, <text>Hello Daniel,\n\nI tried the additional Parameter in /etc/virt-who.d/..., now I get another error:\n\n2017-10-10 06:40:33,709 [virtwho.destination_-3430009366329003554 ERROR] MainProcess(29024):Thread-3 @virt.py:run:389 - Thread 'destination_-3430009366329003554' fails with exception:\nTraceback (most recent call last):\n  File "/usr/lib/python2.7/site-packages/virtwho/virt/virt.py", line 380, in run\n    self._run()\n  File "/usr/lib/python2.7/site-packages/virtwho/virt/virt.py", line 336, in _run\n    self._send_data(data_to_send)\n  File "/usr/lib/python2.7/site-packages/virtwho/virt/virt.py", line 609, in _send_data\n    options=self.options)\n  File "/usr/lib/python2.7/site-packages/virtwho/manager/subscriptionmanager/subscriptionmanager.py", line 178, in hypervisorCheckIn\n    self._connect(report.config)\n  File "/usr/lib/python2.7/site-packages/virtwho/manager/subscriptionmanager/subscriptionmanager.py", line 140, in _connect\n    if not self.connection.ping()['result']:\n  File "/usr/lib64/python2.7/site-packages/rhsm/connection.py", line 883, in ping\n    return self.conn.request_get("/status/")\n  File "/usr/lib64/python2.7/site-packages/rhsm/connection.py", line 646, in request_get\n    return self._request("GET", method, headers=headers)\n  File "/usr/lib64/python2.7/site-packages/rhsm/connection.py", line 672, in _request\n    info=info, headers=headers)\n  File "/usr/lib64/python2.7/site-packages/rhsm/connection.py", line 528, in _request\n    conn.request(request_type, handler, body=body, headers=final_headers)\n  File "/usr/lib64/python2.7/httplib.py", line 1017, in request\n    self._send_request(method, url, body, headers)\n  File "/usr/lib64/python2.7/httplib.py", line 1051, in _send_request\n    self.endheaders(body)\n  File "/usr/lib64/python2.7/httplib.py", line 1013, in endheaders\n    self._send_output(message_body)\n  File "/usr/lib64/python2.7/httplib.py", line 864, in _send_output\n    self.send(msg)\n  File "/usr/lib64/python2.7/httplib.py", line 826, in send\n    self.connect()\n  File "/usr/lib64/python2.7/httplib.py", line 1227, in connect\n    HTTPConnection.connect(self)\n  File "/usr/lib64/python2.7/httplib.py", line 810, in connect\n    self._tunnel()\n  File "/usr/lib64/python2.7/httplib.py", line 792, in _tunnel\n    message.strip()))\nerror: Tunnel connection failed: 403 Forbidden\n2017-10-10 06:40:33,731 [virtwho.destination_-3430009366329003554 INFO] MainProcess(29024):Thread-3 @virt.py:_send_data:563 - Error report received\n2017-10-10 06:40:33,731 [virtwho.destination_-3430009366329003554 INFO] MainProcess(29024):Thread-3 @virt.py:run:408 - Waiting 3600 seconds before performing action again 'destination_-3430009366329003554'\n\nBest regards\nKlaus</text>, <text>Hello Daniel,\n\nI think I configured NO_PROXY not in the right configuration file in the morning. Now I configured it in /etc/sysconfig/virt-who. But also I get one error message:\n\n2017-10-10 08:05:47,453 [virtwho.destination_-3430009366329003554 ERROR] MainProcess(36353):Thread-3 @virt.py:_send_data:625 - Error during hypervisor checkin:\nTraceback (most recent call last):\n  File "/usr/lib/python2.7/site-packages/virtwho/virt/virt.py", line 609, in _send_data\n    options=self.options)\n  File "/usr/lib/python2.7/site-packages/virtwho/manager/subscriptionmanager/subscriptionmanager.py", line 221, in hypervisorCheckIn\n    raise ManagerError("Communication with subscription manager failed with code %d: %s" % (e.code, str(e)))\nManagerError: Communication with subscription manager failed with code 403: Access denied\n\nBest regards\nKlaus</text>, <text>Hello Klaus,\n\nmany thanks for the update.\n\nDo you still have the org id in there and if so can we replace that with the name instead, as Kenny asked for.\nAlso, what is the configuration of the NO_PROXY setting ? Is the proxy just needed to get out side or also \nneeded to connect to the vcenter ?\n\n\nCould you kindly follow Kenny's advice:\n\n-----\nWe can see a proxy is configured for rhsm:\n# grep proxy etc/rhsm/rhsm.conf | grep -v "#"\n~~~\nproxy_hostname = pro-int.noc.fiducia.de\nproxy_port = 8080\nproxy_user =\nproxy_password =\n~~~\n\n=&gt; This can interfere with virt-who communication which could be observed with:\n~~~\n017-10-09 09:09:33,042 [ERROR] @virt.py:389 - Thread 'destination_-3430009366329003554' fails with exception:\nerror: Tunnel connection failed: 403 Forbidden\n~~~\n\n=&gt; To attempt to resolve this lets try to add the following parameter to /etc/sysconfig/virt-who this should tell virt-who not to use any proxy while rhsm still uses the proxy\n~~~\nNO_PROXY=*\n~~~\n\nvirt-who system config:\n# grep -v "#" sysconfig/virt-who  | grep -v "^$"\n~~~\nVIRTWHO_DEBUG=1\nVIRTWHO_SATELLITE6=1\n~~~\n\nYou can comment out # VIRTWHO_SATELLITE6=1\n\n\nThe virt-who configuration:\n# grep -v "#" esx_ka_nonprod.conf \n~~~\n[esx_ka_nonprod]\ntype=esx\nserver=xaiimgvcs001.noc.fiducia.de\nusername=VSPHERE.LOCAL\\rh_satellite\npassword=bGDS67ToaklH2NnaiioB&amp;1yF$QjKOP7cp04lnGXyUgiByUMAkE94kr5AZOdhkhJc\nowner=1\nenv=Library\nhypervisor_id=hostname\nrhsm_hostname=rhsat6-r-01.noc.fiducia.de\nrhsm_username=admin\nrhsm_password=redhat\nrhsm_prefix=/rhsm \n~~~\n\n=&gt; owner should be the name of the organization in Satellite can be verified with # hammer organization list\n-----\n\n\n\nAfter that kindly run a virt-who --one-shot and attach the fill trace as well as \n\n- /etc/sysconfig/virt-who\n- /etc/virt-who.d/*.conf files\n- /etc/virt-who.conf\n- /var/log/rhsm/rhsm.log\n\n\nThanks,\nDaniel</text>, <text>Hello Daniel,\n\nfor NO_PROXY I have configured NO_PROXY=rhsat6-r-01.noc.fiducia.de\n\nThe Proxy is only needed for connection to Red Hat Network.\n\nConfiguration for owner is verified: \n\n[root@rhsat6-r-01 ~]# hammer organization list\n---|------------|------------|------------\nID | NAME       | LABEL      | DESCRIPTION\n---|------------|------------|------------\n1  | FiduciaGAD | FiduciaGAD |\n---|------------|------------|------------\n\nI changed all Settings how described and startet virt-who --one-shot\n\n[root@rhsat6-r-01 ~]# virt-who --one-shot\n2017-10-10 08:50:59,190 INFO: Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-10 08:50:59,190 INFO: Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-10 08:51:19,268 INFO: Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-10 08:51:20,069 INFO: Sending update in hosts-to-guests mapping for config "destination_-3430009366329003554": 183 hypervisors and 4436 guests found\n2017-10-10 08:51:21,008 ERROR: Error during hypervisor checkin:\nManagerError: Communication with subscription manager failed with code 403: Access denied\n\nIs it possible to make a remote session to check all together?\n\nBest regards\nKlaus</text>, <text>Hi,\n\nAre you sure about the credentials to connect to the Satellite admin/redhat ?\n\nError encountered:\n~~~\nManagerError: Communication with subscription manager failed with code 403: Access denied\n~~~\n\nConfig:\n~~~\n[esx_ka_nonprod]\ntype=esx\nserver=xaiimgvcs001.noc.fiducia.de\nusername=VSPHERE.LOCAL\\rh_satellite\npassword=bGDS67ToaklH2NnaiioB&amp;1yF$QjKOP7cp04lnGXyUgiByUMAkE94kr5AZOdhkhJc\nowner=1\nenv=Library\nhypervisor_id=hostname\nrhsm_hostname=rhsat6-r-01.noc.fiducia.de\nrhsm_username=admin\nrhsm_password=redhat\nrhsm_prefix=/rhsm \n~~~\n\nCan you also add the following to /etc/rhsm/rhsm.conf:\n~~~\nno_proxy=rhsat6-r-01.noc.fiducia.de\n~~~\n\n\nIn case of further questions or concerns, please don't hesitate to contact me.\n\n\nThanks\n\nKind regards,\nKenny</text>, <text>*/INTERNAL*/\n\n\n\n$ grep -v '#' etc/sysconfig/virt-who |tr -s '\\n'\n\nVIRTWHO_DEBUG=1\nNO_PROXY=*\n$\n\n$ grep -v '#' etc/virt-who.conf |tr -s '\\n'\n\n$\n\n$ grep -v '#' etc/virt-who.d/esx_ka_nonprod.conf |tr -s '\\n'\n\n[esx_ka_nonprod]\ntype=esx\nserver=xaiimgvcs001.noc.fiducia.de\nusername=VSPHERE.LOCAL\\rh_satellite\npassword=bGDS67ToaklH2NnaiioB&amp;1yF$QjKOP7cp04lnGXyUgiByUMAkE94kr5AZOdhkhJc\nowner=1\nenv=Library\nhypervisor_id=hostname\nrhsm_hostname=rhsat6-r-01.noc.fiducia.de\nrhsm_username=admin\nrhsm_password=redhat\nrhsm_prefix=/rhsm \n$\n\n\n--&gt; we have still owner=1 and asked twice to use Org label ..... could this be the issue ?\n\n--&gt; tesing:\n\n# cat dmoessne-coe.conf \n[coe-rhv-dmoessne]\ntype=rhevm\nhypervisor_id=hostname\n#owner=dmoessne\nowner=12\nenv=Library\nserver=https://inf2.coe.muc.redhat.com:443/ovirt-engine\nusername=admin@internal\nencrypted_password=c7731b728f5ab3d8fd648450af6e3f55\nrhsm_hostname=inf3.coe.muc.redhat.com\nrhsm_username=admin\nrhsm_encrypted_password=c7731b728f5ab3d8fd648450af6e3f55\nrhsm_prefix=/rhsm\n\n[root@inf3 virt-who.d]# virt-who --one-shot\n2017-10-10 09:44:56,114 INFO: Using configuration "coe-rhv-dmoessne" ("rhevm" mode)\n2017-10-10 09:44:56,114 INFO: Using reporter_id='inf3.coe.muc.redhat.com-8cdc33892b934dea8108d0e220bc01ec'\n2017-10-10 09:44:57,975 INFO: Report for config "coe-rhv-dmoessne" gathered, placing in datastore\n2017-10-10 09:44:58,356 INFO: Sending update in hosts-to-guests mapping for config "destination_3805821483717515565": 3 hypervisors and 83 guests found\n2017-10-10 09:44:58,414 ERROR: Error during hypervisor checkin: \nManagerError: Communication with subscription manager failed with code 403: Access denied\n[root@inf3 virt-who.d]# \n\n# cat dmoessne-coe.conf \n[coe-rhv-dmoessne]\ntype=rhevm\nhypervisor_id=hostname\nowner=dmoessne\n#owner=12\nenv=Library\nserver=https://inf2.coe.muc.redhat.com:443/ovirt-engine\nusername=admin@internal\nencrypted_password=c7731b728f5ab3d8fd648450af6e3f55\nrhsm_hostname=inf3.coe.muc.redhat.com\nrhsm_username=admin\nrhsm_encrypted_password=c7731b728f5ab3d8fd648450af6e3f55\nrhsm_prefix=/rhsm\n#\n\n[root@inf3 virt-who.d]# virt-who --one-shot\n2017-10-10 09:45:14,243 INFO: Using configuration "coe-rhv-dmoessne" ("rhevm" mode)\n2017-10-10 09:45:14,243 INFO: Using reporter_id='inf3.coe.muc.redhat.com-8cdc33892b934dea8108d0e220bc01ec'\n2017-10-10 09:45:16,102 INFO: Report for config "coe-rhv-dmoessne" gathered, placing in datastore\n2017-10-10 09:45:16,497 INFO: Sending update in hosts-to-guests mapping for config "destination_-7627749307845664756": 3 hypervisors and 83 guests found\n[root@inf3 virt-who.d]# \n\n\n\n\ncustomer logs:\n\n2017-10-10 08:49:01,701 [INFO] @virt.py:563 - Error report received\n2017-10-10 08:50:59,190 [INFO] @main.py:183 - Using configuration "esx_ka_nonprod" ("esx" mode)\n2017-10-10 08:50:59,190 [INFO] @main.py:185 - Using reporter_id='rhsat6-r-01.noc.fiducia.de-01902ea8e39243b4a87f8383c43ef8bb'\n2017-10-10 08:51:19,268 [INFO] @virt.py:888 - Report for config "esx_ka_nonprod" gathered, placing in datastore\n2017-10-10 08:51:20,069 [INFO] @subscriptionmanager.py:202 - Sending update in hosts-to-guests mapping for config "destination_-3430009366329003554": 183 hypervisors and 4436 guests found\n2017-10-10 08:51:21,008 [ERROR] @virt.py:625 - Error during hypervisor checkin: \nManagerError: Communication with subscription manager failed with code 403: Access denied</text>, <text>Hello Klaus,\n\nthanks for the time on the phone. as we replaced the owner=1 with FiduciaGAD virt-who --oneshot worked\nand you could see the hypervisors in Satellite 6.\n\nAs discussed the next issue is that you do have 2 Satellites one for test and one for production but still\nconnecting to the same VCenter and hosts. Question is now how to match these withot having to have VDCs \ntwice.\n\nAs as soon as I do have news, I'll let you know.\n\nThanks,\nDaniel</text>, <text>Hello Daniel,\n\nas we discussed a few minutes ago after confiration of our Integration satellite System with a subset of esx hosts and running virt-who with Parameter --one-shot the desired esx hosts are displayed in menu "All Hosts" with association of the installed test systems. But the 4 Systems are always consuming a subscription for RHEL with 2 sockets. We expect that if the system is linked to a subscribed esx host the association to the other subscription will be removed automatically.\n\nBest regards\nKlaus</text>, <text>Hello Klaus,\n\n\nthat would be another RFE  then. I have discussed this case internally and by now we do not automatically \nremove attached subscriptions but required to have accordingly build activation keys as outlined by the \ndocumentation:\n\n- https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html-single/virtual_instances_guide/#applying_virtual_guest_subscriptions\n\nSo this would mean you do have actually 2 possibilities at the moment:\n- build different activation keys, one for physical and one for VDC hosts\n- manually remove the subscription for physical host after deployment.\n\n\nI you like I can file another RFE for this but I am afraid, at the moment we do have only the possibilities outlined\nabove.\n\nThanks,\nDaniel</text>, <text>Hello Daniel,\n\nthanks for the informations, but they are not satisfying at the moment.\n\nFor possibility number 2 I want to do this in a script and I'm searching for the hammer command to get all content hosts attached to a esx host, but I can't find it. If this is also not available we need the next RFE.\n\nTo decide the strategy in the meantime I need some more tests.\n\nBest regards\nKlaus</text>, <text>Hello Klaus,\n\nI'll need some more feedback on this from internal lists and probably discuss with PM.\n\nI understand that this is not satisfying for you and would like to ensure\nwe do get this sorted if possible but I feel just filing an RFE would most\nprobably not lead to the goal we're aiming at.\n\nI'll keep you posted about the progress, but please let me know in case you do \nhave any questions meanwhile.\n\nThanks,\nDaniel</text>, <text>*/INTERNAL*/\n\n\n\n\n-------- Forwarded Message --------\nSubject: \tRe: Fiducia GAD\nDate: \tWed, 8 Nov 2017 09:13:43 +0100\nFrom: \tDaniel Moessner &lt;dmoessne@redhat.com&gt;\nTo: \tRich Jerrido &lt;rjerrido@redhat.com&gt;, David Caplan &lt;dcaplan@redhat.com&gt;\nCC: \tMichael Schwabe &lt;mschwabe@redhat.com&gt;, Marc Schwering &lt;mschweri@redhat.com&gt;, Daniel Moessner &lt;dmoessne@redhat.com&gt;\n\n\nHi Rich,\n\nprobably I have missed your answer. Would you mind commenting on my below questions especially the second one  ?\n\nThanks,\nDaniel\n\nOn 10/24/2017 10:32 AM, Daniel Moessner wrote:\n&gt; Hi Rich, David,\n&gt;\n&gt; thanks for following up on this and sorry for my late reply, I was out unplanned...\n&gt;\n&gt; Rich, If I do get you right, your suggestions all aim to changes in VCenter (DRS, sffinity rules,..), right ? I am asking, because I heard some rumors that such\n&gt; kind of floating vDC could possibly been build with CFME ?\n&gt;\n&gt; Also, as they have 2 Satellites, dev and prod both connected to the same VCenter, this would mean double vDC SKUs so that each Satellite is able to register\n&gt; hosts correctly via VDC (and they really would like to use vDC if possible). Do we have possible solutions to that one ?\n&gt;\n&gt; Thanks,\n&gt; Daniel\n&gt;\n&gt;\n&gt; On 10/20/2017 06:43 PM, Rich Jerrido wrote:\n&gt;&gt;\n&gt;&gt;\n&gt;&gt; On 10/20/2017 10:37 AM, David Caplan wrote:\n&gt;&gt;&gt; https://docs.google.com/document/d/1jCabT8fPyIOGVSsUPT-z2cYfKOsZI42vNPnytuESgp4/edit\n&gt;&gt;&gt;\n&gt;&gt;&gt; Rich,\n&gt;&gt;&gt; There is an issue with floating VDC Subs:\n&gt;&gt;&gt;\n&gt;&gt;&gt;   *\n&gt;&gt;&gt;\n&gt;&gt;&gt;     Aside this they have a floating VM environment, this is, some esx\n&gt;&gt;&gt;     hosts are dedicated to WIN, some to RHEL, some to SUSE but the\n&gt;&gt;&gt;     overall majority is not bound to a certain esx host which could lead\n&gt;&gt;&gt;     to a situation where no RHEL in running on a VDC subscribed host but\n&gt;&gt;&gt;     one one not jet subscribed. As this is floating, where VMS can\n&gt;&gt;&gt;     dynamically migrate there is no chance following up on if all esx\n&gt;&gt;&gt;     hosts currently used have a proper subscription, although the\n&gt;&gt;&gt;     overall available SKUs are not used.  They would require and have\n&gt;&gt;&gt;     requested as Satellite 6 knows all HVs where VMS are running if Sat\n&gt;&gt;&gt;     6 detects a esx host not subscribed with VMs on it but another one\n&gt;&gt;&gt;     empty with regards to RHEL VMs that Satellite 6 automatically moves\n&gt;&gt;&gt;     the VDC SKU around.\n&gt;&gt;&gt;\n&gt;&gt;&gt; I have seen this come up with other customers. I would like hear how we have solved this in the past. My 1st reaction is to tune the virt-who config to constrain reporting of HVs\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt; If you migrate a RHEL VM to a host that doesn't have a proper sub, that VM isn't properly subscribed, and thus Candlepin attaches a sub to it. (Strongly preferring a sub that supports virtual guests, such as a vDC sub)\n&gt;&gt;\n&gt;&gt; Subscriptions are only ever changed when either an administrator explicitly does it, OR when they expire. If you have an empty host (and I am defining 'empty' as a 'host with a subscription attached that currently does not host a virtual machine'), we do not remove the subscription. In fact we can't as we do not know (amongst other things) if the current 'empty' state is temporary (such as due to maintenance) or permanent.\n&gt;&gt;\n&gt;&gt; This gives the customer the following options\n&gt;&gt;\n&gt;&gt; - dont report hypervisors (either via virt-who configuration OR configuration of the user running virt-who) if they do not run RHEL guests. This protects the vDC subs from being consumed, but doesn't protect you from the guests being migrated. To do that you need to...\n&gt;&gt; - Limit which hypervisors guests can migrate to\n&gt;&gt;    - via DRS policy\n&gt;&gt;    - affinity rules.\n&gt;&gt;    - building dedicated RHEL only clusters.\n&gt;&gt; - If you _truly_ need the virtual machines to be mobile within the cluster and you haven't purchased vDC subs for all of the hypervisors, then you are NOT the target market for virtual datacenter subscriptions. You should be purchasing instance based subs.\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;&gt;&gt; Thanks\n&gt;&gt;&gt;\n&gt;&gt;&gt; David\n&gt;&gt;&gt;\n&gt;&gt;\n&gt;&gt;\n&gt;\n&gt; -- \n&gt; Daniel M\xf6\xdfner\n&gt; Senior Technical Account Manager, Red Hat GmbH (Munich Office)\n&gt;\n&gt; mobile  +49.151.12002918\n&gt; desk    +49.89.205071-315\n&gt;\n&gt; Red Hat GmbH, http://www.de.redhat.com/, Registered seat: Grasbrunn, \n&gt; Commercial register: Amtsgericht Muenchen, HRB 153243,\n&gt; Managing Directors: Charles Cachera, Michael Cunningham, Michael O'Neill, Eric Shander\n\n-- \nDaniel M\xf6\xdfner\nSenior Technical Account Manager, Red Hat GmbH (Munich Office)\n\nmobile  +49.151.12002918\ndesk    +49.89.205071-315\n\nRed Hat GmbH, http://www.de.redhat.com/, Registered seat: Grasbrunn, \nCommercial register: Amtsgericht Muenchen, HRB 153243,\nManaging Directors: Charles Cachera, Michael Cunningham, Michael O'Neill, Eric Shander</text>, <text>Hello Klaus,\n\nwhile I am still waiting for further details from product management I have gotten already a first \nanswer regarding my request doing auto movement of VDC Subscriptions:\n\n~~~\nIf you migrate a RHEL VM to a host that doesn't have a proper sub, that VM isn't properly subscribed, and thus Candlepin attaches a sub to it. (Strongly preferring a sub that supports virtual guests, such as a vDC sub)\n\nSubscriptions are only ever changed when either an administrator explicitly does it, OR when they expire. If you have an empty host (and I am defining 'empty' as a 'host with a subscription attached that currently does not host a virtual machine'), we do not remove the subscription. In fact we can't as we do not know (amongst other things) if the current 'empty' state is temporary (such as due to maintenance) or permanent.\n\nThis gives the customer the following options\n\n- dont report hypervisors (either via virt-who configuration OR configuration of the user running virt-who) if they do not run RHEL guests. This protects the vDC subs from being consumed, but doesn't protect you from the guests being migrated. To do that you need to...\n- Limit which hypervisors guests can migrate to\n   - via DRS policy\n   - affinity rules.\n   - building dedicated RHEL only clusters.\n- If you _truly_ need the virtual machines to be mobile within the cluster and you haven't purchased vDC subs for all of the hypervisors, then you are NOT the target market for virtual datacenter subscriptions. You should be purchasing instance based subs.\n\n~~~\n\nSo to sum this up, there will be no kind of auto migration of subscriptions because of the reasons outlined above and therefore manual configurations in virt-who or in VMware need to be done to circumvent this.\n\nAs stated I am still waiting for more clarification but I do not reckon this decision will change.\n\nKind regards,\nDaniel</text>, <text>*/INTERNAL*/\n\n==Handover Note to new TAM==\n\n- customer has actually 2 issues:\n  1. 2 Satellites connected to same VCenter\n     previously we granted additional $0 SKus for that, but answer from PM still overdue\n  2. not happy with how VDC SKUs work at all as they have floating VMs mainly and as they do not have as many \n     SKUS as HVs they would expect auto movement of SKUs, which will not happen according to mail from PM\n     (see below)\n     --&gt; provided this to the customer\n\ntodo:\n=====\n- waiting for feedback on my last question re 2 satellites vs 1 vcenter and then update the customer\n- I hope we do get 0$ SKUs -&gt; work them with $ales to get BU guidance ticket to get that SKUs in cust account</text>, <text>Hello Klaus,\n\nsince we now have the additional Subscriptions for the migration in your account,\ndo you agree to close this case?\n\nBest regards,\nMichael</text>]
spam	[[<description>Our Satellite server indicates the following for a system content host view:-\n\nSubscription Status Fully entitled\nDetails\n\n    Red Hat Enterprise Linux Server: Not supported by a valid subscription.\n\nIt appears that our subscription expired on 2017-11-18\nWe seem to have a renewal active from 2017-11-19, but this was not displayed from the Red Hat subscription view in the Satellite. \nWhen trying to re-register a system to Satellite we get the following message:-\nUnable to verify server's identity: certificate verify failed\n\nWe were trying to patch systems yesterday, but could not enable repos to get the patches due to this problem, so had to cancel the patching due to time constraints, as not sure how long it would have taken to resolve this.</description>], <text>Request Management Escalation: Need to resolve this as soon as possible to allow us to have system patching capability</text>, <text>Checking RME</text>, <text>Hello,\n\nI am Swagato Paul, Escalation Manager with Red Hat Customer Experience and Engagement. I acknowledge your request for management escalation. I observed that the case was created less than an hour ago. I understand the urgency of the issue to you. Our engineer is currently reviewing the provided details, please allow such time to analyze and revert within SLA. Thank you for your patience.\n\nRegards,\nSwagato Paul,\nEscalation Manager, CEE,\nRed Hat Inc.</text>, <text>Hello,\n\nWelcome to Red Hat Technical Support!!!\n\nMy name is Prashant Waghmare and I am assisting you on this case.\n\nCould you please follow below steps to resolve this issue:\n\n# cd /etc/rhsm/ \n\n# cp rhsm.conf.kat-backup rhsm.conf\n\n# service rhsmcertd restart(Rhel6)/systemctl restart rhsmcertd\n\nIf you are using proxy then make sure mention its details in /etc/rhsm/rhsm.conf in following parameteres:\n\n# an http proxy server to use\nproxy_hostname =\n\n# port for http proxy server\nproxy_port =\n\n# user name for authenticating to an http proxy, if needed\nproxy_user =\n\n# password for basic http proxy auth, if needed\nproxy_password =\n\n# rpm -qa katello-ca-*\n\nremove the katello-ca-consumer-satellite.example.com-1.0-2.noarch.rpm which you will get in above commands out-put:\n\n# rpm -e katello-ca-consumer-satellite.example.com-1.0-2.noarch.rpm\n\nThen reinstall it, to client VM as below:\n\nrpm -Uvh katello-ca-consumer-satellite.example.com-1.0-2.noarch.rpm\n\nThen try to register the client VM and let us know if the problem still persist.\n\nFor more details, please refer below articles:\n=====================\nhttps://access.redhat.com/solutions/1211353\n\nhttps://access.redhat.com/solutions/3146801\n====================\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Could we schedule a webex so that you can assist us in doing this please?\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 20 November 2017 10:37\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-20 10:37:12, Waghmare, Prashant commented:\n"Hello,\n\nWelcome to Red Hat Technical Support!!!\n\nMy name is Prashant Waghmare and I am assisting you on this case.\n\nCould you please follow below steps to resolve this issue:\n\n# cd /etc/rhsm/ \n\n# cp rhsm.conf.kat-backup rhsm.conf\n\n# service rhsmcertd restart(Rhel6)/systemctl restart rhsmcertd\n\nIf you are using proxy then make sure mention its details in /etc/rhsm/rhsm.conf in following parameteres:\n\n# an http proxy server to use\nproxy_hostname =\n\n# port for http proxy server\nproxy_port =\n\n# user name for authenticating to an http proxy, if needed proxy_user =\n\n# password for basic http proxy auth, if needed proxy_password =\n\n# rpm -qa katello-ca-*\n\nremove the katello-ca-consumer-satellite.example.com-1.0-2.noarch.rpm which you will get in above commands out-put:\n\n# rpm -e katello-ca-consumer-satellite.example.com-1.0-2.noarch.rpm\n\nThen reinstall it, to client VM as below:\n\nrpm -Uvh katello-ca-consumer-satellite.example.com-1.0-2.noarch.rpm\n\nThen try to register the client VM and let us know if the problem still persist.\n\nFor more details, please refer below articles:\n=====================\nhttps://access.redhat.com/solutions/1211353\n\nhttps://access.redhat.com/solutions/3146801\n====================\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\n---------------------------------------\n\nThank you for your latest interaction with Red Hat Global Support Services. We are currently working to resolve your case.\n\nYour case has transitioned to "Waiting On Customer" status. This means that the Red Hat associate working on your case needs information or action from you to proceed. To help us resolve your case as quickly as possible, please update your case online on the Customer Portal at https://access.redhat.com.\n\nOnce you update the case to provide the requested information, we can continue working to resolve your issue.\n\nIf you wish to contact Red Hat, visit the Customer Portal at https://access.redhat.com to find phone and web contact information relevant to your region and support contract.\n\n\nThank you,\n\nRed Hat Global Support Services\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>Hello,\n\nCould you join on following remote session URl:\n\nSession Key: 9501180 \n\nURL: https://remotesupport.redhat.com/?ak=25afa5fc086cee0be0fe935185fc20a1 \n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hello,\n\nW.r.t. remote session we have seen that 33 hypervisors are reporting to the satellite server, however you have only 29 virtual datacenter subscriptions.\n\nThe out-put of reporting hypervisors are present on virt-who server , compare the names of hypervisors present in /tmp/virt-who.log file w.r.t. the csv file which you have downloaded on your physical server.\n\nThe name of hypervisors present in /tmp/virt.log file of virt-who server but not available in csv file, check for those hypervisors in satellite web-ui and make sure they should have correct subscription for all reporting hypervisor. \n\nFor that you will need to order more 5 virtual data center subscriptions to report all hypervisors and client VM based on them.\n\nOnce you will correct subscription for each hypervisor you need to run below command on client VM:\n\n# subscription-manager refresh\n\n# subscription-manager attach --auto \n\nAnd need to remove Red Hat Enterprise Linux Server with Smart Management, Premium (Physical or Virtual Nodes) subscription from those client VM's as they got consumed wrongly.\n\nFeel free to update the case once you will have correct number subscriptions to subscribe all hypervisors, for that please contact to our sales team to order more Virtual data center subscriptions.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>We cannot register a physical host to the satellite server --&gt;\n[root@ohs-dmz-prod02 ~]# subscription-manager register --org="Default_Organization" --activationkey="1-rhel6-phys-production-x86_64" --force\nThe system with UUID 104bdf14-a957-47a2-a668-c2fcdd2fc4f8 has been unregistered\nThe system has been registered with ID: e10466c2-2b1f-4050-9b5b-5b9dd14abc1d\n\nInstalled Product Current Status:\nProduct Name: Red Hat Enterprise Linux Server\nStatus:       Not Subscribed\n\nUnable to find available subscriptions for all your installed products.\n[root@ohs-dmz-prod02 ~]# subscription-manager status\n+-------------------------------------------+\n   System Status Details\n+-------------------------------------------+\nOverall Status: Invalid\n\nRed Hat Enterprise Linux Server:\n- Not supported by a valid subscription.</text>, <text>Hello,\n\nCould you run following commands on physical server:\n========================\n# subscription-manager refresh\n\n# subscription-manager list --available &lt;== here you will find the available subscriptions and then copy pool-id of subscription which you would like to register to client VM and attach it to the client VM via following command\n\n# subscription-manager attach --pool=pool-id\n=======================\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Can we schedule a webex - this is a physical system we are trying to register, but also have a couple of VM's indicating subscription-manager status --&gt; unknown\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 22 November 2017 12:39\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-22 12:39:15, Waghmare, Prashant commented:\n"Hello,\n\nCould you run following commands on physical server:\n========================\n# subscription-manager refresh\n\n# subscription-manager list --available &lt;== here you will find the available subscriptions and then copy pool-id of subscription which you would like to register to client VM and attach it to the client VM via following command\n\n# subscription-manager attach --pool=pool-id =======================\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\n---------------------------------------\n\nThank you for your latest interaction with Red Hat Global Support Services. We are currently working to resolve your case.\n\nYour case has transitioned to "Waiting On Customer" status. This means that the Red Hat associate working on your case needs information or action from you to proceed. To help us resolve your case as quickly as possible, please update your case online on the Customer Portal at https://access.redhat.com.\n\nOnce you update the case to provide the requested information, we can continue working to resolve your issue.\n\nIf you wish to contact Red Hat, visit the Customer Portal at https://access.redhat.com to find phone and web contact information relevant to your region and support contract.\n\n\nThank you,\n\nRed Hat Global Support Services\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>[root@ohs-dmz-prod02 ~]# subscription-manager list --available\n+-------------------------------------------+\n    Available Subscriptions\n+-------------------------------------------+\nSubscription Name:   mbmodules\nProvides:\nSKU:                 1485242909494\nContract:\nPool ID:             8a94079259b78ddd0159cf60481a3c27\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                01/17/2047\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux for Virtual Datacenters with Smart Management, Premium\nProvides:\nSKU:                 RH00006\nContract:            11466520\nPool ID:             8a9407925f9b1656015fd98851392621\nProvides Management: Yes\nAvailable:           2\nSuggested:           1\nService Level:       Premium\nService Type:        L1-L3\nSubscription Type:   Stackable\nEnds:                11/19/2018\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux for Virtual Datacenters with Smart Management, Premium\nProvides:\nSKU:                 RH00006\nContract:            11466520\nPool ID:             8a9407925f9b1656015fd98852052692\nProvides Management: Yes\nAvailable:           1\nSuggested:           1\nService Level:       Premium\nService Type:        L1-L3\nSubscription Type:   Stackable\nEnds:                11/19/2018\nSystem Type:         Physical\n\nSubscription Name:   migration\nProvides:\nSKU:                 15028779214711439796311\nContract:\nPool ID:             8a9407925dadcf4b015dea8139b12c48\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                08/09/2047\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux for Virtual Datacenters with Smart Management, Premium\nProvides:\nSKU:                 RH00006F3\nContract:            11479498\nPool ID:             8a9407925f9b1656015fd3a6abb90d5f\nProvides Management: Yes\nAvailable:           4\nSuggested:           1\nService Level:       Premium\nService Type:        L1-L3\nSubscription Type:   Stackable\nEnds:                11/19/2020\nSystem Type:         Physical\n\nSubscription Name:   PuppetForge\nProvides:\nSKU:                 1455194184498\nContract:\nPool ID:             8a940792524d471d0152d05520ee23b6\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                02/03/2046\nSystem Type:         Physical\n\n[root@ohs-dmz-prod02 ~]#\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 22 November 2017 12:50\nTo: JONES Martyn\nSubject: (WoRH) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-22 12:49:39, Jones, Martyn commented:\n"Can we schedule a webex - this is a physical system we are trying to register, but also have a couple of VM's indicating subscription-manager status --&gt; unknown\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com]\nSent: 22 November 2017 12:39\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-22 12:39:15, Waghmare, Prashant commented:\n"Hello,\n\nCould you run following commands on physical server:\n========================\n# subscription-manager refresh\n\n# subscription-manager list --available &lt;== here you will find the available subscriptions and then copy pool-id of subscription which you would like to register to client VM and attach it to the client VM via following command\n\n# subscription-manager attach --pool=pool-id =======================\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\n---------------------------------------\n\nThank you for your latest interaction with Red Hat Global Support Services. We are currently working to resolve your case.\n\nYour case has transitioned to "Waiting On Customer" status. This means that the Red Hat associate working on your case needs information or action from you to proceed. To help us resolve your case as quickly as possible, please update your case online on the Customer Portal at https://access.redhat.com.\n\nOnce you update the case to provide the requested information, we can continue working to resolve your issue.\n\nIf you wish to contact Red Hat, visit the Customer Portal at https://access.redhat.com to find phone and web contact information relevant to your region and support contract.\n\n\nThank you,\n\nRed Hat Global Support Services\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************"\n\nhttps://access.redhat.com/support/cases/#/case/01976915?commentId=a0aA000000L4B0FIAV\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>Hello,\n\nCould you run below command client VM :\n\n# subscription-manager attach --pool=8a9407925f9b1656015fd98852052692\n\n# subscription-manager refresh\n\n# subscription-manager status\n\nIf it still shows invalid then check # cat /etc/redhat-release and # rct cat-cert /etc/pki/product/69.pem\n\nBoth versions should be same. If its differ then copy 69.pem from :\n\n#  cd /etc/pki/product-default/\n\n# cp 69.pem /etc/pki/product/\n\n# subscription-manager refresh \n\n# subscription-manager attach --auto\n\n# subscription-manager status\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Could we schedule a webex for tomorrow morning please?\n\nWe have a few systems still with registration/subscription issues. A mixture of VMs and Physicals.\n\nHere is the output from 2 of the VMs:-\n\n[root@av-prod ~]# subscription-manager refresh\nUnable to verify server's identity: certificate verify failed\n\n[root@lpjen001 ~]# subscription-manager attach --pool=8a9407925f9b1656015fd98852052692\nThis system is not yet registered. Try 'subscription-manager register --help' for more information.\n[root@lpjen001 ~]# rct cat-cert /etc/pki/product/69.pem\nThe specified certificate file does not exist.\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 22 November 2017 14:55\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-22 14:55:02, Waghmare, Prashant commented:\n"Hello,\n\nCould you run below command client VM :\n\n# subscription-manager attach --pool=8a9407925f9b1656015fd98852052692\n\n# subscription-manager refresh\n\n# subscription-manager status\n\nIf it still shows invalid then check # cat /etc/redhat-release and # rct cat-cert /etc/pki/product/69.pem\n\nBoth versions should be same. If its differ then copy 69.pem from :\n\n#  cd /etc/pki/product-default/\n\n# cp 69.pem /etc/pki/product/\n\n# subscription-manager refresh \n\n# subscription-manager attach --auto\n\n# subscription-manager status\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\n---------------------------------------\n\nThank you for your latest interaction with Red Hat Global Support Services. We are currently working to resolve your case.\n\nYour case has transitioned to "Waiting On Customer" status. This means that the Red Hat associate working on your case needs information or action from you to proceed. To help us resolve your case as quickly as possible, please update your case online on the Customer Portal at https://access.redhat.com.\n\nOnce you update the case to provide the requested information, we can continue working to resolve your issue.\n\nIf you wish to contact Red Hat, visit the Customer Portal at https://access.redhat.com to find phone and web contact information relevant to your region and support contract.\n\n\nThank you,\n\nRed Hat Global Support Services\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>This is the error for the physical system:-\n\n[root@lagbcdf006 ~]# subscription-manager register --org="Default_Organization" --activationkey="1-rhel6-phys-production-x86_64" --force\nThe system with UUID d2e25fa0-b2d8-4436-ae40-78b9cee7d13c has been unregistered\nThe system has been registered with ID: 7d0a4731-a72f-406a-85d6-f77b7d0436aa\n\nInstalled Product Current Status:\nProduct Name: Red Hat Enterprise Linux Server\nStatus:       Not Subscribed\n\nUnable to find available subscriptions for all your installed products.\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 22 November 2017 15:51\nTo: JONES Martyn\nSubject: (WoRH) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-22 15:51:13, Jones, Martyn commented:\n"Could we schedule a webex for tomorrow morning please?\n\nWe have a few systems still with registration/subscription issues. A mixture of VMs and Physicals.\n\nHere is the output from 2 of the VMs:-\n\n[root@av-prod ~]# subscription-manager refresh Unable to verify server's identity: certificate verify failed\n\n[root@lpjen001 ~]# subscription-manager attach --pool=8a9407925f9b1656015fd98852052692\nThis system is not yet registered. Try 'subscription-manager register --help' for more information.\n[root@lpjen001 ~]# rct cat-cert /etc/pki/product/69.pem The specified certificate file does not exist.\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com]\nSent: 22 November 2017 14:55\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-22 14:55:02, Waghmare, Prashant commented:\n"Hello,\n\nCould you run below command client VM :\n\n# subscription-manager attach --pool=8a9407925f9b1656015fd98852052692\n\n# subscription-manager refresh\n\n# subscription-manager status\n\nIf it still shows invalid then check # cat /etc/redhat-release and # rct cat-cert /etc/pki/product/69.pem\n\nBoth versions should be same. If its differ then copy 69.pem from :\n\n#  cd /etc/pki/product-default/\n\n# cp 69.pem /etc/pki/product/\n\n# subscription-manager refresh \n\n# subscription-manager attach --auto\n\n# subscription-manager status\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\n---------------------------------------\n\nThank you for your latest interaction with Red Hat Global Support Services. We are currently working to resolve your case.\n\nYour case has transitioned to "Waiting On Customer" status. This means that the Red Hat associate working on your case needs information or action from you to proceed. To help us resolve your case as quickly as possible, please update your case online on the Customer Portal at https://access.redhat.com.\n\nOnce you update the case to provide the requested information, we can continue working to resolve your issue.\n\nIf you wish to contact Red Hat, visit the Customer Portal at https://access.redhat.com to find phone and web contact information relevant to your region and support contract.\n\n\nThank you,\n\nRed Hat Global Support Services\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************"\n\nhttps://access.redhat.com/support/cases/#/case/01976915?commentId=a0aA000000L4E0tIAF\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/\nRed Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>Hello,\n\nCould you please join on remote session:\n==================\nSession Key: 0984592 \n\nURL: https://remotesupport.redhat.com/?ak=164cc24245a3caa3c032fd30885b01bc \n==================\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hello,\n\nw.r.t. remote session, we have registered the systems which need to patch and attched subscriptions to them.\n\nNext time perform below steps in order to avoid the inconvenience while registeration:\n\n# subscription-manager unsubscribe --all\n\n#  subscription-manager unregister\n\n# subscription-manager clean\n\n# rpm -qa katello-ca-*\n\nThe out-put you will get in above command remove that package via # rpm -e &lt;katello_pckage&gt;\n\n# cd /etc/rhsm\n\n# ls\n\n# cp rhsm.conf.kat-backup rhsm.conf\n\n# subscription-manager refresh\n\n Then register the system using script.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>We would like to understand why this problem occurred and how to prevent a recurrence at the next subscription expiration/renewal?\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 26 November 2017 11:10\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\n---------------------------------------\n\nHello Martyn Jones,\n\nIt has been 2 days since we requested additional information on your inquiry.  Please review previous case emails and/or case history, and provide the requested information to allow us to continue solving this issue.\n\nRegards,\nRed Hat Customer Experience and Engagement\n\nNote: This is an automated response.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>We have 2 systems that we need to patch on Sunday that are not being registered with valid subscription.\nCan we schedule a webex to resolve please?</text>, <text>Hello,\n\nHave you tred below steps:\n\n\n# subscription-manager unsubscribe --all\n\n#  subscription-manager unregister\n\n# subscription-manager clean\n\n# rpm -qa katello-ca-*\n\nThe out-put you will get in above command remove that package via # rpm -e &lt;katello_pckage&gt;\n\n# cd /etc/rhsm\n\n# ls\n\n# cp rhsm.conf.kat-backup rhsm.conf\n\n# subscription-manager refresh\n\n Then register the system using script.\n\nIf yes, what error are you getting while registration? Provide me the out-put of all above commands on client VM.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>[root@sol-prod-node-13 ~]# subscription-manager unsubscribe --all\nUnit 03087aaa-fa22-4735-8e3b-69da6e63e4c8 has been deleted\n[root@sol-prod-node-13 ~]# subscription-manager unregister\nSystem has been unregistered.\n[root@sol-prod-node-13 ~]# subscription-manager clean\nAll local data removed\n[root@sol-prod-node-13 ~]# rpm -qa katello-ca-*\nkatello-ca-consumer-satellite.atradiusnet.com-1.0-2.noarch\n[root@sol-prod-node-13 ~]# rpm -e katello-ca-consumer-satellite.atradiusnet.com-1.0-2.noarch\n[root@sol-prod-node-13 ~]# cd /etc/rhsm\n[root@sol-prod-node-13 rhsm]# ls\nca  facts  logging.conf  pluginconf.d  rhsm.conf  rhsm.conf.kat-backup  rhsm.conf.rpmnew\n[root@sol-prod-node-13 rhsm]# cp rhsm.conf.kat-backup rhsm.conf\ncp: overwrite `rhsm.conf'? y\n[root@sol-prod-node-13 rhsm]# subscription-manager refresh\nThis system is not yet registered. Try 'subscription-manager register --help' for more information.\n[root@sol-prod-node-13 rhsm]# ll\ntotal 28\ndrwxr-xr-x. 2 root root 4096 Oct 13 15:29 ca\ndrwxr-xr-x. 2 root root 4096 Oct 13 15:18 facts\n-rw-r--r--  1 root root 1662 Feb 10  2017 logging.conf\ndrwxr-xr-x. 2 root root 4096 Oct 13 15:30 pluginconf.d\n-rw-r--r--  1 root root 1659 Nov 30 12:48 rhsm.conf\n-rw-r--r--  1 root root 1659 Aug  4  2016 rhsm.conf.kat-backup\n-rw-r--r--  1 root root 1852 Feb 10  2017 rhsm.conf.rpmnew\n[root@sol-prod-node-13 rhsm]# cd ~\n[root@sol-prod-node-13 ~]# ll\ntotal 7968\n-rw-------. 1 root root    2401 Feb 20  2014 anaconda-ks.cfg\n-rw-r--r--. 1 root root     639 Oct 13  2014 client-config-overrides.txt\n-rw-r--r--. 1 root root    6504 Aug  6  2013 client_config_update.py\n-rw-r--r--. 1 root root   19484 Feb 20  2014 install.log\n-rw-r--r--. 1 root root    7459 Feb 20  2014 install.log.syslog\n-rw-r--r--  1 root root 8087719 Nov  1  2015 NRC_RedHat6_64.spec\n-rw-r--r--. 1 root root     116 Mar 15  2015 prod-systems-rhel6\n-rwxr-xr-x. 1 root root     790 Dec 11  2014 register-rhel6-x86_64\n-rw-r--r--  1 root root    1506 Jul 25  2016 register-system-to-satellite6.sh\n-rwxr--r--  1 root root    1419 Oct 13 15:15 register-system-to-satellite6.sh.1\n-rw-r--r--. 1 root root    1164 Oct 20  2014 rpmsign_public_key.txt\n-rwxr-xr-x  1 root root     133 Jan  5  2016 ssh_change.sh\n[root@sol-prod-node-13 ~]# ./register-system-to-satellite6.sh.1 rhel6 phys production\nUpdate subscription-manager and yum\nLoaded plugins: enabled_repos_upload, package_upload, product-id, rhnplugin, search-disabled-repos, security\nThis system is not registered with RHN Classic or RHN Satellite.\nYou can use rhn_register to register.\nRHN Satellite or RHN Classic support will be disabled.\nSetting up Update Process\nNo Packages marked for Update\nUploading Enabled Repositories Report\nLoaded plugins: product-id, rhnplugin\nThis system is not registered with RHN Classic or RHN Satellite.\nYou can use rhn_register to register.\nRHN Satellite or RHN Classic support will be disabled.\nCannot upload enabled repos report, is this client registered?\nInstall katello certificate\nLoaded plugins: enabled_repos_upload, package_upload, product-id, rhnplugin, search-disabled-repos, security\nThis system is not registered with RHN Classic or RHN Satellite.\nYou can use rhn_register to register.\nRHN Satellite or RHN Classic support will be disabled.\nSetting up Local Package Process\nExamining /var/tmp/yum-root-FbVTGs/katello-ca-consumer-latest.noarch.rpm: katello-ca-consumer-satellite.atradiusnet.                                                                                                                         com-1.0-2.noarch\nMarking /var/tmp/yum-root-FbVTGs/katello-ca-consumer-latest.noarch.rpm to be installed\nResolving Dependencies\n--&gt; Running transaction check\n---&gt; Package katello-ca-consumer-satellite.atradiusnet.com.noarch 0:1.0-2 will be installed\n--&gt; Finished Dependency Resolution\n\nDependencies Resolved\n\n====================================================================================================================\n Package                                          Arch      Version     Repository                             Size\n====================================================================================================================\nInstalling:\n katello-ca-consumer-satellite.atradiusnet.com    noarch    1.0-2       /katello-ca-consumer-latest.noarch     14 k\n\nTransaction Summary\n====================================================================================================================\nInstall       1 Package(s)\n\nTotal size: 14 k\nInstalled size: 14 k\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\nWarning: RPMDB altered outside of yum.\n** Found 1 pre-existing rpmdb problem(s), 'yum check' output follows:\nnagios-plugins-1.4.16-1.ATR.x86_64 has missing requires of perl(Net::SNMP)\n  Installing : katello-ca-consumer-satellite.atradiusnet.com-1.0-2.noarch                                       1/1\nUploading Package Profile\nUnable to upload Package Profile\n  Verifying  : katello-ca-consumer-satellite.atradiusnet.com-1.0-2.noarch                                                                                                                                                                1/1\n\nInstalled:\n  katello-ca-consumer-satellite.atradiusnet.com.noarch 0:1.0-2\n\nComplete!\nUploading Enabled Repositories Report\nLoaded plugins: product-id, rhnplugin\nThis system is not registered with RHN Classic or RHN Satellite.\nYou can use rhn_register to register.\nRHN Satellite or RHN Classic support will be disabled.\nCannot upload enabled repos report, is this client registered?\nRegister system\nThe system has been registered with ID: 42985781-82ab-4e00-b5f3-0211347e35f0\n\nInstalled Product Current Status:\nProduct Name: Red Hat Enterprise Linux Server\nStatus:       Not Subscribed\n\nUnable to find available subscriptions for all your installed products.\nDisable EPEL repository\nRepository 'Default_Organization_Local-repositories_EPEL_6_x86_64' is disabled for this system.\nDisable VMware tools\nError: 'Default_Organization_Local-*vmtools*' does not match a valid repository ID. Use "subscription-manager repos --list" to see valid repositories.\nEnable satellite tools repository\nError: '*server-satellite-tools*' does not match a valid repository ID. Use "subscription-manager repos --list" to see valid repositories.\nInstall katello-agent\nLoaded plugins: enabled_repos_upload, package_upload, product-id, rhnplugin, search-disabled-repos, security\nThis system is not registered with RHN Classic or RHN Satellite.\nYou can use rhn_register to register.\nRHN Satellite or RHN Classic support will be disabled.\nSetting up Install Process\nDefault_Organization_Local-repositories_ELK                                                                                                                                                                           | 2.1 kB     00:00\nDefault_Organization_Local-repositories_IUS_Community_Packages_for_Enterprise_Linux_6                                                                                                                                 | 1.8 kB     00:00\nNot using downloaded repomd.xml because it is older than what we have:\n  Current   : Thu Oct  5 12:48:29 2017\n  Downloaded: Thu Oct  5 11:33:12 2017\nDefault_Organization_Local-repositories_ThirdPartySoftware                                                                                                                                                            | 2.1 kB     00:00\nDefault_Organization_Local-repositories_vmware-tools-ESXI65-10_1_7-RHEL6                                                                                                                                              | 1.8 kB     00:00\nNot using downloaded repomd.xml because it is older than what we have:\n  Current   : Thu Oct  5 12:48:22 2017\n  Downloaded: Thu Oct  5 11:33:07 2017\nNothing to do\nUploading Enabled Repositories Report\nLoaded plugins: product-id, rhnplugin\nThis system is not registered with RHN Classic or RHN Satellite.\nYou can use rhn_register to register.\nRHN Satellite or RHN Classic support will be disabled.\nStopping goferd                                            [  OK  ]\nStarting goferd                                            [  OK  ]\nStarting puppet agent:\n[root@sol-prod-node-13 ~]# subscription-manager status\n+-------------------------------------------+\n   System Status Details\n+-------------------------------------------+\nOverall Status: Invalid\n\nRed Hat Enterprise Linux Server:\n- Not supported by a valid subscription.\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 30 November 2017 10:45\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-30 10:44:54, Waghmare, Prashant commented:\n"Hello,\n\nHave you tred below steps:\n\n\n# subscription-manager unsubscribe --all\n\n#  subscription-manager unregister\n\n# subscription-manager clean\n\n# rpm -qa katello-ca-*\n\nThe out-put you will get in above command remove that package via # rpm -e &lt;katello_pckage&gt;\n\n# cd /etc/rhsm\n\n# ls\n\n# cp rhsm.conf.kat-backup rhsm.conf\n\n# subscription-manager refresh\n\n Then register the system using script.\n\nIf yes, what error are you getting while registration? Provide me the out-put of all above commands on client VM.\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\n---------------------------------------\n\nThank you for your latest interaction with Red Hat Global Support Services. We are currently working to resolve your case.\n\nYour case has transitioned to "Waiting On Customer" status. This means that the Red Hat associate working on your case needs information or action from you to proceed. To help us resolve your case as quickly as possible, please update your case online on the Customer Portal at https://access.redhat.com.\n\nOnce you update the case to provide the requested information, we can continue working to resolve your issue.\n\nIf you wish to contact Red Hat, visit the Customer Portal at https://access.redhat.com to find phone and web contact information relevant to your region and support contract.\n\n\nThank you,\n\nRed Hat Global Support Services\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>Hello,\n\nAs per provided logs it seems hat system got registered without any error. So, could you make sure that you have enough free subscriptions available on satellite server? If no, please make them free from unused systems o from systems which already got patched or add new subscriptions in satellite manifest through customer portal and refresh the manifest via satellite web-ui.\n\nThen add that subscription into the client VM which you would like to patch.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hi Prashant,\n\nThe Satellite tells us we have 10 free:-\n\nRed Hat Enterprise Linux Server with Smart Management, Standard (Physical or Virtual Nodes)\n\n    Details\n    Product Content\n    Associations\n\nSubscription Info\nName Red Hat Enterprise Linux Server with Smart Management, Standard (Physical or Virtual Nodes)\nDescription Red Hat Enterprise Linux\nVirt-Who Usage Required No\nConsumed 90 out of 100\nStarts 11/19/17 5:00 AM\nEnds 11/19/20 4:59 AM\nProduct ID RH00009F3\nContract Number 11479498\nAccount Number 734376\nSupport Level Standard\nSupport Type L1-L3\nArchitecture(s) ia64,ppc,ppc64,ppc64le,s390,s390x,x86,x86_64\nType NORMAL\nLimits Sockets: 2\nMulti-entitlement Yes\nInstance-based Yes\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 30 November 2017 15:12\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-30 15:11:54, Waghmare, Prashant commented:\n"Hello,\n\nAs per provided logs it seems hat system got registered without any error. So, could you make sure that you have enough free subscriptions available on satellite server? If no, please make them free from unused systems o from systems which already got patched or add new subscriptions in satellite manifest through customer portal and refresh the manifest via satellite web-ui.\n\nThen add that subscription into the client VM which you would like to patch.\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\nhttps://access.redhat.com/support/cases/#/case/01976915?commentId=a0aA000000L5nIBIAZ\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>Hello,\n\nCould you check via command line on client VM:\n========================\n# subscription-manager list --available \n========================\n\nThen copy pool id of subscription : Red Hat Enterprise Linux Server with Smart Management, Standard (Physical or Virtual Nodes)\n\nAttach on client VM via command:\n\n# subscription-manager attach --pool=pool_id\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Output states Physical subscriptions available 1 and suggested 2.\n\nWhy does satellite server indicate more than that available?\n\n\n[root@sol-prod-node-13 ~]# subscription-manager list --available\n+-------------------------------------------+\n    Available Subscriptions\n+-------------------------------------------+\nSubscription Name:   mbmodules\nProvides:\nSKU:                 1485242909494\nContract:\nPool ID:             8a94079259b78ddd0159cf60481a3c27\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                17/01/47\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux for Virtual Datacenters with Smart Management, Premium\nProvides:\nSKU:                 RH00006\nContract:            11466520\nPool ID:             8a9407925f9b1656015fd98851392621\nProvides Management: Yes\nAvailable:           2\nSuggested:           1\nService Level:       Premium\nService Type:        L1-L3\nSubscription Type:   Stackable\nEnds:                19/11/18\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux for Virtual Datacenters with Smart Management, Premium\nProvides:\nSKU:                 RH00006\nContract:            11466520\nPool ID:             8a9407925f9b1656015fd98852052692\nProvides Management: Yes\nAvailable:           1\nSuggested:           1\nService Level:       Premium\nService Type:        L1-L3\nSubscription Type:   Stackable\nEnds:                19/11/18\nSystem Type:         Physical\n\nSubscription Name:   migration\nProvides:\nSKU:                 15028779214711439796311\nContract:\nPool ID:             8a9407925dadcf4b015dea8139b12c48\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                09/08/47\nSystem Type:         Physical\n\nSubscription Name:   Local-repositories\nProvides:\nSKU:                 1450947378915\nContract:\nPool ID:             8a94079251c029b70151d33407d002f4\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                16/12/45\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux for Virtual Datacenters with Smart Management, Premium\nProvides:\nSKU:                 RH00006F3\nContract:            11479498\nPool ID:             8a9407925f9b1656015fd3a6abb90d5f\nProvides Management: Yes\nAvailable:           4\nSuggested:           1\nService Level:       Premium\nService Type:        L1-L3\nSubscription Type:   Stackable\nEnds:                19/11/20\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux Server with Smart Management, Standard (Physical or Virtual Nodes)\nProvides:            Oracle Java (for RHEL Server)\n                     Red Hat Enterprise Linux Atomic Host\n                     Red Hat Beta\n                     dotNET on RHEL Beta (for RHEL Server)\n                     Red Hat Developer Tools Beta (for RHEL Server)\n                     Red Hat Container Images\n                     dotNET on RHEL (for RHEL Server)\n                     Red Hat Developer Toolset (for RHEL Server)\n                     Red Hat Software Collections Beta (for RHEL Server)\n                     Red Hat Enterprise Linux Atomic Host Beta\n                     Red Hat Software Collections (for RHEL Server)\n                     Red Hat Enterprise Linux Server\n                     Red Hat Container Images Beta\n                     Red Hat Developer Tools (for RHEL Server)\nSKU:                 RH00009F3\nContract:            11479498\nPool ID:             8a9407925f9b1656015fd3a6ac400dd0\nProvides Management: Yes\nAvailable:           1\nSuggested:           2\nService Level:       Standard\nService Type:        L1-L3\nSubscription Type:   Instance Based\nEnds:                19/11/20\nSystem Type:         Physical\n\nSubscription Name:   PuppetForge\nProvides:\nSKU:                 1455194184498\nContract:\nPool ID:             8a940792524d471d0152d05520ee23b6\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                03/02/46\nSystem Type:         Physical\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 30 November 2017 15:21\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-30 15:20:47, Waghmare, Prashant commented:\n"Hello,\n\nCould you check via command line on client VM:\n========================\n# subscription-manager list --available ========================\n\nThen copy pool id of subscription : Red Hat Enterprise Linux Server with Smart Management, Standard (Physical or Virtual Nodes)\n\nAttach on client VM via command:\n\n# subscription-manager attach --pool=pool_id\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\nhttps://access.redhat.com/support/cases/#/case/01976915?commentId=a0aA000000L5nTgIAJ\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>Freed up an additional subscription and the count went to 98 out of 100 used.\n\nBefore that it displayed 90 out of 100, so something is out of sync.\n\nIt has now registered the system.\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 30 November 2017 15:27\nTo: JONES Martyn\nSubject: (WoRH) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-30 15:26:38, Jones, Martyn commented:\n"Output states Physical subscriptions available 1 and suggested 2.\n\nWhy does satellite server indicate more than that available?\n\n\n[root@sol-prod-node-13 ~]# subscription-manager list --available\n+-------------------------------------------+\n    Available Subscriptions\n+-------------------------------------------+\nSubscription Name:   mbmodules\nProvides:\nSKU:                 1485242909494\nContract:\nPool ID:             8a94079259b78ddd0159cf60481a3c27\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                17/01/47\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux for Virtual Datacenters with Smart Management, Premium\nProvides:\nSKU:                 RH00006\nContract:            11466520\nPool ID:             8a9407925f9b1656015fd98851392621\nProvides Management: Yes\nAvailable:           2\nSuggested:           1\nService Level:       Premium\nService Type:        L1-L3\nSubscription Type:   Stackable\nEnds:                19/11/18\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux for Virtual Datacenters with Smart Management, Premium\nProvides:\nSKU:                 RH00006\nContract:            11466520\nPool ID:             8a9407925f9b1656015fd98852052692\nProvides Management: Yes\nAvailable:           1\nSuggested:           1\nService Level:       Premium\nService Type:        L1-L3\nSubscription Type:   Stackable\nEnds:                19/11/18\nSystem Type:         Physical\n\nSubscription Name:   migration\nProvides:\nSKU:                 15028779214711439796311\nContract:\nPool ID:             8a9407925dadcf4b015dea8139b12c48\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                09/08/47\nSystem Type:         Physical\n\nSubscription Name:   Local-repositories\nProvides:\nSKU:                 1450947378915\nContract:\nPool ID:             8a94079251c029b70151d33407d002f4\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                16/12/45\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux for Virtual Datacenters with Smart Management, Premium\nProvides:\nSKU:                 RH00006F3\nContract:            11479498\nPool ID:             8a9407925f9b1656015fd3a6abb90d5f\nProvides Management: Yes\nAvailable:           4\nSuggested:           1\nService Level:       Premium\nService Type:        L1-L3\nSubscription Type:   Stackable\nEnds:                19/11/20\nSystem Type:         Physical\n\nSubscription Name:   Red Hat Enterprise Linux Server with Smart Management, Standard (Physical or Virtual Nodes)\nProvides:            Oracle Java (for RHEL Server)\n                     Red Hat Enterprise Linux Atomic Host\n                     Red Hat Beta\n                     dotNET on RHEL Beta (for RHEL Server)\n                     Red Hat Developer Tools Beta (for RHEL Server)\n                     Red Hat Container Images\n                     dotNET on RHEL (for RHEL Server)\n                     Red Hat Developer Toolset (for RHEL Server)\n                     Red Hat Software Collections Beta (for RHEL Server)\n                     Red Hat Enterprise Linux Atomic Host Beta\n                     Red Hat Software Collections (for RHEL Server)\n                     Red Hat Enterprise Linux Server\n                     Red Hat Container Images Beta\n                     Red Hat Developer Tools (for RHEL Server)\nSKU:                 RH00009F3\nContract:            11479498\nPool ID:             8a9407925f9b1656015fd3a6ac400dd0\nProvides Management: Yes\nAvailable:           1\nSuggested:           2\nService Level:       Standard\nService Type:        L1-L3\nSubscription Type:   Instance Based\nEnds:                19/11/20\nSystem Type:         Physical\n\nSubscription Name:   PuppetForge\nProvides:\nSKU:                 1455194184498\nContract:\nPool ID:             8a940792524d471d0152d05520ee23b6\nProvides Management: No\nAvailable:           Unlimited\nSuggested:           1\nService Level:\nService Type:\nSubscription Type:   Standard\nEnds:                03/02/46\nSystem Type:         Physical\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com]\nSent: 30 November 2017 15:21\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-30 15:20:47, Waghmare, Prashant commented:\n"Hello,\n\nCould you check via command line on client VM:\n========================\n# subscription-manager list --available ========================\n\nThen copy pool id of subscription : Red Hat Enterprise Linux Server with Smart Management, Standard (Physical or Virtual Nodes)\n\nAttach on client VM via command:\n\n# subscription-manager attach --pool=pool_id\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\nhttps://access.redhat.com/support/cases/#/case/01976915?commentId=a0aA000000L5nTgIAJ\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************"\n\nhttps://access.redhat.com/support/cases/#/case/01976915?commentId=a0aA000000L5nbBIAR\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>Hello,\n\nIf you will refresh the manifest it will show you the correct quantity.\n\nAs you have scarcity of subscriptions and keeps moving subscriptions from 1 system to another it shows the wrong count initially.\n\nAnd regarding quantity as 2 , please refer below article:\n===================\nhttps://access.redhat.com/solutions/1135553\n====================\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Can you update us on why we had the problems with the subscriptions when it was renewed?\n\nHow can this be prevented at the next renewal?\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 30 November 2017 16:39\nTo: JONES Martyn\nSubject: (WoC) (SEV 2) Case #01976915 (Subscriptions) ref:_00DA0HxWH._500A0Z3QHY:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01976915\nCase Title       : Subscriptions\nCase Number      : 01976915\nCase Open Date   : 2017-11-20 09:11:34\nSeverity         : 2 (High)\nProblem Type     : Certification\n\nMost recent comment: On 2017-11-30 16:38:55, Waghmare, Prashant commented:\n"Hello,\n\nIf you will refresh the manifest it will show you the correct quantity.\n\nAs you have scarcity of subscriptions and keeps moving subscriptions from 1 system to another it shows the wrong count initially.\n\nAnd regarding quantity as 2 , please refer below article:\n===================\nhttps://access.redhat.com/solutions/1135553\n====================\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\nhttps://access.redhat.com/support/cases/#/case/01976915?commentId=a0aA000000L5oyvIAB\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z3QHY:ref\n\n****************************************\nThe operating companies affiliated with Atradius N.V. (Atradius Group) conduct insurance, debt collection and information services business through their registered (branch) offices in many countries. For information about the main registration details of Atradius Group offices in your country please visit https://group.atradius.com/contact-us/\n\nImportant Notice\nThis e-mail, including any and all attachments, is intended for the addressee or its representative only. It is confidential and may be under legal privilege. Any form of publication, reproduction, copying or disclosure of the content of this e-mail is not permitted, unless expressly otherwise indicated in the e-mail by the Atradius contact person. If you are not the intended recipient of this e-mail and its contents, please notify the sender immediately by reply e-mail and delete this e-mail and all its attachments subsequently. Although this e-mail and any attachments are believed to be free of any virus or other defect that might affect any computer system into which it is received and/or opened, it is the responsibility of the recipient to ensure that it is virus free and no responsibility is accepted by Atradius Group companies, either jointly or severally, for any loss or damage arising in any way from its use. E-mail received by Atradius Group can be stored for business purposes.\n****************************************</text>, <text>Hello,\n\nWhenever you will renew the subscription, you will not be able to see the old subscription. And you need to reassign subscriptions to the content hosts.\n\nYou could refer following article in order to attach subscriptions to content hosts.\n==================\nhttps://access.redhat.com/blogs/1169563/posts/2630111\n\nhttps://access.redhat.com/blogs/1169563/posts/2882741\n====================\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Is there an explanation for why the virtual systems starting consuming physical licenses?</text>, <text>Hello,\n\nSorry for delay in replying you.\n\nMy name is Mustufa and I am replying you on behalf of my colleague `Prashant` on case#.\n\nBased on query, \n\n1) Could you tell what kind of subscriptions were assigned to VM's machines? .\nIdeally ('temporary guest UN-mapped') subscription is assigned by Satellite Server, which is not physical nor virtual system subscriptions.\n\n2) Was there VDC subscriptions assigned properly to Hypervisor systems? (ESXI,KVM,RHEM) etc where guest VM's are running?.\n\nDid you waited for 12 hours and noted this changes for subscriptions? OR Was this seen/noticed after 12 hours of subscription renewed time?\n\nFeel free to reach us back if you have more questions.\n\nThanks &amp; Regards,\nMustufa M,\nGSS, Red Hat.</text>]
spam	[[<description>What problem/issue/behavior are you having trouble with?  What do you expect to see?\n\nServer rh7-virtwho is used as communication between hypervisor (esxi) with rhsm.\nWe follow the step for register a new hypervisor server. However, we can't fiind that new hypervisor under my account.\n\nThe virtual guest, instead of create the hypervisor as expected, it attached a developer suite subsciption itself, which we haven't manual attach any subscription to it\n\nWhere are you experiencing the behavior?  What environment?\n\nProd env.\n\nWhen does the behavior occur? Frequently?  Repeatedly?   At certain times?\n\nIt happened repeating\n\nWhat information can you provide around timeframes and the business impact?\n\nIt cause all VM guest inside that new box can't receive any packages</description>], <text>This comment has been generated via an automated case assistant.\n\n \n\nThe following resources may be helpful for working with virt-who and virtual Subscriptions (including Virtual Data Center):\n\n \n\n    https://access.redhat.com/labs/virtwhoconfig/ Red Hat Virtualization Agent (virt-who) Configuration Helper - virt-who is required when working with any hypervisor if your subscription name includes the words 'Virtual Datacenters', '4 Guests', or 'Unlimited Guests'. This app helps users configure virt-who. When started, the app requests information about your virt-who usage and generates a configuration file based on he answers. It also provides instructions for updating your virt-who configuration.\n    https://access.redhat.com/blogs/1169563/posts/2190731 Subscription-manager for the former Red Hat Network User: Part 3 - Understanding virt-who \n    https://access.redhat.com/blogs/1169563/posts/2630111 Subscription-manager for the former Red Hat Network user - part 5 - Working with subscriptions that require virt-who\n    https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/virtual_instances_guide/ - Virtual Instances Guide for Satellite 6\n    https://access.redhat.com/documentation/en-us/red_hat_subscription_management/1/html/virtual_instances_guide/ - Virtual Instances Guide for use with Red Hat Subscription Management (RHSM) but no Satellite</text>, <text>*PRIVATE UPDATE, DO NOT MAKE PUBLIC*\n\n    The previous public update is part of a pilot to improve auto-solve and self-solve, by commenting on cases that match certain rules upon creation, to provide answers to common questions/issues.  You can find more information about this project[1], the rule match for this case[2], and how to provide feedback (whether the rule is helpful, suggestions for improvement) here:\n\n    [1] https://mojo.redhat.com/groups/support-delivery/projects/diagnostics-20\n    [2] https://projects.engineering.redhat.com/browse/CPSEARCH-1503\n    [3] https://mojo.redhat.com/thread/942968</text>, <text>Dear Support\nAny finding? Please advise</text>, <text>Hello,\n\nThank you for contacting Red Hat Technical support.\n\nMy name is Sanyam Shah. I have taken ownership of this case so I will be assisting you for the same.\n\nPlease help us with the virt-who configuration.\n\nAwaiting your response.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Dear Support\nplease find the rhsm log and virt-who configuration file</text>, <text>Please also find the uuid mapping from the result of fence_vmware_soap\n\n\nFEE-CER-ESB-2,420e0cc1-4266-2c6d-935d-7777a09b403a\nFEE-CER-ESB-1,564d1788-0c7b-6e9a-fba0-0097c84b698b\nFEE-CER-MDB,420eed70-fe59-3ced-b36c-3e85f959efff\nMIX-CER-TC-01,564d4ef3-fb27-f8c6-7f5a-2b57a23512db\nMIX-CER-TC-02,420e8e9c-dffc-038f-2c67-843e494752ad\nOOS-CER-WEB-02,564d0868-0423-55a1-a52b-61b623c2ed41\nOEP-CER-COG-001,423e9552-593d-e88c-2dea-f1e2b8e60fa4\nOMS-CER-WEB-001,564d0ab8-e10c-abcf-c73d-7db305bf2968\nOCPG-CER-LBR-01,420e3c83-aec8-8022-95e9-eb8812b935d5\nFEE-CER-RDB,564dbd0f-d091-48ca-494b-8dc169995838\nZabbix_Cert_proxy1,420ef5c5-78f6-2150-2dbf-d23e249294be\nFEE-CER-SDB,420ed24f-b6be-ef41-152b-0488665bd015\nMIX-CER-DB-02,420e61d8-8e1e-69ef-91be-722f089ff725\nOIS-CER-APP-01,4234e053-8fa6-091e-e58d-132bc43ed711\nMIX-CER-DB-01,420e8999-15dc-29b1-b3ff-4252db597400\nFEE-CER-TC-2,420e0f0c-ebdb-cb54-fb67-6112a672b174\nFEE-CER-TC-1,420e8d6b-5c5c-7597-b1d0-e335ce102066\nOEP-CER-ESB-001,564d4fc3-035d-09c4-281c-17c3acd99a46\nRH72-templsate-update,420e727d-190e-330d-5c90-9ff63700da47\nMTR-TEST-MQ-01,564d29f3-a7eb-83c9-54f6-684c453ddb36\nOMS-CER-APP-001,423e9c5e-9d98-75a8-3ad0-6c8275339a4e\nODS-CER-RDB,420ed9de-a92b-430f-31aa-db00363db0f1\nOCL-CER-JAS-RPT,564d13a4-e34f-fef5-bac6-eebf444914e8\nOCPG-CER-APP-01,42349811-add2-641c-f156-b15d36063217\nOCL-CER-COG-001,42346b91-2606-f8ff-aa9c-50854828a6ef\nrh7-virtwho,420e1f2e-e728-665b-a545-a699d1979efe</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nPlease let me know if we can have a remote session.\n\nAwaiting your response.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello support\nI'm free for getting remote support\nPlease send me the request</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nKindly start the remote session using below link.\n\nSession Key: 9731488 \n\nURL: https://remotesupport.redhat.com/?ak=86a0bcf21eec6aa0855ff18b073baad5 \n\nAwaiting your response.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello support,\nAs agreed, we will continue the support session on tomorrow</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nPlease help us with the suitable time so we can have a remote session on tomorrow.\n\nAwaiting your response.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Please send us the invition on tomorrow morning 0930\nAnd some more finding, in rhsm.log, there are many Job hypervisor_update not finished, reschduling messages.\nServer tried to send back the info back to portal but failed, and continue in rescheduling loop.\n\nPlease help to investigration\n\n\n2017-12-05 17:16:08,775 [INFO]  @subscriptionmanager.py:165 - Sending update in hosts-to-guests mapping: {\n    "hypervisors": [\n        {\n            "hypervisorId": {\n                "hypervisorId": "4c4c4544-004c-5110-8037-b6c04f574c32"\n            },\n            "name": "SOFX-CER-VM-001.octopuscards.local",\n            "guestIds": [\n                {\n                    "guestId": "564dbd0f-d091-48ca-494b-8dc169995838",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "564d4ef3-fb27-f8c6-7f5a-2b57a23512db",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420e8999-15dc-29b1-b3ff-4252db597400",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "564d29f3-a7eb-83c9-54f6-684c453ddb36",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "564d1788-0c7b-6e9a-fba0-0097c84b698b",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420e0cc1-4266-2c6d-935d-7777a09b403a",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420eed70-fe59-3ced-b36c-3e85f959efff",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420ed24f-b6be-ef41-152b-0488665bd015",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "564d4fc3-035d-09c4-281c-17c3acd99a46",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "423e9c5e-9d98-75a8-3ad0-6c8275339a4e",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420e61d8-8e1e-69ef-91be-722f089ff725",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "564d0ab8-e10c-abcf-c73d-7db305bf2968",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "564d13a4-e34f-fef5-bac6-eebf444914e8",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "42346b91-2606-f8ff-aa9c-50854828a6ef",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "423e9552-593d-e88c-2dea-f1e2b8e60fa4",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420ed9de-a92b-430f-31aa-db00363db0f1",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "42349811-add2-641c-f156-b15d36063217",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420e3c83-aec8-8022-95e9-eb8812b935d5",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420e1f2e-e728-665b-a545-a699d1979efe",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "564d0868-0423-55a1-a52b-61b623c2ed41",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420e8e9c-dffc-038f-2c67-843e494752ad",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420ef5c5-78f6-2150-2dbf-d23e249294be",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420e8d6b-5c5c-7597-b1d0-e335ce102066",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420e0f0c-ebdb-cb54-fb67-6112a672b174",\n                    "state": 1,\n                    "attributes": {\n                        "active": 1,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                },\n                {\n                    "guestId": "420e727d-190e-330d-5c90-9ff63700da47",\n                    "state": 5,\n                    "attributes": {\n                        "active": 0,\n                        "virtWhoType": "esx",\n                        "hypervisorType": "vmware"\n                    }\n                }\n            ]\n        }\n    ]\n}\n\n\n\n..\n2017-12-05 17:16:11,302 [DEBUG]  @virtwho.py:155 - Running method "checkJobStatus"\n2017-12-05 17:16:11,305 [DEBUG]  @subscriptionmanager.py:112 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-05 17:16:12,655 [DEBUG]  @subscriptionmanager.py:181 - Checking status of job hypervisor_update_81110e92-5cf6-4daf-b387-39999829bcf7\n2017-12-05 17:16:14,003 [DEBUG]  @subscriptionmanager.py:186 - Job hypervisor_update_81110e92-5cf6-4daf-b387-39999829bcf7 not finished, rescheduling\n2017-12-05 17:16:16,006 [DEBUG]  @virtwho.py:155 - Running method "checkJobStatus"\n2017-12-05 17:16:16,009 [DEBUG]  @subscriptionmanager.py:112 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-05 17:16:17,463 [DEBUG]  @subscriptionmanager.py:181 - Checking status of job hypervisor_update_81110e92-5cf6-4daf-b387-39999829bcf7\n2017-12-05 17:16:18,812 [DEBUG]  @subscriptionmanager.py:186 - Job hypervisor_update_81110e92-5cf6-4daf-b387-39999829bcf7 not finished, rescheduling\n.....</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nSure, we can have a session after 15 mins.\n\nPlease update the case once you are ready so we can have a remote session.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>We are ready to have remote session\nPlease send us the request</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nPlease start remote session using.\n\nSession Key: 3819006 \n\nURL: https://remotesupport.redhat.com/?ak=e430b93a3ec4b495086d7dff6a5ca348 \n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello,\nafter the remote support session, we found the problem is from our customer portal account.\n\nAs same server register to another account of my colleague work perfect.\n\n\nHere is the summary\n------------------------\naccount that not functional :  \nAccount number:  836265\norg name: 5167740\norg ID: 5167740\n\nAccount that workable\nAccount number:  5427650\norg name: 7147244\norg ID: 7147244\n\nPlease help to fix the issue of account 836265 (Ord ID: 5167740)</text>, <text>As said, new SOSreport generated for further investigation</text>, <text>Hello,\n\nThank you for your time over call.\n\nPlease allow me sometime while I am checking this with senior engineer.\n\nMeanwhile your patience are highly appreciated.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nWe have re-synced your account.\n\nPlease let me know if we can have a remote session today.\n\nAwaiting your response.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello Suppport,\nWe tried to remove all VM subscription and re-register again. It work as expected afterward.\nHowever, after 3 days, the same problem come again. All virt-who servers show Job hypervisor_update_xxxxxxx not finished, rescheduling.\n\nPlease help to check whether it is a bug or not</text>, <text>Hello,\n\nAs discussed over a chat, I informed you that the virt-who issue occurred due to the portal issue. the 'system' page was inaccessible. Later when the system page came up, the virt-who was able to report the host and guests to the portal.\n\nYou confirmed about the case closure. I am closing this case as requested. We are keen to know your feedback on this and we value it much. You may receive an E-mail asking to review the support received, please do send in your valuable feedback and suggestions as this would help us improve the quality of our support. \n\n\nBest Regards, \nAnuja J. \nGlobal Support Services, Red Hat.</text>, <text>Hello Anuja,\nPlease advise the time of remote support</text>, <text>Remote Support Invitation\n\nClick on the link below and follow the directions.\n\nhttps://remotesupport.redhat.com/?ak=d7a38ee16b9aca1585f1e52d35c9b096\n\nBomgar(TM) enables a support representative to view your screen in order to assist you.\nSession traffic is fully encrypted to protect your system's data.\nOnce a session has begun, you will be able to end it at any time</text>, <text>Hello,\n\nHere is the remote session summary :-\n1. We checked that all users from account# 836265 are facing errors while registering the system and configuring virt-who \n2. All the users from another account # 5427650 do not face such issues.\n3. We had kept the same virt-who configuration just the org ID (virt-who owner) was different.\n\nRequest you to upload the fresh rhsm.log file to refer tot eh errors so that we can share those with the portal team.\n\n\n\nBest Regards, \nAnuja J. \nGlobal Support Services, Red Hat</text>, <text>Hello,\n\nThank you for the log file.\n\nCan you please re-register the system using problematic ID, execute :-\n# subscription-manager identity --regenrate \n\nThen, verify the org iD by executing :-\n# subscription-manager identity\n\nCheck if same is mentioned in /etc/virt-who.d/vmware.conf and execute :-\n# subscription-manager refresh\n\nOnce done, check if the host guest mapping is sent and let us know the results.\n\nBest Regards, \nAnuja J. \nGlobal Support Services, Red Hat</text>, <text>Hello,\nno good. The hypervisor still can't register and the log still showing after taken the action as suggested\n\nrhsm.log\n----------\n2017-12-13 09:24:58,951 [INFO] subscription-manager:4394:MainThread @connection.py:551 - Response: status=200, requestUuid=0cd82bd5-a030-4c7c-b848-6ae14e041058, request="GET /subscription/consumers/f2771ce7-9564-4e59-9ab4-cf95ef1a39e1/compliance"\n2017-12-13 09:24:58,953 [INFO] subscription-manager:4394:MainThread @cert_sorter.py:204 - Product status: valid_products= partial_products= expired_products= unentitled_producs=69 future_products= valid_until=None\n2017-12-13 09:24:59,064 [INFO] rhsmd:4387:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=identity_cert ca_dir=/etc/rhsm/ca/ insecure=False\n2017-12-13 09:25:00,508 [INFO] rhsmd:4387:MainThread @connection.py:551 - Response: status=200, requestUuid=bb447632-cc49-4047-982f-ba850366812b, request="GET /subscription/consumers/f2771ce7-9564-4e59-9ab4-cf95ef1a39e1/compliance"\n2017-12-13 09:25:00,511 [INFO] rhsmd:4387:MainThread @cert_sorter.py:204 - Product status: valid_products= partial_products= expired_products= unentitled_producs=69 future_products= valid_until=None\n2017-12-13 09:25:24,552 [INFO] rhsmd:4408:MainThread @rhsmd:270 - rhsmd started\n2017-12-13 09:25:24,640 [INFO] subscription-manager:4402:MainThread @managercli.py:518 - X-Correlation-ID: 18ba69ff7bbb4c9f8a53bdca418a3bcc\n2017-12-13 09:25:24,641 [INFO] subscription-manager:4402:MainThread @managercli.py:407 - Client Versions: {'python-rhsm': '1.19.10-1.el7_4', 'subscription-manager': '1.19.23-1.el7_4'}\n2017-12-13 09:25:24,642 [INFO] subscription-manager:4402:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=identity_cert ca_dir=/etc/rhsm/ca/ insecure=False\n2017-12-13 09:25:24,642 [INFO] subscription-manager:4402:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=none\n2017-12-13 09:25:24,664 [INFO] subscription-manager:4402:MainThread @managercli.py:382 - Consumer Identity name=rh7-virtwho uuid=f2771ce7-9564-4e59-9ab4-cf95ef1a39e1\n2017-12-13 09:25:26,003 [INFO] subscription-manager:4402:MainThread @connection.py:551 - Response: status=200, requestUuid=5e3d92f3-7bd6-4a11-aff5-a37073e1109f, request="GET /subscription/consumers/f2771ce7-9564-4e59-9ab4-cf95ef1a39e1/owner"\n2017-12-13 09:25:27,194 [INFO] subscription-manager:4402:MainThread @connection.py:551 - Response: status=200, request="GET /subscription/"\n2017-12-13 09:25:48,097 [INFO] subscription-manager:4414:MainThread @managercli.py:518 - X-Correlation-ID: 3364d9f7ba0445cda5d923b0ec8a5ab7\n2017-12-13 09:25:48,097 [INFO] subscription-manager:4414:MainThread @managercli.py:407 - Client Versions: {'python-rhsm': '1.19.10-1.el7_4', 'subscription-manager': '1.19.23-1.el7_4'}\n2017-12-13 09:25:48,099 [INFO] subscription-manager:4414:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=identity_cert ca_dir=/etc/rhsm/ca/ insecure=False\n2017-12-13 09:25:48,100 [INFO] subscription-manager:4414:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=none\n2017-12-13 09:25:48,126 [INFO] subscription-manager:4414:MainThread @managercli.py:382 - Consumer Identity name=rh7-virtwho uuid=f2771ce7-9564-4e59-9ab4-cf95ef1a39e1\n2017-12-13 09:25:49,889 [INFO] subscription-manager:4414:MainThread @connection.py:551 - Response: status=200, requestUuid=547a5603-8eea-4e3c-ac53-d2d526656555, request="POST /subscription/consumers/f2771ce7-9564-4e59-9ab4-cf95ef1a39e1"\n2017-12-13 09:25:49,893 [INFO] subscription-manager:4414:MainThread @managerlib.py:74 - Consumer created: {'consumer_name': u'rh7-virtwho', 'uuid': 'f2771ce7-9564-4e59-9ab4-cf95ef1a39e1'}\n2017-12-13 09:25:49,895 [INFO] subscription-manager:4414:MainThread @managercli.py:778 - Successfully generated a new identity from server.\n2017-12-13 09:25:52,987 [INFO] subscription-manager:4419:MainThread @managercli.py:518 - X-Correlation-ID: dcfbedb79f3b436c986e4901979841bf\n2017-12-13 09:25:52,987 [INFO] subscription-manager:4419:MainThread @managercli.py:407 - Client Versions: {'python-rhsm': '1.19.10-1.el7_4', 'subscription-manager': '1.19.23-1.el7_4'}\n2017-12-13 09:25:52,988 [INFO] subscription-manager:4419:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=identity_cert ca_dir=/etc/rhsm/ca/ insecure=False\n2017-12-13 09:25:52,989 [INFO] subscription-manager:4419:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=none\n2017-12-13 09:25:53,016 [INFO] subscription-manager:4419:MainThread @managercli.py:382 - Consumer Identity name=rh7-virtwho uuid=f2771ce7-9564-4e59-9ab4-cf95ef1a39e1\n2017-12-13 09:25:54,421 [INFO] subscription-manager:4419:MainThread @connection.py:551 - Response: status=200, requestUuid=d1baf99a-15b9-48a8-ac7b-984b8d3d544f, request="GET /subscription/consumers/f2771ce7-9564-4e59-9ab4-cf95ef1a39e1/owner"\n2017-12-13 09:25:55,644 [INFO] subscription-manager:4419:MainThread @connection.py:551 - Response: status=200, request="GET /subscription/"\n2017-12-13 09:27:38,490 [INFO] subscription-manager:4450:MainThread @managercli.py:518 - X-Correlation-ID: 21739bf1922b43c89efeb8c1a62d2dad\n2017-12-13 09:27:38,491 [INFO] subscription-manager:4450:MainThread @managercli.py:407 - Client Versions: {'python-rhsm': '1.19.10-1.el7_4', 'subscription-manager': '1.19.23-1.el7_4'}\n2017-12-13 09:27:38,492 [INFO] subscription-manager:4450:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=identity_cert ca_dir=/etc/rhsm/ca/ insecure=False\n2017-12-13 09:27:38,493 [INFO] subscription-manager:4450:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=none\n2017-12-13 09:27:38,494 [INFO] subscription-manager:4450:MainThread @managercli.py:382 - Consumer Identity name=rh7-virtwho uuid=f2771ce7-9564-4e59-9ab4-cf95ef1a39e1\n2017-12-13 09:27:39,721 [INFO] subscription-manager:4450:MainThread @connection.py:551 - Response: status=204, requestUuid=b6a8fad2-e63d-4e70-b2ea-d1c52214e18f, request="PUT /subscription/consumers/f2771ce7-9564-4e59-9ab4-cf95ef1a39e1/certificates?lazy_regen=true"\n2017-12-13 09:27:41,305 [INFO] subscription-manager:4450:MainThread @connection.py:551 - Response: status=200, requestUuid=3c20f059-15f8-48d7-a98f-ae7cee3eeaf4, request="GET /subscription/consumers/f2771ce7-9564-4e59-9ab4-cf95ef1a39e1/certificates/serials"\n2017-12-13 09:27:41,306 [INFO] subscription-manager:4450:MainThread @entcertlib.py:129 - certs updated:\nTotal updates: 0\nFound (local) serial# []\nExpected (UEP) serial# []\nAdded (new)\n  &lt;NONE&gt;\nDeleted (rogue):\n  &lt;NONE&gt;\n2017-12-13 09:27:42,681 [INFO] subscription-manager:4450:MainThread @connection.py:551 - Response: status=200, requestUuid=22793c92-c1d3-4e35-9109-2b8c70463787, request="GET /subscription/status"\n2017-12-13 09:27:42,683 [INFO] subscription-manager:4450:MainThread @managercli.py:699 - Refreshed local data\n2017-12-13 09:27:44,120 [INFO] subscription-manager:4450:MainThread @connection.py:551 - Response: status=200, requestUuid=4cdfa2e3-cba4-43dd-a949-e1e8a4b4e9cb, request="GET /subscription/consumers/f2771ce7-9564-4e59-9ab4-cf95ef1a39e1/compliance"\n2017-12-13 09:27:44,122 [INFO] subscription-manager:4450:MainThread @cert_sorter.py:204 - Product status: valid_products= partial_products= expired_products= unentitled_producs=69 future_products= valid_until=None\n2017-12-13 09:27:44,230 [INFO] rhsmd:4408:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=identity_cert ca_dir=/etc/rhsm/ca/ insecure=False\n2017-12-13 09:27:45,809 [INFO] rhsmd:4408:MainThread @connection.py:551 - Response: status=200, requestUuid=d9bccd0c-22b6-4ce1-9e55-a7e8f95faae6, request="GET /subscription/consumers/f2771ce7-9564-4e59-9ab4-cf95ef1a39e1/compliance"\n2017-12-13 09:27:45,811 [INFO] rhsmd:4408:MainThread @cert_sorter.py:204 - Product status: valid_products= partial_products= expired_products= unentitled_producs=69 future_products= valid_until=None\n2017-12-13 09:28:03,638 [virtwho.init DEBUG] MainProcess(4463):MainThread @executor.py:__init__:52 - Using config named 'vmware'\n2017-12-13 09:28:03,640 [virtwho.init INFO] MainProcess(4463):MainThread @main.py:main:183 - Using configuration "vmware" ("esx" mode)\n2017-12-13 09:28:03,641 [virtwho.init INFO] MainProcess(4463):MainThread @main.py:main:185 - Using reporter_id='rh7-virtwho-cfc1ef5a4dd545a68fe247c23e22ee91'\n2017-12-13 09:28:03,643 [virtwho.main DEBUG] MainProcess(4463):MainThread @executor.py:run:186 - Starting infinite loop with 3600 seconds interval\n2017-12-13 09:28:03,740 [virtwho.vmware DEBUG] MainProcess(4463):Thread-2 @virt.py:run:375 - Thread 'vmware' started\n2017-12-13 09:28:03,744 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(4463):Thread-3 @virt.py:run:375 - Thread 'destination_8734813778660189976' started\n2017-12-13 09:28:03,741 [virtwho.vmware DEBUG] MainProcess(4463):Thread-2 @esx.py:_prepare:132 - Log into ESX\n2017-12-13 09:28:03,984 [virtwho.vmware DEBUG] MainProcess(4463):Thread-2 @esx.py:_prepare:135 - Creating ESX event filter\n2017-12-13 09:28:04,626 [virtwho.vmware INFO] MainProcess(4463):Thread-2 @virt.py:_send_data:885 - Report for config "vmware" gathered, placing in datastore\n2017-12-13 09:28:04,747 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(4463):Thread-3 @subscriptionmanager.py:_connect:131 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-13 09:28:05,995 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(4463):Thread-3 @subscriptionmanager.py:hypervisorCheckIn:179 - Checking if server has capability 'hypervisor_async'\n2017-12-13 09:28:07,567 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(4463):Thread-3 @subscriptionmanager.py:hypervisorCheckIn:186 - Server has capability 'hypervisors_async'\n2017-12-13 09:28:07,569 [virtwho.destination_8734813778660189976 INFO] MainProcess(4463):Thread-3 @subscriptionmanager.py:hypervisorCheckIn:202 - Sending update in hosts-to-guests mapping for config "destination_8734813778660189976": 1 hypervisors and 27 guests found\n2017-12-13 09:28:07,572 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(4463):Thread-3 @subscriptionmanager.py:hypervisorCheckIn:203 - Host-to-guest mapping: {\n    "hypervisors": {\n.....\nlisting of esxi , skip\n........\n}\n2017-12-13 09:30:46,936 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(4463):Thread-3 @subscriptionmanager.py:check_report_state:249 - Job hypervisor_update_879de349-34f5-41c5-837a-a35f76d75dd1 not finished\n2017-12-13 09:30:48,092 [INFO] rhsmcertd-worker:4473:MainThread @rhsmcertd-worker:67 - X-Correlation-ID: ac0ffdb0eee641fa91ef8a138b655810\n2017-12-13 09:30:48,096 [INFO] rhsmcertd-worker:4473:MainThread @connection.py:821 - Connection built: http_proxy=192.168.1.49:3128 host=subscription.rhn.redhat.com port=443 handler=/subscription auth=identity_cert ca_dir=/etc/rhsm/ca/ insecure=False\n2017-12-13 09:30:49,298 [INFO] rhsmcertd-worker:4473:MainThread @connection.py:551 - Response: status=200, request="GET /subscription/"\n2017-12-13 09:30:50,873 [INFO] rhsmcertd-worker:4473:MainThread @connection.py:551 - Response: status=200, requestUuid=5a60fc2c-9a0d-432c-8eb4-150e87fbf94e, request="GET /subscription/consumers/f2771ce7-9564-4e59-9ab4-cf95ef1a39e1/certificates/serials"\n2017-12-13 09:30:50,875 [INFO] rhsmcertd-worker:4473:MainThread @entcertlib.py:129 - certs updated:\nTotal updates: 0\nFound (local) serial# []\nExpected (UEP) serial# []\nAdded (new)\n  &lt;NONE&gt;\nDeleted (rogue):\n  &lt;NONE&gt;</text>, <text>Hello,\nIs that any action can be taken for my account?\nSeem to me that there is update locking in customer portal so virt-who are waiting for update (so call res. Any way to clear, like the resync of last time?</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nPlease allow me sometime while I am checking this internally.\n\nMeanwhile your patience are highly appreciated.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello Mark,\n\nPlease let me know what exactly you see here - https://www.redhat.com/wapps/tnc/termsack?event[]=signIn\nAlso, Let us know if you are asked to accept the terms and conditions. If yes, please accept.\n\nShare the screenshot of above url.\n\nBest Regards, \nAnuja J. \nGlobal Support Services, Red Hat</text>, <text>Hello, \nNot thing show but just direct me to customer portal\n\nBut I got a email from redhat of NDA Acknowledgement Notification</text>, <text>Hello Mark,\n\nOk. Now unregister system :-\n\n# subscription-manager remove --all\n# subscription-manager unregister\n# subscription-manager clean\n\nNow execute tailf /var/log/rhsm/rhsm.log and then register the system (no need to subscribe). Provide us fresh logs for further analysis.\n\n\nBest Regards, \nAnuja J. \nGlobal Support Services, Red Hat</text>, <text>Hello Anuja\nThe time we can register without error, but virt-who still got hypervisor_update not finished.\nDetail please refer to the log\n\nThanks\nMark</text>, <text>Hello Mark,\n\nThe attached file show the logs for 12th Dec. Can you please attach the fresh logs generated during recent system registration? Also, output of date command.\n\nBest Regards, \nAnuja J. \nGlobal Support Services, Red Hat</text>, <text>Just uploaed a wrong one. This is the fresh one for your refenece\nIn fact the log just using putty to log it our. so i got the timestamp at the top of the file</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nPlease allow me sometime while I am checking this with senior engineer.\n\nMeanwhile your patience are highly appreciated.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello Mark,\n\nCould you please run # yum update python-rhsm and then check whether you are facing the same issue again?\n\nAfter updating python-rhsm, restart the virt-who and share the rhsm.log\n\nThanks,\n\nBest Regards,\nSunpreet Bhamrah\nGlobal Support Services\nRed Hat</text>, <text>Hello Support,\nThe python-rhsm is already the latest version.\n\n[root@rh7-virtwho rhsm]# yum update puthon-rhsmython-rhsm\nLoaded plugins: product-id, search-disabled-repos, subscription-manager\nrhel-7-server-rpms                                                                                                                     | 3.5 kB  00:00:00     \nrhel-7-server-rt-beta-rpms                                                                                                             | 4.0 kB  00:00:00     \nrhel-7-server-rt-rpms                                                                                                                  | 4.0 kB  00:00:00     \nNo packages marked for update\n\nPlease check the log for detail.\nThe problem start to affect those new add servers of the VM server. Looking forward to fix it as soon as possible as all ESXi host can't update to portal for thew newcomer</text>, <text>Hello Mark,\n\nIssue is definitely with the user login. Can we again have a remote session at 13:30 (GMT+8) ?\nI will resync the account in the mean time to test the user behaviour.\n\n\nBest Regards, \nAnuja J. \nGlobal Support Services, Red Hat</text>, <text>Hello Anuja\nFine for me. See you at 1330\n\nRegards\nMark</text>, <text>Hello Mark,\n\nSure, I'll send the fresh URL at 13:30\n\nBest Regards, \nAnuja J. \nGlobal Support Services, Red Hat</text>, <text>Remote Support Invitation\n\nClick on the link below and follow the directions.\n\nhttps://remotesupport.redhat.com/?ak=6e35d06daa6c956ce9b037e2f87c0f5a\n\nBomgar(TM) enables a support representative to view your screen in order to assist you.\nSession traffic is fully encrypted to protect your system's data.\nOnce a session has begun, you will be able to end it at any time</text>, <text>Hello,\n\nOn this session :-\n\n1. We again accepted the terms and conditions\n2. I resynced your account for account#836265\n3. We still faced the same issue\n4. We used another system from the same ESX and checked, we faced the same issue\n5. There was no issue with user ocl_dev from account# 5427650\n\nSince we have tried all the possibilities, We will be now escalating this case to the Portal team to verify user account level issue.\n\n\nBest Regards, \nAnuja J. \nGlobal Support Services, Red Hat</text>, <text>// Internal\n\nSanyam, please raise a ticket with the portal team and share the complete information .\nThis needs to be checked at the user account level.</text>, <text>rhsm.log of rh7-virtwho of today</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nWe have escalate this issue with concern team.\n\nI will update the case as I get inputs from the concern team, meanwhile your patience are highly appreciated.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nWe are checking this issue with the concern team.\n\nI will update the case as I get inputs from the concern team, meanwhile your patience are highly appreciated.\n\nPlease feel free to update the case if you have any queries or concerns.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello Support\nAny update for the case? Please advise</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nI have already highlighted this issue to the concern team and the team is working on the issue.\n\nI will update you as I get input from the concern team.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello Support,\nWould you please provide us a target date for the fix. \nWe expecting it can be fix before Christmas.\n\nThanks\nMark Lee</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nWe have already raised this issue to the portal team as this is account related issue.\n\nCurrently we are not able to help you with the exact target date but it will take sometime to resolve this issue.\n\nPlease be assured that we are working on this issue on priority, meanwhile your patience are highly appreciated.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>We have reached out to Purcella Smith ( Sr. Program Manager for Customer Portal ), James Bailey ( Sr. Product Manager for Customer Portal) and also reached out to Shea DeAntonio to get some help on the case apart from the IT ticket that was opened on the case to seek help. As of now most of them are on long vacation and we might have to wait to get some help on the case. \nI will reach out to customer tomorrow and will update them on the delay.</text>, <text>Hello Mark,\n\nGreetings from Red Hat.\n\nWe are still working with the concern team.\n\nMeanwhile your patience are highly appreciated.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nPlease help us with below information.\n\n1. Help us with the hypervisor names which you are not able to see under affected account in customer portal.\n2. Have you checked with any other login id from the affected account ?\n3. Help us with the virtual machine names which are note getting mapped under affected hypervisor ?\n\nAwaiting your response.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello Mark,\n\nGreetings from Red Hat.\n\nI tried to contact you on "+85222662391" but it seems you are unavailable.\n\nPlease update the case once you are available so we can discuss this issue further.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello Support, \nI think phone discussion is not meaningfully for us if you are not come with a solution. \nPlease update the status in here. Thanks</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nPlease help us with below information.\n\n1. Help us with the hypervisor names which you are not able to see under affected account in customer portal.\n2. Have you checked with any other login id from the affected account ?\n3. Help us with the virtual machine names which are note getting mapped under affected hypervisor ?\n\nAwaiting your response.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Hello Support\nAll the information can be find in previous conversion and log. Please refer to\nI would suggest you go through the case before asking the customer the same information as before\n\nJanorkar, Anuja on Dec 15 2017 at 03:19 PM +08:00\nLee, Mark on Dec 14 2017 at 02:00 PM +08:00</text>, <text>Hello,\n\nGreetings from Red Hat.\n\nI have passed all the requested information to the concern team who is working on this issue.\n\nI will update you as I get inputs from them meanwhile your patience are highly appreciated.\n\nThanks &amp; Regards\nSanyam Shah\nCustomer Support Specialist - Technical\nRed Hat</text>, <text>Dear Customer,\n\nThank you for calling Red Hat Technical Support!\n\nAs per conversation, I will take over this case.\n\nPlease allow me some time to review the case and I will revert soon.\n\nThank you!\n\nBest regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>// Internal\n\nI will take over this case as per customer request.\n\nCustomer complained on the phone that this case has been opened for a months and we don't provide any solution.\n\nI am reviewing the case and will further discuss with customer.</text>, <text>Hello Wai\nPlease find the logs and sosreport as tallked.\nI had restart the virt-who on that clients and find it update successfully.\nJust wonder any action taken in portal team?\nAnd shall we register the remain ESXi to Red Hat ?\n\nPlease advise\n\n\n2017-12-27 14:34:23,177 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(20969):Thread-3 @subscriptionmanager.py:_connect:131 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-27 14:34:24,515 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(20969):Thread-3 @subscriptionmanager.py:check_report_state:233 - Checking status of job hypervisor_update_9390d7aa-afc6-4ba5-8f78-5f0cfd76e2b2\n2017-12-27 14:34:25,845 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(20969):Thread-3 @subscriptionmanager.py:check_report_state:258 - Number of mappings unchanged: 0\n2017-12-27 14:34:25,845 [virtwho.destination_8734813778660189976 INFO] MainProcess(20969):Thread-3 @subscriptionmanager.py:check_report_state:259 - Mapping for config "destination_8734813778660189976" updated</text>, <text>Hello Mr.Lee,\n\nThank you for providing the sosreport and rhsm.log\n\nFrom the log, the virt-who is functional, and SOFX-CER-VM-001.octopuscards.local info has been uploaded to Customer Portal.\n~~~\n2017-12-27 14:34:23,177 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(20969):Thread-3 @subscriptionmanager.py:_connect:131 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-27 14:34:24,515 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(20969):Thread-3 @subscriptionmanager.py:check_report_state:233 - Checking status of job hypervisor_update_9390d7aa-afc6-4ba5-8f78-5f0cfd76e2b2\n2017-12-27 14:34:25,845 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(20969):Thread-3 @subscriptionmanager.py:check_report_state:258 - Number of mappings unchanged: 0\n2017-12-27 14:34:25,845 [virtwho.destination_8734813778660189976 INFO] MainProcess(20969):Thread-3 @subscriptionmanager.py:check_report_state:259 - Mapping for config "destination_8734813778660189976" updated\n~~~\n\nChecking the configuration files, I see the "org name" and "hypervisor_id=hostname" is correct, compare to the vmware.conf uploaded in Dec 05. (I see it was corrected in the remote session at Dec 12 2017 02:56 PM)\n\nFrom the internal ticket raised to Customer Portal team, I didn't see changes they made. \n\n\n## Action Plan ##\n\nCustomer:\n\n1. Attach a VDC subscription to SOFX-CER-VM-001.octopuscards.local, and see if guests can register to RHSM.\n\n2. Monitor for a few more days to see if issue recurs.\n\n\nRed Hat:\n\nI will check with Portal team to see if they have done any change. (This might need a few days due to holidays)\n\n\nFeel free to let us know if you need further assistance. Thank you!\n\nBest regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>Hello Wa\nThanks for your prompt reply and effort.\n\nitem 1 had been done. I had turned on virt-who client (rh7-virtwho, rh7-virtwh-101 and CSC-DRC-TC-01) of 3 different VC/ ESXi hostname\nAs they can update to portal at this movment. all ESXi host had been registered to Red Hat\n\nLet keep monitor for a week and see the problem of of rescheduling update come again.\nMeanwhile, we would like to know the cause of the issue.\n\nOne more information may help is all red hat 7 were came with the same VM template. For a new virtual guest, we are simply deploy that VM template and change the hostname, IP setting after clone it out from VMserver. \n\nWill this cause the update conflict?\n\nRegards\nMark</text>, <text>Hello Wa,\nAfter 1 night, the problem of rescheduling come again in one of the virt-who-client (CSC-DRC-TC-01).\nTried restart the virt-who services of all servers, Then all servers prompted Job_hypervisor_update not finished, rescheduling.\n\nThe earliest find the problem is on 04:12 of today of virt-who client CSC-DRC-TC-01, which is used to report our DRC site ESXi server status.\n\n2017-12-28 03:42:49,334 [DEBUG]  @virtwho.py:155 - Running method "checkJobStatus"\n2017-12-28 03:42:49,336 [DEBUG]  @subscriptionmanager.py:112 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-28 03:42:50,490 [DEBUG]  @subscriptionmanager.py:181 - Checking status of job hypervisor_update_e58013bd-18ad-49f5-8e91-3b0f7cc20c8b\n2017-12-28 03:42:51,441 [INFO]  @subscriptionmanager.py:206 - Number of mappings unchanged: 1\n2017-12-28 03:42:51,441 [INFO]  @virtwho.py:211 - virt-who host/guest association update successful\n2017-12-28 03:57:44,715 [DEBUG]  @esx.py:142 - Waiting for ESX changes\n2017-12-28 03:57:44,730 [DEBUG]  @subscriptionmanager.py:112 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-28 03:57:45,997 [DEBUG]  @subscriptionmanager.py:146 - Checking if server has capability 'hypervisor_async'\n2017-12-28 03:57:47,153 [DEBUG]  @subscriptionmanager.py:153 - Server has capability 'hypervisors_async'\n2017-12-28 03:57:47,154 [INFO]  @subscriptionmanager.py:165 - Sending update in hosts-to-guests mapping: {\n2017-12-28 03:57:49,432 [DEBUG]  @virtwho.py:155 - Running method "checkJobStatus"\n2017-12-28 03:57:49,433 [DEBUG]  @subscriptionmanager.py:112 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-28 03:57:50,569 [DEBUG]  @subscriptionmanager.py:181 - Checking status of job hypervisor_update_54f65dc2-bb2e-4e18-8657-b856d0569213\n2017-12-28 03:57:51,556 [INFO]  @subscriptionmanager.py:206 - Number of mappings unchanged: 1\n2017-12-28 03:57:51,556 [INFO]  @virtwho.py:211 - virt-who host/guest association update successful\n2017-12-28 04:12:44,747 [DEBUG]  @esx.py:142 - Waiting for ESX changes\n2017-12-28 04:12:44,760 [DEBUG]  @subscriptionmanager.py:112 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-28 04:12:45,965 [DEBUG]  @subscriptionmanager.py:146 - Checking if server has capability 'hypervisor_async'\n2017-12-28 04:12:47,118 [DEBUG]  @subscriptionmanager.py:153 - Server has capability 'hypervisors_async'\n2017-12-28 04:12:47,119 [INFO]  @subscriptionmanager.py:165 - Sending update in hosts-to-guests mapping: \n2017-12-28 04:12:49,363 [DEBUG]  @virtwho.py:155 - Running method "checkJobStatus"\n2017-12-28 04:12:49,365 [DEBUG]  @subscriptionmanager.py:112 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-28 04:12:50,519 [DEBUG]  @subscriptionmanager.py:181 - Checking status of job hypervisor_update_09aa5bca-3403-464d-982c-887023b01bae\n2017-12-28 04:12:51,696 [DEBUG]  @subscriptionmanager.py:186 - Job hypervisor_update_09aa5bca-3403-464d-982c-887023b01bae not finished, rescheduling</text>, <text>Hello,\nPlease also notified the Job hypervisor update id are different among 3 servers, which they are pointing to different VC/ESXi\n\nFrom rh7-virtwho (using virt-who-0.19-7.el7_4.noarch)\n2017-12-28 09:54:37,631 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(22065):Thread-3 @subscriptionmanager.py:check_report_state:249 - Job hypevisor_update_b984e86a-ccc1-4047-93f1-208913eae0db not finished\n\nfrom rh7-virtwh-101 (using  virt-who-0.19-7.el7_4.noarch)\n2017-12-28 10:04:45,826 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(1031):Thread-3 @subscriptionmanager.py:check_report_state:249 - Job hypervisor_update_c5cf4e08-a4f5-488b-9dde-2e0d8d904e78 not finished\n\nFrom CSC-DRC-TC-01 (using virt-who-0.14-9.el7.noarch)\n2017-12-28 09:24:25,431 [DEBUG]  @subscriptionmanager.py:186 - Job hypervisor_update_7d2f2460-bcbc-45d8-9de0-ea49d63e6f98 not finished, rescheduling</text>, <text>Hello Mark,\n\nSorry to hear that the issue recur and thank you for providing the info.\n\nCould you collect and send me a fresh sosreport from "rh7-virtwho"? I want to check the current status so I can compare it with portal info.\n\nThank you for your effort!\n\nBest regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>Hello Wa\nFiles attached.\nAnd could you please call me, I had some ideas and wanna to discuss with you\n\nThanks\nMark</text>, <text>Hello Mark,\n\nI called you just now but the phone has no answer.\n\nPlease allow me some time to have a check on the log and I will call you later this afternoon to discuss.\n\nFeel free to update the case if you have any finding or idea.\n\nThank you!\n\nBest regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>Hello Mark,\n\nAs per conversation over phone, please leave the current environment unchanged, so our colleagues can compare it with the sosreport.\n\nI am engaging senior engineers to have a deeper look. Please allow us some time and we will revert soon. Thank you!\n\nBest Regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>Hi Mark,\n\nI don't hear feedback from Portal team at the moment. I will update you once I hear any feedback.\n\nAlso, I had a discussion with senior engineers, and providing below info might help boosting the troubleshooting:\n\n1. In rh7-virtwho, stop virt-who service.\n ~~~\n # systemctl stop virt-who\n ~~~\n\n2. Run virt-who in one-shot with debug on. Please provide the output, and the timestamp you run the command.\n ~~~\n # virt-who -o -d\n ~~~\n\n3. Use your another account (5427650) which works fine to have another try.\n ~~~\n # subscription-manager clean\n # subscription-manager register &lt;--- use account 5427650\n # subscription-manager identity &lt;--- get the corresponding org ID.\n ~~~\n Modify the owner in /etc/virt-who.d/vmware.conf to the new org ID.\n\n Try virt-who in one-shot again. Please provide the output and timestamp.\n ~~~\n # virt-who -o -d\n ~~~\n\nThank you for helping with the above operations.\n\nWait for your feedback. Thank you!\n\nBest Regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>Dear Wa\nDetail action had been listed in the log. Please refer to</text>, <text>Hi Mark,\n\nThank you for providing the output! From the output, we can see things clearly:\n\n- Using Account 836265 (org id 5167740), virt-who can get guest-mapping from vmware, but not able to upload it to Customer Portal.\n\n- Using Account 5427650 (org id 7147244), virt-who can get guest-mapping from vmware, and also able to upload it to Customer Portal.\n\nI will forward the log for Customer Portal team to further check. This might take some time and thank you ahead for your patience.\n\n\nIn addition, review the sosreport, I see there's an addition 'o' in the first line, please remove it.\n\n ~/01986014/sosreport-1228$ head -n1 etc/sysconfig/virt-who\n o# Enviromental variables for virt-who service can be specified here.\n\n\nNo further action on your side is needed at the moment. I will keep updating you with the investigation status.\n\nThank you!\n\nBest Regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>Waiting to hear back from  entitlements-list@redhat.com and chainsaw@redhat.com</text>, <text>Hi Mark,\n\nCurrently, we are still investigate the issue internally. Multiple teams are engaged but the issue is not identified so far.\n\nI would like to set the expectation that this might take some days as the issue is rare. I will keep updating you with the latest status.\n\nThank you for your patience!\n\nBest Regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>#### Customer issue\n\nvirt-who not able to upload host-mapping to Customer Portal.\n\nrhsm.log\n----------\n2017-12-28 09:28:52,790 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(22065):Thread-3 @subscriptionmanager.py:_connect:131 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-28 09:28:54,058 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(22065):Thread-3 @subscriptionmanager.py:check_report_state:233 - Checking status of job hypervisor_update_b984e86a-ccc1-4047-93f1-208913eae0db\n2017-12-28 09:28:55,307 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(22065):Thread-3 @subscriptionmanager.py:check_report_state:249 - Job hypervisor_update_b984e86a-ccc1-4047-93f1-208913eae0db not finished\n----------\n\nAll guests could not get updates.\n\n\n#### Already Done\n\n1. Anuja++ and sanshah++ helped customer fixed the configuration issues, problem persist.\n\n   Customer tried use another account 5427650 to register virt-who and set the corresponding org-id in etc/virt-who.d/vmware.conf, everything works fine.\n\n   But use his own account (836265, login: ocl-marklee), issue occurs.\n\n\n2. Raised a ticket to Customer Portal (INC0656134), but they said the permission of customer's account is correct.\n\n3. On Dec 27 I let customer try again, using account 836265, and it works fine. (Dec 27 14:22)\n\n   Then, customer also start the virt-who in his other 2 vmware centers, those virt-who also works fine. (#79)\n\n   Customer can register VM guests and get updates.\n\n4. However, on Dec 28, customer found all 3 virt-who across 3 vmware centers have issue. (#79)\n\n   Customer restarted virt-who at Dec 28 9:28 HKT, and still getting the errors.\n\n5. Let customer run virt-who in one-shot (#87-90), and the result appears to be problems with Customer Portal.\n\n #### Attachments\n\n- Latest sosreport from rh7-virtwho\n  sosreport-rh7-virtwho.1986014-20171227142542.tar.xz (10.0 MB)\n\n- Latest rhsm.log\n  rhsm.log (48.7 kB)\n\n- One-shot virt-who output. Account 5427650 is fine but account 836265 is bad.\n\n#### Questions\n\nPlease help to investigate and fix the issue.\n\nIf you need further information, please set it to Waiting on Owner.\n\nThank you!\n\nBest Regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat\n\n\nCustomer tried again at Dec 28 14:22 HKT(GMT+8), and using his account\n836265 he can register the virt-who VM to rhsm, and virt-who can collect\nand upload host-mapping properly. Then, customer also enable another 2\nvirt-who machine in other 2 vmware sites, and very thing was fine.\n\nAfter 1 night, on Dec 29 morning, virt-who failed to upload the\nhost-mapping to customer portal. From the log, the connection can be\nestablished, but the job never finish.\n\nrhsm.log\n----------\n2017-12-28 09:28:52,790 [virtwho.destination_8734813778660189976 DEBUG]\nMainProcess(22065):Thread-3 @subscriptionmanager.py:_connect:131 -\nAuthenticating with certificate: /etc/pki/consumer/cert.pem\n2017-12-28 09:28:54,058 [virtwho.destination_8734813778660189976 DEBUG]\nMainProcess(22065):Thread-3 @subscriptionmanager.py:check_report_state:233\n- Checking status of job\nhypervisor_update_b984e86a-ccc1-4047-93f1-208913eae0db\n2017-12-28 09:28:55,307 [virtwho.destination_8734813778660189976 DEBUG]\nMainProcess(22065):Thread-3 @subscriptionmanager.py:check_report_state:249\n- Job hypervisor_update_b984e86a-ccc1-4047-93f1-208913eae0db not finished\n----------</text>, <text>Since this bug report was entered in Red Hat Bugzilla, the release flag has been set to ? to ensure that it is properly evaluated for this release.</text>, <text>Created attachment 1377495\nRHEL 7 virt who log</text>, <text>Set this bug private as it involves customer info.\n------\nThis issue has been blocking customer from patching for more than a month, and customer is unhappy with it.</text>, <text>Hi Mark,\n\nWe don't have further news from Engineering team at the moment.\n\nCurrently, internal bug 1530920 and 1531548 are opened to track this issue, and I've already informed my manager Winston about this case.\n\nI will keep updating you the latest status about this issue.\n\nThank you!\n\nBest Regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>I'm currently out of office and will back on 10-Jan-2018\nThanks you.\n\n________________________________\n\n---------------------------------------------------------------------------------------------------------------\nThe information contained in this e-mail and any attachments is confidential and is intended for the addressee only. If you have received this transmission in error, please notify the sender immediately and delete it from your system. Any unauthorised use, copying, printing or dissemination of any part of this e-mail and/or its attachments is prohibited. Octopus Cards Limited does not accept responsibility for the content of any e-mail transmitted by its staff for any reason other than bona fide business purposes.\n\nInternet communications cannot be guaranteed to be secure or error-free. Octopus Cards Limited does not accept liability for any errors or omissions in the context of this message which arise as a result of transmission via the Internet.\n-----------------------------------------------------------------------------------------------</text>, <text>Looking at the Case there are errors that the job hypervisor_update_b984e86a-ccc1-4047-93f1-208913eae0db is still in progress. Looking at this particular customer, what jobs do they have currently in progress? My first recommendation would be to cancel any existing hypervisor update jobs so that they can cleanly schedule new ones. Rich, if the existing jobs are canceled are they able to proceed? \n\nThis is not a permanent fix but may work well enough as a workaround until we can work through the reasons the hypervisor update jobs are piling up.</text>, <text>Hi Barnaby,\n\n"cancel any existing hypervisor update jobs" &lt;- Can this be done on client side by any command?  Tried restarting virt-who, didn't help.  Or should it be done on Customer Portal side?\n\nPlease shed some lights.  Thank you!</text>, <text>Hi Mark,\n\nI don't have further update at the moment.\n\nAlready engaged the concern team but still need some time to wait for their reply.\n\nI will keep updating you the latest status about this issue. Thank you!\n\nBest Regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>According to Rich Bernleithner this is waiting on response from the candlepin team. From INC0656134\n--------------------------------------------------------------------------------\n2018-01-11 18:30:27  Richard Bernleithner  Changed:  Work notes\nwaiting for candlepin team to respond in https://bugzilla.redhat.com/show_bug.cgi?id=1531548\n2018-01-04 20:59:31  INC0656134 - Customer is not able to map the ESXi (hypervisor) to the customer portal using the virtual datacenter. subscription. Email sent\nSent: chainsaw-issues@redhat.com\n2018-01-04 18:55:44  SN Incident INC0656134 Work Notes Update Notification : Customer is not able to map the ESXi (hypervisor) to the customer portal using the virtual datacenter. s Email sent\n[Email]\n\n2018-01-04 18:17:21  Richard Bernleithner  Changed:  Work notes\nWaiting to hear back from entitlement and chainsaw teams.\n2018-01-04 18:16:16  INC0656134 - Customer is not able to map the ESXi (hypervisor) to the customer portal using the virtual datacenter. subscription. Email sent\n[Email]\nSent: chainsaw@redhat.com\n--------------------------------------------------------------------------------\n\nBarnaby is this something your team is aware of an can help with. Thanks in advance.</text>, <text>(In reply to Thomas "Shea" DeAntonio from comment #1)\n\n&gt; Barnaby is this something your team is aware of an can help with. Thanks in\n&gt; advance.\n\nFrom last notes in  1531548, it looks like support is needing instructions on how to cancel job.</text>, <text>Hello,  \t\n\nWe are still waiting for feedback from candlepin team.\n\nI will update the latest status once there's any news. Or at least I will update this ticket every 3 days.\n\nThank you!\n\nBest regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>Hi,\n\nI don't have update at the moment.\n\nI understand this issue has been opened for a long time. I will keep connecting with engineering team and let you know once we have any further findings.\n\nThank you!\n\nBest regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>Dear Support,\nWe tried to use 1 virt-who client only (another 1 Red hat 7 server) to report all ESXi servers on today.\nHowever, the result still soing Job_hypervisor_update not finished in the log.\n\nFrom the log, we can see all esxis info had been got and in JSON format.\n\nAs before, the log shows\n\n2018-01-24 12:58:39,915 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(8463):Thread-7 @subscriptionmanager.py:_connect:131 - Authenticating with certificate: /etc/pki/consumer/cert.pem\n2018-01-24 12:58:41,447 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(8463):Thread-7 @subscriptionmanager.py:check_report_state:233 - Checking status of job hypervisor_update_9fe8f149-24c3-4446-a26f-21edcee5b5e9\n2018-01-24 12:58:43,151 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(8463):Thread-7 @subscriptionmanager.py:check_report_state:249 - Job hypervisor_update_9fe8f149-24c3-4446-a26f-21edcee5b5e9 not finished\n\nWe will keep using that host for report all ESXi record and will keep it virt-who on\na new sosreport and rhsm.log uploaded for your reference</text>, <text>Dear Support\nPlease also help to chase the case of update from your backend team.\nWe had wait for over a month and still didn't get any clues for this issue\n\nRegards\nMark</text>, <text>Hi Mark,\n\nThank you for providing the information.\n\nWe can also see the "job not finished" message this time.\n~~~\n2018-01-24 13:19:14,957 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(8659):Thread-7 @subscriptionmanager.py:check_report_state:249 - Job hypervisor_update_7ef89494-936b-4898-b4c4-5ec9710ec4d5 not finished                                           \n~~~\n\nAnd this is using a different server, so I suppose it is not related to system itself, but related to customer portal.\n\nI provide these information for engineering team and chase for an update.\n\nThank you!\n\nBest regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>Hi,\n\nCustomer tested again using anther virt-who VM, and get the same issue.\n~~~\n2018-01-24 13:19:14,957 [virtwho.destination_8734813778660189976 DEBUG] MainProcess(8659):Thread-7 @subscriptionmanager.py:check_report_state:249 - Job hypervisor_update_7ef89494-936b-4898-b4c4-5ec9710ec4d5 not finished                                           \n~~~\n\nAttach rhsm.log-Jan24 and sosreport for your reference.</text>, <text>Created attachment 1385351\nrhsm.log-Jan24</text>, <text>Created attachment 1385355\nsosreport from another server - part1</text>, <text>Created attachment 1385358\nsosreport from another server - part2</text>, <text>There are no virt-who client side commands to cancel all existing hypervisor update jobs. That would be an interesting RFE.</text>, <text>So it is possible for you to clean the "never done" job in server side? So I can let customer to try again and resume production.\n\nIf not, is there any workaround? For example, creating a new account for customer and transfer all subscriptions to that account? Please advise.\n\nThank you!</text>, <text>Hi Mark,\n\nCurrently I don't hear further feedback from engineering team. I will chase for updates and see if there's any alternative workaround.\n\nThank you!\n\nBest Regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>Hi Mark,\n\nI don't have further update at the moment.\n\nI will keep updating you with the latest status.\n\nThank you!\n\nBest regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>I'm currently out of office and will back on 06-Feb-2018\nThanks you.\n\n________________________________\n\n---------------------------------------------------------------------------------------------------------------\nThe information contained in this e-mail and any attachments is confidential and is intended for the addressee only. If you have received this transmission in error, please notify the sender immediately and delete it from your system. Any unauthorised use, copying, printing or dissemination of any part of this e-mail and/or its attachments is prohibited. Octopus Cards Limited does not accept responsibility for the content of any e-mail transmitted by its staff for any reason other than bona fide business purposes.\n\nInternet communications cannot be guaranteed to be secure or error-free. Octopus Cards Limited does not accept liability for any errors or omissions in the context of this message which arise as a result of transmission via the Internet.\n-----------------------------------------------------------------------------------------------</text>, <text>Hi Mark,\n\nI don't have further update at the moment.\n\nI will keep updating you with the latest status.\n\nThank you!\n\nBest regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>This support case is still open with the customer and last update to the customer is they are waiting on engineering. It is not clear to me what needs to be done and where. \n\nRich is this a issue that Customer Portal Subscription Management (IT-PnT) can assist with? The service now ticket states it is waiting on Candlepin team but this bugzilla references that information may all ready be available.\n\nIf this is a defect in the Customer Portal lets get it identified and in the backlog. If its not a portal issue I would like to close out this bugzilla in order to clear up path forward.</text>, <text>*** Bug 1531548 has been marked as a duplicate of this bug. ***</text>, <text>*** This bug has been marked as a duplicate of bug 1530920 ***</text>, <text>We Finally fixed by using 1 virt-who client only for all ESXi hosts across the site</text>, <text>Hello Mark,\n\nThank you for your feedback!\n\nI apologize the inconvenience it brought you.\n\nI am closing this case. Thank you!\n\nBest regards,\nSiu Wa, Wu\nCustomer Engagement and Experience, Red Hat</text>, <text>The reporter closed the associated case 01986014 on 3/7/18 based on the customer's feedback[1]. However, the reporter did not update or close Bugzilla.\n\n[1]\n"We Finally fixed by using 1 virt-who client only for all ESXi hosts across the site"</text>]
spam	[[<description>What problem/issue/behavior are you having trouble with?  What do you expect to see?\n\nI am trying to setup a RHVM system.   We purchased an HPE DL360 G10 with 256gb RAM as our entry into trying out this product.   With the Server we were sold a RHEL for Virtual Datacentres, Premium Subscription that promised unlimited VMs.\n\n\nI have installed a fresh RHEL 7.4 and registered the system using subscription manager.  The problem is that when *just* the RHEL for Virt DC Sub is selected I have no access to any repositories, I had to auto-attach and I found myself with a 60-day trial.  The subscription manager shows no Products listed under the 'Provides' section.\n\n\nSubscription Name:   Red Hat Enterprise Linux for Virtual Datacenters, Premium (L3 Only)\nProvides:\nSKU:                 RH00077F3\nContract:            11512782\nPool ID:             8a85f98c5f53926e015f53c7666e0967\nProvides Management: No\nAvailable:           1\nSuggested:           1\nService Level:       Premium\nService Type:        L3\nSubscription Type:   Stackable\nEnds:                10/25/2020\nSystem Type:         Physical\n\nWhere are you experiencing the behavior?  What environment?\n\nOn the host th1lvh01\n\nThe guide for install suggests we need \n\nRed Hat Enterprise Linux Server and Red Hat Virtualization.\n\nWhat information can you provide around timeframes and the business impact?\n\nWe are running behind schedule on a business project and I hope to introduce RH Virtualization into our organization.  We currently run VMWare and Oracle OVM.</description>], <text>This comment has been generated via an automated case assistant.\n\n \n\nThe following resources may be helpful for working with virt-who and virtual Subscriptions (including Virtual Data Center):\n\n \n\n    https://access.redhat.com/labs/virtwhoconfig/ Red Hat Virtualization Agent (virt-who) Configuration Helper - virt-who is required when working with any hypervisor if your subscription name includes the words 'Virtual Datacenters', '4 Guests', or 'Unlimited Guests'. This app helps users configure virt-who. When started, the app requests information about your virt-who usage and generates a configuration file based on he answers. It also provides instructions for updating your virt-who configuration.\n    https://access.redhat.com/blogs/1169563/posts/2190731 Subscription-manager for the former Red Hat Network User: Part 3 - Understanding virt-who \n    https://access.redhat.com/blogs/1169563/posts/2630111 Subscription-manager for the former Red Hat Network user - part 5 - Working with subscriptions that require virt-who\n    https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/virtual_instances_guide/ - Virtual Instances Guide for Satellite 6\n    https://access.redhat.com/documentation/en-us/red_hat_subscription_management/1/html/virtual_instances_guide/ - Virtual Instances Guide for use with Red Hat Subscription Management (RHSM) but no Satellite</text>, <text>*PRIVATE UPDATE, DO NOT MAKE PUBLIC*\n\n    The previous public update is part of a pilot to improve auto-solve and self-solve, by commenting on cases that match certain rules upon creation, to provide answers to common questions/issues.  You can find more information about this project[1], the rule match for this case[2], and how to provide feedback (whether the rule is helpful, suggestions for improvement) here:\n\n    [1] https://mojo.redhat.com/groups/support-delivery/projects/diagnostics-20\n    [2] https://projects.engineering.redhat.com/browse/CPSEARCH-1503\n    [3] https://mojo.redhat.com/thread/942968</text>, <text>Thanks for the pointers.  We may need some assistance with this.\n\nAt this stage, we have a vanilla RHEL 7.4 install on this HPE server that we intend will be the Hypervisor.  Do the steps to configure this start with installing and configuring virt-who on this system?</text>, <text>Hello,\n\nThanks for your update on this case. I would request you to please check the below link to know how you can configure KVM for VDC subscriptions.\n\na) https://access.redhat.com/solutions/1515983\n   How to configure virt-who for a KVM host ? \n\nPlease let me know if it helped you or you need further assistance on this. I will be glad to assist you with this.\n\nThanks &amp; Regards,\nShah Imran.\nGss, Red Hat Inc.</text>, <text>Thanks for your patience with this.  We are now at a stage where we have a working VDSM host and an RHEVM node to manage it.  We are able to create VMs, etc.  \n\nI will start looking at virt-who now, I need to get this all register with our datacentre subscription.  I would like to leave this case open a little longer, in case I need support.</text>, <text>OK, I do need some guidance please.\n\nI have 2 hosts.\n\n1.  TH2LVH02. \n    -  This host runs my Red Hat Virtualization Manager software.  It is a single physical host, HP DL380, that runs only RHEL7.4 and the Manager GUI.\n\n2.  TH1LVH01.\n    - This host runs as my RHEV host that is managed by the server in (1).  It runs LibVirt, VDSM, and is where I am creating my guest VM.  It is, in short, the Hypervisor host.\n\nSo, I am reading:  https://access.redhat.com/documentation/en-us/red_hat_subscription_management/1/html/virtual_instances_guide/configuration_and_services\n\nI don't know which of the examples - 5.3 and 5.4 apply to which of my hosts?\n\nDo I perform the steps in 5.3 on TH2LVH02 and the steps in 5.4 on TH1LVH01 ?\n\nThanks\n\n\nPaul.</text>, <text>I've got a little further.\n\nOn the TH1LVH01 server, I decided to give it a go as the virt-who host.  I setup two files and tried a one-shot registration:\n\n[root@th1lvh01 virt-who.d]# cat th1lvh01.conf\n\n[th1lvh01]\ntype=vdsm\nhypervisor_id=hostname\n[root@th1lvh01 virt-who.d]# cat th2lvh02.conf\n[th2lvh02]\ntype=rhevm\nhypervisor_id=hostname\nowner=SpireHC\nenv=Library\nserver=https://th2lvh02.gb.spire.ads:443/ovirt-engine\nusername=admin@internal\nencrypted_password=555d426517e1d82ffa21bfba8a97dbe7\n\n[root@th1lvh01 virt-who.d]# virt-who --one-shot\n2017-11-29 21:47:03,143 INFO: Using configuration "th1lvh01" ("vdsm" mode)\n2017-11-29 21:47:03,143 INFO: Using configuration "th2lvh02" ("rhevm" mode)\n2017-11-29 21:47:03,143 INFO: Using reporter_id='th1lvh01-57b5c478273040368ee35102f48281a1'\n2017-11-29 21:47:03,271 INFO: Report for config "th1lvh01" gathered, placing in datastore\n2017-11-29 21:47:03,929 INFO: Report for config "th2lvh02" gathered, placing in datastore\n2017-11-29 21:47:06,137 INFO: Sending update in guests lists for config "th1lvh01": 7 guests found\n2017-11-29 21:47:07,108 INFO: Sending update in hosts-to-guests mapping for config "destination_3177978192667182968": 1 hypervisors and 7 guests found\n2017-11-29 21:47:08,091 ERROR: Error during hypervisor checkin:\nManagerError: Communication with subscription manager failed with code 404: Organization with id SpireHC could not be found.\n[root@th1lvh01 virt-who.d]#\n\nAdvice please?</text>, <text>More progress.\nI noticed that the subscription manager identity was :\n\n[root@th1lvh01 virt-who.d]# subscription-manager identity\nsystem identity: 52f9d00b-e880-4ce2-ab89-6225660343b4\nname: th1lvh01\norg name: 5927122\norg ID: 5927122\n[root@th1lvh01 virt-who.d]#\n\n\n[root@th1lvh01 virt-who.d]# grep owner *\nth2lvh02.conf:owner=5927122\n\nSo, I have changed this in the virt-who configuration file and it seems to have run through OK.\n\nI could do with a telephone conversation tomorrow to discuss what I should expect to see and just to get confirmation this is all correct.</text>, <text>Hello,\n\nThank you for your update on this case. Could you please verify by checking status of virt-who service.\n\n#service virt-who status\n\nIf it shows running and the hypervisor on the customer portal are reflecting properly assigned it means everything is working fine. Also you can try attaching a virtual machine VDC subscription and see if it shows subscribed status or invalid.\n\nPlease feel free to udpate the case with your queries and concerns. I will be glad to assist you with this.\n\nThanks &amp; Regards,\nShah Imran\nRed Hat India.</text>, <text>Hi Shah,\n\nSo,  it is checking in.  I have the right Org ID, as you see.  But my host appears twice in the portal...\n\nth1lvh01  --  Once as a "server" -- 3 subscriptions.\n\nth1lvh01 -- Once as a "hypervisor" -- 0 subscriptions.\n\nCan you please take a look and tell me if I need to move the VDC subscription from the "server" entry to the "hypervisor" entry.\n\nThanks\n\n\nPaul</text>, <text>Hello,\n\nThanks for your update on this case. Please accept my apology for the delay.\n\nIn this case to verify the right host/hypervisor kindly take any of the Vm which is being hosted on that hypervisor. Go to Vm's profile and check the host link.\n\nIt will bring you to the host. Then delete the duplicate host from the portal and see if the machine is properly subscribed.\n\nPlease feel free to update the case with your queries and concerns. I will be glad to assist you with this.\n\nThanks &amp; Regards,\nShah Imran.\nGss, Red Hat Inc.</text>]
spam	[[<description>What problem/issue/behavior are you having trouble with?  What do you expect to see?\n\nafter registered capsule is everything is good \nafter some time capsule covert to subscription unentitled  \nand red icon appeared ?</description>], <text>it's show that 16 virtual \nit mean we can register 16 vm ? are you right ?\nso i can't register with cloud suite all just only 2 vm ? why</text>, <text>sorry for conflict \nthe capsules repos in cloud suite \nand i wanna i nstall 20 capsules \nwhat is solution ?</text>, <text>Hello,\n\nThanks for your update.\n\nRequest you to allow me some time to look into the case and update the case with my findings.\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>in attached file show that \navailable : unlimited \ni understood from this we can register more than two capsules \nso why i can't do this ?</text>, <text>Hello,\n\nThanks for your update.\n\nRequest you to let me know which account you are using for this subscription.\n\nAs checking the Account details from which the case is logged 633200 dont have Red Hat cloud suite subscription.\n\nSo request you to update the case with the Account Number used to get this subscription.\n\nPlease let me know for any further query/concern regarding this case.\n\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>1376693 this is Account Number</text>, <text>any update for issue</text>, <text>Hello\n\nMy name is Paul and I will be assisting you with this case.\n\nIn the SUB.jpg screenshot you uploaded, you have the Red Hat Cloud Suite subscription selected (and it's associated content hosts) but underlined a different subscription, Red Hat CloudForms. These subscriptions are different and provide different products.\n\nThe Red Hat Cloud Suite provides the Capsule product, as you can see in sub1.jpg, but these subscriptions function differently that the CloudForms subscriptions you highlighted. The Cloud Suite subscriptions will provide an unlimited subscription when the base subscription (one of the 2 subscriptions you have) to a hypervisor that is being reported to the Satellite by virt-who - but this subscription can also cover a normal physical or VM host.\n\nThe best way to optimize the use of the Cloud Suite subscription would be to place it on a hypervisor and use the unlimited subscription provided after that to subscribe the systems hosted on that hypervisor.\n\nPlease let me know if you have any questions about this.\n\nThanks so much\nPaul</text>, <text>actually i did this  i subscription with hypervisor</text>, <text>Hello\n\nAlright, thanks for letting me know.\n\nSo going by the screenshot, then, dchqcpsl05 and dchqcps101 are both hypervisors? Are they rhel-based hypservisors? Have you installed the virt-who service on these systems to have them report their host-to-guest mapping to the Satellite?\n\nAlso, please let me know if we may reduce the severity of this case to, at least, a 2. Severity 1 would imply that production is down in this situation, which is not the case, and does not let us prioritize problems as we should be. This is especially the case since the problem can currently be worked around by running subscription-manager attach --auto on the systems and they will regain their temporary unlimited subscriptions, and access to the Capsule repositories as well.\n\nThanks\nPaul</text>, <text>what do you mean exactly "hypervisors" here do you mean host on hypervisors \nin my environment vmware \nso we need Specific steps to solve this issue\nalso i understood from your reply this subscription only two hypervisors that register with it \nare you right ?</text>, <text>Hello,\n\nWhat system is hosting your VMWare instance? Is that system a RHEL system?\n\nCandace Sheremeta\nTechnical Support Engineer\nSupport Delivery - Platform Technologies\nCustomer Experience and Engagement\nRed Hat, Inc.\n\n1.888.GO.REDHAT\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>Hello\n\nA hypervisor in this context would be your VMware hosts.\n\nYou will need to set up the virt-who service on your Satellite to report the host-to-guest mapping in order to take full advantage of this subscription. You can reference this article to set this up: https://access.redhat.com/solutions/2109221\n\nYou can refer to this page to help you with the configuration set up: https://access.redhat.com/labsinfo/virtwhoconfig - scroll down and click Go To Application to go through the lab.\n\nIf you have any questions to set this up, please let me know.\n\nThanks\nPaul</text>, <text>just setup only on satellite we don't need on hypervisor ?</text>, <text>also these steps required to unregister two servers that already using cloud suite (dchqcpsl05 and dchqcps101)?</text>, <text>Hello\n\nCorrect. This only needs to be done on the Satellite. \n\nYou won't need to unregister the two systems, only unsubscribe them (remove their subscriptions in the webui).\n\nWhen virt-who runs successfully you will see your hypervisors show up in the Content Hosts page of the Satellite webui as virt-who-&lt;hypervisor hostname&gt;. Once these are here you should be able to place the two Cloud Suite subscriptions on the two hypervisors you like, and then the guests of those two hypervisors will have access to the Cloud Suite subscription.\n\nPlease let me know if you have any more questions.\n\nThanks\nPaul</text>, <text>Hello\n\nAs discussed earlier, this case does not meet the standards for a severity 1 case. We will continue to work on this case during normal business hours. If you have any questions for us please don't hesitate to contact us: https://access.redhat.com/support/contact/technicalSupport\n\nThanks\nPaul</text>, <text>hello \ni would like to clarify some points \nin our environment \n- satellite in zone 1\n- we have 8 vcenters \n- every vcenter in different zone also we have firewall between zones \n\nso according to all points we can  already register the Satellite</text>, <text>any update ?</text>, <text>Hello,\n\nThanks for your patience with us.\n\nRequest you to let me know your contact details to check your query.\n\nPlease let me know for other further query/concern.\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>Hello,\n\nThanks for your patience.\n\nBelow is the document which refer to the port required for the satellite to the client system.\n\nhttps://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/single/installation_guide/index#ports_prerequisites.\n\nAs for the client to subscribe and register we do required 443 and 80 port from client to satellite.\n\nRequest you to share the contact details to further discuss your query/.\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>it's already done \ni asked about different vcenter in different zones so if i need anything to do steps</text>, <text>Hello,\n\nThanks for your update.\n\nyou dont have to do anything on the vcenter, but once you configure virt-who.\n\non the configuration file, you need to specify the username and password for the vcenter who can reach and query the guest cretaed on the hypervisor.\n\nAlso, I would request you to provide your contact details to check your query clearly.\n\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>in part of configuration file \nurl or ip of vcenter\nwhich is vcenter ?</text>, <text>we have more one vcenter</text>, <text>Hello,\n\nThanks for your update.\n\nBelow is the configuration file for your reference.\n[vcenter1]\ntype=esx\nserver=&lt;hostname_of_your_vcenter1&gt;\nusername=&lt;username&gt;\npassword=&lt;password&gt;\nowner=&lt;owner1&gt;\nenv=Library\nhypervisor_id=hostname  &lt;----- to list hostname , see virt-who config manpage\n\n[vcenter2]\ntype=esx\nserver=&lt;hostname_of_your_vcenter2&gt;\nusername=&lt;username&gt;\npassword=&lt;password&gt;\nowner=&lt;owner1&gt;\nenv=Library\nhypervisor_id=hostname\n\n\nbelow is the document to configure multiple VMware vCenter's by running virt-who in single RHEL instance?\n\nhttps://access.redhat.com/solutions/1412123\n\n\n\nPlease let me know for any further query/concern regarding this case.\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>so we need open ports ( 443 and 80 ) between satellite and all vcenter\nif it is right ? \nfrom vcenter to satellite or from satellite to vcenter</text>, <text>Hello,\n\nThanks for your update.\n\nyou require opening port from both ends(satellite as well vcenter)\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>so only 40 &amp; 443 ?\nwhat about capsules in all zone do you need any configuration or open ports for it ?</text>, <text>Hello,\n\nI would request you to reduce the severity as it seems the query does not meet for severity 1 standard.\n\nBelow is the document for your reference.\n\nhttps://access.redhat.com/support/policy/severity\n\nAlso, Below is the port requirement in satellite and capsule.\n\nhttps://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/single/installation_guide/index#ports_prerequisites\n\nAs for virt-who configuration, you require port 80 and 443 from satellite and capsule.\n\nPlease let me know for any further query/concern.\n\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>so Let's be more organized\nto configure it we need open theses ports on firewall \nsource \t      destination\t      ports\t\nsatellite \tcapsule\t             80,443\t\ncapsule \tsatellite\t             80,443\t\nsatellite\t        vcenter\t             80,443\t\nvcenter\t        satellite        \t     80,443\t\ncapsule         vcenter\t            80,443\t\nvcenter\t        capsule\t            80,443\t\n \nIs it possible to make sure to me that I have understood correctly</text>, <text>Hello,\n\nThanks for your update.\n\nyou are on right track regarding virt-who configuration.\n\nPlease let me know for any further query/concern.\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>regarding to these configuration \ni can register any host to capsule with cloud suite</text>, <text>Hello,\n\nyes, you can register with cloud suite as mentioned with Paul Dudley.\n\nThe best way to optimize the use of the Cloud Suite subscription would be to place it on a hypervisor and use the unlimited subscription provided after that to subscribe the systems hosted on that hypervisor.\n\nAlso, As it does not seems to be any production impact.I am reducing this severity to 2 and removing the 24x7 flag.We will continue to work on this case during normal business hours. If you have any questions for us please don't hesitate to contact us: https://access.redhat.com/support/contact/technicalSupport\n\nYou can increase the severity from your end, at any point.\n\nBelow is the SLA of RedHat.\n\nhttps://access.redhat.com/support/policy/severity\n\nAlso, I requested you to provide the contact details to answer your query.\n\nPlease let me know for any further query/concern.\n\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>as per you speech "The best way to optimize the use of the Cloud Suite subscription would be to place it on a hypervisor"\ni have 8 vcenter and 2 cloud suite \nwhat is the best practice ?</text>, <text>any update ?</text>, <text>Hello,\n\nThanks for your update.\n\nRequest you to provide your contact details to answer your query.\n\nYes The best way to optimize the use of the Cloud Suite subscription would be to place it on a hypervisor"\n\nIf you have two subscription request you to attach to two hypervisors and the client created on that hypervisor will get the subscription which require virt-who.\n\n\n\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>but i need register all 8 vcenters \nhow to do this ?</text>, <text>Hello,\n\nRequest you to provide me with your contact details to answer your query.\n\nAs It does not the seems to be product down impact.\n\n\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>i decreased it yeastday \nand i asked about something at  07:59 PM and no reply till change again to 1 (urgent)</text>, <text>so i need specific solution urgent\ncan i register 2 cloud suit with all vcenters in my environment  ?\nand if we have this solution please send me specific configuration to \n\n- satellite in zone a\n- we have 13 capsules in different zones  \n- we have 8 vcenters (all hosts Spread over all vcenters )\n- every vcenter in different zone also we have firewall between zones \n- we have 2 cloud suite\n\nso we need when create any host i can register it using cloud suite</text>, <text>Hello,\n\nThanks for your update.\n\nAs is not the production down the issue I am reducing the severity to 2, but I will keep case to be in 24X7 as it seems you need 24 X 7 support.\n\nso I need specific solution urgent\ncan I register 2 cloud suit with all vcenters in my environment?\n\nFor details below is the document, which specifies why and when do you require virt-who.\nhttps://access.redhat.com/articles/1300283\n\nNo, you can only register 2 hypervisors with 2 cloud suite subscription.\nAnd the client created on that hypervisor would get the cloud suite subscription.\n\nand if we have this solution please send me specific configuration to \n\n- satellite in zone a   -- satellite would be having the satellite subscriptions\n- we have 13 capsules in different zones  ---&gt;&gt; capsule server should only have capsule subscriptions\n- we have 8 vcenters (all hosts Spread over all vcenters ) as cloud suite is subscription which is used to attached to a hypervisor not to the physical or virtual system.\n\n- every vcenter in the different zone also we have a firewall between zones --&gt;&gt; there are already a port requirement document shared with you.\n\nFor reference once again sharing the same link.As port mentioned in this document is only required nothing else.\nhttps://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/single/installation_guide/index#ports_prerequisites\n\n- we have 2 cloud suite\nso we need when creating any host I can register it using cloud suite\n\nAs you requirement, if any host is created the host should get cloud suite subscription.\nThen you require to configure virt-who, and cloud-suite subscription is attached to the hypervisor.\nBelow is the document for installing and configuring the virt-who.\n\n\nAlso please let me know for any further query/concern.\n\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>i already registered capsules using cloud suite, include repo relatedt capsule in cloud suite \n \ni according your speech "No, you can only register 2 hypervisors with 2 cloud suite subscription.\nAnd the client created on that hypervisor would get the cloud suite subscription."\n\nbut if you review the case form   start\nsomeone in redhat tell me can do this through satellite \nso i have conflict \n\nin official doc nothing referred to firewall ports with vcenter</text>, <text>Hello,\n\nThanks for your update.\n\nRequest you to have your contact details and let me know your availability for the remote session.\nBelow is the link for  remote session.\n\nSession Key: 2131347 \n\nURL: https://remotesupport.redhat.com/?ak=58574275a9e212e5947f411edcd77e8b\n\nCan we have a call and remote session to check your issue .\n\nPlease let me know for any further query/concern.\n\n\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>OK I will ready now</text>, <text>Hello,\n\n\nBelow is the link to the remote session.\n\nSession Key: 2131347 \n\nURL: https://remotesupport.redhat.com/?ak=58574275a9e212e5947f411edcd77e8b\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>Hello,\n\nThanks for your time on remote session.\n\nAs on the remote we have not performed any steps.\n\nyou need time to  configure virt-who and remove subscription from the host using cloud-suite subscription.\nBelow is the doc for virt-who configuration\n\nhttps://access.redhat.com/solutions/2109221\n\nBelow is the document to create config File for virt-who.\n\nhttps://access.redhat.com/labs/virtwhoconfig/\n\nPlease let me know for any further query/concern.\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>as per our conversation i did all configuration but didn't work \nso i am waiting for remote session as per agreed to check</text>, <text>Hello,\n\nRequest you to join the same session.\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>, <text>i guess you have issue</text>, <text>Hello,\n\nThanks for your time in the remote session.\n\nAs we have checked the port for reporting do require port 443 and 80.\n\nRequest you to update the case after port opening\n\nI am reducing the severity of the case.\n\n\n\nBest Regards,\nRam Singh\nGlobal Support Services\nRed Hat Technical Support.</text>]
spam	[[<description>What problem/issue/behavior are you having trouble with?  What do you expect to see?\n\nTrying to register ESX hypervisors in Customer Portal using virt-who. ESX hosts are not appearing.\nEntries in the log indicate that virt-who is trying to send information:\nrhsm.log-20171203:2017-12-02 10:44:53,890 [INFO]  @subscriptionmanager.py:185 - Sending update in hosts-to-guests mapping for config "hrvcswa6": 5 hypervisors and 41 guests found\n\nWhere are you experiencing the behavior?  What environment?\n\nProduction environment.\n\nWhat information can you provide around timeframes and the business impact?\n\nBusiness impact is that I am unable to register and subscribe systems and therefore cannot begin patching our systems</description>], <text>This comment has been generated via an automated case assistant.\n\n \n\nThe following resources may be helpful for working with virt-who and virtual Subscriptions (including Virtual Data Center):\n\n \n\n    https://access.redhat.com/labs/virtwhoconfig/ Red Hat Virtualization Agent (virt-who) Configuration Helper - virt-who is required when working with any hypervisor if your subscription name includes the words 'Virtual Datacenters', '4 Guests', or 'Unlimited Guests'. This app helps users configure virt-who. When started, the app requests information about your virt-who usage and generates a configuration file based on he answers. It also provides instructions for updating your virt-who configuration.\n    https://access.redhat.com/blogs/1169563/posts/2190731 Subscription-manager for the former Red Hat Network User: Part 3 - Understanding virt-who \n    https://access.redhat.com/blogs/1169563/posts/2630111 Subscription-manager for the former Red Hat Network user - part 5 - Working with subscriptions that require virt-who\n    https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/virtual_instances_guide/ - Virtual Instances Guide for Satellite 6\n    https://access.redhat.com/documentation/en-us/red_hat_subscription_management/1/html/virtual_instances_guide/ - Virtual Instances Guide for use with Red Hat Subscription Management (RHSM) but no Satellite</text>, <text>*PRIVATE UPDATE, DO NOT MAKE PUBLIC*\n\n    The previous public update is part of a pilot to improve auto-solve and self-solve, by commenting on cases that match certain rules upon creation, to provide answers to common questions/issues.  You can find more information about this project[1], the rule match for this case[2], and how to provide feedback (whether the rule is helpful, suggestions for improvement) here:\n\n    [1] https://mojo.redhat.com/groups/support-delivery/projects/diagnostics-20\n    [2] https://projects.engineering.redhat.com/browse/CPSEARCH-1503\n    [3] https://mojo.redhat.com/thread/942968</text>, <text>Hello!\n \nThank you for contacting Red Hat Tech Support.\n\nMy name is Bhushan G Dangre. I am taking ownership of this case and will be assisting you for the same.\n\nHope you have followed the below steps to configure the virt-who.\n\nFirst you will need to install "virt-who" package on any one of the VMs so that, host guest mapping can be sent to Subscription-Manager \n\nIf package is not installed then configure local repo by referring to below support article and install "virt-who" package. \n\nHow to create local repository distributed through apache of Red Hat Enterprise Linux 5/6/7 using DVD iso for update or installation ? \n https://access.redhat.com/solutions/7227 \n\nAfter installing package successfully disable local repo. \n\n~~~~~~~~~~~~~\n # rpm -qa | grep virt-who*\n~~~~~~~~~~~~~\n\nNote: Latest virt-who package for RHEL 6.x is  Package - virt-who-0.16-8.el6.noarch.rpm \n\nRegister the VM using below command: \n\n# subscription-manager register \n\nFind out the identity using below command: \n\n# subscription-manager identity \n\nConfigure virt-who (for virt-who version 0.14+): \n\nOn the RHEL guest, edit the /etc/virt-who.d/ file to identify the location of your ESX management server (there are different blocks of values for HyperV and RHEVM datacenters): \n\t[vmware] \n\ttype=esx \n\tserver=vCenter.domain.com \n\tusername=vCenterusername \n\tpassword=vCenterpassword \n\towner=org ID \n\tenv=Library \n\thypervisor_id=hostname \n\nFor complete list of options, see virt-who-config(5) manual page. \n\nAnd add below values in the /etc/sysconfig/virt-who file \n\tVIRTWHO_INTERVAL=300 \n\tVIRTWHO_BACKGROUND=1 \n\tVIRTWHO_DEBUG=1 \n\nThen Start and enable virt-who \n\t# service virt-who start \n\t# service virt-who enable \n\nOnce done, you will be able to see the host profile on the Customer Portal at \n\nSubscriptions--&gt;Subscription-Management--&gt;Units--&gt;Hypervisors. \nClick on the Hypervisor name and click on 'Attach Subscriptions'. \nNow, go back to the VM (on which virt-who was configured ) and subscribe it by :- \n\n\t# subscription-manager attach --auto \n\nYou can now register and subscribe multiple VMs running on subscribed hosts with the following commands :- \n\n\t# subscription-manager register \n\t# subscription-manager attach --auto \n\nRefer below knowledgebase article for more details: &gt;&gt; https://access.redhat.com/articles/480693 \n\n\n\nNote: If hypervisors were not reporting to customer portal. \n\nCheck for /var/log/rhsm/rhsm.log file:\n\nError Type:\n\tMainThread @subscriptionmanager.py:hypervisorCheckIn:193 - hypervisorCheckIn method in python-rhsm doesn't understand options paramenter, ignoring\n\tMainThread @virtwho.py:send:216 - Error in communication with subscription manager\n\nUpdated the subscription-manager and python-rhsm package\n\nOnce you see the list of hypervisors under customer portal you will need to follow below steps: \n\nClick on the Hypervisor name and click on 'Attach Subscriptions'. \nNow, go back to the VM (on which virt-who was configured ) and subscribe it by :- \n\n# subscription-manager attach --auto \nOR \n# subscription-manager list --available --all \n# subscription-manager attach --pool=&lt;POOL ID&gt; \n\nYou can now register and subscribe multiple VMs running on subscribed hosts with the following commands :- \n# subscription-manager register \n# subscription-manager attach --auto\n\nPlease do let me know if you still have any other query.\n\n\n\nAwaiting for your reply.\n\n\nBest Regards,\n\nBhushan G Dangre\nGSS \nRed Hat India</text>, <text>Hi Bhushan,\nThank you for your email. I didn't realise that sosreport didn't tar up all of the logs.\n\nIf you look in rhsm.log-20171203 you will see the following entries\n\n2017-12-03 00:04:20,675 [virtwho.main INFO] MainProcess(24314):MainThread @subscriptionmanager.py:hypervisorCheckIn:185 - Sending update in hosts-to-guests mapping for config "hrvcswa6": 5 hypervisors and 42 guests found\n2017-12-03 00:04:22,318 [virtwho.main DEBUG] MainProcess(24314):MainThread @virtwho.py:send_report:161 - Report for config "hrvcswa6" sent\n2017-12-03 00:05:28,116 [virtwho.main INFO] MainProcess(24314):MainThread @subscriptionmanager.py:hypervisorCheckIn:185 - Sending update in hosts-to-guests mapping for config "hrvcswa6": 5 hypervisors and 41 guests found\n\nThese messages sugges that the information has been sent. Why don't I see it in the Customer Portal?\n\nThanks and kind regards,\nSteven Childs</text>, <text>Regarding the suggestion to add VIRTWHO_BACKGROUND=1\nAccording to the following article https://access.redhat.com/solutions/2015963:  "As per bugzilla 1247922 background is now depreceated."</text>, <text>I uncommented VIRTWHO_SAM=1 in /etc/sysconfig/virt-who and restarted the virt-who service.\nI've attached the latest /var/log/rhsm/rhsm.log\n\nfrom /etc/virt-who.d/hrvcswa6.conf\nowner=4927749</text>, <text>Hello,\n\nCan you please provide your contact number along with the country code so that we can have detailed telephonic discussion.\nAwaiting for your reply.\n\n\nBest Regards,\n\nBhushan G Dangre\nGSS\nRed Hat India</text>, <text>HI Bhushan\n\nHere is my number.\n\n +44 (0) 7918755657\n\nI look forward to speaking with you.\n\nRegards,\n\nSteven Childs |Senior Technology Engineer, EUR I IT Infrastructure\nTravelers UK\n2nd Floor | 23-27 Alie Street\nLondon, E1 8DS\nT: +44 (0) 1737 787055   M: +44 (0) 7918755657\n\n\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com]\nSent: 05 December 2017 14:44\nTo: Childs,Steven &lt;SCHILDS@travelers.com&gt;\nSubject: (WoC) (SEV 3) Case #01985949 (hypervisors not appearing in customer portal virt-who) ref:_00DA0HxWH._500A0Z50kS:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01985949\nCase Title       : hypervisors not appearing in customer portal virt-who\nCase Number      : 01985949\nCase Open Date   : 2017-12-04 09:36:09\nSeverity         : 3 (Normal)\nProblem Type     : Other\n\nMost recent comment: On 2017-12-05 15:44:24, Dangre, Bhushan G commented:\n"Hello,\n\nCan you please provide your contact number along with the country code so that we can have detailed telephonic discussion.\nAwaiting for your reply.\n\n\nBest Regards,\n\nBhushan G Dangre\nGSS\nRed Hat India"\n\nhttps://access.redhat.com/support/cases/#/case/01985949?commentId=a0aA000000L6ln6IAB\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z50kS:ref\n\n________________________________\n\nDISCLAIMER\n\nThis material has been checked by us for computer viruses and, although none has been found, we cannot guarantee that it is completely free from such problems and we do not accept liability for loss or damage which may be caused.\n\nThis message is intended only for use of the individual or entity to whom it is addressed and may contain information which may be privileged and confidential. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this communication is strictly prohibited. If you have received this e-mail in error, please notify the sender immediately via e-mail and delete the message. Thank you.\n\n*******************************************************\n\nTravelers Insurance Company Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority in the UK and is regulated by the Central Bank of Ireland for conduct of business rules. Registered in England 1034343. Registered as a branch in Ireland 903382.\n\nTravelers Syndicate Management Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority. Registered in England 03207530.\n\nTravelers Underwriting Agency Limited is authorised and regulated by the Financial Conduct Authority. Registered in England 03708247.\n\nTravelers Professional Risks Limited is an appointed representative of Travelers Insurance Company Limited which is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority. Registered in England 05201980\n\nTravelers Management Limited. Registered in England 00972175.\n\nThe registered offices for all companies listed above is: Exchequer Court, 33 St Mary Axe, London, EC3A 8AG.\nAll other branch offices are available from our websites.\n\ntravelers.co.uk\ntravelers.ie\n\nIssues to: mailto: intadmin@travelers.com\n________________________________\nThis communication, including attachments, is confidential, may be subject to legal privileges, and is intended for the sole use of the addressee. Any use, duplication, disclosure or dissemination of this communication, other than by the addressee, is prohibited. If you have received this communication in error, please notify the sender immediately and delete or destroy this communication and all copies.\n\nTRVDiscDefault::1201</text>, <text>Please note I am located in London, England, UK.\nThus I work on GMT hours.\n\nThanks\n\nSteven Childs |Senior Technology Engineer, EUR I IT Infrastructure\nTravelers UK\n2nd Floor | 23-27 Alie Street\nLondon, E1 8DS\nT: +44 (0) 1737 787055   M: +44 (0) 7918755657\n\n\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com]\nSent: 05 December 2017 15:37\nTo: Childs,Steven &lt;SCHILDS@travelers.com&gt;\nSubject: (WoRH) (SEV 3) Case #01985949 (hypervisors not appearing in customer portal virt-who) ref:_00DA0HxWH._500A0Z50kS:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01985949\nCase Title       : hypervisors not appearing in customer portal virt-who\nCase Number      : 01985949\nCase Open Date   : 2017-12-04 09:36:09\nSeverity         : 3 (Normal)\nProblem Type     : Other\n\nMost recent comment: On 2017-12-05 16:37:20, Childs, Steven commented:\n"HI Bhushan\n\nHere is my number.\n\n +44 (0) 7918755657\n\nI look forward to speaking with you.\n\nRegards,\n\nSteven Childs |Senior Technology Engineer, EUR I IT Infrastructure Travelers UK 2nd Floor | 23-27 Alie Street London, E1 8DS\nT: +44 (0) 1737 787055   M: +44 (0) 7918755657\n\n\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com]\nSent: 05 December 2017 14:44\nTo: Childs,Steven &lt;SCHILDS@travelers.com&gt;\nSubject: (WoC) (SEV 3) Case #01985949 (hypervisors not appearing in customer portal virt-who) ref:_00DA0HxWH._500A0Z50kS:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01985949\nCase Title       : hypervisors not appearing in customer portal virt-who\nCase Number      : 01985949\nCase Open Date   : 2017-12-04 09:36:09\nSeverity         : 3 (Normal)\nProblem Type     : Other\n\nMost recent comment: On 2017-12-05 15:44:24, Dangre, Bhushan G commented:\n"Hello,\n\nCan you please provide your contact number along with the country code so that we can have detailed telephonic discussion.\nAwaiting for your reply.\n\n\nBest Regards,\n\nBhushan G Dangre\nGSS\nRed Hat India"\n\nhttps://access.redhat.com/support/cases/#/case/01985949?commentId=a0aA000000L6ln6IAB\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z50kS:ref\n\n________________________________\n\nDISCLAIMER\n\nThis material has been checked by us for computer viruses and, although none has been found, we cannot guarantee that it is completely free from such problems and we do not accept liability for loss or damage which may be caused.\n\nThis message is intended only for use of the individual or entity to whom it is addressed and may contain information which may be privileged and confidential. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this communication is strictly prohibited. If you have received this e-mail in error, please notify the sender immediately via e-mail and delete the message. Thank you.\n\n*******************************************************\n\nTravelers Insurance Company Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority in the UK and is regulated by the Central Bank of Ireland for conduct of business rules. Registered in England 1034343. Registered as a branch in Ireland 903382.\n\nTravelers Syndicate Management Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority. Registered in England 03207530.\n\nTravelers Underwriting Agency Limited is authorised and regulated by the Financial Conduct Authority. Registered in England 03708247.\n\nTravelers Professional Risks Limited is an appointed representative of Travelers Insurance Company Limited which is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority. Registered in England 05201980\n\nTravelers Management Limited. Registered in England 00972175.\n\nThe registered offices for all companies listed above is: Exchequer Court, 33 St Mary Axe, London, EC3A 8AG.\nAll other branch offices are available from our websites.\n\ntravelers.co.uk\ntravelers.ie\n\nIssues to: mailto: intadmin@travelers.com ________________________________ This communication, including attachments, is confidential, may be subject to legal privileges, and is intended for the sole use of the addressee. Any use, duplication, disclosure or dissemination of this communication, other than by the addressee, is prohibited. If you have received this communication in error, please notify the sender immediately and delete or destroy this communication and all copies.\n\nTRVDiscDefault::1201"\n\nhttps://access.redhat.com/support/cases/#/case/01985949?commentId=a0aA000000L6mucIAB\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z50kS:ref\n\n________________________________\n\nDISCLAIMER\n\nThis material has been checked by us for computer viruses and, although none has been found, we cannot guarantee that it is completely free from such problems and we do not accept liability for loss or damage which may be caused.\n\nThis message is intended only for use of the individual or entity to whom it is addressed and may contain information which may be privileged and confidential. If you are not the intended recipient you are hereby notified that any dissemination, distribution or copying of this communication is strictly prohibited. If you have received this e-mail in error, please notify the sender immediately via e-mail and delete the message. Thank you.\n\n*******************************************************\n\nTravelers Insurance Company Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority in the UK and is regulated by the Central Bank of Ireland for conduct of business rules. Registered in England 1034343. Registered as a branch in Ireland 903382.\n\nTravelers Syndicate Management Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority. Registered in England 03207530.\n\nTravelers Underwriting Agency Limited is authorised and regulated by the Financial Conduct Authority. Registered in England 03708247.\n\nTravelers Professional Risks Limited is an appointed representative of Travelers Insurance Company Limited which is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority. Registered in England 05201980\n\nTravelers Management Limited. Registered in England 00972175.\n\nThe registered offices for all companies listed above is: Exchequer Court, 33 St Mary Axe, London, EC3A 8AG.\nAll other branch offices are available from our websites.\n\ntravelers.co.uk\ntravelers.ie\n\nIssues to: mailto: intadmin@travelers.com</text>, <text>When might I get a phone call?  Here is another you can use: +44 (0) 1737 787055</text>, <text>tcpdump attached</text>, <text>tcpdump pcap files</text>, <text>Hi, \n\nThank you for contacting Red Hat Technical Support Services\n\nMy name is Akash Sakpal &amp; I have taken ownership of this case \n\nAs per our discussion on the chat session,  I would like to inform you that there is no issues with the virt-who configurations, \nbut the issue here is, your Login ID does not have the Org Admin permissions and only Org Admin Login IDs have access to view the hypervisors reported via virt-who on the Customer Portal\n\nSo to get the list of Hypervisors visible, you will need to gain the Org Admin permissions for your Login ID by contacting existing Org Admin of your Account who are as follows,\nMatthew J. Fitzpatrick\nJames Griffin\nChad Hinkel\nVishy Kasinadhuni\n\nKindly share your observations on the case after making the changes to the Login Address\n\nRegards,\nAkash Sakpal\nGlobal Support Services\nRed Hat</text>, <text>My account has been updated with Org Admin permissions. I can now see the hypervisors. Please close the case. Thanks.</text>, <text>Hi,\n\nThanks for the confirmation. As agreed, I am now moving this case into our archives. I appreciate your cooperation and patience and hope you would give us more opportunities to assist you in future.\nThank you for continuing to use Red Hat Support. \nHave a nice day!\n\n\nBest Regards,\n\nBhushan G Dangre\nGSS \nRed Hat India</text>, <text>Hello, all!\n\nI just had a communication from Travelers that makes me think this issue might\nnot yet be resolved, so I've re-opened it.\n\nI'll supply more context as soon as I understand the issue.\n\nRegards,\n Mason\n\n-- \nMason Loring Bliss\nTechnical Account Manager\nRed Hat, Inc.</text>, <text>Note, I'm taking ownership of the case while we clarify account roles.\n\nThere is current a call planned for Monday to discuss.\n\nRegards,\n Mason\n\n-- \nMason Loring Bliss\nTechnical Account Manager\nRed Hat, Inc.</text>, <text>*** internal ***\n\nSet 76 hour NEP to accomodate Monday afternoon call.</text>, <text>We're archiving this again, and I'll be opening new cases (here and internally)\nto explore entitlement consumption, as Steven is not seeing entitlements that\nlook like they should be available.\n\nRegards,\n Mason\n\n-- \nMason Loring Bliss\nTechnical Account Manager\nRed Hat, Inc.</text>]
spam	[[<description>We are spinning up openstack automation environments on daily basis. We are registering the hosts with satellite 6 on deployment of VM's. We would need a script or any automated solution to purge all the entries from satellite once the environment is decommissioned.</description>], <text>Hello Thomas,\n\n Thank you for contacting Red Hat Technical Support. My name is Luke and I will be taking temporary ownership of your case while we transition your case to our Support Team.\n\nI will transition your case to our Satellite Team to see how we can help. Once we have an engineer assigned to your case, you will receive an update from them with further instructions. I have your contact number on file as 704-758-5037. Please update the ticket if this is incorrect.\n\nIf you have any further questions or concerns, please feel free to update this case as you need.\n\nRegards,\n\nLuke Grunewald\nRed Hat, Inc.\nCustomer Support Specialist</text>, <text>Hello,\n \nWelcome to Red Hat Technical Support. My name is Harshad and  I will be assisting you with this service request.\n\nHere is my understanding of the requirement.\n&gt;&gt; You wanted to remove content hosts entry from satellite server once the host is decommissioned.\n---------\n\nIf its right then you can enable "unregister_delete_host" option in katello settings. So when run # subscription-manager unregister command on the client will remove host entry from satellite server.\n\nAdminister --&gt; Settings --&gt; Katello --&gt; unregister_delete_host --&gt; True\n\nRegards,\nHarshad More\nGSS, Red Hat India.\n1.888.GO.REDHAT</text>, <text>Hi Harshad,\n\nIs there any automated way may be a script to purge servers from satellite?\nWe deploy and destroy environments on demand. So, there will be bunch of servers which are destroyed and the stale entries still exist on satellite. \nWe cannot use subscription-manager unregister command from client because the client machine no longer exist after the environment is destroyed. \nAre there any hammer cli commands or scripts to find the hosts which are not connected to satellite for past 10 days or more and delete their registries and clean up satellite?\n\nRegards,\nHarish Goura.</text>, <text>Hello,\n\nThere is also an option to search host based on "Last Checkin" under content host tab.\n\nHosts --&gt; Content Hosts --&gt; search  last_checkin = --&gt; will give you option \neg: last_checkin = "6 days ago"\n------------------\n\nVerify the list and select all the hosts then perform Bulk Actions to "Remove Hosts"\n\nRegards,\nHarshad More\nGSS, Red Hat India.\n1.888.GO.REDHAT</text>, <text>Hi Harshad,\n\nI can delete hosts from GUI. But, we deploy environments on daily basis. So, if there are any "command line" options (may be hammer commands or something) to achieve the same, that would help us. We can script it and add to crontab or some jenkins job to run the script. \n\nThanks,\nHarish Goura.</text>, <text>Hello,\n\nIt's not recommended to do this type of scripting for deleting hosts because if for some reason's client failed to report to satellite its status due to some network issue/rhsmcertd/virt-who etc. then other hosts too might get deleted. So always go with GUI and confirm the same.\nAlso, katello setting as suggested in earlier is the another option.\n\nFrom hammer you can search host which don't have a valid subscription and then can use it to delete the host. However, it won't give you list of hosts not reporting from last 10 days or so.\n\n# hammer host list --search "subscription_status = invalid"\n# hammer host delete --id &lt;hostID&gt;\n\nRegards,\nHarshad More\nGSS, Red Hat India.\n1.888.GO.REDHAT</text>]
spam	[[<description>What problem/issue/behavior are you having trouble with?  What do you expect to see?\nWhen running virt-who -p command expect to see ALL ESX hosts attached to this vCenter\nWe only see appx 30 out of 40 ESX hosts. \nAll of our prod VCenters are running at ver 6.0 update 2\nvirt-who-0.19-7 .e17_4.noarch\nWhen we login to the vCenter using the \u2018Satellite@vsphere.local\u2019 user id, We DO see all of the ESX hosts\u2026. This implies we think that there is some sort of additional \u201cfiltering\u201d happening with the virt-who program that we aren\u2019t seeing. \nWhere are you experiencing the behavior?  What environment?\n\nNew Prod Satellite\n\nWhen does the behavior occur? Frequently?  Repeatedly?   At certain times?\n\nalways when running against this VCenter\n\nWhat information can you provide around timeframes and the business impact?\n\nCurrently no business impact. We are using older version of Satellite and have not cutover to new satellite yet.</description>], <text>This comment has been generated via an automated case assistant.\n\n \n\nThe following resources may be helpful for working with virt-who and virtual Subscriptions (including Virtual Data Center):\n\n \n\n    https://access.redhat.com/labs/virtwhoconfig/ Red Hat Virtualization Agent (virt-who) Configuration Helper - virt-who is required when working with any hypervisor if your subscription name includes the words 'Virtual Datacenters', '4 Guests', or 'Unlimited Guests'. This app helps users configure virt-who. When started, the app requests information about your virt-who usage and generates a configuration file based on he answers. It also provides instructions for updating your virt-who configuration.\n    https://access.redhat.com/blogs/1169563/posts/2190731 Subscription-manager for the former Red Hat Network User: Part 3 - Understanding virt-who \n    https://access.redhat.com/blogs/1169563/posts/2630111 Subscription-manager for the former Red Hat Network user - part 5 - Working with subscriptions that require virt-who\n    https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/virtual_instances_guide/ - Virtual Instances Guide for Satellite 6\n    https://access.redhat.com/documentation/en-us/red_hat_subscription_management/1/html/virtual_instances_guide/ - Virtual Instances Guide for use with Red Hat Subscription Management (RHSM) but no Satellite</text>, <text>*PRIVATE UPDATE, DO NOT MAKE PUBLIC*\n\n    The previous public update is part of a pilot to improve auto-solve and self-solve, by commenting on cases that match certain rules upon creation, to provide answers to common questions/issues.  You can find more information about this project[1], the rule match for this case[2], and how to provide feedback (whether the rule is helpful, suggestions for improvement) here:\n\n    [1] https://mojo.redhat.com/groups/support-delivery/projects/diagnostics-20\n    [2] https://projects.engineering.redhat.com/browse/CPSEARCH-1503\n    [3] https://mojo.redhat.com/thread/942968</text>, <text>Hello,\n\nWelcome to Red Hat Technical Support!!!\n\nMy name is Prashant Waghmare and I am assisting you on this case.\n\nIn order to investigate further. could you provide me below details:\n=======================\na) Are you running virt-who on satellite server or do you have separate server on which virt-who is configured?\nProvide me the out-put of following commands, from the server on which virt-who is configured:\n \n# cat /etc/virt-who.d/*\n\n# grep -v ^# /etc/sysconfig/virt-who\n\n# service virt-who stop\n\n# virt-who -d -o &gt;&gt; /tmp/virt.log 2&gt;&amp;1   &lt;== upload virt.log to the case.\n\n# service virt-who start\n===========================\n\nLooking forward to your reply.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>I am currently out of the office, returning on Tuesday 12/12 To report something that is not working, make an urgent request, obtain the status of an existing request, or for a general IT questions please call IT Customer Service at 4-HELP\n\n\n\n\nThis e-mail transmission may contain information that is proprietary, privileged and/or confidential and is intended exclusively for the person(s) to whom it is addressed. Any use, copying, retention or disclosure by any person other than the intended recipient or the intended recipient's designees is strictly prohibited. If you are not the intended recipient or their designee, please notify the sender immediately by return e-mail and delete all copies\n\n****  This message was sent via secure encryption.  ****</text>, <text>Slight change in behavior now... It IS now reporting at least one additional ESX host, esxsadr01,  which is included in a filter_hosts= line in our *.conf file. When we run without the filter_hosts= line and do a "virt-who -p", we are still not seeing 6 of the ESX hosts we can see in vCenter (using the service ID from the *.conf file).\n\n\n[prdlx10174:root]/etc/virt-who.d# ll\ntotal 8\n-rw-r--r-- 1 root root 4091 Dec  7 07:29 isbccmgtdr21.conf\n-rw-r--r-- 1 root root 1213 Oct 25 14:09 template.conf\n[prdlx10174:root]/etc/virt-who.d# cat *\n[vcenter2]\ntype=esx\nserver=isbccmgtdr21.private.massmutual.com\nusername=Satellite@vsphere.local\npassword=xxxxxxxxxxxx\nowner=MMFG\nenv=Library\nhypervisor_id=hostname\nfilter_hosts=esxdcsdr007.private.massmutual.com,esxdcsdr008.private.massmutual.\ncom,esxdcsdr009.private.massmutual.com,esxdcsdr010.private.massmutual.com,esxdcs\ndr011.private.massmutual.com,esxdcsdr012.private.massmutual.com,esxden001.privat\ne.massmutual.com,esxden002.private.massmutual.com,esxden003.private.massmutual.c\nom,esxden004.private.massmutual.com,esxden005.private.massmutual.com,esxden006.p\nrivate.massmutual.com,esxden007.private.massmutual.com,esxden008.private.massmut\nual.com,esxden009.private.massmutual.com,esxden010.private.massmutual.com,esxden\n011.private.massmutual.com,esxecmpr001.private.massmutual.com,esxecmpr002.privat\ne.massmutual.com,esxecmpr005.private.massmutual.com,esxecspr001.private.massmutu\nal.com,esxecspr005.private.massmutual.com,esxecspr006.private.massmutual.com,esx\necspr012.private.massmutual.com,esxecspr013.private.massmutual.com,esxecspr016.p\nrivate.massmutual.com,esxecspr017.private.massmutual.com,esxecspr018.private.mas\nsmutual.com,esxecspr019.private.massmutual.com,esxecspr020.private.massmutual.co\nm,esxecspr021.private.massmutual.com,esxecspr024.private.massmutual.com,esxecspr\n025.private.massmutual.com,esxenf001.private.massmutual.com,esxenf002.private.ma\nssmutual.com,esxenf003.private.massmutual.com,esxenf004.private.massmutual.com,e\nsxenf005.private.massmutual.com,esxenf006.private.massmutual.com,esxenf007.priva\nte.massmutual.com,esxenf008.private.massmutual.com,esxenf009.private.massmutual.\ncom,esxenf010.private.massmutual.com,esxenf011.private.massmutual.com,esxsadr01.\nprivate.massmutual.com,esxsadr02.private.massmutual.com,esxscmpr001.private.mass\nmutual.com,esxscmpr002.private.massmutual.com,esxscmpr005.private.massmutual.com\n,esxscspr001.private.massmutual.com,esxscspr002.private.massmutual.com,esxscspr0\n03.private.massmutual.com,esxscspr004.private.massmutual.com,esxscspr009.private\n.massmutual.com,esxscspr012.private.massmutual.com,esxscspr013.private.massmutua\nl.com,esxscspr014.private.massmutual.com,esxscspr015.private.massmutual.com,esxs\ncspr016.private.massmutual.com,esxscspr017.private.massmutual.com,esxscspr018.pr\nivate.massmutual.com,esxscspr019.private.massmutual.com,esxscspr021.private.mass\nmutual.com,esxscspr022.private.massmutual.com,esxscspr038.private.massmutual.com\n,esxscspr039.private.massmutual.com,esxscspr040.private.massmutual.com,esxscspr0\n44.private.massmutual.com,esxsipr01.private.massmutual.com,esxsipr02.private.mas\nsmutual.com,esxsipr03.private.massmutual.com,esxsipr04.private.massmutual.com,es\nxsipr05.private.massmutual.com,esxsipr06.private.massmutual.com,esxsipr07.privat\ne.massmutual.com,esxsipr08.private.massmutual.com,esxskpr01.private.massmutual.c\nom,esxskpr02.private.massmutual.com,esxskpr03.private.massmutual.com,esxskpr04.p\nrivate.massmutual.com,esxskpr05.private.massmutual.com,esxskpr06.private.massmut\nual.com,esxskpr07.private.massmutual.com,esxskpr08.private.massmutual.com,esxspr\n001.private.massmutual.com,esxspr002.private.massmutual.com,esxspr003.private.ma\nssmutual.com,esxspr004.private.massmutual.com,esxspr005.private.massmutual.com,e\nsxspr006.private.massmutual.com,esxspr007.private.massmutual.com,esxspr008.priva\nte.massmutual.com,esxspr009.private.massmutual.com,esxspr010.private.massmutual.\ncom,esxspr011.private.massmutual.com,esxspr012.private.massmutual.com,esxspr013.\nprivate.massmutual.com,esxspr014.private.massmutual.com,esxspr015.private.massmu\ntual.com,esxspr016.private.massmutual.com,esxspr017.private.massmutual.com,esxsp\nr018.private.massmutual.com,esxspr019.private.massmutual.com,esxspr020.private.m\nassmutual.com,esxspr021.private.massmutual.com,esxspr022.private.massmutual.com,\nesxspr036.private.massmutual.com,esxspr037.private.massmutual.com,esxspr038.priv\nate.massmutual.com,esxspr039.private.massmutual.com,esxspr040.private.massmutual\n.com,esxspr041.private.massmutual.com,esxspr042.private.massmutual.com,esxspr043\n.private.massmutual.com,esxspr044.private.massmutual.com,esxspr045.private.massm\nutual.com\n\n\n## This is a template for virt-who configuration files. Please see\n## virt-who-config(5) manual page for detailed information.\n##\n## virt-who checks all files in /etc/virt-who.d/ if they're valid ini-like\n## files and uses them as configuration. Each file might contain more configs.\n##\n## You can uncomment and fill following template or create new file with\n## similar content.\n\n#[config name]\n#type=               ; insert one of libvirt/esx/hyperv/rhevm/vdsm/fake\n#server=             ; insert hostname or ip address of the server to connect to\n#username=           ; username for server authentication\n#password=           ; password for server authentication\n#encrypted_password=  ; password encrypted using virt-who-password utility\n#owner=              ; owner for use with SAM, Customer Portal, or Satellite 6\n#env=                ; environment for use with SAM, Customer Portal, or Satelli\nte 6\n#hypervisor_id=      ; how will be the hypervisor identified, one of: uuid, host\nname, hwuuid\n\n## For complete list of options, see virt-who-config(5) manual page.\n\n## Terse version of the config template:\n#[config name]\n#type=\n#server=\n#username=\n#password=\n#encrypted_password=\n#owner=\n#env=\n#hypervisor_id=\n\n[prdlx10174:root]/etc/virt-who.d# grep -v ^# /etc/sysconfig/virt-who\n\nVIRTWHO_DEBUG=0\n\n\n\n\n[prdlx10174:root]/etc/virt-who.d#</text>, <text>I forgot to say that we are running virt-who on the Satellite server itself.</text>, <text>Hello,\n\nThank you for update. I am looking into the logs and will get back to you with further details.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Possible spam comment\n\nI am currently out of the office, returning on Tuesday 12/12 To report something that is not working, make an urgent request, obtain the status of an existing request, or for a general IT questions please call IT Customer Service at 4-HELP\n\n\n\n\nThis e-mail transmission may contain information that is proprietary, privileged and/or confidential and is intended exclusively for the person(s) to whom it is addressed. Any use, copying, retention or disclosure by any person other than the intended recipient or the intended recipient's designees is strictly prohibited. If you are not the intended recipient or their designee, please notify the sender immediately by return e-mail and delete all copies\n\n****  This message was sent via secure encryption.  ****</text>, <text>Hello,\n\nCould you enable the below line in : \n\n# vi /etc/sysconfig/virt-who \n\n# Report to Satellite version 6\nVIRTWHO_SATELLITE6=1\n\n\nAs per provided out-put , it shows that the 118 hypervisors are reporting via configuration file:\n==================\nPWVCENT002.cntr.thrivent.corp\n=====================\n\nSo, do you have any other server on which virt-who is configured? because in provided configuration you have configured conf file vcenter2.\n\nIn order to find it, could you run below command on satellite server:\n========================\n# grep POST /var/log/httpd/foreman-ssl_access_ssl.log | grep cmd=virt-who | awk '{print $1}' | sort | uniq -c\n\n# grep -i "cmd=virt-who" /var/log/httpd/foreman-ssl_access_ssl.log | less\n===========================\n\nLooking forward to your reply.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Thanks, and a couple of points.\n\n1. We have a separate virt-who running on a different server, feeding a completely different, and much older, Satellite v6.2.2. That's our "old Sat" environment and as soon as we get this new Satellite (6.2.12) configured and operating correctly, we will be migrating all RHEL servers from old to new Satellite.\n\n2. I am not sure what file is PWVCENT002.cntr.thrivent.corp is, but it's not ours.\n\n3. We have a total of 4 vCenters that our virt-who needs to poll and report into Satellite. Using filter_hosts=, there are 116 total subscribed ESX hosts that should be reporting.\n\n\n4. When I only use our Denver / DR vCenter config, isbccmgtdr21.conf, and remove the "filter_hosts=" line and run "virt-who -p", it does NOT report ALL ESX hosts controlled by that vCenter. As of right now, it reports 34 ESX hosts out of 40.\n\n5. When I log in to the vCenter using the credentials from the *.conf file, I can see all 40 ESX hosts.\n\n6. When I run those two commands that search the log, I get no output:\n\n[prdlx10174:root]/root# grep POST /var/log/httpd/foreman-ssl_access_ssl.log | grep cmd=virt-who | awk '{print $1}' | sort | uniq -c\n[prdlx10174:root]/root# \n[prdlx10174:root]/root# grep -i "cmd=virt-who" /var/log/httpd/foreman-ssl_access_ssl.log\n[prdlx10174:root]/root#</text>, <text>Hello,\n\nThank you for update. I am looking into the details ad will get back to you with further details.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hello,\n\n1) From Satellite server check the Hypervisor hostname is resolving to its name correctly using nslookup. Also from VCentre terminal run nslookup for the ESXi host.\n\nCheck domain is set appropriately for the Hypervisor. Compare it with the one that reported correctly to the Satellite.\n\nvCentre --&gt; host1.example.com --&gt; Configuration --&gt; DNS and Routing\n\nHere check Name and Domain for the ESX host. The Domain information should be mentioned.\n\nThen restart virt-who service, run \n\n# service virt-who stop\n\n# virt-who -d -o &gt;&gt; /tmp/virt.log4 &lt;== upload virt.log4 to the case.\n\n# service virt-who start\n\nFor more details, please refer below article:\n=================\nhttps://access.redhat.com/solutions/3186841\n==================\n\nAnd see if the mentioned hypervisors are reporting into /tmp/virt.log4\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>DNS resolution on the Satellite host appears to be working ok forward and back for ESX hostname:\n\n[prdlx10174:root]/tmp# nslookup esxsbdr06\nServer:         170.6.240.26\nAddress:        170.6.240.26#53\n\nName:   esxsbdr06.private.massmutual.com\nAddress: 170.6.227.227\n\n[prdlx10174:root]/tmp# nslookup 170.6.227.227\nServer:         170.6.240.26\nAddress:        170.6.240.26#53\n\n227.227.6.170.in-addr.arpa      name = esxsbdr06.private.massmutual.com.\n\nI ran "virt-who -d" but with "-p" option with filter_hosts= line commented out. The ESX hosts that are not reporting are ones I would filter out (due to not being licensed for RHEL). I just want to be sure that ALL hosts are being captured by virt-who, even if not reported back to Satellite due to filter.\n\nI redirected stdout to virt.log4, and debug (stderr) output to virt-p.log4, both attach.\n\nOne thing I did notice, and have emailed my VMware engineer, is that on the Hosts / Configuration / Networking screen in vCenter, I do not see any information for the ESX hosts that are not being reported by virt-who. I will let you know what he says when I hear from him. The "DNS and Routing" panel looks OK to me.</text>, <text>Hello,\n\nThank you for update. I am looking into logs and will get back to youwith further details.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hello,\n\nIf hostname on the guest is xyz.example.com then in virt-who config, "filter_hosts" parameter should be\n\nfilter_hosts="xyz.example.com"\n\nIf hostname on the guest is xyz then in virt-who config, "filter_hosts" parameter should be\n\nfilter_hosts="xyz"\n\nand then check by restarting the virt-who whether hosts reports or not?\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>OK, but what I'm saying is that when I *completely remove* filter_hosts=, I still do not see some of my ESX hosts.\n\nBelow is my conf file modified to remove filter_hosts= line, a run of "virt-who -p", and I'm attaching the output of that command.\n\nAs you can see from the "grep" following that returns nothing found in the output file, there are at least 3 ESX hosts that we know exist, can see in vCenter, but are not captured in virt-who output.\n\n[prdlx10174:root]/etc/virt-who.d# cat isbccmgtdr21.conf\n[Denver_DR]\ntype=esx\nserver=isbccmgtdr21.private.massmutual.com\nusername=Satellite@vsphere.local\npassword=XXXXXXXXXX\nowner=MMFG\nenv=Library\nhypervisor_id=hostname\n\n[prdlx10174:root]/etc/virt-who.d# virt-who -p &gt; virt-who-p.out\n2017-12-14 07:30:56,427 INFO: Using configuration "Denver_DR" ("esx" mode)\n2017-12-14 07:30:56,428 INFO: Using reporter_id='prdlx10174-25178675ec2742d5a689b0073b40b6a9'\n2017-12-14 07:31:01,709 INFO: Report for config "Denver_DR" gathered, placing in datastore\n\n\nesxsbdr05.private.massmutual.com &lt;&lt; NOT reported\nesxsbdr06.private.massmutual.com &lt;&lt; NOT reported\nesxsbdr07.private.massmutual.com &lt;&lt; NOT reported\n\n[prdlx10174:root]/etc/virt-who.d# grep esxsbdr0[567] virt-who-p.out\n[prdlx10174:root]/etc/virt-who.d#</text>, <text>The previous update was supposed to have been posted the other day, but evidently the RH portal / case website is having issues and it never actually posted... \n\nIn any case, we have resolved this. I started to poke around in the networking portion of vCenter and alerted our VMware support to some "oddities".\n\nThey said they "restarted the services", and now ALL ESX hosts are being reported correctly thru virt-who.\n\nThis ticket can be closed.\n\nThank you</text>, <text>Hello Mark,\n\nWe are archiving this case with your confirmation.\n\nAfter this case has been archived, you may receive an email request to fill a survey regarding our support services. Please take this opportunity to let us know what we did well and where we can improve.\n\nIn case you don't receive the email request but would like to share your feedback, please feel free to reply back to this email or update the case. Your comments help us continually refine the customer experience to provide you with better service and quicker resolution.\n\nThank you for contacting Red Hat Technical Support.\n\nRegards,\nGauravi Patil \nRed Hat Technical Support</text>]
spam	[[<description>What problem/issue/behavior are you having trouble with?  What do you expect to see?\n\nHello Support team,\n\nwe purchased 20 Red Hat Enterprise Linux Developer Suite subscriptions for our internal developers to give them the possibility to use RHEL 7 based Vagrant boxes within VirtualBox on their Windows laptops for development purposes. \n\nThe idea is to connect the Vagrant VirtualBox virtual machines to our Satellite server to offer the developers the same package repositories and content we use on our RHEL 7 machines in our datacenters. \n\nNow we started 21 virtual machines on the same physical Windows laptop and registered the virtual machines with subscription-manager and an according activation key (which I created on our Satellite server). Each Vagrant VirtualBox virtual machine has a unique hostname. The 21st virtual machines is now not getting a valid subscription because we reached our purchased subscription of 20.\n\nBullet point 8 on page https://developers.redhat.com/articles/no-cost-rhel-faq/ says that you can run as many virtual machines you want with this subscription on one physical node. But each "subscription-manager register" command consumes one of our subscription even if the virtual machine is running on the same physical laptop.\n\nIs there something I misunderstood or configured wrong? What is the correct configuration for this use case?\n\nBest regards\n\nFrank</description>], <text>Greetings,\n\nThank you for contacting Red Hat Technical Support. I am Karnvir and I will be assisting you on this service request.\n\nAs per the case description, I have understood that you are using Satellite 6 and for Red Hat Enterprise Linux Developer Suite subscriptions your host machines are consuming the all 20 subscriptions, Now you are facing problem . Correct me if I am wrong\n\nIn order to troubleshoot this issue please provide us the following details:-\n\n1. What is the minor version of Satellite ? \n\n2. Screenshot of this page from Satellite WebGUI\n\nSatellite WebGUI &gt;&gt; Content &gt;&gt; Red Hat Subscriptions\n\n3. Output of following command from Satellite server\n\n# hammer subscription list --organization-id &lt;id&gt;\n\n4. Output of following commands from one of the host machines subscribed to Red Hat Developer Suite Subscription.\n\n# subscription-manager list --consumed\n\n# subscription-manager list --available\n\nFeel free to get back to us, if you have any concerns.\n\nThank you for your patience and for choosing Red Hat.\n\nBest Regards,\nKarnvir Singh\nRed Hat Technical Support</text>, <text>Hello Karnvir,\n\nplease find my answers below:\n\n1) Version 6.2.11\n2) Please find screenshot attached to this ticket (2017-11-27 12_13_25-Subscriptions.jpg)\n3) Please find command output attached to this ticket (hammer_subscriptions.txt)\n4) Please find attached output from within a registered VM (shell_output_VM.txt)\n\nBest regards\n\nFrank\n\n\n(In reply to Singh, Karnvir)\n&gt; Greetings,\n&gt; \n&gt; Thank you for contacting Red Hat Technical Support. I am Karnvir and I will be assisting you on this service request.\n&gt; \n&gt; As per the case description, I have understood that you are using Satellite 6 and for Red Hat Enterprise Linux Developer Suite subscriptions your host machines are consuming the all 20 subscriptions, Now you are facing problem . Correct me if I am wrong\n&gt; \n&gt; In order to troubleshoot this issue please provide us the following details:-\n&gt; \n&gt; 1. What is the minor version of Satellite ? \n&gt; \n&gt; 2. Screenshot of this page from Satellite WebGUI\n&gt; \n&gt; Satellite WebGUI &gt;&gt; Content &gt;&gt; Red Hat Subscriptions\n&gt; \n&gt; 3. Output of following command from Satellite server\n&gt; \n&gt; # hammer subscription list --organization-id &lt;id&gt;\n&gt; \n&gt; 4. Output of following commands from one of the host machines subscribed to Red Hat Developer Suite Subscription.\n&gt; \n&gt; # subscription-manager list --consumed\n&gt; \n&gt; # subscription-manager list --available\n&gt; \n&gt; Feel free to get back to us, if you have any concerns.\n&gt; \n&gt; Thank you for your patience</text>, <text>Hello Frank,\n\nThank you for writing back,\n\nPlease allow me some time to go through the logs, I would update the case with my findings soon.\n\nIn the mean time could you please re-upload the following screenshot again, As from the provided screenshot I am not able to see the last field "Requires virt-who usage" of subscriptions tab.\n\nSatellite WebGUI &gt;&gt; Content &gt;&gt; Red Hat Subscriptions\n\nBest Regards,\nKarnvir Singh\nRed Hat Technical Support</text>, <text>Hi Karnvir,\n\nplease find attached 2017-11-27 12_13_25-Subscriptions_2.jpg.\n\nBest regards\n\nFrank\n\n(In reply to Singh, Karnvir)\n&gt; Hello Frank,\n&gt; \n&gt; Thank you for writing back,\n&gt; \n&gt; Please allow me some time to go through the logs, I would update the case with my findings soon.\n&gt; \n&gt; In the mean time could you please re-upload the following screenshot again, As from the provided screenshot I am not able to see the last field "Requires virt-who usage" of subscriptions tab.\n&gt; \n&gt; Satellite WebGUI &gt;&gt; Content &gt;&gt; Red Hat Subscriptions\n&gt; \n&gt; Best Regards,\n&gt; Karnvir Singh\n&gt; Red Hat Technical Support</text>, <text>Hello,\n\nThank you for writing back,\n\nI have gone through the attachments, on screenshot we can see "Red Hat Enterprise Linux Developer Suite" requires virt-who, So in order to get the proper benefit of "Red Hat Enterprise Linux Developer Suite" you need to configure the virt-who to report the info about the hypervisor on which you are running the guest machines and also attach the subscription to the hypervisor.\n\n1. First add the information of your hypervisor to virt-who machine, so that it can report the info to Satellite server.\n\n2. Then your hypervisor would be visible in Satellite WebGUI hosts tab, here add the Developer subscription to the hypervisor by following below steps:-\n\nSatellite WebGUI &gt;&gt; hosts &gt;&gt; Content hosts &gt;&gt; select the host hypervisor &gt;&gt; subscriptions &gt;&gt; add &gt;&gt; Here Select the "Red Hat Enterprise Linux Developer Suite".\n\n3. Then try to register the host machines running on this hypervisor again.\n\nLet us know the results\n\nBest Regards,\nKarnvir Singh\nRed Hat Technical Support</text>]
ham	[[<description>esansodevweb3 (\u4eee\u60f3\u30de\u30b7\u30f3)\u306eyum\u767b\u9332\u3092\u884c\u3044\u307e\u3057\u305f\u304c\u3001subscription\u7d42\u4e86\u65e5\u304c2017/11/29\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u672c\u6765\u306f2018/05/12 \u306e\u306f\u305a\u3067\u3059\u3002esansodevweb3\u306e\u72b6\u614b\u3092\u78ba\u8a8d\u3059\u308b\u3068\u4ee5\u4e0b\u306e\u8868\u793a\u304c\u3042\u308a\u307e\u3057\u305f\u3002\n\u300c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u6e08\u307f\u306e\u88fd\u54c1\u306e\u4e00\u90e8\u306f\u3001\u30a2\u30bf\u30c3\u30c1\u3055\u308c\u3066\u3044\u308b\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u306e\u5bfe\u8c61\u3068\u3057\u3066\u6b63\u3057\u304f\u6307\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002 2017\u5e7411\u670829\u65e5(\u6c34) 16\u664258\u520656\u79d2 +09:00 \u307e\u3067\u66f4\u65b0\u3092\u53d7\u4fe1\u3067\u304d\u307e\u3059\u3002\u300d\n\u539f\u56e0\u3068\u5bfe\u7b56\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002</description>], <text>\u3044\u3064\u3082\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002\n\nRed Hat Global Support Services \u3067\u3059\u3002\n\n\u672c\u30b5\u30dd\u30fc\u30c8\u30b5\u30fc\u30d3\u30b9\u3092\u3054\u5229\u7528\u9802\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u3054\u6307\u5b9a\u9802\u304d\u307e\u3057\u305f\u91cd\u5927\u5ea6\u306e\u5185\u5bb9\u3001\u304a\u3088\u3073\u305d\u308c\u305e\u308c\u306e\u5fdc\u7b54\u6642\u9593\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3068\u306a\u308a\u307e\u3059\u3002\n\u203b\u91cd\u5927\u5ea6\u306f\u62c5\u5f53\u8005\u304c\u9069\u5207\u306a\u30ec\u30d9\u30eb\u306b\u8abf\u6574\u3055\u305b\u3066\u9802\u304f\u5834\u5408\u304c\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u30fb\u91cd\u5927\u5ea61 (\u7dca\u6025)\n\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u7528\u9014\u3067\u306e\u672c\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u4f7f\u7528\u306b\u6df1\u523b\u306a\u5f71\u97ff\u3092\u53ca\u307c\u3059\u554f\u984c (\u4f8b: \u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u7d1b\u5931\u3001\u307e\u305f\u306f\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u30b7\u30b9\u30c6\u30e0\u306e\u6a5f\u80fd\u55aa\u5931)\u3002 \u696d\u52d9\u3092\u505c\u6b62\u305b\u3056\u308b\u3092\u5f97\u306a\u3044\u72b6\u6cc1\u3067\u3001\u56de\u907f\u7b56\u304c\u306a\u3044\u3002\n\n\u30fb\u91cd\u5927\u5ea62 (\u9ad8)\n\u672c\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306f\u6a5f\u80fd\u3057\u3066\u3044\u308b\u304c\u3001\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u7528\u9014\u3067\u306e\u4f7f\u7528\u306b\u91cd\u5927\u306a\u6e1b\u901f\u30fb\u6e1b\u5c11\u3092\u53ca\u307c\u3059\u554f\u984c\u3002\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u7528\u9014\u306b\u304a\u3044\u3066\u3001\u696d\u52d9\u306e\u4e00\u90e8\u306b\u6df1\u523b\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u72b6\u6cc1\u3067\u3001\u56de\u907f\u7b56\u304c\u306a\u3044\u3002\n\n\u30fb\u91cd\u5927\u5ea63 (\u4e2d)\n\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u307e\u305f\u306f\u958b\u767a\u7528\u9014\u306b\u304a\u3044\u3066\u3001\u90e8\u5206\u7684\u3067\u3001\u304b\u3064\u81f4\u547d\u7684\u3067\u306f\u306a\u3044\u652f\u969c\u3092\u3082\u305f\u3089\u3059\u554f\u984c\u3002\u4e2d\u304b\u3089\u4f4e\u7a0b\u5ea6\u306e\u5f71\u97ff\u304c\u3042\u308b\u304c\u3001\u56de\u907f\u7b56\u304c\u3042\u308a\u696d\u52d9\u306f\u7d9a\u3051\u3089\u308c\u308b\u3002\n\n\u30fb\u91cd\u5927\u5ea64 (\u4f4e)\n\u4e00\u822c\u7684\u306a\u4f7f\u7528\u306b\u95a2\u3059\u308b\u8cea\u554f\u3002\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u7528\u9014\u306b\u304a\u3044\u3066\u3001\u5f71\u97ff\u304c\u4f4e\u3044\u3001\u3082\u3057\u304f\u306f\u306a\u3044\u554f\u984c\u3002\u958b\u767a\u7528\u9014\u306b\u304a\u3044\u3066\u3001\u30d3\u30b8\u30cd\u30b9\u306b\u4e2d\u304b\u3089\u4f4e\u7a0b\u5ea6\u306e\u5f71\u97ff\u304c\u3042\u308b\u304c\u3001\u56de\u907f\u7b56\u304c\u3042\u308a\u696d\u52d9\u306f\u7d9a\u3051\u3089\u308c\u308b\u3002\n\n+----------+-----------------+-----------------------------------------------------+\n|\u30d7\u30ed\u30c0\u30af\u30c8|     Standard    |                        Premium                      |\n+----------+-----------------+-------------+---------------------------------------+\n|  \u91cd\u5927\u5ea6  |\u521d\u671f/\u7d99\u7d9a\u5fdc\u7b54\u6642\u9593| \u521d\u671f\u5fdc\u7b54\u6642\u9593|              \u7d99\u7d9a\u5fdc\u7b54\u6642\u9593             |\n+----------+-----------------+-------------+---------------------------------------+\n|     1    |    1 \u55b6\u696d\u6642\u9593   |    1 \u6642\u9593   | 1 \u6642\u9593\u307e\u305f\u306f\u304a\u5ba2\u69d8\u3068\u306e\u5408\u610f\u306b\u57fa\u3065\u304f    |\n+----------+-----------------+-------------+---------------------------------------+\n|     2    |    4 \u55b6\u696d\u6642\u9593   |    2 \u6642\u9593   | 4 \u6642\u9593\u307e\u305f\u306f\u304a\u5ba2\u69d8\u3068\u306e\u5408\u610f\u306b\u57fa\u3065\u304f    |\n+----------+-----------------+-------------+---------------------------------------+\n|     3    |    1 \u55b6\u696d\u65e5     |  4 \u55b6\u696d\u6642\u9593 | 8 \u55b6\u696d\u6642\u9593\u307e\u305f\u306f\u304a\u5ba2\u69d8\u3068\u306e\u5408\u610f\u306b\u57fa\u3065\u304f|\n+----------+-----------------+-------------+---------------------------------------+\n|     4    |    2 \u55b6\u696d\u65e5     |  8 \u55b6\u696d\u6642\u9593 | 2 \u55b6\u696d\u65e5\u307e\u305f\u306f\u304a\u5ba2\u69d8\u3068\u306e\u5408\u610f\u306b\u57fa\u3065\u304f  |\n+----------+-----------------+-------------+---------------------------------------+\n\n\uff1c \u88fd\u54c1\u30b5\u30dd\u30fc\u30c8\u306e\u30b5\u30fc\u30d3\u30b9\u30ec\u30d9\u30eb\u30a2\u30b0\u30ea\u30fc\u30e1\u30f3\u30c8(SLA) \uff1e\nhttps://access.redhat.com/ja/support/offerings/production/sla\n\uff1c Red Hat \u30b5\u30dd\u30fc\u30c8\u306b\u304a\u3051\u308b\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\u306e\u5b9a\u7fa9 \uff1e\nhttps://access.redhat.com/support/policy/severity\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>Red Hat Global Support Services \u306e\u4e2d\u6839\u3068\u7533\u3057\u307e\u3059\u3002\n\u3053\u306e\u5ea6\u306f\u672c\u30b5\u30dd\u30fc\u30c8\u30b5\u30fc\u30d3\u30b9\u3092\u3054\u5229\u7528\u6234\u304d\u307e\u3057\u3066\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u3059\u3067\u306b\u30cf\u30a4\u30d1\u30fc\u30d0\u30a4\u30b6\u30fc\u304c\u30ab\u30b9\u30bf\u30de\u30fc\u30dd\u30fc\u30bf\u30eb\u306b\u8868\u793a\u3055\u308c\u3066\u304a\u308a\u307e\u3057\u305f\u306e\u3067\u3001virt-who \u306e\n\u8a2d\u5b9a\u306f\u884c\u3063\u3066\u3044\u305f\u3060\u3044\u3066\u3044\u308b\u3068\u304a\u898b\u53d7\u3051\u3044\u305f\u3057\u305f\u304c\u3001\u305d\u306e\u7406\u89e3\u3067\u3088\u308d\u3057\u3044\u3067\u3057\u3087\u3046\u304b\uff1f\n\n\u305d\u306e\u5834\u5408\u3001Red Hat Enterprise Linux for Virtual Datacenters \u306e\u4eee\u60f3\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u304c\n\u6b63\u3057\u304f\u30b5\u30d6\u30b9\u30af\u30e9\u30a4\u30d6\u3055\u308c\u306a\u3044\u5834\u5408\u3001\u4e0b\u8a18\u306e\u72b6\u614b\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u3002\n\n1. \u8a72\u5f53\u306e\u4eee\u60f3\u30b2\u30b9\u30c8\u304c\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u304c\u4ed8\u4e0e\u3055\u308c\u305f\u30cf\u30a4\u30d1\u30fc\u30d0\u30a4\u30b6\u30fc\u4e0a\u3067\u7a3c\u50cd\u3057\u3066\u3044\u306a\u3044\n\u3000\u3000\u4e0b\u8a18\u306e\u30ea\u30f3\u30af\u5148\u3092\u53c2\u7167\u306b\u30ab\u30b9\u30bf\u30de\u30fc\u30dd\u30fc\u30bf\u30eb\u306eUUID\u306e\u30ea\u30b9\u30c8\u3068\u30cf\u30a4\u30d1\u30fc\u30d0\u30a4\u30b6\u30fc\u540d\u3092\u4e00\u81f4\u3055\u305b\u3066\u3001\n\u3000\u3000\u8a72\u5f53\u306e\u4eee\u60f3\u30b2\u30b9\u30c8\u306e\u30cf\u30a4\u30d1\u30fc\u30d0\u30a4\u30b6\u30fc\u306b Red Hat Enterprise Linux for Virtual \n\u3000\u3000\u3000\u3000Datacenters \u306e\u7269\u7406\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u3044\u308b\u306e\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\u3000\u3000*\u3000\u30ab\u30b9\u30bf\u30de\u30fc\u30dd\u30fc\u30bf\u30eb / Satelite 6 \u5185\u306e UUID \u306e\u30ea\u30b9\u30c8\u3068 VMware\u3001RHEV-M \n\u3000\u3000\u3000\u3000\u3000\u3000Host/Hypervisor \u540d\u3092\u4e00\u81f4\u3055\u305b\u78ba\u8a8d\u3059\u308b\u65b9\u6cd5 \n\u3000\u3000\u3000\u3000\u3000\u3000https://access.redhat.com/ja/solutions/1382023\n\n\n2. virt-who \u30b5\u30fc\u30d3\u30b9\u3092\u5c0e\u5165\u3057\u305f\u4eee\u60f3\u30b2\u30b9\u30c8\u304c\u505c\u6b62\u3057\u3066\u3044\u308b/virt-who \u304c\u6b63\u5e38\u306b\u8d77\u52d5\u3057\u3066\u3044\u306a\u3044\n\u3000\u3000\u3000/virt-who \u306b\u306a\u3093\u3089\u304b\u554f\u984c\u304c\u3042\u308b\n\u3000\u3000\u3000- virt-who \u30b5\u30fc\u30d3\u30b9\u3092\u5c0e\u5165\u3057\u305f\u4eee\u60f3\u30b2\u30b9\u30c8\u306f\u8d77\u52d5\u3057\u3066\u3044\u307e\u3059\u304b\uff1f \n\u3000\u3000\u3000- virt-who \u30b5\u30fc\u30d3\u30b9\u306f\u8d77\u52d5\u3057\u3066\u307e\u3059\u304b\uff1f service virt-who status \u3067\u78ba\u8a8d\u3057\u3066\u304f\n\u3000\u3000\u3000\u3060\u3055\u3044\u3002\u3082\u3057\u505c\u6b62\u3057\u3066\u3044\u308b\u5834\u5408\u306f service virt-who start \u3067\u8d77\u52d5\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u3000\u3000\u3000\u3059\u3067\u306b\u8d77\u52d5\u3057\u3066\u3044\u308b\u306b\u3082\u95a2\u308f\u3089\u305a\u554f\u984c\u304c\u89e3\u6c7a\u3057\u306a\u3044\u5834\u5408\u306f\u3001service virt-who restart \n      \u3067\u30b5\u30fc\u30d3\u30b9\u3092\u518d\u8d77\u52d5\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u3000\u3000\u3000- virt-who \u306b\u8a2d\u5b9a\u3057\u305f vCenter \u306e\u60c5\u5831\u306b\u5909\u66f4\u306f\u3042\u308a\u307e\u305b\u3093\u304b\uff1f\n\n\n\u4e0a\u8a18\u78ba\u8a8d\u5f8c\u3001\u30b7\u30b9\u30c6\u30e0\u767b\u9332/\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u306e\u89e3\u9664\u3001\u518d\u30b7\u30b9\u30c6\u30e0\u767b\u9332\u3001\u30b5\u30d6\u30b9\u30af\u30e9\u30a4\u30d6\u3092\u5b9f\u65bd\u304f\u3060\u3055\u3044\u3002\n\u5177\u4f53\u7684\u306a\u64cd\u4f5c\u306f\u4e0b\u8a18\u306b\u306a\u308a\u307e\u3059\u3002\n\n\u30b7\u30b9\u30c6\u30e0\u767b\u9332\u3001\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u306e\u89e3\u9664\n# subscription-manager remove --all\n# subscription-manager unregister\n# subscription-manager clean\n\n\u518d\u30b7\u30b9\u30c6\u30e0\u767b\u9332\u3001\u30b5\u30d6\u30b9\u30af\u30e9\u30a4\u30d6\nhttps://access.redhat.com/ja/articles/1435793 \u306e\uff62\u624b\u9806 #3: RHEL \u30b2\u30b9\u30c8\u3092\u767b\u9332/\u30b5\u30d6\u30b9\n\u30af\u30e9\u30a4\u30d6\uff63\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n\n\u305d\u306e\u5f8c\u3001# subscription-manager list \u306b\u3066\uff62\u72b6\u614b:\u30b5\u30d6\u30b9\u30af\u30e9\u30a4\u30d6\u6e08\u307f\uff63\u3068\u306a\u308a\u3001\uff62\u7d42\u4e86:\uff63\u304c\u9069\u7528\u3055\n\u308c\u305f\u30b5\u30d6\u30b9\u30af\u30ea\u30d7\u30b7\u30e7\u30f3\u306e\u7d42\u4e86\u65e5\u306b\u306a\u3063\u3066\u3044\u308b\u304b\u3054\u78ba\u8a8d\u304f\u3060\u3055\u3044\u3002\n\n\n[\u53c2\u7167\u8cc7\u6599]\n*\u3000virt-who \u304c\u30c0\u30a6\u30f3\u3059\u308b\u3068\u3069\u306e\u3088\u3046\u306a\u554f\u984c\u304c\u767a\u751f\u3057\u307e\u3059\u304b? \n\u3000(\u6587\u66f8\u5185\u306f\u5c11\u3057\u5185\u5bb9\u304c\u53e4\u3044\u70ba\u3001\u6709\u52b9\u671f\u9650\u304c\u4e00\u65e5\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u73fe\u5728\u306f\u4e00\u9031\u9593\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002)\n\u3000\u3000https://access.redhat.com/ja/solutions/2910221\u3000(\u65e5\u672c\u8a9e\u7248)\n\u3000\u3000https://access.redhat.com/solutions/2405341\u3000(\u82f1\u8a9e\u7248\u3000\u6700\u65b0)\n\n\n\u3082\u3057\u4e0a\u8a18\u3092\u5b9f\u65bd\u3044\u305f\u3060\u3044\u3066\u3082\u554f\u984c\u304c\u89e3\u6c7a\u81f4\u3057\u307e\u305b\u3093\u5834\u5408\u3001\u4e0b\u8a18\u306e\u8cc7\u6599\u306e\u63d0\u4f9b\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n- virt-who \u30b5\u30fc\u30d3\u30b9\u3092\u8a2d\u5b9a\u3057\u3066\u3044\u308b\u4eee\u60f3\u30b2\u30b9\u30c8\u306e sosreport\n- \u30b5\u30d6\u30b9\u30af\u30e9\u30a4\u30d6\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u4eee\u60f3\u30b2\u30b9\u30c8\u306e sosreport\n\u3000\u3000*\u3000Red Hat Enterprise Linux 4.6 \u4ee5\u964d\u306b\u304a\u3051\u308b sosreport \u306e\u5f79\u5272\u3068\u53d6\u5f97\u65b9\u6cd5\n\u3000\u3000\u3000\u3000https://access.redhat.com/ja/solutions/78443\n\n-\u3000virt-who \u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u304c /etc/virt-who.d/ \u914d\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u5834\u5408\u3001\n\u3000\u3000/etc/virt-who.d/ \u914d\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u3002\u3082\u3057 /etc/sysconfig/virt-who \u3067\n\u3000\u3000\u8a2d\u5b9a\u3057\u3066\u304a\u308a\u3001\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u5fc5\u8981\u3042\u308a\u307e\u305b\u3093\u3002\n\u3000\u3000virt-who \u306e\u8a2d\u5b9a\u306f\u3069\u3061\u3089\u304b\u4e00\u65b9\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u307f\u3067\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n- virt-who \u8a2d\u5b9a\u3057\u3066\u3044\u308b\u4eee\u60f3\u30b2\u30b9\u30c8\u3068\u30b5\u30d6\u30b9\u30af\u30e9\u30a4\u30d6\u306b\u5931\u6557\u3057\u3066\u3044\u308b\u4eee\u60f3\u30b2\u30b9\u30c8\u306e\u30b3\u30de\u30f3\u30c9\u7d50\u679c\n\u3000# subscription-manager list\n\u3000# subscription-manager list --available\n\u3000# subscription-manager list --consumed\n\n- virt-who \u8a2d\u5b9a\u3057\u3066\u3044\u308b\u4eee\u60f3\u30b2\u30b9\u30c8\u3067\u306e # service virt-who status \u3068 # subscription-manager identity \n\u3000\u306e\u51fa\u529b\u7d50\u679c\n\n\n\n\u672c\u30b1\u30fc\u30b9\u3001\u91cd\u5927\u5ea6 2 \u3092\u3054\u6307\u5b9a\u3044\u305f\u3060\u3044\u3066\u304a\u308a\u307e\u3059\u304c\u3001\u3053\u306e\u91cd\u5927\u5ea6\u306f\u904b\u7528\u4e2d  \n\u306e\u30b7\u30b9\u30c6\u30e0\u306b\u3066\u73fe\u6642\u70b9\u3067\u767a\u751f\u3057\u3066\u3044\u308b\u969c\u5bb3\u306b\u3088\u308b\u30c7\u30fc\u30bf\u640d\u5931\u3084\u640d\u5bb3\u304c\u767a\u751f\u3057  \n\u7d9a\u3051\u308b\u72b6\u6cc1\u3092\u8fc5\u901f\u306b\u56de\u907f\u3059\u308b\u305f\u3081\u306e\u64cd\u4f5c\u65b9\u6cd5\u3084\u56de\u907f\u7b56\u3092\u3054\u6848\u5185\u3059\u308b\u30b9\u30c6\u30fc\u30bf  \n\u30b9\u3067\u3054\u3056\u3044\u307e\u3059\u3002  \n\u304a\u77e5\u3089\u305b\u3044\u305f\u3060\u3044\u305f\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3068\u3053\u308d\u3001\u73fe\u6642\u70b9\u3067\u3054\u6307\u5b9a\u306e\u91cd\u5927\u5ea6\u306b  \n\u76f8\u5f53\u3059\u308b\u72b6\u6cc1\u3068\u306f\u898b\u3089\u308c\u305a\u3001\u304a\u6642\u9593\u3092\u9802\u6234\u3057\u3001\u8abf\u67fb\u3055\u305b\u3066\u3044\u305f\u3060\u304f\u30b9\u30c6\u30fc\u30bf\n\u30b9\u3068\u898b\u3089\u308c\u307e\u3057\u305f\u3002\u3053\u306e\u5834\u5408\u306f\u8fc5\u901f\u306b\u56de\u907f\u65b9\u6cd5\u3092\u3054\u6848\u5185\u3059\u308b\u30b9\u30c6\u30fc\u30bf\u30b9\u3067\u306f\n\u306a\u304f\u304a\u6642\u9593\u3092\u3044\u305f\u3060\u3044\u3066\u8abf\u67fb\u3092\u884c\u3046\u30b9\u30c6\u30fc\u30bf\u30b9\u3092\u898b\u53d7\u3051\u3089\u308c\u3001\u91cd\u5927\u5ea6\u306f 3 \n\u306b\u76f8\u5f53\u81f4\u3057\u307e\u3059\u3002  \n  \n\u91cd\u5927\u5ea6\u306f\u8907\u6570\u306e\u304a\u5ba2\u69d8\u3078\u306e\u516c\u6b63\u306a\u30b5\u30fc\u30d3\u30b9\u306e\u63d0\u4f9b\u3092\u884c\u3046\u305f\u3081\u306b\u3001\u73fe\u5728\u306e\u304a\u5ba2  \n\u69d8\u306e\u30b7\u30b9\u30c6\u30e0\u306e\u72b6\u614b\u306b\u5fdc\u3058\u3066\u5b9a\u3081\u3066\u304a\u308a\u307e\u3059\u3002\u8907\u6570\u306e\u304a\u5ba2\u69d8\u3078\u306e\u516c\u6b63\u306a\u30b5\u30fc  \n\u30d3\u30b9\u63d0\u4f9b\u3092\u884c\u3046\u305f\u3081\u3001\u91cd\u5927\u5ea6\u306e\u9069\u5207\u306a\u6307\u5b9a\u306b\u3064\u3044\u3066\u3054\u7406\u89e3\u3068\u3054\u5354\u529b\u3092\u304a\u9858\u3044  \n\u3044\u305f\u3057\u307e\u3059\u3002  \n  \n\u304a\u5ba2\u69d8\u306e\u30b7\u30b9\u30c6\u30e0\u72b6\u614b\u304c\u3088\u308a\u9ad8\u3044\u91cd\u5927\u5ea6\u306b\u8a72\u5f53\u3057\u3066\u3044\u308b\u3068\u8003\u3048\u3089\u308c\u308b\u5834\u5408\u306f\u3001  \n\u30b7\u30b9\u30c6\u30e0\u304c\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u7528\u9014\u3067\u3042\u308b\u304b\u3001\u30b7\u30b9\u30c6\u30e0\u304c\u4e0e\u3048\u308b\u30d3\u30b8\u30cd\u30b9\u30a4\u30f3  \n\u30d1\u30af\u30c8\u7b49\u3092\u3054\u63d0\u793a\u9802\u3051\u308c\u3070\u91cd\u5927\u5ea6\u3092\u518d\u5ea6\u691c\u8a0e\u3044\u305f\u3057\u307e\u3059\u3002\u91cd\u5927\u5ea6\u306e\u5b9a\u7fa9\u306b  \n\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u306e URL \u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002  \n  \n  https://access.redhat.com/support/policy/severity/  \n\n\n\n\u4ee5\u4e0a\u3001\u5b9c\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>]
ham	[[<description>virt-who\u30b5\u30fc\u30d3\u30b9\u304c\u4ee5\u4e0b\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u51fa\u529b\u3057\u7570\u5e38\u7d42\u4e86\u3057\u307e\u3057\u305f\u3002\n===\u30b7\u30b9\u30c6\u30e0\u30ed\u30b0\u629c\u7c8b===\nNov 21 15:00:08 echtclxrr01cc1z &lt;user.err&gt; /usr/bin/virt-who: [ERROR] @__main__.py:16 - Fatal error: ManagerError: Communication with subscription manager interrupted\nNov 21 15:00:09 echtclxrr01cc1z &lt;user.err&gt; /usr/bin/virt-who: [ERROR] @esx.py:200 - Waiting for ESX events fails: WebFault: Server raised fault: '\\u30bf\\u30b9\\u30af\\u306f\\u3001\\u30e6\\u30fc\\u30b6\\u30fc\\u306b\\u3088\\u3063\\u3066\\u30ad\\u30e3\\u30f3\\u30bb\\u30eb\\u3055\\u308c\\u307e\\u3057\\u305f\\u3002'\nNov 21 15:00:09 echtclxrr01cc1z &lt;daemon.notice&gt; systemd: virt-who.service: main process exited, code=exited, status=1/FAILURE\nNov 21 15:00:09 echtclxrr01cc1z &lt;daemon.notice&gt; systemd: Unit virt-who.service entered failed state.\nNov 21 15:00:09 echtclxrr01cc1z &lt;daemon.warning&gt; systemd: virt-who.service failed.\n=====\n\n\u304a\u5ba2\u69d8\u3082\u4f7f\u7528\u3057\u59cb\u3081\u3066\u3044\u308b\u74b0\u5883\u3067\u767a\u751f\u3057\u305f\u30a8\u30e9\u30fc\u3068\u306a\u308a\u307e\u3059\u306e\u3067\u3001\u5927\u81f3\u6025\u4ee5\u4e0b\u306e\u78ba\u8a8d\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\u30fb\u4e8b\u8c61\u304c\u767a\u751f\u3057\u305f\u539f\u56e0\u306e\u8abf\u67fb\u3092\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n\u30fb\u5fa9\u65e7\u65b9\u6cd5\u3092\u3054\u6559\u6388\u9858\u3044\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</description>], <text>\u5165\u96fb\u3001\u65e9\u3081\u306e\u56de\u7b54\u304c\u307b\u3057\u3044\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u3002</text>, <text>__internal__\n\nNov 21 15:00:09 echtclxrr01cc1z &lt;user.err&gt; /usr/bin/virt-who: [ERROR] @esx.py:200 - Waiting for ESX events fails: WebFault: Server raised fault: '\\u30bf\\u30b9\\u30af\\u306f\\u3001\\u30e6\\u30fc\\u30b6\\u30fc\\u306b\\u3088\\u3063\\u3066\\u30ad\\u30e3\\u30f3\\u30bb\\u30eb\\u3055\\u308c\\u307e\\u3057\\u305f\\u3002'\n\n&gt;\n\nNov 21 15:00:09 echtclxrr01cc1z &lt;user.err&gt; /usr/bin/virt-who: [ERROR] @esx.py:200 - Waiting for ESX events fails: WebFault: Server raised fault: '\u30bf\u30b9\u30af\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u307e\u3057\u305f\u3002'</text>, <text>\u3044\u3064\u3082\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002\n\nRed Hat Global Support Services \u3067\u3059\u3002\n\n\u672c\u30b5\u30dd\u30fc\u30c8\u30b5\u30fc\u30d3\u30b9\u3092\u3054\u5229\u7528\u9802\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u3054\u6307\u5b9a\u9802\u304d\u307e\u3057\u305f\u91cd\u5927\u5ea6\u306e\u5185\u5bb9\u3001\u304a\u3088\u3073\u305d\u308c\u305e\u308c\u306e\u5fdc\u7b54\u6642\u9593\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3068\u306a\u308a\u307e\u3059\u3002\n\u203b\u91cd\u5927\u5ea6\u306f\u62c5\u5f53\u8005\u304c\u9069\u5207\u306a\u30ec\u30d9\u30eb\u306b\u8abf\u6574\u3055\u305b\u3066\u9802\u304f\u5834\u5408\u304c\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u30fb\u91cd\u5927\u5ea61 (\u7dca\u6025)\n\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u7528\u9014\u3067\u306e\u672c\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u4f7f\u7528\u306b\u6df1\u523b\u306a\u5f71\u97ff\u3092\u53ca\u307c\u3059\u554f\u984c (\u4f8b: \u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u7d1b\u5931\u3001\u307e\u305f\u306f\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u30b7\u30b9\u30c6\u30e0\u306e\u6a5f\u80fd\u55aa\u5931)\u3002 \u696d\u52d9\u3092\u505c\u6b62\u305b\u3056\u308b\u3092\u5f97\u306a\u3044\u72b6\u6cc1\u3067\u3001\u56de\u907f\u7b56\u304c\u306a\u3044\u3002\n\n\u30fb\u91cd\u5927\u5ea62 (\u9ad8)\n\u672c\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306f\u6a5f\u80fd\u3057\u3066\u3044\u308b\u304c\u3001\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u7528\u9014\u3067\u306e\u4f7f\u7528\u306b\u91cd\u5927\u306a\u6e1b\u901f\u30fb\u6e1b\u5c11\u3092\u53ca\u307c\u3059\u554f\u984c\u3002\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u7528\u9014\u306b\u304a\u3044\u3066\u3001\u696d\u52d9\u306e\u4e00\u90e8\u306b\u6df1\u523b\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u72b6\u6cc1\u3067\u3001\u56de\u907f\u7b56\u304c\u306a\u3044\u3002\n\n\u30fb\u91cd\u5927\u5ea63 (\u4e2d)\n\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u307e\u305f\u306f\u958b\u767a\u7528\u9014\u306b\u304a\u3044\u3066\u3001\u90e8\u5206\u7684\u3067\u3001\u304b\u3064\u81f4\u547d\u7684\u3067\u306f\u306a\u3044\u652f\u969c\u3092\u3082\u305f\u3089\u3059\u554f\u984c\u3002\u4e2d\u304b\u3089\u4f4e\u7a0b\u5ea6\u306e\u5f71\u97ff\u304c\u3042\u308b\u304c\u3001\u56de\u907f\u7b56\u304c\u3042\u308a\u696d\u52d9\u306f\u7d9a\u3051\u3089\u308c\u308b\u3002\n\n\u30fb\u91cd\u5927\u5ea64 (\u4f4e)\n\u4e00\u822c\u7684\u306a\u4f7f\u7528\u306b\u95a2\u3059\u308b\u8cea\u554f\u3002\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u7528\u9014\u306b\u304a\u3044\u3066\u3001\u5f71\u97ff\u304c\u4f4e\u3044\u3001\u3082\u3057\u304f\u306f\u306a\u3044\u554f\u984c\u3002\u958b\u767a\u7528\u9014\u306b\u304a\u3044\u3066\u3001\u30d3\u30b8\u30cd\u30b9\u306b\u4e2d\u304b\u3089\u4f4e\u7a0b\u5ea6\u306e\u5f71\u97ff\u304c\u3042\u308b\u304c\u3001\u56de\u907f\u7b56\u304c\u3042\u308a\u696d\u52d9\u306f\u7d9a\u3051\u3089\u308c\u308b\u3002\n\n+----------+-----------------+-----------------------------------------------------+\n|\u30d7\u30ed\u30c0\u30af\u30c8|     Standard    |                        Premium                      |\n+----------+-----------------+-------------+---------------------------------------+\n|  \u91cd\u5927\u5ea6  |\u521d\u671f/\u7d99\u7d9a\u5fdc\u7b54\u6642\u9593| \u521d\u671f\u5fdc\u7b54\u6642\u9593|              \u7d99\u7d9a\u5fdc\u7b54\u6642\u9593             |\n+----------+-----------------+-------------+---------------------------------------+\n|     1    |    1 \u55b6\u696d\u6642\u9593   |    1 \u6642\u9593   | 1 \u6642\u9593\u307e\u305f\u306f\u304a\u5ba2\u69d8\u3068\u306e\u5408\u610f\u306b\u57fa\u3065\u304f    |\n+----------+-----------------+-------------+---------------------------------------+\n|     2    |    4 \u55b6\u696d\u6642\u9593   |    2 \u6642\u9593   | 4 \u6642\u9593\u307e\u305f\u306f\u304a\u5ba2\u69d8\u3068\u306e\u5408\u610f\u306b\u57fa\u3065\u304f    |\n+----------+-----------------+-------------+---------------------------------------+\n|     3    |    1 \u55b6\u696d\u65e5     |  4 \u55b6\u696d\u6642\u9593 | 8 \u55b6\u696d\u6642\u9593\u307e\u305f\u306f\u304a\u5ba2\u69d8\u3068\u306e\u5408\u610f\u306b\u57fa\u3065\u304f|\n+----------+-----------------+-------------+---------------------------------------+\n|     4    |    2 \u55b6\u696d\u65e5     |  8 \u55b6\u696d\u6642\u9593 | 2 \u55b6\u696d\u65e5\u307e\u305f\u306f\u304a\u5ba2\u69d8\u3068\u306e\u5408\u610f\u306b\u57fa\u3065\u304f  |\n+----------+-----------------+-------------+---------------------------------------+\n\n\uff1c \u88fd\u54c1\u30b5\u30dd\u30fc\u30c8\u306e\u30b5\u30fc\u30d3\u30b9\u30ec\u30d9\u30eb\u30a2\u30b0\u30ea\u30fc\u30e1\u30f3\u30c8(SLA) \uff1e\nhttps://access.redhat.com/ja/support/offerings/production/sla\n\uff1c Red Hat \u30b5\u30dd\u30fc\u30c8\u306b\u304a\u3051\u308b\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\u306e\u5b9a\u7fa9 \uff1e\nhttps://access.redhat.com/support/policy/severity\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>\u3054\u62c5\u5f53\u8005\u69d8\n\n\u3044\u3064\u3082\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3001Red Hat Global Support Service \u306e\u9234\u6728\u3067\u3059\u3002\n\u3053\u306e\u5ea6\u306f\u672c\u30b5\u30dd\u30fc\u30c8\u30b5\u30fc\u30d3\u30b9\u3092\u3054\u5229\u7528\u6234\u304d\u307e\u3057\u3066\u8aa0\u306b\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u304a\u554f\u3044\u5408\u308f\u305b\u306e\u91cd\u5927\u5ea6\u3092 2 \u3068\u3054\u6307\u5b9a\u3044\u305f\u3060\u3044\u3066\u304a\u308a\u307e\u3059\u304c\u3001\u91cd\u5927\u5ea6 2 \u306e\u554f\u984c\u306f \n\u304a\u5ba2\u69d8\u306e\u30b7\u30b9\u30c6\u30e0\u304c\u6a5f\u80fd\u3057\u306a\u304c\u3089\u73fe\u6642\u70b9\u3067\u305d\u306e\u80fd\u529b\u304c\u5927\u5e45\u306b\u4f4e\u4e0b\u3057\u3066\u3044\u308b\u72b6\u6cc1\u3001 \n\u30b5\u30fc\u30d3\u30b9\u306e\u55aa\u5931\u3084\u4e2d\u65ad\u306e\u5371\u967a\u306b\u3055\u3089\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u7dca\u6025\u3067\u64cd\u4f5c\u65b9\u6cd5\u7b49\u306e\u5bfe\u7b56 \n\u3092\u3054\u6848\u5185\u3059\u308b\u5185\u5bb9\u3068\u306a\u308a\u307e\u3059\u3002 \n\u65e9\u6025\u306b\u91cd\u5927\u5ea6 2 \u306b\u76f8\u5f53\u3059\u308b\u56de\u907f\u5bfe\u5fdc\u3059\u308b\u70ba\u3001\u73fe\u5728\u306e\u88ab\u5bb3\u72b6\u6cc1\u306e\u8a73\u7d30\u3092\u304a\u77e5\u3089\u305b \n\u304f\u3060\u3055\u3044\u307e\u3059\u3088\u3046\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002 \n \n\u306a\u304a\u3001\u672c\u30b1\u30fc\u30b9\u306e\u5f53\u8a72\u30b7\u30b9\u30c6\u30e0\u306e\u52d5\u4f5c\u72b6\u6cc1\u304c\u4e0a\u8a18\u306b\u76f8\u5f53\u3057\u3066\u3044\u306a\u3044\u3001\u3082\u3057\u304f\u306f \n\u73fe\u6642\u70b9\u3067\u30b7\u30b9\u30c6\u30e0\u306e\u52d5\u4f5c\u306b\u5927\u304d\u306a\u5f71\u97ff\u304c\u767a\u751f\u3057\u3066\u3044\u308b\u4e8b\u3092\u3054\u78ba\u8a8d\u3067\u304d\u3066\u3044\u306a\u3044 \n\u5834\u5408\u306b\u306f\u3001\u9069\u5207\u306a\u91cd\u5927\u5ea6\u3078\u5909\u66f4\u3057\u305f\u4e0a\u3067\u5bfe\u5fdc\u3044\u305f\u3057\u307e\u3059\u3002 \n\u4f55\u5352\u3054\u4e86\u627f\u3044\u305f\u3060\u304d\u305f\u304f\u5b58\u3058\u307e\u3059\u3002\n\n\uff1c \u88fd\u54c1\u30b5\u30dd\u30fc\u30c8\u306e\u30b5\u30fc\u30d3\u30b9\u30ec\u30d9\u30eb\u30a2\u30b0\u30ea\u30fc\u30e1\u30f3\u30c8(SLA) \uff1e\nhttps://access.redhat.com/ja/support/offerings/production/sla\n\uff1c Red Hat \u30b5\u30dd\u30fc\u30c8\u306b\u304a\u3051\u308b\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\u306e\u5b9a\u7fa9 \uff1e\nhttps://access.redhat.com/support/policy/severity\n\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>\u3054\u62c5\u5f53\u8005\u69d8\n\n\u3044\u3064\u3082\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3001Red Hat Global Support Service \u306e\u9234\u6728\u3067\u3059\u3002\n\n\n/var/log/messages\nNov 21 15:00:08 echtclxrr01cc1z &lt;user.err&gt; /usr/bin/virt-who: [ERROR] @__main__.py:16 - Fatal error: ManagerError: Communication with subscription manager interrupted\nNov 21 15:00:09 echtclxrr01cc1z &lt;user.err&gt; /usr/bin/virt-who: [ERROR] @esx.py:200 - Waiting for ESX events fails: WebFault: Server raised fault: '\\u30bf\\u30b9\\u30af\\u306f\\u3001\\u30e6\\u30fc\\\nu30b6\\u30fc\\u306b\\u3088\\u3063\\u3066\\u30ad\\u30e3\\u30f3\\u30bb\\u30eb\\u3055\\u308c\\u307e\\u3057\\u305f\\u3002'\nNov 21 15:00:09 echtclxrr01cc1z &lt;daemon.notice&gt; systemd: virt-who.service: main process exited, code=exited, status=1/FAILURE\nNov 21 15:00:09 echtclxrr01cc1z &lt;daemon.notice&gt; systemd: Unit virt-who.service entered failed state.\nNov 21 15:00:09 echtclxrr01cc1z &lt;daemon.warning&gt; systemd: virt-who.service failed.\n\n\nWebFault: Server raised fault: '\\u30bf\\u30b9\\u30af\\u306f\\u3001\\u30e6\\u30fc\\u30b6\\u30fc\\u306b\\u3088\\u3063\\u3066\\u30ad\\u30e3\\u30f3\\u30bb\\u30eb\\u3055\\u308c\\u307e\\u3057\\u305f\\u3002'\n\n\u4e0a\u8a18\u306e\u30a8\u30e9\u30fc\u306f\u6587\u5b57\u5316\u3051\u3057\u3066\u304a\u308a\u307e\u3059\u304c "\u30bf\u30b9\u30af\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u307e\u3057\u305f\u3002" \u3068\u3044\u3046\u6587\u5b57\u3092\u51fa\u529b\u3057\u3066\u304a\u308a\u307e\u3059\u3002\n\u3053\u306e\u30e6\u30fc\u30b6\u30fc\u306f virt-who \u304c\u901a\u4fe1\u3092\u884c\u3063\u3066\u3044\u308b ESX/vCenter \u3068\u306a\u308a\u307e\u3059\u3002\u4e0a\u8a18\u30bf\u30a4\u30df\u30f3\u30b0\u306b\u3066 VMware\u793e \u88fd\u54c1\u304c\n\u901a\u4fe1\u3092\u505c\u6b62\u3055\u305b\u305f\u7406\u7531\u306b\u3064\u3044\u3066\u3054\u78ba\u8a8d\u9802\u3051\u308c\u3070\u3068\u5b58\u3058\u307e\u3059\u3002\n\n\u307e\u305f\u3001virt-who \u304c\u505c\u6b62\u3057\u305f\u4e8b\u306b\u4ed8\u304d\u307e\u3057\u3066\u306f\u5207\u308a\u5206\u3051\u306e\u305f\u3081\u306b virt-who-0.17-11.el7_3 \u304b\u3089\u4ee5\u4e0b\u306e\u6700\u65b0\u7248\u306b\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3057\u3066\u9802\u3051\u308c\u3070\u3068\u5b58\u3058\u307e\u3059\u3002\nhttps://access.redhat.com/downloads/content/rhel---7/x86_64/2456/virt-who/0.19-6.el7_4/noarch/fd431d51/package\n\n\n/etc/sysconfig/virt-who \u30d5\u30a1\u30a4\u30eb\u306b\u8a2d\u5b9a\u9802\u3044\u3066\u3044\u308b VIRTWHO_ESX \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u73fe\u5728\u3067\u306f\u5ec3\u6b62\u3055\u308c\u3066\u304a\u308a\u307e\u3059\u3002\nvirt-who \u304c\u3001Satellite 5\u3001Satellite 6\u3001\u307e\u305f\u306f\u30ab\u30b9\u30bf\u30de\u30fc\u30dd\u30fc\u30bf\u30eb\u306b\u3001UUID \u306e\u4ee3\u308f\u308a\u306b\u30cf\u30a4\u30d1\u30fc\u30d0\u30a4\u30b6\u30fc\u306e\u30db\u30b9\u30c8\u540d\u3092\u5831\u544a\u3059\u308b\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b\nhttps://access.redhat.com/ja/solutions/3015901\n\n\u30ab\u30b9\u30bf\u30de\u30fc\u30dd\u30fc\u30bf\u30eb \u884c\u3092\u53c2\u8003\u306b /etc/virt-who.d \u914d\u4e0b\u306b "\u4efb\u610f\u306e\u540d\u524d.conf" \u30d5\u30a1\u30a4\u30eb\u3092\u8a2d\u5b9a\u3057\u3066\u9802\u3051\u308c\u3070\u3068\u5b58\u3058\u307e\u3059\u3002\n\n\u672c\u3054\u9023\u7d61\u3092\u6301\u3061\u307e\u3057\u3066\u91cd\u5927\u5ea6\u30923\u3078\u5909\u66f4\u3055\u305b\u3066\u9802\u304d\u307e\u3057\u305f\u3002\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>Red Hat Global Support Service )\u9234\u6728\u69d8\n\n\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002\n\u65e5\u7acb\u88fd\u4f5c\u6240)\u5185\u5800\u3067\u3059\u3002\n\n\u672c\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3054\u78ba\u8a8d\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\u56de\u7b54\u9802\u3044\u305f\u5185\u5bb9\u306b\u3064\u304d\u307e\u3057\u3066\u78ba\u8a8d\u3092\u3055\u305b\u3066\u4e0b\u3055\u3044\u3002\n\n(1)\n\u4e0a\u8a18\u306e\u30a8\u30e9\u30fc\u306f\u6587\u5b57\u5316\u3051\u3057\u3066\u304a\u308a\u307e\u3059\u304c "\u30bf\u30b9\u30af\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u307e\u3057\u305f\u3002" \u3068\u3044\u3046\u6587\u5b57\u3092\u51fa\u529b\u3057\u3066\u304a\u308a\u307e\u3059\u3002\n\u3053\u306e\u30e6\u30fc\u30b6\u30fc\u306f virt-who \u304c\u901a\u4fe1\u3092\u884c\u3063\u3066\u3044\u308b ESX/vCenter \u3068\u306a\u308a\u307e\u3059\u3002\u4e0a\u8a18\u30bf\u30a4\u30df\u30f3\u30b0\u306b\u3066 VMware\u793e \u88fd\u54c1\u304c\n\u901a\u4fe1\u3092\u505c\u6b62\u3055\u305b\u305f\u7406\u7531\u306b\u3064\u3044\u3066\u3054\u78ba\u8a8d\u9802\u3051\u308c\u3070\u3068\u5b58\u3058\u307e\u3059\u3002\n\u3000\u21d2\u3000\u4e0a\u8a18\u306b\u3064\u304d\u307e\u3057\u3066\u300cVMware\u793e \u88fd\u54c1\u304c\u901a\u4fe1\u3092\u505c\u6b62\u3055\u305b\u305f\u300d\u3068\u3042\u308a\u307e\u3059\u304c\u3001VMware\u793e\u554f\u5408\u305b\u306e\u305f\u3081\u306b\u5177\u4f53\u7684\u306a\u5185\u5bb9\u3092\u6559\u3048\u3066\u4e0b\u3055\u3044\u3002\n\u3000\u3000\u3000\u3000\u4eca\u56de\u9802\u3044\u305f\u5185\u5bb9\u3060\u3051\u3067VMware\u5074\u306b\u78ba\u8a8d\u3092\u884c\u3044\u307e\u3057\u305f\u304c\u3001\u8a72\u5f53\u6642\u9593\u5e2f\u306e\u30a4\u30d9\u30f3\u30c8\u30ed\u30b0\u4e0a\u3067\u306f\u7570\u5e38\u306f\u767a\u751f\u3057\u3066\u304a\u3089\u305a\u3001virt-who\u306e\u901a\u4fe1\u3068\u601d\u308f\u308c\u308b\u3082\u306e\u304c\n\u3000\u3000\u3000\u3000\u6b63\u5e38\u306b\u30ed\u30b0\u30a4\u30f3\u30fb\u30ed\u30b0\u30a2\u30a6\u30c8\u3057\u305f\u5f62\u8de1\u3057\u304b\u3054\u3056\u3044\u307e\u305b\u3093\u3067\u3057\u305f\u3002\uff08\u9001\u4ed8\u306e\u30a4\u30d9\u30f3\u30c8\u30ed\u30b0\u3092\u53c2\u7167\u9858\u3044\u307e\u3059\uff09\n\u3000\u3000\u3000\u3000\u305d\u306e\u305f\u3081\u3001VMware\u5074\u304b\u3089\u306fvirt-who\u304c\uff56Center\u306b\u5bfe\u3057\u3066\u4f55\u3092\u884c\u3063\u3066\u3044\u304a\u308a\u3001\u4f55\u3092\u78ba\u8a8d\u3059\u308c\u3070\u3088\u3044\u306e\u304b\u6559\u3048\u3066\u307b\u3057\u3044\u3068\u8a00\u308f\u308c\u3066\u304a\u308a\u307e\u3059\u3002\n\n(2)\n\u307e\u305f\u3001virt-who \u304c\u505c\u6b62\u3057\u305f\u4e8b\u306b\u4ed8\u304d\u307e\u3057\u3066\u306f\u5207\u308a\u5206\u3051\u306e\u305f\u3081\u306b virt-who-0.17-11.el7_3 \u304b\u3089\u4ee5\u4e0b\u306e\u6700\u65b0\u7248\u306b\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3057\u3066\u9802\u3051\u308c\u3070\u3068\u5b58\u3058\u307e\u3059\u3002\n\u3000\u21d2\u3000\u5207\u308a\u5206\u3051\u65b9\u6cd5\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u4ee5\u5916\u3067\u4f55\u304b\u5fa1\u5ea7\u3044\u307e\u3059\u3067\u3057\u3087\u3046\u304b\u3002\n\u3000\u3000\u3000\u3000\u304a\u5ba2\u69d8\u304c\u4f7f\u7528\u3057\u59cb\u3081\u3066\u3044\u308b\u74b0\u5883\u3067\u3042\u308b\u4e8b\u304b\u3089\u3001\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u30fb\u30c0\u30a6\u30f3\u30b0\u30ec\u30fc\u30c9\u306b\u3064\u3044\u3066\u7c21\u5358\u306b\u306f\u884c\u3048\u306a\u3044\u305f\u3081\u3001\n\u3000\u3000\u3000\u3000\u5207\u308a\u5206\u3051\u306e\u305f\u3081\u3060\u3051\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3068\u3044\u3046\u4e8b\u3067\u3042\u308c\u3070\u4ed6\u306e\u65b9\u6cd5\u304c\u7121\u3044\u304b\u3054\u78ba\u8a8d\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n(3)\n/etc/sysconfig/virt-who \u30d5\u30a1\u30a4\u30eb\u306b\u8a2d\u5b9a\u9802\u3044\u3066\u3044\u308b VIRTWHO_ESX \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u73fe\u5728\u3067\u306f\u5ec3\u6b62\u3055\u308c\u3066\u304a\u308a\u307e\u3059\u3002\n\u3000\u21d2\u3000\u4e0a\u8a18\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8a2d\u5b9a\u5909\u66f4\u306f\u5fc5\u9808\u3068\u306a\u308a\u307e\u3059\u3067\u3057\u3087\u3046\u304b?\n\u3000\u3000\u3000\u3000\u4ee5\u524d\u306e\u30b5\u30dd\u30fc\u30c8\u30b1\u30fc\u30b9\u3067virt-who\u306e\u8a2d\u5b9a\u3067\u4e0a\u8a18\u306e\u8a71\u304c\u7121\u304b\u3063\u305f\u3053\u3068\u304b\u3089\u8a2d\u5b9a\u3057\u3066\u304a\u3089\u305a\u3001\u307e\u305f\u3001\u304a\u5ba2\u69d8\u3082\u4f7f\u7528\u3057\u59cb\u3081\u3066\u3044\u308b\u74b0\u5883\u3067\u3042\u308b\u4e8b\u304b\u3089\n\u3000\u3000\u3000\u3000\u5f71\u97ff\u5185\u5bb9\u306b\u3088\u3063\u3066\u8a2d\u5b9a\u3092\u884c\u3046\u304b\u6c7a\u3081\u308b\u5fc5\u8981\u304c\u5fa1\u5ea7\u3044\u307e\u3059\u306e\u3067\u3001\u78ba\u8a8d\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n\u306a\u304a\u3001\u539f\u56e0\u306b\u3064\u304d\u307e\u3057\u3066\u306f\u30bf\u30b9\u30af\u304c\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u305f\u3068\u3044\u3046\u4e8b\u3067\u3042\u308c\u3070\u3001virt-who\u30b5\u30fc\u30d3\u30b9\u3092\u518d\u5ea6\u8d77\u52d5\u3059\u308c\u3070\u5fa9\u65e7\u3044\u305f\u3057\u307e\u3059\u3067\u3057\u3087\u3046\u304b\u3002\n\n\u4ee5\u4e0a\u3001\u3054\u78ba\u8a8d\u306e\u7a0b\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>\u3054\u62c5\u5f53\u8005\u69d8\n\n\u3044\u3064\u3082\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3001Red Hat Global Support Service \u306e\u9234\u6728\u3067\u3059\u3002\n\n&gt; (1)\n&gt; \u4e0a\u8a18\u306e\u30a8\u30e9\u30fc\u306f\u6587\u5b57\u5316\u3051\u3057\u3066\u304a\u308a\u307e\u3059\u304c "\u30bf\u30b9\u30af\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u307e\u3057\u305f\u3002" \u3068\u3044\u3046\u6587\u5b57\u3092\u51fa\u529b\u3057\u3066\u304a\u308a\u307e\u3059\u3002\n&gt; \u3053\u306e\u30e6\u30fc\u30b6\u30fc\u306f virt-who \u304c\u901a\u4fe1\u3092\u884c\u3063\u3066\u3044\u308b ESX/vCenter \u3068\u306a\u308a\u307e\u3059\u3002\u4e0a\u8a18\u30bf\u30a4\u30df\u30f3\u30b0\u306b\u3066 VMware\u793e \u88fd\u54c1\u304c\n&gt; \u901a\u4fe1\u3092\u505c\u6b62\u3055\u305b\u305f\u7406\u7531\u306b\u3064\u3044\u3066\u3054\u78ba\u8a8d\u9802\u3051\u308c\u3070\u3068\u5b58\u3058\u307e\u3059\u3002\n\nvirt-who \u306f \u30cf\u30a4\u30d1\u30fc\u30d0\u30a4\u30b6\u30fc\u4e0a\u3067\u52d5\u4f5c\u3057\u3066\u3044\u308b\u30b2\u30b9\u30c8OS\u306e\u60c5\u5831\u3092\u53ce\u96c6\u3057\u3066\u3044\u307e\u3059\u3002\n$ echo -e '\\u30bf\\u30b9\\u30af\\u306f\\u3001\\u30e6\\u30fc\\u30b6\\u30fc\\u306b\\u3088\\u3063\\u3066\\u30ad\\u30e3\\u30f3\\u30bb\\u30eb\\u3055\\u308c\\u307e\\u3057\\u305f\\u3002'\n\u30bf\u30b9\u30af\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u307e\u3057\u305f\u3002\n\u82f1\u6587\u3067\u306f "The task was canceled by a user." \u3068\u51fa\u529b\u3055\u308c\u307e\u3059\u3002\n\n\u3053\u306e\u30a8\u30e9\u30fc\u304c\u51fa\u529b\u3055\u308c\u3066\u3044\u308b\u539f\u56e0\u306f VMware \u88fd\u54c1\u5074\u304c\u63a5\u7d9a\u3092\u62d2\u5426\u3057\u305f\u4e8b\u306b\u3088\u308b\u3082\u306e\u3068\u8aac\u660e\u3055\u305b\u3066\u9802\u304d\u307e\u3057\u305f\u304c\u3001\n\u6b63\u78ba\u306b\u306f\u30bf\u30b9\u30af\u3092\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u305f\u3082\u306e\u3068\u8a02\u6b63\u3055\u305b\u3066\u9802\u304d\u307e\u3059\u3002\u6050\u3089\u304f vmware knowledge base \u306b\u3042\u308b\u4ee5\u4e0b\u306e\u5185\u5bb9\u3068\n\u95a2\u9023\u3059\u308b\u3082\u306e\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u306e\u3067\u3001\u3053\u306e\u70b9\u3092 VMware\u88fd\u54c1\u30b5\u30dd\u30fc\u30c8\u3078\u3054\u78ba\u8a8d\u9802\u308c\u3070\u3068\u5b58\u3058\u307e\u3059\u3002\n\nUsing vCloud Director and importing a virtual machine from vCenter Server fails before 99% with the error: The task was canceled by a user (2016408)\nhttps://kb.vmware.com/s/article/2016408\n\n\n&gt; (2)\n&gt; \u307e\u305f\u3001virt-who \u304c\u505c\u6b62\u3057\u305f\u4e8b\u306b\u4ed8\u304d\u307e\u3057\u3066\u306f\u5207\u308a\u5206\u3051\u306e\u305f\u3081\u306b virt-who-0.17-11.el7_3 \u304b\u3089\u4ee5\u4e0b\u306e\u6700\u65b0\u7248\u306b\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3057\u3066\u9802\u3051\u308c\u3070\u3068\u5b58\u3058\u307e\u3059\u3002\n&gt; \u3000\u21d2\u3000\u5207\u308a\u5206\u3051\u65b9\u6cd5\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u4ee5\u5916\u3067\u4f55\u304b\u5fa1\u5ea7\u3044\u307e\u3059\u3067\u3057\u3087\u3046\u304b\u3002\n&gt; (3)\n&gt; /etc/sysconfig/virt-who \u30d5\u30a1\u30a4\u30eb\u306b\u8a2d\u5b9a\u9802\u3044\u3066\u3044\u308b VIRTWHO_ESX \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u73fe\u5728\u3067\u306f\u5ec3\u6b62\u3055\u308c\u3066\u304a\u308a\u307e\u3059\u3002\n&gt; \u3000\u21d2\u3000\u4e0a\u8a18\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8a2d\u5b9a\u5909\u66f4\u306f\u5fc5\u9808\u3068\u306a\u308a\u307e\u3059\u3067\u3057\u3087\u3046\u304b?\n&gt; \u3000\u3000\u3000\u3000\u4ee5\u524d\u306e\u30b5\u30dd\u30fc\u30c8\u30b1\u30fc\u30b9\u3067virt-who\u306e\u8a2d\u5b9a\u3067\u4e0a\u8a18\u306e\u8a71\u304c\u7121\u304b\u3063\u305f\u3053\u3068\u304b\u3089\u8a2d\u5b9a\u3057\u3066\u304a\u3089\u305a\u3001\u307e\u305f\u3001\u304a\u5ba2\u69d8\u3082\u4f7f\u7528\u3057\u59cb\u3081\u3066\u3044\u308b\u74b0\u5883\u3067\u3042\u308b\u4e8b\u304b\u3089\n&gt; \u3000\u3000\u3000\u3000\u5f71\u97ff\u5185\u5bb9\u306b\u3088\u3063\u3066\u8a2d\u5b9a\u3092\u884c\u3046\u304b\u6c7a\u3081\u308b\u5fc5\u8981\u304c\u5fa1\u5ea7\u3044\u307e\u3059\u306e\u3067\u3001\u78ba\u8a8d\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\u4e0a\u8a18\u306b\u4ed8\u304d\u307e\u3057\u3066\u306f\u78ba\u8a8d\u4e2d\u3068\u306a\u308a\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002\n\n\n\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002</text>, <text>Red Hat Global Support Service )\u9234\u6728\u69d8\n\n\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002\n\u65e5\u7acb\u88fd\u4f5c\u6240)\u5185\u5800\u3067\u3059\u3002\n\n\u672c\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3054\u56de\u7b54\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\u9023\u643a\u9802\u304d\u307e\u3057\u305fVMware\u306e\u60c5\u5831\u3092\u78ba\u8a8d\u3055\u305b\u3066\u9802\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\u307e\u305f\u3001\u6b8b\u308a\u306e(2)\u3001(3)\u306b\u3064\u304d\u307e\u3057\u3066\u3082\u5f15\u304d\u7d9a\u304d\u3054\u78ba\u8a8d\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n1\u4ef6\u60c5\u5831\u306e\u9023\u643a\u3092\u3055\u305b\u3066\u9802\u304d\u307e\u3059\u3002\n\u4e8b\u8c61\u304c\u767a\u751f\u3057\u305fOS\u3067\u3059\u304c\u300122\u65e5\u306e\u591c\u9593\u306b\u304a\u5ba2\u69d8\u4f5c\u696d\u3067OS\u306e\u518d\u8d77\u52d5\u306e\u5b9f\u65bd\u3092\u884c\u3044\u307e\u3057\u305f\u6240\u3001\nOS\u518d\u8d77\u52d5\u5f8c\u306f\u4e00\u5ea6virt-who\u306e\u30a8\u30e9\u30fc\u304c\u8868\u793a\u3055\u308c\u307e\u3057\u305f\u304c\u3001\u305d\u306e\u5f8c\u6b63\u5e38\u306b\u901a\u4fe1\u304c\u884c\u3048\u3066\u3044\u308b\u4e8b\u3092\u78ba\u8a8d\u81f4\u3057\u307e\u3057\u305f\u3002\n\u5ff5\u306e\u305f\u3081\u3001OS\u518d\u8d77\u52d5\u5f8c\u306e\u60c5\u5831\u3082\u9001\u4ed8\u3055\u305b\u3066\u9802\u304d\u307e\u3059\u306e\u3067\u3001\u3054\u53c2\u8003\u306b\u3057\u3066\u9802\u3051\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3001\u3054\u78ba\u8a8d\u306e\u7a0b\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>\u3054\u56de\u7b54\u306b\u304a\u6642\u9593\u3092\u3044\u305f\u3060\u3044\u3066\u304a\u308a\u7533\u3057\u8a33\u3042\u308a\u307e\u305b\u3093\u3002 \n\u73fe\u5728\u3001\u672c\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u306f\u8abf\u67fb\u4e2d\u306e\u72b6\u6cc1\u3067\u3059\u3002\u9032\u5c55\u304c\u3042\u308a\u6b21\u7b2c\u3054\u9023\u7d61\u3044\u305f\u3057\u307e\u3059\u306e\u3067\u3001\u4eca\u66ab\u304f\u304a\u5f85\u3061\u4e0b\u3055\u3044\u3002</text>, <text>Red Hat Global Support Service )\u9234\u6728\u69d8\n\n\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002\n\u65e5\u7acb\u88fd\u4f5c\u6240)\u5185\u5800\u3067\u3059\u3002\n\n\u7d99\u7d9a\u8abf\u67fb\u4e2d\u306e\u3054\u9023\u7d61\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u672c\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3001VMware\u793e\u3067\u78ba\u8a8d\u3092\u884c\u3046\u4e0a\u3067\u4ee5\u4e0b\u306e\u4e8b\u3092\u6559\u3048\u3066\u4e0b\u3055\u3044\u3002\n\n(1)\nvirt-who\u306e\u52d5\u4f5c\u306b\u3064\u3044\u3066\u3001\u30b2\u30b9\u30c8OS\u306e\u60c5\u5831\u3092\u53ce\u96c6\u3057\u3066\u3044\u308b\u3068\u3044\u3046\u4e8b\u3067\u3059\u304c\u3001\u5177\u4f53\u7684\u306a\u5185\u90e8\u306e\u52d5\u4f5c\u3092\u6559\u3048\u3066\u9802\u304f\u4e8b\u306f\u53ef\u80fd\u3067\u3057\u3087\u3046\u304b\u3002\nVMware\u5074\u3067\u8abf\u67fb\u3059\u308b\u306b\u5f53\u305f\u308a\u3001virt-who\u304cESXi/vCenter\u306b\u5bfe\u3057\u3066\u3069\u306e\u3088\u3046\u306aAPI\u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u306e\u304b\u3001\u3069\u306e\u3088\u3046\u306a\u30b3\u30de\u30f3\u30c9\u3092ESXi/vCenter\u4e0a\u3067\u5b9f\u884c\u3057\u60c5\u5831\u3092\u63a1\u53d6\u3057\u3066\u3044\u308b\u306e\u304b\u304c\u5206\u304b\u3089\u306a\u3044\u3068\u8abf\u67fb\u304c\u96e3\u3057\u3044\u3068\u3044\u3046\u72b6\u6cc1\u306b\u306a\u308a\u307e\u3059\u3002\n\u307e\u305f\u3001\u3054\u9023\u643a\u9802\u304d\u307e\u3057\u305f\u60c5\u5831\u306b\u3064\u304d\u307e\u3057\u3066\u306f\u51fa\u529b\u3055\u308c\u3066\u3044\u308b\u30e1\u30c3\u30bb\u30fc\u30b8\u306f\u540c\u3058\u3060\u304c\u3001\u767a\u751f\u4e8b\u8c61\u3068\u306e\u95a2\u9023\u6027\u306f\u7121\u3055\u305d\u3046\u3068\u306e\u4e8b\u306b\u306a\u308a\u307e\u3059\u3002\n\u7533\u3057\u8a33\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u3055\u3089\u306a\u308b\u8abf\u67fb\u3092\u9032\u3081\u308b\u305f\u3081\u306b\u60c5\u5831\u306e\u3054\u63d0\u4f9b\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>/// COLLABO NOTE ///\n\nHi sbr-sysmgmt\n\nPlease see case summary for CU environment.\nvirt-who stopped suddenly and output log.\n\n/var/log/messages\nNov 21 15:00:08 echtclxrr01cc1z &lt;user.err&gt; /usr/bin/virt-who: [ERROR] @__main__.py:16 - Fatal error: ManagerError: Communication with subscription manager interrupted\nNov 21 15:00:09 echtclxrr01cc1z &lt;user.err&gt; /usr/bin/virt-who: [ERROR] @esx.py:200 - Waiting for ESX events fails: WebFault: Server raised fault: '\\u30bf\\u30b9\\u30af\\u306f\\u3001\\u30e6\\u30fc\\u30b6\\u30fc\\u306b\\u3088\\u3063\\u3066\\u30ad\\u30e3\\u30f3\\u30bb\\u30eb\\u3055\\u308c\\u307e\\u3057\\u305f\\u3002'\nNov 21 15:00:09 echtclxrr01cc1z &lt;daemon.notice&gt; systemd: virt-who.service: main process exited, code=exited, status=1/FAILURE\nNov 21 15:00:09 echtclxrr01cc1z &lt;daemon.notice&gt; systemd: Unit virt-who.service entered failed state.\nNov 21 15:00:09 echtclxrr01cc1z &lt;daemon.warning&gt; systemd: virt-who.service failed.\n\n'\\u30bf\\u30b9\\u30af\\u306f\\u3001\\u30e6\\u30fc\\u30b6\\u30fc\\u306b\\u3088\\u3063\\u3066\\u30ad\\u30e3\\u30f3\\u30bb\\u30eb\\u3055\\u308c\\u307e\\u3057\\u305f\\u3002'\nThe task was canceled by a user\n\nvirtwho/virt/esx/esx.py\n184             except (suds.WebFault, HTTPException) as e:\n185                 suppress_exception = False\n186                 try:\n187                     if hasattr(e, 'fault'):\n188                         if e.fault.faultstring == 'The session is not authenticated.':\n189                             # Do not print the exception if we get 'not authenticated',\n190                             # it's quite normal behaviour and nothing to worry about\n191                             suppress_exception = True\n192                         if e.fault.faultstring == 'The task was canceled by a user.':\n193                             # Do not print the exception if we get 'canceled by user',\n194                             # this happens when the wait is terminated when\n195                             # virt-who is being stopped\n196                             continue\n197                 except Exception:\n198                     pass\n199                 if not suppress_exception:\n200                     self.logger.exception("Waiting for ESX events fails:")\n201                 self._cancel_wait()\n202                 version = ''\n203                 self._prepare()\n204                 continue\n\nVmware said, this kcs has nothing to do with this issue.\nhttps://kb.vmware.com/s/article/2016408\n\nWhich API did virt-who use? CU need the answer for research VMWARE it.\n\nThank you for reading\nhasuzuki</text>, <text>\u3054\u56de\u7b54\u306b\u304a\u6642\u9593\u3092\u3044\u305f\u3060\u3044\u3066\u304a\u308a\u7533\u3057\u8a33\u3042\u308a\u307e\u305b\u3093\u3002 \n\u73fe\u5728\u3001\u672c\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u306f\u8abf\u67fb\u4e2d\u306e\u72b6\u6cc1\u3067\u3059\u3002\u9032\u5c55\u304c\u3042\u308a\u6b21\u7b2c\u3054\u9023\u7d61\u3044\u305f\u3057\u307e\u3059\u306e\u3067\u3001\u4eca\u66ab\u304f\u304a\u5f85\u3061\u4e0b\u3055\u3044\u3002</text>, <text>//\u5165\u96fb\nQ. \u89e3\u6c7a\u307e\u3067\u306e\u76ee\u51e6\u3092\u6559\u3048\u3066\u6b32\u3057\u3044\u3002\u51fa\u6765\u308c\u3070\u65e9\u6025\u306b\u5bfe\u5fdc\u3057\u3066\u6b32\u3057\u3044\nA. \u62c5\u5f53\u8005\u306b\u4f1d\u3048\u307e\u3059\u3002\u672c\u30b1\u30fc\u30b9\u306b\u3066\u672c\u65e5\u4e2d\u306b\u4f55\u3089\u304b\u306e\u3054\u9023\u7d61\u306f\u3059\u308b\u4e88\u5b9a\u3068\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002</text>, <text>///</text>, <text>\u3054\u62c5\u5f53\u8005\u69d8\n\n\u3044\u3064\u3082\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3001Red Hat Global Support Service \u306e\u9234\u6728\u3067\u3059\u3002\n\u8abf\u67fb\u306b\u304a\u6642\u9593\u304c\u304b\u304b\u308a\u7533\u3057\u8a33\u3054\u3056\u3044\u307e\u305b\u3093\u3067\u3057\u305f\u3002\n\n&gt; (2)\n&gt; \u307e\u305f\u3001virt-who \u304c\u505c\u6b62\u3057\u305f\u4e8b\u306b\u4ed8\u304d\u307e\u3057\u3066\u306f\u5207\u308a\u5206\u3051\u306e\u305f\u3081\u306b virt-who-0.17-11.el7_3 \u304b\u3089\u4ee5\u4e0b\u306e\u6700\u65b0\u7248\u306b\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3057\u3066\u9802\u3051\u308c\u3070\u3068\u5b58\u3058\u307e\u3059\u3002\n&gt; \u3000\u21d2\u3000\u5207\u308a\u5206\u3051\u65b9\u6cd5\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u4ee5\u5916\u3067\u4f55\u304b\u5fa1\u5ea7\u3044\u307e\u3059\u3067\u3057\u3087\u3046\u304b\u3002\n\n\u4eca\u56de\u767a\u751f\u3057\u3066\u3044\u308b\u4e8b\u8c61\u306f\u65e2\u77e5\u306e\u4e8b\u4f8b\u304c\u307b\u3068\u3093\u3069\u7121\u3044\u7269\u3068\u306a\u308a\u307e\u3059\u3002\n\u540c\u4e00\u4e8b\u8c61\u3068\u601d\u308f\u308c\u308b\u304a\u554f\u3044\u5408\u308f\u305b\u304c\u3082\u30461\u4ef6\u3042\u308a\u307e\u3059\u304c\u3001\n\u4eca\u56de\u304a\u5ba2\u69d8\u304c\u5229\u7528\u3055\u308c\u3066\u3044\u308b virt-who \u3068\u540c\u4e00\u30d0\u30fc\u30b8\u30e7\u30f3\u3068\u306a\u308a\u307e\u3059\u3002\n\nPackage version\n  virt-who-0.17-11.el7_3.noarch                               Tue Sep 26 15:11:09 2017\n  python-rhsm-1.17.9-1.el7.x86_64                             Mon Jun 26 17:16:09 2017\n  subscription-manager-1.17.15-1.el7.x86_64                   Mon Jun 26 17:16:41 2017\n\n\u4e0a\u8a18\u30d1\u30c3\u30b1\u30fc\u30b8\u306f\u95a2\u9023\u3057\u3066\u52d5\u4f5c\u3057\u307e\u3059\u3002\u30d0\u30fc\u30b8\u30e7\u30f3\u30a2\u30c3\u30d7\u306e\u969b\u306b\u4fee\u6b63\u3055\u308c\u305f\u554f\u984c\u306f changelog \u306b\u8a18\u9332\u3057\u3066\u3044\u307e\u3059\u304c\u3001\n\u4fee\u6b63\u3055\u308c\u305f\u30b3\u30fc\u30c9\u306b\u3088\u3063\u3066\u610f\u56f3\u305b\u305a\u306b\u4fee\u6b63\u3055\u308c\u305f\u554f\u984c\u306f\u8a18\u8f09\u3055\u308c\u307e\u305b\u3093\u3002\n\u65e2\u77e5\u306e\u554f\u984c\u306e\u53ef\u80fd\u6027\u3092\u7121\u304f\u3057\u3001\u8abf\u67fb\u304c\u3067\u304d\u308b\u3088\u3046\u4ee5\u4e0b\u6700\u65b0\u30d1\u30c3\u30b1\u30fc\u30b8\u3078\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3092\u518d\u5ea6\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n  subscription-manager-1.19.23-1.el7_4.x86_64.rpm \n  https://access.redhat.com/downloads/content/rhel---7/x86_64/2456/subscription-manager/1.19.23-1.el7_4/x86_64/fd431d51/package\n\n  python-rhsm-1.19.10-1.el7_4.x86_64.rpm\n  https://access.redhat.com/downloads/content/rhel---7/x86_64/2456/python-rhsm/1.19.10-1.el7_4/x86_64/fd431d51/package\n\n  virt-who-0.19-6.el7_4.noarch.rpm\n  https://access.redhat.com/downloads/content/rhel---7/x86_64/2456/virt-who/0.19-6.el7_4/noarch/fd431d51/package\n\n\u4f9d\u5b58\u95a2\u4fc2\u306e\u30a8\u30e9\u30fc\u304c\u51fa\u305f\u5834\u5408\u306f\u3001\u305d\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u3082\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3092\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n\n\u305d\u306e\u4e0a\u3067\u554f\u984c\u304c\u518d\u767a\u3059\u308b\u5834\u5408\u306f\u3001\u4ee5\u4e0b\u306e\u8a2d\u5b9a\u3092\u884c\u3044\u4e8b\u8c61\u767a\u751f\u6642\u306e\u30c7\u30d0\u30c3\u30b0\u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u6709\u52b9\u5316\u3001\u30d1\u30b1\u30c3\u30c8\u30ad\u30e3\u30d7\u30c1\u30e3\u3001sosreport \u306e\u53d6\u5f97\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n1. virt-who \u306e\u30c7\u30d0\u30c3\u30b0\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6709\u52b9\u306b\u3057\u3001\u4ee5\u4e0b\u306e\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3057\u3066\u9802\u304d\u3001\u4e8b\u8c61\u518d\u73fe\u6642\u306b\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3057\u3066\u9802\u3051\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n\n  /etc/sysconfig/virt-who\n  # Enable debugging output.\n  VIRTWHO_DEBUG=1             &lt;== 0 \u304b\u3089 1\u306b\u5909\u66f4\n\n  # systemctl restart virt-who\n\n2. \u4e8b\u8c61\u767a\u751f\u6642\u306e\u901a\u4fe1\u30c7\u30fc\u30bf\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u3001\u4ee5\u4e0b\u30c7\u30fc\u30bf\u306e\u53ce\u96c6\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n   proxy\u30b5\u30fc\u30d0\u30fc 10.11.5.129 \u4e0a\u306b\u3066\u30018080\u756a\u3001443\u756a\u30dd\u30fc\u30c8\u306e\u30d1\u30b1\u30c3\u30c8\u30ad\u30e3\u30d7\u30c1\u30e3\n   ESXi/vCenter \u306b\u3066 443 \u756a\u30dd\u30fc\u30c8\u306e\u30d1\u30b1\u30c3\u30c8\u30ad\u30e3\u30d7\u30c1\u30e3\n\n   \u4eca\u56de\u4e8b\u8c61\u767a\u751f\u3057\u305f\u30b7\u30b9\u30c6\u30e0\u306b\u3066\u3001\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067\u30d1\u30b1\u30c3\u30c8\u3092\u30ad\u30e3\u30d7\u30c1\u30e3\n     # tcpdump -i ens192 -s0 -w /tmp/virt-who.pcap port 443 or 80 or 8080 or icmp or arp\n     /tmp/virt-who.pcap \u306f\u30c7\u30a3\u30b9\u30af\u5bb9\u91cf\u304c\u5145\u5206\u306b\u7a7a\u3044\u3066\u3044\u308b\u9818\u57df\u3092\u6307\u5b9a\u3057\u3066\u4e0b\u3055\u3044\u3002\n\n&gt; (3)\n&gt; /etc/sysconfig/virt-who \u30d5\u30a1\u30a4\u30eb\u306b\u8a2d\u5b9a\u9802\u3044\u3066\u3044\u308b VIRTWHO_ESX \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u73fe\u5728\u3067\u306f\u5ec3\u6b62\u3055\u308c\u3066\u304a\u308a\u307e\u3059\u3002\n&gt; \u3000\u21d2\u3000\u4e0a\u8a18\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8a2d\u5b9a\u5909\u66f4\u306f\u5fc5\u9808\u3068\u306a\u308a\u307e\u3059\u3067\u3057\u3087\u3046\u304b?\n&gt; \u3000\u3000\u3000\u3000\u4ee5\u524d\u306e\u30b5\u30dd\u30fc\u30c8\u30b1\u30fc\u30b9\u3067virt-who\u306e\u8a2d\u5b9a\u3067\u4e0a\u8a18\u306e\u8a71\u304c\u7121\u304b\u3063\u305f\u3053\u3068\u304b\u3089\u8a2d\u5b9a\u3057\u3066\u304a\u3089\u305a\u3001\u307e\u305f\u3001\u304a\u5ba2\u69d8\u3082\u4f7f\u7528\u3057\u59cb\u3081\u3066\u3044\u308b\u74b0\u5883\u3067\u3042\u308b\u4e8b\u304b\u3089\n&gt; \u3000\u3000\u3000\u3000\u5f71\u97ff\u5185\u5bb9\u306b\u3088\u3063\u3066\u8a2d\u5b9a\u3092\u884c\u3046\u304b\u6c7a\u3081\u308b\u5fc5\u8981\u304c\u5fa1\u5ea7\u3044\u307e\u3059\u306e\u3067\u3001\u78ba\u8a8d\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\u73fe\u5728\u3067\u306f\u5ec3\u6b62\u4e88\u5b9a\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u304c\u3001\u4e92\u63db\u6027\u3092\u4fdd\u3064\u305f\u3081\u5229\u7528\u3059\u308b\u3053\u3068\u306f\u53ef\u80fd\u3067\u3059\u3002\n\u73fe\u5728\u76f4\u306b\u554f\u984c\u3092\u5f15\u304d\u8d77\u3059\u3082\u306e\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u5c06\u6765\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u5229\u7528\u3067\u304d\u306a\u304f\u306a\u3063\u305f\u6642\u306e\u305f\u3081\u306b\u5909\u66f4\u3057\u3066\u9802\u304f\u4e8b\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\n\n&gt; 1\u4ef6\u60c5\u5831\u306e\u9023\u643a\u3092\u3055\u305b\u3066\u9802\u304d\u307e\u3059\u3002\n&gt; \u4e8b\u8c61\u304c\u767a\u751f\u3057\u305fOS\u3067\u3059\u304c\u300122\u65e5\u306e\u591c\u9593\u306b\u304a\u5ba2\u69d8\u4f5c\u696d\u3067OS\u306e\u518d\u8d77\u52d5\u306e\u5b9f\u65bd\u3092\u884c\u3044\u307e\u3057\u305f\u6240\u3001\n&gt; OS\u518d\u8d77\u52d5\u5f8c\u306f\u4e00\u5ea6virt-who\u306e\u30a8\u30e9\u30fc\u304c\u8868\u793a\u3055\u308c\u307e\u3057\u305f\u304c\u3001\u305d\u306e\u5f8c\u6b63\u5e38\u306b\u901a\u4fe1\u304c\u884c\u3048\u3066\u3044\u308b\u4e8b\u3092\u78ba\u8a8d\u81f4\u3057\u307e\u3057\u305f\u3002\n&gt; \u5ff5\u306e\u305f\u3081\u3001OS\u518d\u8d77\u52d5\u5f8c\u306e\u60c5\u5831\u3082\u9001\u4ed8\u3055\u305b\u3066\u9802\u304d\u307e\u3059\u306e\u3067\u3001\u3054\u53c2\u8003\u306b\u3057\u3066\u9802\u3051\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n\n/var/log/rhsm/rhsm.log \u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30ed\u30b0\u304c\u51fa\u529b\u3055\u308c\u3066\u3044\u308c\u3070\u3001\u30cf\u30a4\u30d1\u30fc\u30d0\u30a4\u30b6\u30fc\u304b\u3089\n\u30b2\u30b9\u30c8OS\u306e\u60c5\u5831\u3092\u53ce\u96c6\u3067\u304d\u3066\u304a\u308a\u307e\u3059\u306e\u3067\u554f\u984c\u3054\u3056\u3044\u307e\u305b\u3093\u3002\n\n [INFO] @main.py:160 - Using configuration "env/cmdline" ("esx" mode)\n [INFO] @main.py:162 - Using reporter_id='dhcp-217-12.nrt.redhat.com-ba96a401dadc4cba87249c1f62f500ff'\n [INFO] @subscriptionmanager.py:195 - Sending update in hosts-to-guests mapping for config "env/cmdline": 1 hypervisors and 6 guests found\n [INFO] @subscriptionmanager.py:260 - Mapping for config "env/cmdline" updated\n\nESXi/vCenter \u306e\u3069\u306eAPI\u3092\u5229\u7528\u3057\u3066\u3044\u308b\u304b\u306b\u3064\u3044\u3066\u3067\u3059\u304c\u3001\u3053\u3061\u3089\u3082\u540c\u4e8b\u8c61\u3092\u518d\u73fe\u3055\u305b\u3066\u78ba\u8a8d\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3068\n\u601d\u308f\u308c\u307e\u3059\u304c\u3001\u305d\u306e\u969b\u306b\u3069\u306e\u3088\u3046\u306b\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3059\u308c\u3070\u78ba\u8a8d\u3067\u304d\u308b\u304b\u3068\u3044\u3046\u70b9\u3092\u73fe\u5728\u8abf\u67fb\u3057\u3066\u304a\u308a\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002</text>, <text>Red Hat Global Support Service )\u9234\u6728\u69d8\n\n\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002\n\u65e5\u7acb\u88fd\u4f5c\u6240)\u5185\u5800\u3067\u3059\u3002\n\n\u672c\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u8abf\u67fb\u72b6\u6cc1\u306e\u3054\u56de\u7b54\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n\u540c\u4e8b\u8c61\u306e\u518d\u73fe\u3092\u884c\u3063\u3066\u9802\u3044\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u3001virt-who\u304c\u3069\u306e\u3088\u3046\u306a\u30b3\u30de\u30f3\u30c9\u3092ESXi/vCenter\u306b\u5b9f\u884c\u3057\u3066\u3044\u308b\u304b\u3060\u3051\u3067\u3082\n\u5148\u306b\u3054\u9023\u643a\u9802\u304f\u4e8b\u306f\u53ef\u80fd\u3067\u3057\u3087\u3046\u304b\u3002\n\u30b3\u30de\u30f3\u30c9\u304c\u5206\u304b\u308c\u3070\u305d\u3053\u304b\u3089VMware\u793e\u306b\u3082\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u305f\u539f\u56e0\u306b\u3064\u3044\u3066\u8abf\u67fb\u304c\u51fa\u6765\u308b\u3068\u8003\u3048\u3066\u304a\u308a\u307e\u3059\u3002\n\n(2)\u306b\u3064\u304d\u307e\u3057\u3066\u306f\u3001\u304a\u5ba2\u69d8\u304c\u4f7f\u7528\u3092\u958b\u59cb\u3057\u3066\u3044\u308b\u74b0\u5883\u3068\u306a\u308a\u307e\u3059\u306e\u3067\u3001\u7406\u7531\u304c\u5224\u660e\u3057\u78ba\u5b9f\u306b\u4fee\u6b63\u3055\u308c\u308b\u3068\u5206\u304b\u3089\u306a\u3044\u9650\u308a\n\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u306f\u96e3\u3057\u3044\u3068\u601d\u308f\u308c\u307e\u3059\u304c\u3001\u4eca\u5f8c\u306e\u5bfe\u51e6\u65b9\u6cd5\u3068\u3057\u3066\u53c2\u8003\u306b\u3055\u305b\u3066\u9802\u304d\u307e\u3059\u3002\n\n(3)\u306b\u3064\u304d\u307e\u3057\u3066\u306f\u3059\u3050\u306b\u5ec3\u6b62\u3068\u3044\u3046\u308f\u3051\u3067\u306f\u306a\u304f\u4eca\u5f8c\u5ec3\u6b62\u3055\u308c\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3068\u3044\u3046\u4e8b\u627f\u77e5\u3044\u305f\u3057\u307e\u3057\u305f\u3002\n\n\u8d77\u52d5\u5f8c\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u672c\u65e5\u5206\u306e\u30ed\u30b0\u3092\u63a1\u53d6\u3057\u307e\u3057\u305f\u306e\u3067\u5ff5\u306e\u305f\u3081\u3054\u78ba\u8a8d\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>\u3054\u62c5\u5f53\u8005\u69d8\n\n\u3044\u3064\u3082\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3001Red Hat Global Support Service \u306e\u9234\u6728\u3067\u3059\u3002\n\n&gt; \u540c\u4e8b\u8c61\u306e\u518d\u73fe\u3092\u884c\u3063\u3066\u9802\u3044\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u3001virt-who\u304c\u3069\u306e\u3088\u3046\u306a\u30b3\u30de\u30f3\u30c9\u3092ESXi/vCenter\u306b\u5b9f\u884c\u3057\u3066\u3044\u308b\u304b\u3060\u3051\u3067\u3082\n&gt; \u5148\u306b\u3054\u9023\u643a\u9802\u304f\u4e8b\u306f\u53ef\u80fd\u3067\u3057\u3087\u3046\u304b\u3002\n&gt; \u30b3\u30de\u30f3\u30c9\u304c\u5206\u304b\u308c\u3070\u305d\u3053\u304b\u3089VMware\u793e\u306b\u3082\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u305f\u539f\u56e0\u306b\u3064\u3044\u3066\u8abf\u67fb\u304c\u51fa\u6765\u308b\u3068\u8003\u3048\u3066\u304a\u308a\u307e\u3059\u3002\n\u73fe\u5728\u78ba\u8a8d\u4e2d\u3067\u3059\u3002\n\n&gt; \u8d77\u52d5\u5f8c\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u672c\u65e5\u5206\u306e\u30ed\u30b0\u3092\u63a1\u53d6\u3057\u307e\u3057\u305f\u306e\u3067\u5ff5\u306e\u305f\u3081\u3054\u78ba\u8a8d\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\u78ba\u8a8d\u81f4\u3057\u307e\u3057\u305f\u304c\u30b2\u30b9\u30c8OS\u306e\u60c5\u5831\u3092\u30ec\u30dd\u30fc\u30c8\u3057\u3066\u304a\u308a\u3001\u6b63\u5e38\u306b\u52d5\u4f5c\u3057\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u307e\u3059\u3002\nvirt-who \u30b5\u30fc\u30d3\u30b9\u306f\u8907\u6570\u30b7\u30b9\u30c6\u30e0\u3067\u52d5\u4f5c\u3055\u305b\u3066\u9802\u304f\u4e8b\u304c\u63a8\u5968\u3055\u308c\u307e\u3059\u3002\n\u4eca\u56de\u306e\u4e8b\u8c61\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3082\u3044\u305a\u308c\u304b\u306e\u30b7\u30b9\u30c6\u30e0\u304c\u30cf\u30a4\u30d1\u30fc\u30d0\u30a4\u30b6\u30fc\u4e0a\u306e\u30b2\u30b9\u30c8OS\u3092\u5831\u544a\u3059\u308b\u4e8b\u306b\u3088\u308a\u3001\n\u30cf\u30a4\u30d1\u30fc\u30d0\u30a4\u30b6\u30fc\u4e0a\u306e\u30b2\u30b9\u30c8OS\u306f\u6b63\u5e38\u306b\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3092\u3054\u5229\u7528\u9802\u3051\u307e\u3059\u3002\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>**Internal Note**\n\nI think there is more into the issue than virt-who. I saw this permission issue of rhsmcertd in the log:\n\nNov 21 09:49:23 echtclxrr01cc1z &lt;kern.notice&gt; kernel: type=1400 audit(1511225363.931:94): avc:  denied  { sys_admin } for  pid=1076 comm="rhsmcertd" capability=21  scontext=system_u:system_r:rhsmcertd_t:s0 tcontext=system_u:system_r:rhsmcertd_t:s0 tclass=capability  &lt;======== permission or selinux issue\nNov 21 09:49:23 echtclxrr01cc1z &lt;daemon.notice&gt; ntpd[2830]: ntpd exiting on signal 15\nNov 21 09:49:23 echtclxrr01cc1z &lt;authpriv.info&gt; sshd[1560]: Received signal 15; terminating.\nNov 21 09:49:23 echtclxrr01cc1z &lt;daemon.notice&gt; systemd: mcelog.service: main process exited, code=exited, status=15/n/a\nNov 21 09:49:23 echtclxrr01cc1z &lt;daemon.notice&gt; systemd: Unit mcelog.service entered failed state.\nNov 21 09:49:23 echtclxrr01cc1z &lt;daemon.warning&gt; systemd: mcelog.service failed.\nNov 21 09:49:24 echtclxrr01cc1z &lt;kern.notice&gt; kernel: [13342((kill))]: gsch_mount_hook_fn(,/,,84000,          (null)) done\nNov 21 09:49:24 echtclxrr01cc1z &lt;kern.notice&gt; kernel: [13342((kill))]: gsch_mount_hook_fn(/tmp/systemd-private-714fc4cb2ac14f70b70ade8a417f5f4c-httpd.ser,/tmp,,5000,          (null)) done\nNov 21 09:49:24 echtclxrr01cc1z &lt;kern.notice&gt; kernel: [13342((kill))]: gsch_mount_hook_fn(/var/tmp/systemd-private-714fc4cb2ac14f70b70ade8a417f5f4c-httpd,/var/tmp,,5000,          (null)) done\nNov 21 09:49:24 echtclxrr01cc1z &lt;kern.notice&gt; kernel: [13342((kill))]: gsch_mount_hook_fn(,/tmp,,1020,          (null)) done\nNov 21 09:49:24 echtclxrr01cc1z &lt;kern.notice&gt; kernel: [13342((kill))]: gsch_mount_hook_fn(,/var/tmp,,1020,          (null)) done\nNov 21 09:49:24 echtclxrr01cc1z &lt;kern.notice&gt; kernel: [13342((kill))]: gsch_mount_hook_fn(,/,,104000,          (null)) done\nNov 21 09:49:24 echtclxrr01cc1z &lt;user.err&gt; /usr/bin/virt-who: [ERROR] @esx.py:200 - Waiting for ESX events fails: WebFault: Server raised fault: '\\u30bf\\u30b9\\u30af\\u306f\\u3001\\u30e6\\u30fc\\u30b6\\u30fc\\u306b\\u3088\\u3063\\u3066\\u30ad\\u30e3\\u30f3\\u30bb\\u30eb\\u3055\\u308c\\u307e\\u3057\\u305f\\u3002'   &lt;====== this message actually means: task is cancelled by a user (in English) probably due to the wait/interval. Please see case 01860456 comment#33/34</text>, <text>\u5165\u96fb\u3042\u308a\u3002\n\n\u304a\u5ba2\u69d8:\n(2017/11/29 17:07) \u306b\u8cea\u554f\u3057\u305f\u4e0b\u8a18\u5185\u5bb9\u306b\u3064\u3044\u3066\u3060\u3051\u3067\u3082\u512a\u5148\u3057\u3066\n\u9023\u7d61\u3057\u3066\u6b32\u3057\u3044\u3002\n\n---\n\u540c\u4e8b\u8c61\u306e\u518d\u73fe\u3092\u884c\u3063\u3066\u9802\u3044\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u3001virt-who\u304c\u3069\u306e\u3088\u3046\u306a\u30b3\u30de\u30f3\u30c9\u3092ESXi/vCenter\u306b\u5b9f\u884c\u3057\u3066\u3044\u308b\u304b\u3060\u3051\u3067\u3082\n\u5148\u306b\u3054\u9023\u643a\u9802\u304f\u4e8b\u306f\u53ef\u80fd\u3067\u3057\u3087\u3046\u304b\u3002\n\u30b3\u30de\u30f3\u30c9\u304c\u5206\u304b\u308c\u3070\u305d\u3053\u304b\u3089VMware\u793e\u306b\u3082\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u305f\u539f\u56e0\u306b\u3064\u3044\u3066\u8abf\u67fb\u304c\u51fa\u6765\u308b\u3068\u8003\u3048\u3066\u304a\u308a\u307e\u3059\u3002\n---\n\n\u5bfe\u5fdc:\n\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u306b\u3064\u3044\u3066\u306f SLA \u6e96\u62e0\u3067\u3042\u308b\u3053\u3068\u3092\u6848\u5185\u3002\u307e\u305f\u3001\u62c5\u5f53\u8005\u306b\u30ea\u30af\u30a8\u30b9\u30c8\u304c\n\u3042\u3063\u305f\u3053\u3068\u306f\u4f1d\u3048\u308b\u3053\u3068\u306f\u3067\u304d\u308b\u304c\u3001\u3054\u8981\u671b\u306b\u305d\u3048\u308b\u304b\u5426\u304b\u306f\u4e0d\u660e\u3067\u3042\u308b\u65e8\u4f1d\u3048\u3066\n\u3042\u308a\u307e\u3059\u3002\n\n\n\u4ee5\u4e0a\u3067\u3059\u3002</text>, <text>\u3054\u62c5\u5f53\u8005\u69d8\n\n\u3044\u3064\u3082\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3001Red Hat Global Support Service \u306e\u9234\u6728\u3067\u3059\u3002\n\u3054\u9023\u7d61\u9802\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\nvirt-who \u306f SOAP \u3092\u4f7f\u3063\u3066 ESX \u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u3057\u3066\u3044\u307e\u3059\u3002\n\u4f8b\u5916\u304c\u767a\u751f\u3057\u305f\u30dd\u30a4\u30f3\u30c8\u304b\u3089 virt-who \u304c \u30b3\u30fc\u30eb\u3059\u308b\u53ef\u80fd\u6027\u306e\u3042\u308b API \u306f WaitForUpdatesEx \u3067\u3059\u3002\n\u304a\u5ba2\u69d8\u304c\u3054\u5229\u7528\u30d0\u30fc\u30b8\u30e7\u30f3\u306e virt-who \u306f\u8fd4\u3055\u308c\u305f\u5fdc\u7b54\u304c\u6709\u52b9\u306a HTTP \u5fdc\u7b54\u306b\u898b\u3048\u306a\u3044\u5834\u5408\u30b5\u30fc\u30d3\u30b9\u3092\u7d42\u4e86\u3057\u307e\u3059\u304c\n\u6700\u65b0\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u306f\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u305f\u30ed\u30b0\u3092\u51fa\u529b\u3059\u308b\u306e\u307f\u3067\u7d42\u4e86\u3057\u307e\u305b\u3093\u306e\u3067\u3001\u3053\u306e\u3088\u3046\u306a\u89b3\u70b9\u304b\u3089\u3082\u30d0\u30fc\u30b8\u30e7\u30f3\u30a2\u30c3\u30d7\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\n\n- WaitForUpdatesEx ( https://www.vmware.com/support/developer/converter-sdk/conv60_apireference/vmodl.query.PropertyCollector.html#waitForUpdatesEx )\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002</text>, <text>In 01956425 information.\n\n------------------------------------\nHi,\n\nI can provide a few details of the api that virt-who is using.\n\nVirt-who communicates with ESX using SOAP.\nThere are a few methods that virt-who calls in this way (in no meaningful order):\n- WaitForUpdatesEx ( https://www.vmware.com/support/developer/converter-sdk/conv60_apireference/vmodl.query.PropertyCollector.html#waitForUpdatesEx )\n- CancelWaitForUpdates ( https://www.vmware.com/support/developer/converter-sdk/conv60_apireference/vmodl.query.PropertyCollector.html#cancelWaitForUpdates )\n- RetriveServiceContent ( https://pubs.vmware.com/vi3/sdk/ReferenceGuide/vim.ServiceInstance.html#retrieveContent )\n- Login\n- Logout\n- CreateFilter ( https://www.vmware.com/support/developer/converter-sdk/conv60_apireference/vmodl.query.PropertyCollector.html#CreateFilter )\n- DestoryPropertyFilter\n\n\nFrom the point at which the exception is raised, I can tell that the api that virt-who was likely calling was WaitForUpdatesEx.\nFurthermore, it appears that the error handling logic for a WebFaults during that call does not account for translated faultstrings (please note, no version of virt-who known to me does this).\n\n\nWith regards to the early exit, it appears to me that this version of virt-who would only produce such a traceback if there were an issue connecting with Satellite or with RHSM hosted. Specifically if the response that came back did not look like a valid HTTP response. In this version of virt-who, this causes the service to exit. In newer versions the service will not exit any more (only log that the error occurred).\n\n\nHoping this helps,\nChris Snyder</text>, <text>Red Hat Global Support Service )\u9234\u6728\u69d8\n\n\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002\n\u65e5\u7acb\u88fd\u4f5c\u6240)\u5185\u5800\u3067\u3059\u3002\n\nvirt-who\u304c\u547c\u3073\u51fa\u3059\u53ef\u80fd\u6027\u306e\u3042\u308bAPI\u306b\u3064\u304d\u307e\u3057\u3066\u3054\u56de\u7b54\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\nVMware\u793e\u306b\u306f\u4e0a\u8a18API\u306e\u60c5\u5831\u3092\u5143\u306b\u554f\u5408\u305b\u3092\u3055\u305b\u3066\u9802\u304d\u307e\u3059\u3002\n\u307e\u305f\u3001\u5f15\u304d\u3064\u3065\u304d\u5185\u90e8\u3067\u5b9f\u884c\u3057\u3066\u3044\u308b\u30b3\u30de\u30f3\u30c9\u306b\u3064\u304d\u307e\u3057\u3066\u3082\u3054\u78ba\u8a8d\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n\u306a\u304a\u3001\u6700\u65b0\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u306f\u300c\u30a8\u30e9\u30fc\u306f\u51fa\u308b\u304c\u30b5\u30fc\u30d3\u30b9\u306f\u7d42\u4e86\u3057\u306a\u3044\u300d\u3068\u3042\u308a\u307e\u3059\u304c\u3001\n\u30b5\u30fc\u30d3\u30b9\u304c\u7d42\u4e86\u3059\u308b\u3053\u3068\u81ea\u4f53\u304c\u7570\u5e38(virt-who\u306e\u30d0\u30b0?)\u3067\u3042\u3063\u305f\u3068\u3044\u3046\u4e8b\u3067\u3057\u3087\u3046\u304b\u3002\n\u307e\u305f\u3001\u30d0\u30fc\u30b8\u30e7\u30f3\u30a2\u30c3\u30d7\u4ee5\u5916\u3067\u5bfe\u51e6\u3059\u308b\u4e8b\u306f\u53ef\u80fd\u3067\u3057\u3087\u3046\u304b?\n\n\u4ee5\u4e0a\u3001\u3054\u78ba\u8a8d\u306e\u7a0b\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>\u3054\u62c5\u5f53\u8005\u69d8\n\n\u3044\u3064\u3082\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3001Red Hat Global Support Service \u306e\u9234\u6728\u3067\u3059\u3002\n\u3054\u9023\u7d61\u9802\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n&gt; virt-who\u304c\u547c\u3073\u51fa\u3059\u53ef\u80fd\u6027\u306e\u3042\u308bAPI\u306b\u3064\u304d\u307e\u3057\u3066\u3054\u56de\u7b54\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n&gt; VMware\u793e\u306b\u306f\u4e0a\u8a18API\u306e\u60c5\u5831\u3092\u5143\u306b\u554f\u5408\u305b\u3092\u3055\u305b\u3066\u9802\u304d\u307e\u3059\u3002\n&gt; \u307e\u305f\u3001\u5f15\u304d\u3064\u3065\u304d\u5185\u90e8\u3067\u5b9f\u884c\u3057\u3066\u3044\u308b\u30b3\u30de\u30f3\u30c9\u306b\u3064\u304d\u307e\u3057\u3066\u3082\u3054\u78ba\u8a8d\u3092\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\nAPI\u306e\u4e8b\u3092\u30b3\u30de\u30f3\u30c9\u3068\u8868\u73fe\u3055\u308c\u3066\u3044\u305f\u306e\u304b\u3068\u8003\u3048\u3066\u304a\u308a\u307e\u3057\u305f\u304c\u5225\u306e\u4e8b\u3092\u3054\u8cea\u554f\u3055\u308c\u3066\u3044\u3089\u3063\u3057\u3083\u308b\u306e\u3067\u3057\u3087\u3046\u304b\u3002\nvirt-who \u306f ESXi/vCenter \u3078 ssh \u30ed\u30b0\u30a4\u30f3\u3057\u3001ESXi/vCenter \u306e\u30b3\u30f3\u30bd\u30fc\u30eb\u306b\u3066\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\n\u8a33\u3067\u306f\u3054\u3056\u3044\u307e\u305b\u3093\u304c\u3069\u306e\u3088\u3046\u306a\u52d5\u4f5c\u3092\u8a00\u308f\u308c\u3066\u3044\u308b\u306e\u3067\u3057\u3087\u3046\u304b\u3002\n\n&gt; \u306a\u304a\u3001\u6700\u65b0\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u306f\u300c\u30a8\u30e9\u30fc\u306f\u51fa\u308b\u304c\u30b5\u30fc\u30d3\u30b9\u306f\u7d42\u4e86\u3057\u306a\u3044\u300d\u3068\u3042\u308a\u307e\u3059\u304c\u3001\n&gt; \u30b5\u30fc\u30d3\u30b9\u304c\u7d42\u4e86\u3059\u308b\u3053\u3068\u81ea\u4f53\u304c\u7570\u5e38(virt-who\u306e\u30d0\u30b0?)\u3067\u3042\u3063\u305f\u3068\u3044\u3046\u4e8b\u3067\u3057\u3087\u3046\u304b\u3002\n\u6050\u3089\u304f\u304a\u5ba2\u69d8\u306e\u3054\u8a8d\u8b58\u901a\u308a\u304b\u3068\u5b58\u3058\u307e\u3059\u3002\n\u610f\u56f3\u3057\u306a\u3044\u5fdc\u7b54\u304c\u3042\u3063\u305f\u969b\u306b\u30b5\u30fc\u30d3\u30b9\u304c\u505c\u6b62\u3059\u308b\u3079\u304d\u3067\u306f\u7121\u3044\u3068\u8003\u3048\u3089\u308c\u307e\u3059\u3002\n\n&gt; \u307e\u305f\u3001\u30d0\u30fc\u30b8\u30e7\u30f3\u30a2\u30c3\u30d7\u4ee5\u5916\u3067\u5bfe\u51e6\u3059\u308b\u4e8b\u306f\u53ef\u80fd\u3067\u3057\u3087\u3046\u304b?\n\u8a72\u5f53\u306e\u554f\u984c\u304c\u767a\u751f\u3057\u305f\u969b\u306b\u3001ps \u30b3\u30de\u30f3\u30c9\u3067 virt-who \u304c\u52d5\u4f5c\u3057\u3066\u3044\u305f\u3089 kill \u3067\u505c\u6b62\u3057\u3001\nvirt-who \u30b5\u30fc\u30d3\u30b9\u3092\u8d77\u52d5\u3059\u308b\u65b9\u6cd5\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u304c\u3001\u305d\u308c\u4ee5\u5916\u3067\u306f\u30d0\u30fc\u30b8\u30e7\u30f3\u30a2\u30c3\u30d7\u3057\u3066\u9802\u304f\u3057\u304b\u3054\u3056\u3044\u307e\u305b\u3093\u3002\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002</text>, <text>Red Hat Global Support Service)\u9234\u6728\u69d8\n\n\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002\n\u65e5\u7acb\u88fd\u4f5c\u6240)\u5185\u5800\u3067\u3059\u3002\n\n\u672c\u4ef6\u306b\u3064\u304d\u3054\u56de\u7b54\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n&gt; virt-who \u306f ESXi/vCenter \u3078 ssh \u30ed\u30b0\u30a4\u30f3\u3057\u3001ESXi/vCenter \u306e\u30b3\u30f3\u30bd\u30fc\u30eb\u306b\u3066\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\n&gt; \u8a33\u3067\u306f\u3054\u3056\u3044\u307e\u305b\u3093\u304c\u3069\u306e\u3088\u3046\u306a\u52d5\u4f5c\u3092\u8a00\u308f\u308c\u3066\u3044\u308b\u306e\u3067\u3057\u3087\u3046\u304b\u3002\n  \u21d2\u3000vCenter\u306eID/\u30d1\u30b9\u30ef\u30fc\u30c9\u3092\u767b\u9332\u3057\u3066\u52d5\u4f5c\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u5bfe\u8c61\u3078\u30ed\u30b0\u30a4\u30f3\u5f8c\u4f55\u304b\u3057\u3089\u306e\u30b3\u30de\u30f3\u30c9\u3092\u767a\u884c\u3057\u3066\u3044\u308b\u304b\u3068\n\u3000\u3000\u3000 \u60f3\u5b9a\u3057\u3066\u304a\u308a\u307e\u3057\u305f\u304c\u3001\u4eca\u56de\u306e\u56de\u7b54\u3067\u30ed\u30b0\u30a4\u30f3\u3092\u884c\u3044\u30b3\u30de\u30f3\u30c9\u3092\u767a\u884c\u3059\u308b\u52d5\u4f5c\u306f\u3057\u3066\u3044\u306a\u3044\u4e8b\u627f\u77e5\u3044\u305f\u3057\u307e\u3057\u305f\u3002\n\n\u5bfe\u51e6\u65b9\u6cd5\u3068\u3057\u3066\u306f\u4e8b\u8c61\u767a\u751f\u6642\u306b\u30b5\u30fc\u30d3\u30b9\u306e\u518d\u8d77\u52d5\u3092\u884c\u3046\u304b\u3001\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u30a2\u30c3\u30d7\u304c\u5fc5\u8981\u3067\u3042\u308b\u3053\u3068\u627f\u77e5\u3057\u307e\u3057\u305f\u3002\n\u30d0\u30fc\u30b8\u30e7\u30f3\u30a2\u30c3\u30d7\u3092\u884c\u3046\u4e0a\u3067\u4ee5\u4e0b\u306e2\u70b9\u3092\u6559\u3048\u3066\u4e0b\u3055\u3044\u3002\n\u30fbvirt-who\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb(/etc/sysconfig/virt-who)\u306b\u65b0\u898f\u3067\u8a2d\u5b9a\u304c\u5fc5\u8981\u306a\u9805\u76ee\u7b49\u306e\u5dee\u7570\u306f\u3054\u3056\u3044\u307e\u3059\u3067\u3057\u3087\u3046\u304b\u3002\n\u30fb\u5ff5\u62bc\u3057\u78ba\u8a8d\u3068\u306a\u308a\u307e\u3059\u304c\u3001\u3054\u9023\u7d61\u9802\u3044\u305f\u30d1\u30c3\u30b1\u30fc\u30b8\u306fRHEL7.4\u304b\u3089\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u300cvirt-who-0.19-6.el7_4.noarch.rpm \u300d\u3068\n \u898b\u53d7\u3051\u3089\u308c\u307e\u3059\u304c\u3001\u4f9d\u5b58\u30d1\u30c3\u30b1\u30fc\u30b8\u542b\u3081\u3001RHEL7.3\u74b0\u5883(kernel-3.10.0-514.el7.x86_64)\u306b\u5c0e\u5165\u3057\u3066\u3082OS\u3068\u3057\u3066\n \u52d5\u4f5c\u5f71\u97ff\u306f\u3054\u3056\u3044\u307e\u305b\u3093\u3067\u3057\u3087\u3046\u304b\u3002\n\n\u307e\u305f\u3001VMware\u793e\u3078\u4ee5\u524d\u3054\u56de\u7b54\u3044\u305f\u3060\u3044\u305f\u60c5\u5831\u3092\u5143\u306b\u78ba\u8a8d\u3092\u884c\u3044\u307e\u3057\u305f\u304c\u904e\u53bb\u306e\u4e8b\u4f8b\u306f\u7121\u3044\u3068\u306e\u56de\u7b54\u304c\u6765\u3066\u304a\u308a\u307e\u3059\u3002\n======\n\u3054\u5831\u544a\u9802\u3044\u305f\u4e0a\u8ff0\u306e\u60c5\u5831\u304b\u3089 Reahat \u793e\u306e virt-who \u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u3054\u5229\u7528\u6642\u306b WaitForUpdatesEx \u306eAPI Call \u6642\u306b\u300c\u30bf\u30b9\u30af\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u307e\u3057\u305f\u300d \u3067\u51e6\u7406\u304c\u505c\u6b62\u3057\u305f\u3068\u306e\u3053\u3068\u3067\u3059\u304c\u3001\n\u3053\u308c\u3089\u306e\u60c5\u5831\u304b\u3089\u306e\u4e8b\u4f8b\u306f\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\n\u3053\u306e\u300c\u30bf\u30b9\u30af\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u307e\u3057\u305f\u300d \u304c\u51fa\u529b\u3055\u308c\u305f\u3068\u601d\u308f\u308c\u307e\u3059\u304c\u3001\u3053\u3061\u3089\u306f virt-who\u306e\u30ed\u30b0\u60c5\u5831\u307e\u305f\u306f\u4f55\u3089\u304b\u306e\u4ed5\u7d44\u307f\u3067\u3054\u78ba\u8a8d\u9802\u3044\u3066\u3044\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u5185\u5bb9\u304b\u3089\u3059\u308b\u3068\u30e6\u30fc\u30b6\u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u305f\u3088\u3046\u306b\u3082\u898b\u3048\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n\u672c\u4ef6\u306e\u539f\u56e0\u8abf\u67fb\u3092\u884c\u3046\u70ba\u306b\u306f\u3001RedHat\u5074\u3067\u306e\u8abf\u67fb\u7d50\u679c\u304b\u3089 vCenter/ESXi \u30db\u30b9\u30c8\u306e\u72b6\u6cc1\u3092\u8abf\u3079\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n\u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u5185\u5bb9\u304b\u3089\u306f vCenter/ESXi \u30db\u30b9\u30c8\u3068\u306e\u63a5\u7d9a\u306b\u554f\u984c\u304c\u767a\u751f\u3057\u305f\u3068\u306f\u601d\u3048\u307e\u305b\u3093\u306e\u3067\u3001\u307e\u305a\u306f\u3001Redhat \u5074\u304b\u3089\u8abf\u67fb\u9802\u3051\u307e\u3059\u3088\u3046\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\u305d\u306e\u969b\u306b vCenter/ESXi \u30db\u30b9\u30c8\u5074\u3067\u8abf\u67fb\u304c\u5fc5\u8981\u306a\u3088\u3046\u3067\u3057\u305f\u3089\u3001 Redhat\u793e\u304b\u3089\u306e\u5fc5\u8981\u306a\u8abf\u67fb\u5185\u5bb9\u3092\u6559\u3048\u3066\u9802\u3051\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n======\n\u21d2\u3000RedHat\u5074\u3067\u3082\u3053\u308c\u4ee5\u4e0a\u306e\u8abf\u67fb\u3092\u884c\u3046\u5834\u5408\u306f\u3001\u4ee5\u524d\u3054\u56de\u7b54\u3044\u305f\u3060\u304d\u307e\u3057\u305fvirt-who\u30c7\u30d0\u30c3\u30af\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6709\u52b9\u306b\u3057\u3001\n\u3000\u3000 \u4e8b\u8c61\u767a\u751f\u4e2d\u306btcpdump\u306b\u3088\u308b\u60c5\u5831\u53d6\u5f97\u304c\u5fc5\u8981\u3068\u3044\u3046\u8a8d\u8b58\u3067\u76f8\u9055\u3054\u3056\u3044\u307e\u305b\u3093\u3067\u3057\u3087\u3046\u304b\u3002\n\n\u4ee5\u4e0a\u3001\u3054\u78ba\u8a8d\u306e\u7a0b\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>\u3054\u62c5\u5f53\u8005\u69d8\n\n\u3044\u3064\u3082\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3001Red Hat Global Support Service \u306e\u9234\u6728\u3067\u3059\u3002\n\u5fa1\u9023\u7d61\u6234\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n\n&gt; \u5bfe\u51e6\u65b9\u6cd5\u3068\u3057\u3066\u306f\u4e8b\u8c61\u767a\u751f\u6642\u306b\u30b5\u30fc\u30d3\u30b9\u306e\u518d\u8d77\u52d5\u3092\u884c\u3046\u304b\u3001\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u30a2\u30c3\u30d7\u304c\u5fc5\u8981\u3067\u3042\u308b\u3053\u3068\u627f\u77e5\u3057\u307e\u3057\u305f\u3002\n&gt; \u30d0\u30fc\u30b8\u30e7\u30f3\u30a2\u30c3\u30d7\u3092\u884c\u3046\u4e0a\u3067\u4ee5\u4e0b\u306e2\u70b9\u3092\u6559\u3048\u3066\u4e0b\u3055\u3044\u3002\n&gt; \u30fbvirt-who\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb(/etc/sysconfig/virt-who)\u306b\u65b0\u898f\u3067\u8a2d\u5b9a\u304c\u5fc5\u8981\u306a\u9805\u76ee\u7b49\u306e\u5dee\u7570\u306f\u3054\u3056\u3044\u307e\u3059\u3067\u3057\u3087\u3046\u304b\u3002\n&gt; \u30fb\u5ff5\u62bc\u3057\u78ba\u8a8d\u3068\u306a\u308a\u307e\u3059\u304c\u3001\u3054\u9023\u7d61\u9802\u3044\u305f\u30d1\u30c3\u30b1\u30fc\u30b8\u306fRHEL7.4\u304b\u3089\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u300cvirt-who-0.19-6.el7_4.noarch.rpm \u300d\u3068\n&gt;  \u898b\u53d7\u3051\u3089\u308c\u307e\u3059\u304c\u3001\u4f9d\u5b58\u30d1\u30c3\u30b1\u30fc\u30b8\u542b\u3081\u3001RHEL7.3\u74b0\u5883(kernel-3.10.0-514.el7.x86_64)\u306b\u5c0e\u5165\u3057\u3066\u3082OS\u3068\u3057\u3066\n&gt;  \u52d5\u4f5c\u5f71\u97ff\u306f\u3054\u3056\u3044\u307e\u305b\u3093\u3067\u3057\u3087\u3046\u304b\u3002\n\u5f71\u97ff\u306e\u7121\u3044\u3088\u3046\u306b\u4f5c\u3089\u308c\u3066\u304a\u308a\u307e\u3059\u304c\u3001\u3054\u5fc3\u914d\u306a\u3088\u3046\u3067\u3057\u305f\u3089\u691c\u8a3c\u74b0\u5883\u306b\u3066\u3054\u78ba\u8a8d\u9802\u304d\u3001\n\u554f\u984c\u304c\u3042\u308b\u3088\u3046\u3067\u3057\u305f\u3089\u3054\u9023\u7d61\u9802\u3051\u308c\u3070\u30b5\u30dd\u30fc\u30c8\u3055\u305b\u3066\u6234\u304d\u307e\u3059\u3002\n\n&gt; \u307e\u305f\u3001VMware\u793e\u3078\u4ee5\u524d\u3054\u56de\u7b54\u3044\u305f\u3060\u3044\u305f\u60c5\u5831\u3092\u5143\u306b\u78ba\u8a8d\u3092\u884c\u3044\u307e\u3057\u305f\u304c\u904e\u53bb\u306e\u4e8b\u4f8b\u306f\u7121\u3044\u3068\u306e\u56de\u7b54\u304c\u6765\u3066\u304a\u308a\u307e\u3059\u3002\n&gt; ======\n&gt; \u3054\u5831\u544a\u9802\u3044\u305f\u4e0a\u8ff0\u306e\u60c5\u5831\u304b\u3089 Reahat \u793e\u306e virt-who \u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u3054\u5229\u7528\u6642\u306b WaitForUpdatesEx \u306eAPI Call \u6642\u306b\u300c\u30bf\u30b9\u30af\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u307e\u3057\u305f\u300d \u3067\u51e6\u7406\u304c\u505c\u6b62\u3057\u305f\u3068\u306e\u3053\u3068\u3067\u3059\u304c\u3001\n&gt; \u3053\u308c\u3089\u306e\u60c5\u5831\u304b\u3089\u306e\u4e8b\u4f8b\u306f\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\n&gt; \u3053\u306e\u300c\u30bf\u30b9\u30af\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306b\u3088\u3063\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u307e\u3057\u305f\u300d \u304c\u51fa\u529b\u3055\u308c\u305f\u3068\u601d\u308f\u308c\u307e\u3059\u304c\u3001\u3053\u3061\u3089\u306f virt-who\u306e\u30ed\u30b0\u60c5\u5831\u307e\u305f\u306f\u4f55\u3089\u304b\u306e\u4ed5\u7d44\u307f\u3067\u3054\u78ba\u8a8d\u9802\u3044\u3066\u3044\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u5185\u5bb9\u304b\u3089\u3059\u308b\u3068\u30e6\u30fc\u30b6\u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u3066\u30ad\u30e3\u30f3\u30bb\u30eb\u3055\u308c\u305f\u3088\u3046\u306b\u3082\u898b\u3048\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n&gt; \u672c\u4ef6\u306e\u539f\u56e0\u8abf\u67fb\u3092\u884c\u3046\u70ba\u306b\u306f\u3001RedHat\u5074\u3067\u306e\u8abf\u67fb\u7d50\u679c\u304b\u3089 vCenter/ESXi \u30db\u30b9\u30c8\u306e\u72b6\u6cc1\u3092\u8abf\u3079\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n&gt; \u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u5185\u5bb9\u304b\u3089\u306f vCenter/ESXi \u30db\u30b9\u30c8\u3068\u306e\u63a5\u7d9a\u306b\u554f\u984c\u304c\u767a\u751f\u3057\u305f\u3068\u306f\u601d\u3048\u307e\u305b\u3093\u306e\u3067\u3001\u307e\u305a\u306f\u3001Redhat \u5074\u304b\u3089\u8abf\u67fb\u9802\u3051\u307e\u3059\u3088\u3046\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n&gt; \u305d\u306e\u969b\u306b vCenter/ESXi \u30db\u30b9\u30c8\u5074\u3067\u8abf\u67fb\u304c\u5fc5\u8981\u306a\u3088\u3046\u3067\u3057\u305f\u3089\u3001 Redhat\u793e\u304b\u3089\u306e\u5fc5\u8981\u306a\u8abf\u67fb\u5185\u5bb9\u3092\u6559\u3048\u3066\u9802\u3051\u308c\u3070\u3068\u601d\u3044\u307e\u3059\u3002\n&gt; ======\n&gt; \u21d2\u3000RedHat\u5074\u3067\u3082\u3053\u308c\u4ee5\u4e0a\u306e\u8abf\u67fb\u3092\u884c\u3046\u5834\u5408\u306f\u3001\u4ee5\u524d\u3054\u56de\u7b54\u3044\u305f\u3060\u304d\u307e\u3057\u305fvirt-who\u30c7\u30d0\u30c3\u30af\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6709\u52b9\u306b\u3057\u3001\n&gt; \u3000\u3000 \u4e8b\u8c61\u767a\u751f\u4e2d\u306btcpdump\u306b\u3088\u308b\u60c5\u5831\u53d6\u5f97\u304c\u5fc5\u8981\u3068\u3044\u3046\u8a8d\u8b58\u3067\u76f8\u9055\u3054\u3056\u3044\u307e\u305b\u3093\u3067\u3057\u3087\u3046\u304b\u3002\n\n\u4ee5\u524d\u3054\u6848\u5185\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u305f\u5185\u5bb9\u306b\u52a0\u3048\u3066\nvirt-who \u30b5\u30fc\u30d3\u30b9\u306f\u8d77\u52d5\u305b\u305a\u306b\u4e0b\u8a18\u306e\u30b3\u30de\u30f3\u30c9\u306b\u3066\u8d77\u52d5\u3057\u3066\u4e0b\u3055\u3044\u3002\n\n  # /usr/bin/strace -ttfo /root/who.str -s 65535 /usr/bin/virt-who\n\n/root/who.str \u306b\u306f\u5927\u91cf\u306e\u30c7\u30fc\u30bf\u304c\u8a18\u9332\u3055\u308c\u308b\u4e8b\u304c\u4e88\u60f3\u3055\u308c\u307e\u3059\u306e\u3067\u3001\n\u30c7\u30a3\u30b9\u30af\u5bb9\u91cf\u306e\u7a7a\u304d\u304c\u3042\u308b\u5834\u6240\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u4e8b\u8c61\u518d\u73fe\u5f8c\u306b\u4e0a\u8a18\u30c7\u30fc\u30bf\u3001 sosreport \u3092\u53ce\u96c6\u9802\u304d\u3001\u4e8b\u8c61\u767a\u751f\u65e5\u6642\u3092\u3054\u9023\u7d61\u4e0b\u3055\u3044\u3002\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002</text>, <text>\u3054\u62c5\u5f53\u8005\u3000\u69d8 \n \n\u5e73\u7d20\u3088\u308a\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002\u30ec\u30c3\u30c9\u30cf\u30c3\u30c8\u30b0\u30ed\u30fc\u30d0\u30eb\u30b5\u30dd\u30fc\u30c8\u3067 \n\u3054\u3056\u3044\u307e\u3059\u3002 \n \n\u672c\u30b1\u30fc\u30b9\u306b\u3064\u304d\u307e\u3057\u3066\u7d99\u7d9a\u8abf\u67fb\u306e\u305f\u3081\u306b\u8cc7\u6599\u306e\u3054\u63d0\u4f9b\u3092\u304a\u9858\u3044\u3057\u3066\u304a\u308a \n\u307e\u3059\u304c\u305d\u306e\u5f8c\u306e\u72b6\u6cc1\u306f\u3044\u304b\u304c\u3067\u3057\u3087\u3046\u304b\uff1f\u3082\u3057\u60c5\u5831\u53d6\u5f97\u306b\u3057\u3070\u3089\u304f\u6642\u9593 \n\u3092\u8981\u3059\u308b\u898b\u8fbc\u307f\u306e\u5834\u5408\u306b\u306f\u3001\u304a\u624b\u6570\u3067\u306f\u3054\u3056\u3044\u307e\u3059\u304c\u3054\u63d0\u4f9b\u4e88\u5b9a\u306b\u3064\u304d \n\u307e\u3057\u3066\u304a\u77e5\u3089\u305b\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u5e78\u3044\u3067\u3054\u3056\u3044\u307e\u3059\u3002 \n \n\u3082\u3057\u8cc7\u6599\u306e\u3054\u63d0\u4f9b\u304c\u56f0\u96e3\u306a\u5834\u5408\u3084\u6e96\u5099\u306b\u304a\u6642\u9593\u3092\u8981\u3059\u308b\u5834\u5408\u306b\u306f\u3001\u4e00\u65e6 \n\u672c\u30b1\u30fc\u30b9\u3092\u30af\u30ed\u30fc\u30ba\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u3066\u8cc7\u6599\u3092\u3054\u63d0\u4f9b\u3044\u305f\u3060\u3044\u305f\u30bf\u30a4 \n\u30df\u30f3\u30b0\u3067\u518d\u30aa\u30fc\u30d7\u30f3\u3055\u305b\u3066\u3044\u305f\u3060\u304f\u3053\u3068\u3084\u3001\u60c5\u5831\u3092\u53d6\u5f97\u3044\u305f\u3060\u3044\u305f\u969b\u306b \n\u6539\u3081\u3066\u65b0\u898f\u30b1\u30fc\u30b9\u306b\u3066\u304a\u554f\u3044\u5408\u308f\u305b\u3044\u305f\u3060\u304f\u3053\u3068\u3082\u691c\u8a0e\u3044\u305f\u3060\u3051\u307e\u3059\u3068 \n\u5e78\u3044\u3067\u3054\u3056\u3044\u307e\u3059\u3002 \n \n\u5f53\u30b5\u30dd\u30fc\u30c8\u3067\u306f\u304a\u5ba2\u69d8\u304c\u30ab\u30b9\u30bf\u30de\u30fc\u30dd\u30fc\u30bf\u30eb\u3092\u3054\u5229\u7528\u3044\u305f\u3060\u304f\u969b\u306e\u5229\u4fbf \n\u6027\u3092\u5411\u4e0a\u3055\u305b\u3066\u3044\u305f\u3060\u304f\u305f\u3081\u3001\u304a\u554f\u3044\u5408\u308f\u305b\u30ea\u30b9\u30c8\u306e\u6574\u7406\u3092\u63d0\u6848\u3055\u305b\u3066 \n\u3044\u305f\u3060\u3044\u3066\u304a\u308a\u307e\u3059\u3002 \n \n\u306a\u304a\u3001\u304a\u5ba2\u69d8\u5074\u306b\u3066\u7d99\u7d9a\u5bfe\u5fdc\u306e\u5fc5\u8981\u304c\u306a\u304f\u306a\u3063\u305f\u72b6\u6cc1\u306e\u5834\u5408\u306b\u306f\u672c\u30b1\u30fc \n\u30b9\u3092\u30af\u30ed\u30fc\u30ba\u51e6\u7406\u3078\u9032\u3081\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u30ab\u30b9\u30bf\u30de\u30fc\u30dd\u30fc\u30bf\u30eb\u306e\u30b5\u30dd\u30fc\u30c8 \n\u30e1\u30cb\u30e5\u30fc\u306b\u8868\u793a\u3055\u308c\u308b\u304a\u554f\u3044\u5408\u308f\u305b\u30ea\u30b9\u30c8\u306e\u6574\u7406\u3001\u304a\u3088\u3073\u304a\u5ba2\u69d8\u304c\u8cea\u554f \n\u72b6\u6cc1\u3092\u7ba1\u7406\u3057\u6613\u304f\u3059\u3079\u304f\u5bfe\u5fdc\u3055\u305b\u3066\u3044\u305f\u3060\u3051\u308c\u3070\u3068\u5b58\u3058\u307e\u3059\u3002 \n \n\u304a\u5ba2\u69d8\u304b\u3089\u306e\u4eca\u5f8c\u306e\u3054\u8981\u671b\u306b\u3064\u304d\u307e\u3057\u3066\u3054\u9023\u7d61\u3092\u304a\u5f85\u3061\u3059\u308b\u6240\u5b58\u3067\u3054\u3056 \n\u3044\u307e\u3059\u304c\u3001\u3082\u3057\u7d99\u7d9a\u5bfe\u5fdc\u304c\u4e0d\u8981\u306a\u5834\u5408\u3084\u3057\u3070\u3089\u304f\u3054\u9023\u7d61\u3044\u305f\u3060\u304f\u3053\u3068\u304c \n\u3067\u304d\u306a\u3044\u5834\u5408\u306f\u30af\u30ed\u30fc\u30ba\u53ef\u5426\u306e\u305f\u3081\u3060\u3051\u306e\u3054\u9023\u7d61\u306e\u304a\u624b\u9593\u3092\u304b\u3051\u3066\u3057\u307e \n\u3046\u4e8b\u3082\u61f8\u5ff5\u3055\u308c\u307e\u3057\u305f\u306e\u3067\u3001\u672c\u3054\u6848\u5185\u304b\u3089 3 \u55b6\u696d\u65e5\u307b\u3069\u7d4c\u904e\u5f8c\u306b\u81ea\u52d5\u30af \n\u30ed\u30fc\u30ba\u3068\u306a\u308b\u3088\u3046\u306b\u624b\u914d\u3092\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u305f\u304f\u5b58\u3058\u307e\u3059\u3002\u8aa0\u306b\u52dd\u624b\u3068\u306f \n\u5b58\u3058\u307e\u3059\u304c\u3054\u7406\u89e3\u304a\u3088\u3073\u3054\u5354\u529b\u8cdc\u308a\u305f\u304f\u5b58\u3058\u307e\u3059\u3002 \n \n\u3082\u3057\u30b1\u30fc\u30b9\u306e\u30af\u30ed\u30fc\u30ba\u5f8c\u306b\u672c\u30b1\u30fc\u30b9\u3067\u306e\u56de\u7b54\u5185\u5bb9\u306b\u3064\u3044\u3066\u3054\u78ba\u8a8d\u306a\u3055\u308a \n\u305f\u3044\u4e8b\u9805\u306a\u3069\u3054\u3056\u3044\u307e\u3057\u305f\u3089\u3001\u304a\u624b\u6570\u3067\u306f\u3054\u3056\u3044\u307e\u3059\u304c\u672c\u30b1\u30fc\u30b9\u306e\u30b1\u30fc\u30b9 \nID \u3092\u660e\u8a18\u306e\u4e0a\u3001\u65b0\u898f\u30b1\u30fc\u30b9\u3092\u8d77\u7968\u3044\u305f\u3060\u3051\u307e\u3059\u3068\u5e78\u3044\u3067\u3059\u3002 \n \n\u4eca\u5f8c\u3068\u3082\u3069\u3046\u305e\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u7533\u3057\u4e0a\u3052\u307e\u3059\u3002</text>, <text>Red Hat Global Support Service)\u9234\u6728\u69d8\n\n\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u304a\u308a\u307e\u3059\u3002\n\u65e5\u7acb\u88fd\u4f5c\u6240)\u5185\u5800\u3067\u3059\u3002\n\n\u672c\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u8fd4\u4fe1\u304c\u9045\u308c\u7533\u3057\u8a33\u3042\u308a\u307e\u305b\u3093\u3002\n\u7d99\u7d9a\u8abf\u67fb\u306e\u305f\u3081\u306e\u60c5\u5831\u53d6\u5f97\u306b\u3064\u304d\u307e\u3057\u3066\u306f\u3001\u518d\u73fe\u304c\u3044\u3064\u3055\u308c\u308b\u304b\u7b49\u3082\u3054\u3056\u3044\u307e\u3059\u306e\u3067\u3001\n\u4e00\u5ea6\u30af\u30ed\u30fc\u30ba\u3068\u3057\u3066\u9802\u304d\u307e\u3059\u3088\u3046\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002\n\n\u4e8b\u8c61\u306e\u518d\u73fe\u304c\u3055\u308c\u307e\u3057\u305f\u3089\u307e\u305f\u65b0\u898f\u3067\u30b5\u30dd\u30fc\u30c8\u30b1\u30fc\u30b9\u306e\u4f5c\u6210\u3092\u3055\u305b\u3066\u9802\u304d\u307e\u3059\u3002\n\n\u3053\u306e\u305f\u3073\u306f\u3054\u5bfe\u5fdc\u3044\u305f\u3060\u304d\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f\u3002\n\n\u4ee5\u4e0a\u3001\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u81f4\u3057\u307e\u3059\u3002</text>, <text>\u3054\u9023\u7d61\u9802\u304d\u307e\u3057\u3066\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002 \n\u672c\u4ef6\u306f\u30af\u30ed\u30fc\u30ba\u3068\u3055\u305b\u3066\u9802\u304d\u307e\u3057\u305f\u3002 \n\u307e\u305f\u4f55\u304b\u3054\u4e0d\u660e\u306a\u70b9\u306a\u3069\u3054\u3056\u3044\u307e\u3057\u305f\u3089\u65b0\u898f\u30b1\u30fc\u30b9\u306b\u3066\u3054\u9023\u7d61\u304f\u3060\u3055\u3044\u3002 \n \n\u5f0a\u793e\u306b\u304a\u304d\u307e\u3057\u3066\u306f\u3001\u5f0a\u793e\u30b5\u30dd\u30fc\u30c8\u3092\u3054\u5229\u7528\u3044\u305f\u3060\u3044\u305f\u304a\u5ba2\u69d8\u306e\u3054\u610f\u898b\u3001 \n\u3054\u8981\u671b\u3092\u3082\u3068\u306b\u3001\u3055\u3089\u306a\u308b\u30b5\u30dd\u30fc\u30c8\u30b5\u30fc\u30d3\u30b9\u306e\u5411\u4e0a\u3092\u76ee\u6307\u3057\u3066\u304a\u308a\u307e\u3059\u3002 \n\u672c\u30b1\u30fc\u30b9\u306e\u30af\u30ed\u30fc\u30ba\u306b\u4f34\u3044\u3001\u6539\u3081\u307e\u3057\u3066\u30e1\u30fc\u30eb\u306b\u3066\u9867\u5ba2\u6e80\u8db3\u5ea6\u8abf\u67fb\u306b \n\u3064\u3044\u3066\u306e\u3054\u6848\u5185\u3092\u9001\u3089\u305b\u3066\u9802\u304f\u5834\u5408\u304c\u3054\u3056\u3044\u307e\u3059\u3002\u540c\u7a2e\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3092 \n\u53d7\u4fe1\u3055\u308c\u305f\u5834\u5408\u3001\u3054\u591a\u7528\u4e2d\u3068\u306f\u5b58\u3058\u307e\u3059\u304c\u3054\u5354\u529b\u3092\u9802\u3051\u308c\u3070\u5e78\u3044\u3067\u3059\u3002</text>]
ham	[[<description>How would I specify a Role for a user group to only be able to apply patches?</description>], <text>Good Afternoon,\n\nThank you for contacting Red Hat Technical Support. My name is Charlie and I am a member of our Customer Experience &amp; Engagement team for North America.  \n\nI have taken temporary ownership of this case to transition it to an engineer for technical assistance.  I appreciate your patience during this process. When an engineer has been assigned, please allow some time for your case to be reviewed and they will reach out to you with the next steps for moving forward.\n\nThank you for choosing Red Hat!\n\nBest,\n\nCharles Speight | CSS\nCustomer Experience &amp; Engagement\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems.\nLearn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>Hello,\n\nWelcome to Red Hat Technical Support! My name is Renu and I will be assisting you on this case.\n\nBased on the provided description, it is my understanding that you want to know about the role set to do patching and to make effective on a Host group.\n\nIs this correct? If I have misunderstood or omitted details key to your issue, please feel free to update the case with any clarifying points.\n\nPlease try the below role set :\n\nTask Manager\nTask Reader\nView Hosts\nEdit Hosts\n\nFeel free  to get back to us, in case you have any further concerns.\n\n\nBest Regards,\nRenu Chauhan\nAssociate Technical Support Engineer\nRed Hat Global Support services</text>, <text>I have applied the recommended role set and looks like it would be done on an individual basis? So my attempt here is that they be able to patch an entire lifecycle environment?\n\nRoberto Rivera | Senior Datacenter Engineer | Pegasystems Inc. \nOffice: (617) 866.6032 | E-Mail: roberto.rivera@pega.com | www.pega.com\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: Tuesday, October 31, 2017 6:07 AM\nTo: Rivera, Roberto &lt;Roberto.Rivera@pega.com&gt;\nSubject: (WoC) (SEV 3) Case #01961707 (Roles and Permissions) ref:_00DA0HxWH._500A0Yfm2n:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01961707\nCase Title       : Roles and Permissions\nCase Number      : 01961707\nCase Open Date   : 2017-10-27 08:31:21\nSeverity         : 3 (Normal)\nProblem Type     : Usage / Documentation Help\n\n---------------------------------------\n\nHello Roberto Rivera,\n\nIt has been 3 days since we requested additional information on your inquiry.  Please review previous case emails and/or case history, and provide the requested information to allow us to continue solving this issue.\n\nRegards,\nRed Hat Customer Experience and Engagement\n\nNote: This is an automated response.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Yfm2n:ref</text>, <text>Hello,\n\nI am checking for the entire lifecycle environment on my end. \n\nPlease allow me sometime to further update you on this.\n\nBest Regards,\nRenu Chauhan\nAssociate Technical Support Engineer\nRed Hat Global Support services</text>, <text>Hello,\n\nGreetings of the day ..!\n\nWhile checking into this, yes the given role set is only for the individual patching.\n\n\nBest Regards,\nRenu Chauhan\nAssociate Technical Support Engineer\nRed Hat Global Support services</text>, <text>Ok, so how do I achieve the alternative?\n\nRoberto Rivera | Senior Datacenter Engineer | Pegasystems Inc. \nOffice: (617) 866.6032 | E-Mail: roberto.rivera@pega.com | www.pega.com\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: Monday, November 6, 2017 10:27 PM\nTo: Rivera, Roberto &lt;Roberto.Rivera@pega.com&gt;\nSubject: (WoRH) (SEV 3) Case #01961707 (Roles and Permissions) ref:_00DA0HxWH._500A0Yfm2n:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01961707\nCase Title       : Roles and Permissions\nCase Number      : 01961707\nCase Open Date   : 2017-10-27 08:31:21\nSeverity         : 3 (Normal)\nProblem Type     : Usage / Documentation Help\n\nMost recent comment: On 2017-11-06 22:26:58, Chauhan, Renu commented:\n"Hello,\n\nGreetings of the day ..!\n\nWhile checking into this, yes the given role set is only for the individual patching.\n\n\nBest Regards,\nRenu Chauhan\nAssociate Technical Support Engineer\nRed Hat Global Support services"\n\nhttps://access.redhat.com/support/cases/#/case/01961707?commentId=a0aA000000Kwa7kIAB\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Yfm2n:ref</text>, <text>Best Regards,\nRenu Chauhan\nAssociate Technical Support Engineer\nRed Hat Global Support services</text>, <text>There is nothing in the case notes? I mean what is "Best Regards," Was that a part of the signature and was there supposed to be an attachment?\n\nRoberto Rivera | Senior Datacenter Engineer | Pegasystems Inc. \nOffice: (617) 866.6032 | E-Mail: roberto.rivera@pega.com | www.pega.com\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: Saturday, November 11, 2017 6:05 AM\nTo: Rivera, Roberto &lt;Roberto.Rivera@pega.com&gt;\nSubject: (WoC) (SEV 3) Case #01961707 (Roles and Permissions) ref:_00DA0HxWH._500A0Yfm2n:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01961707\nCase Title       : Roles and Permissions\nCase Number      : 01961707\nCase Open Date   : 2017-10-27 08:31:21\nSeverity         : 3 (Normal)\nProblem Type     : Usage / Documentation Help\n\n---------------------------------------\n\nHello Roberto Rivera,\n\nIt has been 3 days since we requested additional information on your inquiry.  Please review previous case emails and/or case history, and provide the requested information to allow us to continue solving this issue.\n\nRegards,\nRed Hat Customer Experience and Engagement\n\nNote: This is an automated response.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Yfm2n:ref</text>, <text>Hello,\n\nGreetings from Red Hat Technical Support. My name is Hradayesh Shukla and I will assist you with this case. \nSincere apologies for the delay, I appreciate your patience. \nI understand that you wish to assign a role to a user to be able to patch all the systems in a Host Collection. \n\n-&gt; Do you have a specific list of hosts or a host collection on which you wish to apply patches? \nPlease provide the details by sharing the output of following commands:\n# hammer user list                                                    (this command will provide output of all users)\n# hammer user info --login &lt;username&gt;          (replace "username" with the username you wish to assign the role)\n\nNext you may go to Hosts =&gt; Host Collection =&gt; Under "Collection Actions"  Tab =&gt; If you are trying to install an Errata, click on "Errata Installation" \n=&gt; Under the "Content Host Content" tab, you can select the Lifecycle Environment and its respective Content View.  \n\n\nThanks &amp; regards,\nHradayesh Shukla\nAssociate Technical Support Engineer, RHCE\nGlobal Support Services, \nRed Hat India Pvt Ltd</text>, <text>I have created a group of Hosts Collection and Lifecycle Environments to the same DEV, QA, PRD. Below is the output of each command\nhammer user list:\n---|----------------|------------------|--------------------------\nID | LOGIN          | NAME             | EMAIL                    \n---|----------------|------------------|--------------------------\n3  | admin          | Admin User       | dcops@pega.com           \n10 | hajda          | Arkadiusz Hajduk | Arkadiusz.Hajduk@pega.com\n7  | johnd1         | David Johnson    | David.Johnson@pega.com   \n11 | esxtest        | esx test         | test1@test.com           \n6  | alyo1          | Omar Aly         | Omar.Aly@pega.com        \n9  | lingp          | Philip Ling      | Philip.Ling@pega.com     \n5  | river          | Roberto Rivera   | Roberto.Rivera@pega.com  \n8  | virt-who-admin | Virt-Who Admin   |                          \n---|----------------|------------------|--------------------------\n\n\nhammer user info --login esxtest\nId:                   11\nLogin:                esxtest\nName:                 esx test\nEmail:                test1@test.com\nAdmin:                no\nAuthorized by:        rpega.com\nLocale:               default\nTimezone:             \nLast login:           2017/11/03 15:34:50\nDefault organization: \nDefault location:     \nRoles:                \n    Anonymous\n    Edit hosts\n    Tasks Manager\n    Tasks Reader\n    View hosts\nUser groups:          \n    Satellite Patch Users\nLocations:            \n    Cambridge\nOrganizations:        \n    Pegasystems\nCreated at:           2017/11/03 15:28:14\nUpdated at:           2017/11/03 15:34:50</text>, <text>Hi Roberto,\n\nGreetings for the day. I have reviewed the update. I would like to know more about the patching task that is being attempted.  \nAre you trying to use bulk action through katello-agent, or by remote execution or goferd-less? This information is required to help you with the roles that further need to be added.   \nPlease do let me know if I have understood your problem properly or not. \n\nThanks &amp; regards,\nHradayesh Shukla\nAssociate Technical Support Engineer, RHCE\nGlobal Support Services, \nRed Hat India Pvt Ltd</text>, <text>Basically we have a group of individuals in which we will be handing off the patching of the hosts and want to ensure that they have the ability to perform this action, but not grant them full priviledge of the environment.</text>, <text>Hi Roberto,\n\nGreetings for the day. I have reviewed the update.  \nIf you wish to patch all the servers within an environment of the Host Collections, then a filter of Host Collections is required within the role. \n\nIf the same is not added, to add this filter to an existing role, \n1. Login using the admin account.\n2. Go to Administer -&gt; Roles -&gt; Edit an existing Role (which is assigned to the user)-&gt; \n3. Select the Role -&gt;  Click Add filter -&gt; Under "Resource type", choose "Host Collections" -&gt; Add all the permissions -&gt; Submit\n4. Go to users and assign this role to the user. \n5. Login with the user and try to perform patching. \n\nIf you are still facing issue, please attach a screenshot with details of error &amp; roles,filters selected for the user. \n\nThanks &amp; regards,\nHradayesh Shukla\nAssociate Technical Support Engineer, RHCE\nGlobal Support Services, \nRed Hat India Pvt Ltd</text>, <text>Added the filter to the role of the user. And after selection host collection actions for ERRATA installation it continues to spin and an error that a host needs to be selected.</text>, <text>Hello Roberto,\n\nThanks for your update.  \n\nTo understand more about your issue, can you please share following details: \n1. Screen-shot of this error. This is required to know exactly where you are facing the error. \n\n2. Confirm the version (major &amp; minor)  of Satellite Server which you are using? eg: Red Hat Satellite 6.2.11\n\n3. Details of the user role &amp; filter used using following command: \n# hammer user info --login esxtest\n\n4. Please confirm if content hosts are added in the "Host Collection".  \nTo create Host Collection and to add hosts to an Host Collection, go to "Hosts" -&gt; "Host Collection" -&gt; New -&gt; Fill the details -&gt; Select the hosts you want to add in this collection.\nYou can then continue to patch the hosts. \n\nThanks &amp; regards,\nHradayesh Shukla\nTechnical Support - Red Hat</text>, <text>I have attached 4 jpg for the 4 questions.</text>, <text>Hello Roberto,\n\nGreetings from Red Hat Technical Support. Thanks for your update.\nI see that in the screenshot, while patching using the Host Collections method, you are facing the issue. \n\nTo resolve this we have to add following filters to any existing role assigned to the user:\nFor your convenience, I have provided the below table with Resource &amp; Filter details:\n\n  Resource                                     Filters\nContent Host\t                        view_content_hosts\t\nEnvironment\t                         view_environments\t\t\nHost\t                                   view_hosts\t\t \nHost Collections\t                  view_host_collections\t\t\nHost Group\t                          view_hostgroups\t\t\nJob invocation\t               create_job_invocations, view_job_invocations\t\t\nLifecycle Environment\t        view_lifecycle_environments\t\nLocation\t                            view_locations\t\t\nOrganization\t                         view_organizations\t\t\n\nPlease ignore some already added filters. \nUsing the above, the error we encountered must go away.\n\nThanks &amp; regards,\nHradayesh Shukla\nTechnical Support - Red Hat</text>, <text>Thanks this works, can you send me of what exact filters are needed to be able to lock down group to only be able to login and apply errata from content view, without being able to modify anything else?</text>, <text>Hello Roberto,\n\nGreetings from Red Hat Technical Support. Thanks for your update. Glad to know that the solution worked for you. \n\nCan you please send screen-shots of what users are able to view/edit currently &amp; what changes do you expect? \n\n\nThanks &amp; regards,\nHradayesh Shukla\nTechnical Support - Red Hat</text>, <text>Basically I would simply want the user to login and only be able to patch systems after I have promoted a lifecycle environment to the new published content view.</text>, <text>Hello Roberto, \n\nThanks for your update. \nPlease use below roles &amp; permissions using which the specific user will only be able to install errata on the Host Collections after you promote it to Content View. \n\nEdit Hosts \nView Hosts \nTask Manager \nTask Reader \nHost Collections\t(view_host_collections)\t\t\nLocation\t        (view_locations)\t\t\nOrganization\t        (view_organizations)\t\t\nHost Collections\t(edit_host_collections)\n\nPlease let me know if you still face issue. A screenshot would help if you do not get the desired results. \n\nThanks &amp; regards,\nHradayesh Shukla\nTechnical Support - Red Hat</text>, <text>Looks like these permissions will work. Please archive case, if we run into any issues we will create new case. Thanks</text>, <text>Hello Roberto, \n\nGreetings for the day! \nThank you for the  update. With your permission I am closing this case. Upon closing this case, you may receive an email asking for your feedback. \n\nWe would appreciate you taking a few minutes to fill up the survey and provide your valuable feedback. \nIt will help us at Red Hat to assist you better in the future.\n\nI am archiving this case for now. You can update this same case if you have any difficulty in future related to Roles. This will help save time for resolution. \n\nThanks &amp; regards,\nHradayesh Shukla\nTechnical Support - Red Hat</text>]
ham	[[<description>ref. case 01952296\n\nNov. 6 11:52:\n\nHi, we plan to move the VMs to their own storage domains as per Javier's instructions below and update the queue-depth as per Jon's instructions below on November 8th, 2017 starting at 3AM EST to 11:59PM EST. The plan is to do one VM at a time, do the disk storage migration, then test, then update the queue-depth on the host for that particular volume on all the compute hosts, then test again.\n\nCan we have someone in RedHat support on standby for this ticket just in case we run into any issues?\n\nThanks,\nSonya Grewal\n647-456-5392</description>], <text>Thank you for contacting Red Hat Technical Support. My name is Shivam and I will be working on this case with you let us know if any assistance is needed. \n\nI understand that you are planning to do storage migration on RHEL4.1 from 3am-11:59pm EST on 8/11/17.  Thank you for proactively providing the time frame you have planned, and contact information for you. \n \nAs a proactive measure, please upload the sos-report of the servers or any of the server that are involved in the migration. \n\nIf you encounter any issues, please provide the following information:\n\n A description of the issue, including any steps taken thus far and the expected vs. observed outcome.\n\n- What is the current state of the impacted system? Is it up and running?\n\n- What is the present production impact?\n\n- Any screenshots you have taken to illustrate this issue.\n\n- What are the complete error messages/outputs you are receiving?\n\n\n- A log collector: \n\n    How to collect logs in RHEV 3 and RHV 4\n    https://access.redhat.com/solutions/61546\n\n- An sosreport from the affected system(s):\n\n    What is a sosreport and how to create one in Red Hat Enterprise Linux 4.6 and later? \n    https://access.redhat.com/solutions/3592\n\nIf you have any questions or further details to provide, feel free to update the case\n\n\nRegards,\nShivam Gupta\nRed Hat Global Support Service</text>, <text>FYI, attached are all the SOS reports from the physical hosts and the RHEVM engine. Thanks.</text>, <text>Hello \n\nThanks for your update on this, we will be standby in this request do let us know if you face any issues.\n\n\nRegards,\nShivam Gupta\nRed Hat Global Support Service</text>, <text>Hi, I'm Allan. I'm monitoring this case during North American operating hours.\n\nLet me know how I can assist.\n\nThanks,\nAllan Voss, RHCE, RHCVA\nSenior Technical Support Engineer\nRed Hat, Inc.\n\nIf your systems are registered on Red Hat Network (RHN), visit https://access.redhat.com/products/red-hat-subscription-management/  to learn why you need to migrate to the Red Hat Subscription Management (RHSM) interface by July 2017.\n\nRHEV 3.x series was retired on September 30, 2017:\nhttps://access.redhat.com/support/policy/updates/rhev\n\nPreparing to upgrade? Try the RHEV Upgrade Helper tool:\nhttps://access.redhat.com/labs/rhevupgradehelper/\n\nHave you configured regular database backups? \nhttps://access.redhat.com/solutions/61993  (RHEV 3.0 to 3.2)\nhttps://access.redhat.com/solutions/797463 (RHEV 3.3 to 3.6)\n\nIf you're using Self Hosted Engine, perform you database backups as per the Hosted Engine Guide:\nhttps://access.redhat.com/documentation/en-us/red_hat_virtualization/4.1/html/self-hosted_engine_guide/chap-backing_up_and_restoring_a_rhel-based_self-hosted_environment</text>, <text>Hi Allan,\nAre you able to setup bomgar right now? I see the 100% CPU on the VMs. Before I powercycle the VMs to restart it, do you want to take a look? I see the SPM is contending and the datacenter status is contending. I'm grabbing sosreport from one of the hypervisors. I can't do kdump of the guest VM because kdump is disabled on all the compute hosts still.\n\nLet me know.\nThanks,\nSonya</text>, <text>Hi,\n\nHere's the remote session information:\n\nSession Key: 9784833\n\nURL: https://remotesupport.redhat.com/?ak=c99a0507c4b900ee2dfc74472c49a8f2\n\nThanks,\nAllan Voss, RHCE, RHCVA\nSenior Technical Support Engineer\nRed Hat, Inc.\n\nIf your systems are registered on Red Hat Network (RHN), visit https://access.redhat.com/products/red-hat-subscription-management/  to learn why you need to migrate to the Red Hat Subscription Management (RHSM) interface by July 2017.\n\nRHEV 3.x series was retired on September 30, 2017:\nhttps://access.redhat.com/support/policy/updates/rhev\n\nPreparing to upgrade? Try the RHEV Upgrade Helper tool:\nhttps://access.redhat.com/labs/rhevupgradehelper/\n\nHave you configured regular database backups? \nhttps://access.redhat.com/solutions/61993  (RHEV 3.0 to 3.2)\nhttps://access.redhat.com/solutions/797463 (RHEV 3.3 to 3.6)\n\nIf you're using Self Hosted Engine, perform you database backups as per the Hosted Engine Guide:\nhttps://access.redhat.com/documentation/en-us/red_hat_virtualization/4.1/html/self-hosted_engine_guide/chap-backing_up_and_restoring_a_rhel-based_self-hosted_environment</text>, <text>Hi,\n\nAs we discussed on the phone and Bomgar session, we'll need a LogCollector and sosreports from each host. Once we have that, we'll dig in to determine:\n- Why the switch reboot caused the VMs to hang/panic\n- Why the switch reboot triggered an SPM failover\n\nFrom this, we can determine if this is an issue that's likely to occur in a real-world scenario and determine if this is a bug that needs to be addressed, or if something else is misconfigured somewhere along the line.\n\nMy shift is coming to a close, so I'll leave 24x7 enabled on this case so we can investigate tonight to ensure you're not going to continue to have issues as you continue to perform the storage operations you have planned.\n\nThanks,\nAllan Voss, RHCE, RHCVA\nSenior Technical Support Engineer\nRed Hat, Inc.\n\nIf your systems are registered on Red Hat Network (RHN), visit https://access.redhat.com/products/red-hat-subscription-management/  to learn why you need to migrate to the Red Hat Subscription Management (RHSM) interface by July 2017.\n\nRHEV 3.x series was retired on September 30, 2017:\nhttps://access.redhat.com/support/policy/updates/rhev\n\nPreparing to upgrade? Try the RHEV Upgrade Helper tool:\nhttps://access.redhat.com/labs/rhevupgradehelper/\n\nHave you configured regular database backups? \nhttps://access.redhat.com/solutions/61993  (RHEV 3.0 to 3.2)\nhttps://access.redhat.com/solutions/797463 (RHEV 3.3 to 3.6)\n\nIf you're using Self Hosted Engine, perform you database backups as per the Hosted Engine Guide:\nhttps://access.redhat.com/documentation/en-us/red_hat_virtualization/4.1/html/self-hosted_engine_guide/chap-backing_up_and_restoring_a_rhel-based_self-hosted_environment</text>, <text>Hello,\n\nI see where a logcollector and 6 sosreports were provided but we are still missing sosreports from the Hosted Engine Hosts 04 and 05. Please let us know once those have been uploaded.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Hello,\n\nI am Kumar from Virtualization team and I will be assisting you during APAC hours on this case.\n\nLooking at the Logs you have provided, Below is my understanding: \n\nYour Data Center: Barlow is in contending state. This is because there is no active SPM (storage pool manager) host.\nRight now Host: CLGRABGUHV10 is contending to become SPM host but looks like it is facing issues while accessing the storage.\n~~~\nNov 9, 2017 8:10:12 AM  Data Center is being initialized, please wait for initialization to complete.\n~~~\n\nLooking at the storage domains, all the iscsi storage domains are marked as down. \n\nAfter looking at the Hosts logs, I can see there was some issue with the Network and due to which storage domains were inaccessible.\nAlso it has very large time drift which will cause major issues.\n-audit logs:\n~~~\nNov 9, 2017 7:08:12 AM  Slave ens2f1 of bond bond0 on host CLGRABGUHV10, changed state to down\nNov 9, 2017 7:08:12 AM  Slave ens2f1 of bond bond0 on host CLGRABGUHV10, changed state to down\n~~~\n-engine logs:\n~~~\nNov  8 14:12:30 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000012: remaining active paths: 0\nNov  8 14:12:30 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000007: remaining active paths: 0\nNov  8 14:12:31 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001f: remaining active paths: 0\nNov  8 14:12:31 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000017: remaining active paths: 0\nNov  8 14:12:31 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000001: remaining active paths: 0\nNov  8 14:12:31 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000019: remaining active paths: 0\nNov  8 14:12:31 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000018: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000015: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000003: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000016: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000006: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001a: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001b: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001e: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000020: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000023: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000024: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000026: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000004: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000002: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000008: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000005: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001c: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001d: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000021: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000022: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000025: remaining active paths: 0\nNov  8 14:59:10 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000001: remaining active paths: 0\nNov  8 14:59:10 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000012: remaining active paths: 0\nNov  8 14:59:11 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000004: remaining active paths: 0\nNov  8 14:59:11 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000017: remaining active paths: 0\nNov  8 14:59:11 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001e: remaining active paths: 0\nNov  8 14:59:11 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000021: remaining active paths: 0\nNov  8 14:59:11 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000025: remaining active paths: 0\nNov  8 15:38:42 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000002: remaining active paths: 0\nNov  8 15:38:42 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000012: remaining active paths: 0\nNov  8 15:38:42 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000007: remaining active paths: 0\nNov  8 15:38:42 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000008: remaining active paths: 0\nNov  8 15:38:42 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000004: remaining active paths: 0\nNov  8 15:38:42 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000005: remaining active paths: 0\nNov  8 15:38:42 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000016: remaining active paths: 0\nNov  8 15:38:42 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001f: remaining active paths: 0\nNov  8 15:38:42 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000018: remaining active paths: 0\nNov  8 15:38:42 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001e: remaining active paths: 0\nNov  8 15:38:43 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000019: remaining active paths: 0\nNov  8 15:38:43 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000023: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000006: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000003: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000001: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000015: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000017: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001a: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001b: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001c: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001d: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000020: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000021: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000022: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000024: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000025: remaining active paths: 0\nNov  8 15:38:44 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000026: remaining active paths: 0\n~~~\n\nAll the analysis shows that there was complete network outage for Data center Barlow due to which hosts,storage,NTP server were inaccessible and storage domains were marked as down and VM were paused. Hosts were too marked as non-responding for sometime.\n\nAs the network was UP later, Have the storage domains marked UP now?\n~~~\nNov 9, 2017 7:36:01 AM  Slave ens2f1 of bond bond0 on host CLGRABGUHV10, changed state to up\n~~~\n\nPlease confirm the Network is up for all the host and current status of the environment.\n\nRegards,\nKumar Mashalkar\nCEE, Red Hat</text>, <text>Hi, this happened earlier in the day. I'm providing the sosreports as per Allan to understand root cause. Is it a configuration issue or a bug? Because it seems like it happens just when the switch is powering back where either network connectivity is lost or hindered for a bit. I can't tell because every time we have done it so far in the past, we were working on fixing other issues and not watching for this anomaly to occur. Current status of RHEV environment is stable. We will wait for your further analysis and what game plan we can come up with on how best to troubleshoot this.\n\nThanks,\nSonya</text>, <text>BTW, we configured everything redundantly so that one switch upgrade *shouldn't* cause an issue. Do you see the bond0 interfaces go down on all the compute hosts when it happened?</text>, <text>Topology-wise\nServer port eno50 + ens2f1 are in bond0.\nServer port eno50 --&gt; connects to backend-switch#1\nServer port ens2f1 --&gt; connects to backend-switch#2\nStorage nodes are dually connected to both backend-switch#1 and backend-switch#2 using LACP.\nSo it is expected we should see server port ens2f1 to go down when we reboot backend-switch#2.\nBut why does having one of the bond0 ports going down (e.g. ens2f1) affect the server's ability to talk to the storage nodes which should still be accessible over backend-switch#1?</text>, <text>Also, why does the issue only affect Barlow datacenter and not default datacenter?</text>, <text>Hello Sonya,\n\nEven the default data center was affected.\n~~~ \nNov 9, 2017 5:42:34 AM Invalid status on Data Center Default. Setting Data Center status to Non Responsive (On host clgrabguhv05, Error: Cannot access master storage domain).\n~~~\n\nBut as this Data center has less resources, it recovered quickly and the status was UP when you generated the Log collector.\n\nGood to know the environment is up and stable.\n\nTo check why the network redundancy failed, I would pass this case to our networking team expert who will analyze the logs. Before that is there any more help/assistance do you require from Virtualization team regrading this issue?\n\nRegards,\nKumar Mashalkar\nCEE, Red Hat</text>, <text>Hello,\nI'm seeing some weird behavior on Barlow datacenter. I saw host HV11 which was the SPM go into connecting state, then now it's in UP state. I see all the storage domains as showing down but the VMs are up.\n\nCan you send a bomgar to take a look? I have one VM that is still doing a disk migration in progress.\nAnother VM disk migration just finished but I can't start that VM back up.\n\nThanks,\nSonya\n647-456-5392</text>, <text>Remote Support Invitation\n\nClick on the link below and follow the directions.\n\nhttps://remotesupport.redhat.com/?ak=be374f999e07574887636e4a5e64e93e\n\nBomgar(TM) enables a support representative to view your screen in order to assist you.\nSession traffic is fully encrypted to protect your system's data.\nOnce a session has begun, you will be able to end it at any time</text>, <text>Hello Sonya,\n\nThank you for your time on call and remote session.\n\nAs we saw on remote session, your Data center, SPM host are down. From the vdsm logs we saw that it was not able to access one of the storage domain.\n\nThat storage domain was ISO domain which is hosted on one of the RHV host. We tried to mount it manually and it was stuck.\n\nIt is not recommended to have any storage domain hosted on RHV host (expect the local on host storage). To come out of this situation, I will take experts advice by putting this case on collaboration. We will quickly get assistance from them as I have already communicated them. \n\nTo put the case in collaboration, I have asked you few logs. Please attach the same.\n\nRegards,\nKumar Mashalkar\nCEE, Red Hat</text>, <text>Sorry it took so long. HV11 sosreport was taking long because it was timing out in certain places:\n[root@clgrabguhv11 ~]# sosreport\n\nsosreport (version 3.3)\n\nThis command will collect diagnostic and configuration information from\nthis Red Hat Enterprise Linux system and installed applications.\n\nAn archive containing the collected information will be generated in\n/var/tmp/sos.ziXDfJ and may be provided to a Red Hat support\nrepresentative.\n\nAny information provided to Red Hat will be treated in accordance with\nthe published support policies at:\n\n  https://access.redhat.com/support/\n\nThe generated archive may contain data considered sensitive and its\ncontent should be reviewed by the originating organization before being\npassed to any third party.\n\nNo changes will be made to system configuration.\n\nPress ENTER to continue, or CTRL-C to quit.\n\nPlease enter your first initial and last name [clgrabguhv11]: sghv11\nPlease enter the case id that you are generating this report for []: 01967706\n\n Setting up archive ...\n Setting up plugins ...\ncaught exception in plugin method "vdsm.setup()"\nwriting traceback to sos_logs/vdsm-plugin-errors.txt\n Running plugins. Please wait ...\n\n  Running 87/94: vdsm...        [plugin:vdsm] command '/bin/su vdsm -s /bin/sh -c '/usr/bin/tree -l /rhev/data-center'' timed out after 300s\n  Running 94/94: yum...\nCreating compressed archive...\n\nYour sosreport has been generated and saved in:\n  /var/tmp/sosreport-sghv11.01967706-20171108225407.tar.xz\n\nThe checksum is: d889779701bff7fbb3c46c39bb40e389\n\nPlease send this file to your support representative.\n\n[root@clgrabguhv11 ~]#</text>, <text>Is it worth trying to select a different host as SPM?</text>, <text>// Need Collaboration //\n\nIssue: SPM host going non-responsive due to which Storage and DC going down due to which enable to start VM and complete disk migration activity.\n\nEnvironment: RHV 4.1\n\nAudit logs:\n++++\nNov 9, 2017 2:16:05 PM  VDSM CLGRABGUHV11 command GetCapabilitiesVDS failed: Message timeout which can be caused by communication issues\nNov 9, 2017 2:13:01 PM  VDSM CLGRABGUHV11 command GetCapabilitiesVDS failed: Message timeout which can be caused by communication issues\nNov 9, 2017 2:09:57 PM  VDSM CLGRABGUHV11 command GetCapabilitiesVDS failed: Message timeout which can be caused by communication issues\nNov 9, 2017 2:06:53 PM  Host CLGRABGUHV11 became Non Responsive and was not restarted due to disabled fencing in the Cluster Fencing Policy.\nNov 9, 2017 2:06:53 PM  Host CLGRABGUHV11 is non responsive.\n++++\n\nSPM host messages:\n++++\nNov  8 22:54:36 clgrabguhv11 kernel: nfs: server 100.70.85.101 not responding, timed out\nNov  8 22:56:39 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:56:42 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:56:43 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:56:46 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:56:49 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:56:50 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:56:53 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:56:57 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:56:57 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:57:00 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:57:03 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\nNov  8 22:57:06 clgrabguhv11 journal: vdsm jsonrpc.JsonRpcServer ERROR could not allocate request thread#012Traceback (most recent call last):#012  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest#012    self._threadFactory(partial(self._serveRequest, ctx, request))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch#012    self._tasks.put(Task(callable, timeout))#012  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put#012    raise TooManyTasks()#012TooManyTasks\n++++\n\nVDSM logs:\n++++\n2017-11-08 22:59:02,157-0600 ERROR (JsonRpcServer) [jsonrpc.JsonRpcServer] could not allocate request thread (__init__:626)\nTraceback (most recent call last):\n  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest\n    self._threadFactory(partial(self._serveRequest, ctx, request))\n  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch\n    self._tasks.put(Task(callable, timeout))\n  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put\n    raise TooManyTasks()\nTooManyTasks\n2017-11-08 22:59:05,198-0600 ERROR (JsonRpcServer) [jsonrpc.JsonRpcServer] could not allocate request thread (__init__:626)\nTraceback (most recent call last):\n  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest\n    self._threadFactory(partial(self._serveRequest, ctx, request))\n  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch\n    self._tasks.put(Task(callable, timeout))\n  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put\n    raise TooManyTasks()\nTooManyTasks\n2017-11-08 22:59:07,794-0600 WARN  (itmap/0) [storage.scanDomains] Could not collect metadata file for domain path /rhev/data-center/mnt/100.70.85.101:_NFS_images (fileSD:805)\nTraceback (most recent call last):\n  File "/usr/share/vdsm/storage/fileSD.py", line 794, in collectMetaFiles\n    sd.DOMAIN_META_DATA))\n  File "/usr/lib/python2.7/site-packages/vdsm/storage/outOfProcess.py", line 107, in glob\n    return self._iop.glob(pattern)\n  File "/usr/lib/python2.7/site-packages/ioprocess/__init__.py", line 570, in glob\n    return self._sendCommand("glob", {"pattern": pattern}, self.timeout)\n  File "/usr/lib/python2.7/site-packages/ioprocess/__init__.py", line 455, in _sendCommand\n    raise Timeout(os.strerror(errno.ETIMEDOUT))\nTimeout: Connection timed out\n~~~\nStorageDomainDoesNotExist: Storage domain does not exist: (u'886e21b3-5868-475c-9944-472d7bb16baf',)\n2017-11-08 22:59:09,211-0600 ERROR (JsonRpcServer) [jsonrpc.JsonRpcServer] could not allocate request thread (__init__:626)\nTraceback (most recent call last):\n  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest\n    self._threadFactory(partial(self._serveRequest, ctx, request))\n  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch\n    self._tasks.put(Task(callable, timeout))\n  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put\n    raise TooManyTasks()\nTooManyTasks\n2017-11-08 22:59:11,213-0600 ERROR (JsonRpcServer) [jsonrpc.JsonRpcServer] could not allocate request thread (__init__:626)\nTraceback (most recent call last):\n  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest\n    self._threadFactory(partial(self._serveRequest, ctx, request))\n  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch\n    self._tasks.put(Task(callable, timeout))\n  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put\n    raise TooManyTasks()\nTooManyTasks\n2017-11-08 22:59:11,879-0600 INFO  (monitor/67c9bfc) [IOProcessClient] Closing client ioprocess-246 (__init__:593)\n2017-11-08 22:59:13,030-0600 ERROR (JsonRpcServer) [jsonrpc.JsonRpcServer] could not allocate request thread (__init__:626)\nTraceback (most recent call last):\n  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 624, in _runRequest\n    self._threadFactory(partial(self._serveRequest, ctx, request))\n  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 148, in dispatch\n    self._tasks.put(Task(callable, timeout))\n  File "/usr/lib/python2.7/site-packages/vdsm/executor.py", line 361, in put\n    raise TooManyTasks()\nTooManyTasks\n++++\n\nLooks like host is unable to respond back to the engine in timely manner and thus going unresponsive. \nHost looks busy with finding storage domain: "Storage domain does not exist: (u'886e21b3-5868-475c-9944-472d7bb16baf',)" which is ISO_images and multiple tasks on it.\n++++\n                  id                  | storage_name \n--------------------------------------+--------------\n 886e21b3-5868-475c-9944-472d7bb16baf | ISO_images\n++++\n\nThis ISO_images is hosted on RHV-Host: "CLGRABGUHV06". I have already informed customer this is not recommended and supported.\n\nCollaboration needed to bring back the SPM host up so that DC is initialized and VM can be started.\n\nRegards,\nKumar Mashalkar\nRed Hat Global Support Services</text>, <text>Hello Sonya,\n\nYou have unfinished disk related task in your environment so you wont be able to change the SPM host till then.\n\nWe have completed the analysis with our SME team and soon will share the action plan.\n\nRegards,\nKumar Mashalkar\nCEE, Red Hat</text>, <text>Hello Sonya,\n\nThank you for your patience.\n\nBelow is the action plan suggested by our SME:\n1) Destroy the ISO domain.\nFrom RHV-Manager portal go to Storage tab -&gt; Select "ISO_images" -&gt; Right click and select "Destroy". \nThis will just remove the entry for ISO domain and will not have any data loss. As you have only 1 ISO in that ISO domain, you can later copy it to new ISO domain created in supported way.\n\n2) unmount if not then Force un-mount the ISO domain from SPM host.\nOn SPM host run below command:\n# umount -lf /rhev/data-center/mnt/100.70.85.101:_NFS_images\n\n3) restart the vdsm service\nOn SPM host run below command:\n# systemctl restart vdsmd\n\nPlease let us know if you have any doubt on above action plan.\n\nRegards,\nKumar Mashalkar\nCEE, Red Hat</text>, <text>Can I call you and you watch on bomgar to see if it fixes the issue?\nWill the disk migration for SDB3a resume? Right now it showing locked at 17%.</text>, <text>Remote Support Invitation\n\nClick on the link below and follow the directions.\n\nhttps://remotesupport.redhat.com/?ak=19d32c860005f0ef55c879c965d8d7ce\n\nBomgar(TM) enables a support representative to view your screen in order to assist you.\nSession traffic is fully encrypted to protect your system's data.\nOnce a session has begun, you will be able to end it at any time</text>, <text>Hello Sonya,\n\nThank you for your time on remote session. \n\nWe were able to resolve the issue by force un-mounting the ISO domain on SPM host. Also I tried my best to explain the situation that happened and how we resolved.\n\nUnfortunately we do not have any guide which talk about ISO domain do's and don't but I will write one solution article and see if we could add that information on documentation.\n\nReason behind not having ISO domain on RHV host is that it will go into loop. As same host is serving nfs share and it is again mounting it. I have not gone through the logs for this but this issue has been faced earlier with other customers.\n\nReason we ask ISO domain should be out of RHV environment is to avoid issue cascading. You can refer this solution on what happens when ISO domain is not available: https://access.redhat.com/solutions/528793\n\nAlso I jumped in this case at later time, I am unaware of the previous issue. As you said you want this case to be in 24x7 sev2 to get RCA of some issue, Can you please re-confirm you want the RCA of exactly which issue? Is that about network issue happened during redundant network switch?\n\nRegards,\nKumar Mashalkar\nCEE, Red Hat</text>, <text>Yes, thanks, can we keep the ticket open for the RCA that happened during the redundant network switch reboot that caused the VMs to go to 100% spike in CPU and SPM into contending state.\n\nThanks,\nSonya</text>, <text>// Need Collaboration // SBR-Networking\n\nIssue: Hosts storage path were affected where there was network switch. Customer claims they have redundant path for both host and storage but still path failed.\n\nEnvironment: Red Hat Enterprise Linux release 7.3\n\nLogs:\n++++\nNov  8 14:12:24 clgrabguhv11 kernel: connection5:0: ping timeout of 5 secs expired, recv timeout 5, last rx 11999342752, last ping 11999347760, now 11999352768\nNov  8 14:12:24 clgrabguhv11 kernel: connection5:0: detected conn error (1022)\nNov  8 14:12:24 clgrabguhv11 kernel: connection9:0: ping timeout of 5 secs expired, recv timeout 5, last rx 11999342880, last ping 11999347888, now 11999352896\nNov  8 14:12:24 clgrabguhv11 kernel: connection9:0: detected conn error (1022)\nNov  8 14:12:24 clgrabguhv11 kernel: connection8:0: ping timeout of 5 secs expired, recv timeout 5, last rx 11999342904, last ping 11999347920, now 11999352928\nNov  8 14:12:24 clgrabguhv11 kernel: connection8:0: detected conn error (1022)\nNov  8 14:12:24 clgrabguhv11 kernel: connection7:0: ping timeout of 5 secs expired, recv timeout 5, last rx 11999342960, last ping 11999347968, now 11999352976\nNov  8 14:12:24 clgrabguhv11 kernel: connection7:0: detected conn error (1022)\nNov  8 14:12:24 clgrabguhv11 kernel: connection16:0: ping timeout of 5 secs expired, recv timeout 5, last rx 11999343078, last ping 11999348080, now 11999353088\nNov  8 14:12:24 clgrabguhv11 kernel: connection16:0: detected conn error (1022)\nNov  8 14:12:24 clgrabguhv11 kernel: connection2:0: ping timeout of 5 secs expired, recv timeout 5, last rx 11999343254, last ping 11999348255, now 11999353264\nNov  8 14:12:24 clgrabguhv11 kernel: connection2:0: detected conn error (1022)\n~~~\nNov  8 14:12:25 clgrabguhv11 kernel: connection15:0: ping timeout of 5 secs expired, recv timeout 5, last rx 11999343895, last ping 11999348896, now 11999353904\nNov  8 14:12:25 clgrabguhv11 kernel: connection15:0: detected conn error (1022)\nNov  8 14:12:25 clgrabguhv11 kernel: connection23:0: ping timeout of 5 secs expired, recv timeout 5, last rx 11999344006, last ping 11999349008, now 11999354016\nNov  8 14:12:25 clgrabguhv11 kernel: connection23:0: detected conn error (1022)\nNov  8 14:12:25 clgrabguhv11 kernel: connection11:0: ping timeout of 5 secs expired, recv timeout 5, last rx 11999344095, last ping 11999349096, now 11999354112\nNov  8 14:12:25 clgrabguhv11 kernel: connection11:0: detected conn error (1022)\nNov  8 14:12:25 clgrabguhv11 kernel: connection17:0: ping timeout of 5 secs expired, recv timeout 5, last rx 11999344378, last ping 11999349379, now 11999354384\nNov  8 14:12:25 clgrabguhv11 kernel: connection17:0: detected conn error (1022)\n~~~\nNov  8 14:12:30 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000012: remaining active paths: 0\nNov  8 14:12:30 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000007: remaining active paths: 0\nNov  8 14:12:31 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001f: remaining active paths: 0\nNov  8 14:12:31 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000017: remaining active paths: 0\nNov  8 14:12:31 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000001: remaining active paths: 0\nNov  8 14:12:31 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000019: remaining active paths: 0\nNov  8 14:12:31 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000018: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000015: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000003: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000016: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000006: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001a: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001b: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e0000001e: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000020: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000023: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000024: remaining active paths: 0\nNov  8 14:12:32 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000026: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000004: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000002: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000008: remaining active paths: 0\nNov  8 14:12:33 clgrabguhv10 multipathd: 36f47acc1000000006478386e00000005: remaining active paths: 0\n++++\n\nLogs clearly shows there was network issue but as customer says they have network redundancy, we need to re-check this and confirm there was network loss. So, I am moving this case for collaboration to sbr-networking.\n\n\nRegards,\nKumar Mashalkar\nRed Hat Global Support Services</text>, <text>I don't think we can tell any more.  The system logged there was a network outage, so from it's point of view there was no connectivity.\n\nIf the situation can be reproduced, and maintained for an indefinite period then we could aid in suggesting active probing tests -\nbut this is unlikely to be viable for the customer.</text>, <text>Hi, i have another problem. I did the disk migration successfully on the VM abbrlwpcrfsdb4a, then activated the disk, then tried to activate the VM, but i get the following error messages on the console and I cannot ssh to it. I tried powering it off and then powering it on and same thing. Whats going on? I need your help! Please send me bomgar link asap.\n\nThanks,\nSonya</text>, <text>Hello Sonya,\n\nMy name is Sara Ferguson and I am a member of the Networking team. \n\nThank you for the update. As this appears to be a new issue, can you please open a new case with the details and screenshot you've provided here? This will allow the proper team to assist you. \n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>Can you please transfer this case back to the RHV virtualization team so they can help me?\n\nThanks,\nSonya</text>, <text>Hey Sonya,\n\nThank you for your time and patience over the phone.\n\nYou called in to speak with you engineer. At the time of request, there was no engineer available, but we are currently working to align a resource up for you. As Sara mentioned, it would be very helpful if you can create a new case for the Virtualization issue you have.\n\nI've noted your best contact number as the following:\n\n1-647-456-5392\n\nIf you do end up creating a new case for us, feel free to call in and let us know!\n\nBest,\n\nChris Wise\nRed Hat, Inc.\nCustomer Support Specialist</text>, <text>Hi Sonya, I switched it back to virt.\n\nDon</text>, <text>Hi Sonya, sorry, I will switch this back to networking so that they can assist with the network outage RCA.  Can you open a new virt case for the VM storage issue you are having?\n\nThanks.\n\nDon</text>, <text>I have opened case 01970514 for the VM bootup issue. Sev 1.</text>, <text>Hi Sonya,\n\nI wanted to let you know that I am currently investigating the logs for the RCA on the network outage. \n\nI'll update you again shortly with my findings. \n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>Hi Sonya,\n\nI've been reviewing the case and the sosreport. However I do have a couple of questions moving forward:\n\n- If I am understanding the topology correctly, you have 2 bonds, each with VLAN tagged interfaces. The slaves for these bonds are each plugged into separate switches. Is this an accurate understanding of what occurred?\n- After the switch was rebooted, all connectivity on bond0 ceased to work? Or did this happen on both bond interfaces? \n- As this case is now for the RCA of the network outage, would it be ok to remove this case from 24x7 support? I can continue my analysis tomorrow beginning at 8am EST. \n\n I appreciate your time and responses to the above. I will turn the case to Waiting on Customer for now. \n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>//// INTERNAL HANDOVER NOTE\n\nPlacing the handover note internally as there is a lot happening in this case. \n\nIssue: If I understand correctly, they were rebooting switches one at a time and lost total connectivity. They have 2 mode 4 bonds, so they should not have lost connectivity. \n\nThe only relevant logs that mention bond* are below:\n\nsosreport-sghv11.01967706-20171108225407]$ grep bond var/log/messages\nNov  8 10:45:05 clgrabguhv11 kernel: bond1: link status definitely down for interface eno49, disabling it\nNov  8 11:23:22 clgrabguhv11 kernel: bond1: link status definitely up for interface eno49, 10000 Mbps full duplex\nNov  8 12:30:12 clgrabguhv11 kernel: bond1: link status definitely down for interface ens2f0, disabling it\nNov  8 12:30:12 clgrabguhv11 kernel: bond1: first active interface up!\nNov  8 13:50:19 clgrabguhv11 kernel: bond1: link status definitely up for interface ens2f0, 10000 Mbps full duplex\nNov  8 14:12:59 clgrabguhv11 kernel: bond0: link status definitely down for interface eno50, disabling it\nNov  8 14:54:39 clgrabguhv11 kernel: bond0: link status definitely up for interface eno50, 10000 Mbps full duplex\nNov  8 15:39:13 clgrabguhv11 kernel: bond0: link status definitely down for interface ens2f1, disabling it\nNov  8 15:39:13 clgrabguhv11 kernel: bond0: first active interface up!\nNov  8 16:06:43 clgrabguhv11 kernel: bond0: link status definitely up for interface ens2f1, 10000 Mbps full duplex\n\n\nTime of issue: Nov 8 ~14:12 (system time)\n\nAs this is an RCA I've asked that we remove 24x7.</text>, <text>Hi Sara,\nYes we have 2 bonds on each physical server, each with VLAN tagged interfaces. Bond0 goes towards storage switches, Bond1 goes towards data network switches. Each slave interface for each bond is plugged into separate switches.\n\n After the switch was rebooted, I don't know if all connectivity to bond0 broke? That's what Red Hat support seems to keep saying from their log analysis. When I was logging into the switch at the time switch was rebooting, I only saw one of the slave interfaces as down, but the bond0 was still showing up. Single switch reboot on data network switches doesn't seem to cause bond1 to go down, but I wasn't checking there and the traffic from the VMs was migrated off temporarily to accommodate for the switch upgrades, so we couldn't tell if there was an impact on the data side.\n\nYes you can remove this case from 24x7 support. I'm still doing the original scope of work of the proactive case but I hope to finish it in a few hours. If any new issues pop-up, I'll make new cases.\n\nThanks,\nSonya</text>, <text>Hi Sonya,\n\nThank you for the quick reply. I'll remove 24x7 support from this case for now. \n\nI will leave the case Waiting on Red Hat for now. I'll give you an update tomorrow morning with my status or findings. \n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>see similar case 01941504\nwe concluded in that case that there must have been some issue with the other switch, so that when they rebooted the other one they lost all storage connections</text>, <text>Hi Sonya,\n\nI am still working to review the data. So far the only things I can see in the logs are each link going down one by one over the course of about five and half hours:\n\nsosreport-sghv11.01967706-20171108225407]$ grep bond var/log/messages\nNov  8 10:45:05 clgrabguhv11 kernel: bond1: link status definitely down for interface eno49, disabling it\nNov  8 11:23:22 clgrabguhv11 kernel: bond1: link status definitely up for interface eno49, 10000 Mbps full duplex\nNov  8 12:30:12 clgrabguhv11 kernel: bond1: link status definitely down for interface ens2f0, disabling it\nNov  8 12:30:12 clgrabguhv11 kernel: bond1: first active interface up!\nNov  8 13:50:19 clgrabguhv11 kernel: bond1: link status definitely up for interface ens2f0, 10000 Mbps full duplex\nNov  8 14:12:59 clgrabguhv11 kernel: bond0: link status definitely down for interface eno50, disabling it\nNov  8 14:54:39 clgrabguhv11 kernel: bond0: link status definitely up for interface eno50, 10000 Mbps full duplex\nNov  8 15:39:13 clgrabguhv11 kernel: bond0: link status definitely down for interface ens2f1, disabling it\nNov  8 15:39:13 clgrabguhv11 kernel: bond0: first active interface up!\nNov  8 16:06:43 clgrabguhv11 kernel: bond0: link status definitely up for interface ens2f1, 10000 Mbps full duplex\n\nI'm still investigating the logs, but after speaking with your TAM, Don Berry, it was brought to my attention that in a previous case there was an issue with a switch. That issue caused your systems to lose storage connectivity when the functional switch was rebooted. I know you mentioned that you were monitoring the switches at the time; did you notice anything that sounds similar to the previous issue you saw? \n\nLooking into the below errors shortly after eno50 went down:\n\nNov  8 14:13:02 clgrabguhv11 kernel: connection13:0: detected conn error (1020)\nNov  8 14:13:02 clgrabguhv11 kernel: connection19:0: detected conn error (1020)\n\nI was able to find an article which discusses this. \n\nReceiving error on iscsi device: "connectionX:0: detected conn error" in RHEL\n        https://access.redhat.com/solutions/423473\n\nThat being said, this is just another symptom of the issue. \n\nLooking back through the case, you've mentioned that this has happened before, but you've not been able to actively troubleshoot it, as other more pressing issues present at the time. Would you be able to reproduce this on a test server to gather some data while it occurs? That would be useful to diagnose it and provide an action plan moving forward. \n\nLet me know your thoughts, and how you'd like to move forward. At this time, I'm not sure there is enough information for a root-cause on this one. I will keep reviewing the sosreport in the meantime however. \n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>Hi Sara,\nThe first time this happened, we had to reboot the switches due to a memory leak on the switches to apply the fix. That was the first time we noticed this behaviour. We tried to reproduce it in the lab, but we couldn't. We can reboot switches one at a time in the lab, and the VMs were fine. The only way we somehow reproduced it in the lab, was if we shutdown both switchports facing the servers towards the storage network. So my theory for the first time it happened was maybe the redundant paths were broken due to the memory leak and traffic was not getting through the other link properly and that's why the VMs lost access to storage temporarily when we rebooted one switch. However, this time around, we verified all the interfaces were up before the reboot but still the problem happened. Which leads me to believe, there is something else going on either misconfiguration or bug. We have another similar RHEV cluster in Ontario. We have not tried rebooting the switches in Ontario to see if the same issue exists over there.\n\nHope that sheds some light.\nApproximate timeframes of when switches were rebooted on Nov. 8th:\nOn or before 2:10 PM - rebooting PF01\n12:20 PM - rebooting PF01\n1:22 PM - rebooting PF02\n2:57 PM - rebooting PB01\n4:32 PM - rebooting PB02\n\nPF01 connects to eno49  } bond1\nPF02 connects to ens2f0 } bond1\nPB01 connects to eno50 } bond0\nPB02 connects to ens2f1 } bond0\n\nWe may have an opportunity to try a switch reboot on Nov. 15th when we fail traffic over from Ontario to Calgary when we do the vm disk migration work on Ontario. The first priority will be to finish the vm rework, but later on we may be able to squeeze in one reboot if I can find the right resource to do it. This will at least pinpoint if the issue is environmental or not because now both sites are exactly the same.\n\nCan you do any audit from the logs if it is a configuration issue? I can provide sosreports from Ontario as a comparison?\n\nThanks,\nSonya</text>, <text>Hi Sonya,\n\nThanks for the clarification.\n\nI do think it would be beneficial to try to reproduce it; but can continue to look at the logs. \n\nI will need some additional logging if you have it. I noticed that there are sar reports until until the 5th, but none following. \n\nsosreport-sghv11.01967706-20171108225407]$ ll var/log/sa/sar*\n-rw-r--r--. 1 sferguso sferguso 3966222 Nov  2 01:53 var/log/sa/sar01\n-rw-r--r--. 1 sferguso sferguso 3966222 Nov  3 01:53 var/log/sa/sar02\n-rw-r--r--. 1 sferguso sferguso 3966222 Nov  4 01:53 var/log/sa/sar03\n-rw-r--r--. 1 sferguso sferguso 3966222 Nov  5 01:53 var/log/sa/sar04\n-rw-r--r--. 1 sferguso sferguso 3966222 Nov  6 00:53 var/log/sa/sar05\n\nDo you happen to have the logs from November 8? If so, can you please upload them to the case?\n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>*** internal ***\n\nSeeing if duchess is about today - if not, will see who can push this forward.</text>, <text>*** internal ***\n\nsbr-networking folks noted sos_commands/sar/sar08 with the requested data, but\nwe're not clear on the direction to take with it.\n\nIt seems unlikely that there's anything on the box that's problematic here.\nI'll update the customer and note that we're continuing the investigation, to\ngive duchess a chance to weigh in when she's back in the office.\n\nRegards,\n Mason\n\n-- \nMason Loring Bliss\nTechnical Account Manager\nRed Hat, Inc.</text>, <text>Sonya,\n\nThank you for your patience. We are combing over the logs to narrow down the\nprospects for what you might have seen during the single-switch reboot. We'll\nhope to have additional analysis for you tomorrow.\n\nIn the meantime, we're wondering if you've weighed the pros and cons of trying\na test-cycle of your switches on Ontario. The risk clearly is that if there\nis a systemic flaw, this could cause an outage, with the benefit being con-\nfirmation of the configuration and software.\n\nWe'll explore all the options before we recommend such a test, of course.\n\nRegards,\n Mason\n\n-- \nMason Loring Bliss\nTechnical Account Manager\nRed Hat, Inc.</text>, <text>*** internal ***\n\nNote: I left this in WoO despite the question, as I didn't want to be seen as\nfreezing the clock meaninglessly. That said, the log dig warrants updating\nthe SBT.</text>, <text>Hi Sonya,\n\nThe reason I was hoping to check the sar logs was to see the behavior of each slave in each bond as the switches were rebooted. \n\nIt is interesting to note, that some of these results make little sense to me, as the times the slaves are down prior to the switch reboot, or they do not go down at all durign a reboot time. \n\nReboot times and bond-pairs. \n\n12:20 PM - rebooting PF01\n1:22 PM - rebooting PF02\n2:57 PM - rebooting PB01\n4:32 PM - rebooting PB02\n\nPF01 connects to eno49  } bond1\nPF02 connects to ens2f0 } bond1\nPB01 connects to eno50 } bond0\nPB02 connects to ens2f1 } bond0\n\nAt the 12:20 reboot, we expect to see only eno49 go down, but it never loses connectivity... \n\n\tLinux 3.10.0-514.16.1.el7.x86_64 (clgrabguhv11)         11/08/17        _x86_64_        (56 CPU)\n\n\n\t00:00:01        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\n\t11:40:01        eno49      3.14      3.41      1.06      2.73      0.00      0.00      0.02\n\t11:50:01        eno49      9.05      8.80      2.83      5.70      0.00      0.00      0.03\n\t12:00:01        eno49     15.06      8.74      4.44      5.70      0.00      0.00      0.03\n\t12:10:01        eno49     30.82      8.74      8.00      5.70      0.00      0.00      0.03\n\t12:20:02        eno49     25.37      8.76      6.51      5.71      0.00      0.00      0.03\n\t12:30:01        eno49     10.71      8.54      3.19      5.66      0.00      0.00      0.04\n\t12:40:01        eno49     27.08     32.41      9.81     20.39      0.00      0.00      0.04\n\t12:50:01        eno49     27.83     33.16     10.05     20.75      0.00      0.00      0.03\n\t13:00:01        eno49     27.60     32.94      9.98     20.65      0.00      0.00      0.03\n\t13:10:01        eno49     27.81     33.10     10.04     20.69      0.00      0.00      0.03\n\t13:20:01        eno49     27.79     33.15     10.04     20.71      0.00      0.00      0.03\n\t13:30:01        eno49     27.71     33.14     10.04     20.72      0.00      0.00      0.03\n\t13:40:01        eno49     27.31     32.75      9.96     20.61      0.00      0.00      0.03\n\n\nWhereas ens2f0 goes down 30 minutes prior to the reboot of PF02 (13:22) and remains down until 14:00. This slave also stays up during the reboot of PF01.\n\n\t00:00:01        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\n\t11:40:01       ens2f0     18.52     25.95      5.04     17.08      0.00      0.00      0.04\n\t11:50:01       ens2f0     12.44     21.36      3.25     14.26      0.00      0.00      0.03\n\t12:00:01       ens2f0     15.09     28.38      4.01     15.99      0.00      0.00      0.03\n\t12:10:01       ens2f0     23.35     46.46      6.06     20.24      0.00      0.00      0.03\n\t12:20:02       ens2f0     20.65     40.17      5.05     19.05      0.00      0.00      0.03\n\t12:30:01       ens2f0     13.72     23.72      3.45     14.64      0.00      0.00      0.03\n\t12:40:01       ens2f0      0.17      0.15      0.03      0.03      0.00      0.00      0.00\n\t12:50:01       ens2f0      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t13:00:01       ens2f0      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t13:10:01       ens2f0      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t13:20:01       ens2f0      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t13:30:01       ens2f0      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t13:40:01       ens2f0      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t13:50:01       ens2f0      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t14:00:01       ens2f0     15.78     23.69      5.29     14.66      0.00      0.00      0.04\n\n\nDuring the reboot of PB01, eno50 goes down, but again this is ~20 minutes before the switch reboot, and is back up (at least briefly, likely toward the end of the 10 minute interval) moments after the switch reboot. \n\n\t00:00:01        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\n\t14:00:01        eno50    580.67    146.68    813.83     89.70      0.00      0.00      0.03\n\t14:10:01        eno50    575.19    148.93    806.57     93.34      0.00      0.00      0.03\n\t14:20:01        eno50    141.97     42.62    198.72     36.05      0.00      0.00      0.01\n\t14:30:01        eno50      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t14:40:01        eno50      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t14:50:01        eno50      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t15:00:01        eno50      1.81      0.47      0.23      0.05      0.00      0.00      0.01\n\t15:10:01        eno50   2408.59    400.02   3230.39     35.55      0.00      0.00      0.03\n\t15:20:01        eno50   2499.92    409.25   3434.69     36.32      0.00      0.00      0.03\n\t15:30:01        eno50 8071559.98 773124.22 9725282.00 693462.02      0.00      0.00    428.07\n\nDuring this same time, ens2f1 remains up and continues to pass traffic indicating there was no issue for it at the time. \n\n\t00:00:01        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\n\t14:00:01       ens2f1    819.20    322.09   1178.89    254.81      0.00      0.00      0.03\n\t14:10:01       ens2f1    813.76    316.47   1171.61    247.15      0.00      0.00      0.03\n\t14:20:01       ens2f1   1282.25    280.78   1853.98     91.67      0.00      0.00      0.04\n\t14:30:01       ens2f1   1361.36    224.64   1976.40     19.62      0.00      0.00      0.03\n\t14:40:01       ens2f1   4366.75   2141.96   6296.00   2306.69      0.00      0.00      0.03\n\t14:50:01       ens2f1   2411.32   4431.22   3222.98   5976.64      0.00      0.00      0.03\n\t15:00:01       ens2f1   4768.39   5751.55   6668.53   7592.84      0.00      0.00      0.04\n\t15:10:01       ens2f1   2066.34   4774.90   2827.64   5232.68      0.00      0.00      0.03\n\t15:20:01       ens2f1   1707.08   2739.51   2380.49   2405.85      0.00      0.00      0.03\n\t15:30:01       ens2f1 6767837.44 46565332.48 5991021.41 65021118.62      0.00      0.00    428.25\n\nFinally, during the reboot of PB02 (16:32), we can see that it also goes down briefly ~40 minutes prior to the reboot, but comes back up before the reboot is said to have occured. \n\n\t00:00:01        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s\n\t15:40:01       ens2f1   1251.36    893.89   1743.96    180.20      0.00      0.00      0.03\n\t15:50:01       ens2f1      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t16:00:01       ens2f1      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\t16:10:01       ens2f1    869.80    477.26   1207.15     42.66      0.00      0.00      0.02\n\t16:20:01       ens2f1   2382.38    863.73   3391.73     76.67      0.00      0.00      0.03\n\t16:30:01       ens2f1   2930.25   1231.83   4143.95    109.65      0.00      0.00      0.03\n\t16:40:01       ens2f1   2901.02   1241.23   4096.39    110.51      0.00      0.00      0.03\n\t16:50:01       ens2f1   2674.03   1444.31   3747.95    128.30      0.00      0.00      0.03\n\t17:00:02       ens2f1 6786279.29 46583254.19 6016030.38 65036614.42      0.00      0.00    428.48\n\nAnd eno50 remains up. \n\n\t15:40:01        eno50   1883.83    356.30   2649.76    111.06      0.00      0.00      0.04\n\t15:50:01        eno50   5258.90   2023.36   7345.85    183.67      0.00      0.00      0.03\n\t16:00:01        eno50   5105.25   1762.61   7183.44    157.53      0.00      0.00      0.03\n\t16:10:01        eno50   4207.83   1296.47   5935.87    117.63      0.00      0.00      0.04\n\t16:20:01        eno50   1610.43    387.73   2265.73     33.82      0.00      0.00      0.03\n\t16:30:01        eno50   2098.19    516.65   2933.19     45.40      0.00      0.00      0.03\n\t16:40:01        eno50   2105.37    516.10   2939.55     45.31      0.00      0.00      0.03\n\t16:50:01        eno50   2426.66    516.68   3391.07     45.83      0.00      0.00      0.03\n\t17:00:02        eno50 8098687.22 780962.54 9763249.74 694274.66      0.00      0.00    428.41\n\nFrom an OS perspective, none of the slaves lost connectivity at the same time, during a switch reboot. I can admit that the behavior appears odd to me, but perhaps there is a timing issue between this server and the switch, or some other issue I cannot see in the data. \n\nI think it would be best to consider reproducing the issue, and collecting packet captures, sosreports, and monitor.sh data during that time. The script for monitor.sh can be found in the article below. It will allow us to see what is happening on the system at different intervals, as well as compare connection streams in packet captures with `ss` output. This set of data may prove more useful in diagnosing the issue. \n\nIn summary, the data we would need is:\n\n1.) Packet captures on each slave while the issue is reproduced. They should be started before attempting to reproduce the issue. You can use the below command, replacing 'INTERFACE-NAME' with the appropriate interface. The '&amp;' will allow it to run in the background. \n\n\t# tcpudmp -i INTERFACE-NAME -w /tmp/01967706-INTERFACE-NAME-$(hostname).pcap &amp;\n\n2.) A packet capture from the switch-side (if possible) to see what the switch-ports are doing during failover\n3.) The monitor.sh script should be started at the same time as the packet capture to ensure we collect the same data. \n\nHow to begin Network performance debugging\n\thttps://access.redhat.com/articles/1311173#monitorsh-script-3\n\nThank you for your patience while I checked into this. Please let me know if you have any questions or concerns. \n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>Hi Sara,\nThe times for the switch reboots were approximate so I might have been off since I wasn't doing the actual rebooting and the switches had to be rebooted multiple times in order for the upgrade to be completed so that might explain why you saw it go down at other times too. Anyways, I see the tcpdump commands you are asking for us to do next time. At this point, we don't have a window secured for this. We are planning to failover traffic from our Ontario site to Calgary, which means Ontario will be freed up on Monday, Nov. 20th-21st because we have to do the disk migrations over there as well. We might have an opportunity to see if this same problem exists in Ontario after the disk migrations are completed. Also, there's an embargo this week for another maintenance activity going on so I can't try the captures this week. If the interfaces seem to be connected to the correct switches and the vlans are configured correctly, then what is going on at the storage level? Is there anything in the logs that show SPM renegotiating or iSCSI paths going down? Why would the VMs go into pause state if the network paths towards the storage networks seem to have always been up? Or do we need the captures to answer that question?</text>, <text>Hi Sonya,\n\nThanks for that. \n\nI'm currently working to coordinate with our storage team to help answer your questions. I appreciate your patience while I do so.\n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>/// Internal Comment - Collab Request for SBR-Storage\n-----------------------------------------------------\n\nShort summary\n-------------\n\nDuring a disk migration and switch reboot, there is an interruption in access to the storage. I have taken a look at things from a networking perspective, and I can see that traffic continued on at least one slave at a time during the switch failovers. \n\nHer questions are below:\n-----------------------\n\n- If the interfaces seem to be connected to the correct switches and the vlans are configured correctly, then what is going on at the storage level? \n- Is there anything in the logs that show SPM renegotiating or iSCSI paths going down? \n- Why would the VMs go into pause state if the network paths towards the storage networks seem to have always been up? Or do we need the captures to answer that question?\n\nI know that captures would likely answer the last question, but that will take until ~Nov 20th to answer. I am not sure of the rest of the requests. \n\nPlease also let me know if I need to loop sbr-virt back in, as they are making storage changes in RHV, or if I have misunderstood, and this is not a storage question. \n\nPlease also note that they are currently unable to make any maintenance changes right now, as there is an embargo currently.</text>, <text>The collab note isn't very descriptive and there are many sosreports attached to the case.  Looking at one of the latest: sosreport-sghv11.01967706-20171108225407.tar.xz\n\nBased on the logs it appears the issue is a target authentication failure due to failed tcp connection: \n\nNov  8 15:39:13 clgrabguhv11 kernel: i40e 0000:05:00.1 ens2f1: NIC Link is Down\nNov  8 15:39:13 clgrabguhv11 kernel: i40e 0000:05:00.1 ens2f1: speed changed to 0 for port ens2f1\n\nNov  8 15:39:13 clgrabguhv11 kernel: bond0: link status definitely down for interface ens2f1, disabling it\nNov  8 15:39:13 clgrabguhv11 kernel: bond0: first active interface up!\n[..]\nNov  8 15:39:14 clgrabguhv11 iscsid: connect to 100.70.84.193:3260 failed (No route to host)\nNov  8 15:39:14 clgrabguhv11 iscsid: connect to 100.70.84.193:3260 failed (No route to host)\n\nNov  8 15:39:16 clgrabguhv11 iscsid: Login authentication failed with target iqn.2010-01.com.solidfire:dx8n.vol-brlwvpcrfgd8a.35\nNov  8 15:39:16 clgrabguhv11 iscsid: Kernel reported iSCSI connection 24:0 error (1020 - ISCSI_ERR_TCP_CONN_CLOSE: TCP connection closed) state (1)\nNov  8 15:39:16 clgrabguhv11 iscsid: connection24:0 is operational after recovery (5 attempts)\n\n\n\nThe IP for this target is \n\nnode.conn[0].address = 100.70.84.193\n\nUsing 'default' iface:\n\n# BEGIN RECORD 6.2.0.873-35\nnode.name = iqn.2010-01.com.solidfire:dx8n.vol-brlwvpcrfgd8a.35\nnode.tpgt = 1\nnode.startup = manual\nnode.leading_login = No\niface.iscsi_ifacename = default\niface.transport_name = tcp\n[..]\nnode.session.timeo.replacement_timeout = 120\n\n\nWhich would be: \n---\n13: STORAGE_COMPUTE    inet 100.70.84.203/26 brd 100.70.84.255 scope global STORAGE_COMPUTE\\       valid_lft forever preferred_lft forever\n\nbridge name     bridge id               STP enabled     interfaces\nSTORAGE_COMPUTE         8000.1402ec6e27f8       no              bond0.2010\n\n\nBonding Mode: IEEE 802.3ad Dynamic link aggregation\n\nSlave Interface: eno50\nMII Status: up\nSpeed: 10000 Mbps\nDuplex: full\nLink Failure Count: 2\n\nSlave Interface: ens2f1\nMII Status: up\nSpeed: 10000 Mbps\nDuplex: full\nLink Failure Count: 2\n\niSCSI is single session per tcp connection and does not round-robin or failover w/ lacp.  If loosing a nic due to switch reboot, etc, iSCSI would need to re-establish the session on the next active slave which seems to be occurring here.</text>, <text>Hi Sonya,\n\nI reached out to our storage team, and it seems likely that the switch reboot caused the issue with storage. \n\nBased on the logs it appears the issue is a target authentication failure due to failed tcp connection: \n\nNov  8 15:39:13 clgrabguhv11 kernel: i40e 0000:05:00.1 ens2f1: NIC Link is Down\nNov  8 15:39:13 clgrabguhv11 kernel: i40e 0000:05:00.1 ens2f1: speed changed to 0 for port ens2f1\n\nNov  8 15:39:13 clgrabguhv11 kernel: bond0: link status definitely down for interface ens2f1, disabling it\nNov  8 15:39:13 clgrabguhv11 kernel: bond0: first active interface up!\n[..]\nNov  8 15:39:14 clgrabguhv11 iscsid: connect to 100.70.84.193:3260 failed (No route to host)\nNov  8 15:39:14 clgrabguhv11 iscsid: connect to 100.70.84.193:3260 failed (No route to host)\n\nNov  8 15:39:16 clgrabguhv11 iscsid: Login authentication failed with target iqn.2010-01.com.solidfire:dx8n.vol-brlwvpcrfgd8a.35\nNov  8 15:39:16 clgrabguhv11 iscsid: Kernel reported iSCSI connection 24:0 error (1020 - ISCSI_ERR_TCP_CONN_CLOSE: TCP connection closed) state (1)\nNov  8 15:39:16 clgrabguhv11 iscsid: connection24:0 is operational after recovery (5 attempts)\n\n\n\nThe IP for this target is \n\nnode.conn[0].address = 100.70.84.193\n\nUsing 'default' iface:\n\n# BEGIN RECORD 6.2.0.873-35\nnode.name = iqn.2010-01.com.solidfire:dx8n.vol-brlwvpcrfgd8a.35\nnode.tpgt = 1\nnode.startup = manual\nnode.leading_login = No\niface.iscsi_ifacename = default\niface.transport_name = tcp\n[..]\nnode.session.timeo.replacement_timeout = 120\n\n\nWhich would be: \n---\n13: STORAGE_COMPUTE    inet 100.70.84.203/26 brd 100.70.84.255 scope global STORAGE_COMPUTE\\       valid_lft forever preferred_lft forever\n\nbridge name     bridge id               STP enabled     interfaces\nSTORAGE_COMPUTE         8000.1402ec6e27f8       no              bond0.2010\n\n\nBonding Mode: IEEE 802.3ad Dynamic link aggregation\n\nSlave Interface: eno50\nMII Status: up\nSpeed: 10000 Mbps\nDuplex: full\nLink Failure Count: 2\n\nSlave Interface: ens2f1\nMII Status: up\nSpeed: 10000 Mbps\nDuplex: full\nLink Failure Count: 2\n\niSCSI is single session per TCP connection and does not round-robin or failover with LACP. In this instance, the switch was rebooted, so iSCSI needed to re-establish the session on the next active slave. It is likely what is occurring here. \n\nWe could likely further confirm this with a packet capture at the same time. We would likely not need the level of information I requested before, but would be helpful if we determine that this is not the cause. Although, this does seem to fit with the issue well. \n\nPlease let me know your thoughts and if you have any questions or concerns. \n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>Hi Sara,\nIs it because the "node.session.timeo.replacement_timeout = 120" (120 seconds = 2 minutes). And if the bond interface doesn't switch over to the other available interface within that time, then the iSCSI session times out?\n\nSo is it possible that we need to tweak the bond timers or maybe the iSCSI timers or is the answer to do multipathing so that we have two simultaneous TCP sessions and if one session dies, iSCSI will just use the other TCP session and then there should be no storage issue?\n\nThanks,\nSonya</text>, <text>Hi Sonya,\n\nI apologize for the delay in my response. I've been unexpectedly out of the office this week.\n\nI will need to do some research and reach out to my storage team for further input. I appreciate your patience, and I will work to get an answer for you by the end of my shift today (5 PM EST). I will update you by then with an answer, or a status update. \n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>/// Internal Comment\n\nCollaboration Note meant for SBR-Storage:\n\nBackground: During switch failover, connections are lost to iSCSI, even though there is an LACP bond in place, which should provide failover. \n\nJmag++ recently commented on this, and noted that iSCSI is single session per TCP connection and does not round-robin or failover with LACP. In this instance, the switch was rebooted, so iSCSI needed to re-establish the session on the next active slave. It is likely what is occurring here. \n\nI've copied and pasted his findings here:\n\nBased on the logs it appears the issue is a target authentication failure due to failed tcp connection: \n\nNov  8 15:39:13 clgrabguhv11 kernel: i40e 0000:05:00.1 ens2f1: NIC Link is Down\nNov  8 15:39:13 clgrabguhv11 kernel: i40e 0000:05:00.1 ens2f1: speed changed to 0 for port ens2f1\n\nNov  8 15:39:13 clgrabguhv11 kernel: bond0: link status definitely down for interface ens2f1, disabling it\nNov  8 15:39:13 clgrabguhv11 kernel: bond0: first active interface up!\n[..]\nNov  8 15:39:14 clgrabguhv11 iscsid: connect to 100.70.84.193:3260 failed (No route to host)\nNov  8 15:39:14 clgrabguhv11 iscsid: connect to 100.70.84.193:3260 failed (No route to host)\n\nNov  8 15:39:16 clgrabguhv11 iscsid: Login authentication failed with target iqn.2010-01.com.solidfire:dx8n.vol-brlwvpcrfgd8a.35\nNov  8 15:39:16 clgrabguhv11 iscsid: Kernel reported iSCSI connection 24:0 error (1020 - ISCSI_ERR_TCP_CONN_CLOSE: TCP connection closed) state (1)\nNov  8 15:39:16 clgrabguhv11 iscsid: connection24:0 is operational after recovery (5 attempts)\n\n\n\nThe IP for this target is \n\nnode.conn[0].address = 100.70.84.193\n\nUsing 'default' iface:\n\n# BEGIN RECORD 6.2.0.873-35\nnode.name = iqn.2010-01.com.solidfire:dx8n.vol-brlwvpcrfgd8a.35\nnode.tpgt = 1\nnode.startup = manual\nnode.leading_login = No\niface.iscsi_ifacename = default\niface.transport_name = tcp\n[..]\nnode.session.timeo.replacement_timeout = 120\n\n\nWhich would be: \n---\n13: STORAGE_COMPUTE    inet 100.70.84.203/26 brd 100.70.84.255 scope global STORAGE_COMPUTE\\       valid_lft forever preferred_lft forever\n\nbridge name     bridge id               STP enabled     interfaces\nSTORAGE_COMPUTE         8000.1402ec6e27f8       no              bond0.2010\n\n\nBonding Mode: IEEE 802.3ad Dynamic link aggregation\n\nSlave Interface: eno50\nMII Status: up\nSpeed: 10000 Mbps\nDuplex: full\nLink Failure Count: 2\n\nSlave Interface: ens2f1\nMII Status: up\nSpeed: 10000 Mbps\nDuplex: full\nLink Failure Count: 2\n\n\nHowever, Sonya still has a few questions moving forward:\n\n- "Is it because the "node.session.timeo.replacement_timeout = 120" (120 seconds = 2 minutes). And if the bond interface doesn't switch over to the other available interface within that time, then the iSCSI session times out?"\n\nThis seems logical to me, however, not being familiar with iSCSI beyond the RHCE, I'm not sure if this would be the proper solution. In some cases, it seemed the switch took a very long to reboot, while in other cases, it booted up quickly enough that there was no traffic loss. In my opinion, increasing this timeout is only a band-aid, and will still cause failures in the event that the next switch reboot takes longer than \n\n- "So is it possible that we need to tweak the bond timers or maybe the iSCSI timers or is the answer to do multipathing so that we have two simultaneous TCP sessions and if one session dies, iSCSI will just use the other TCP session and then there should be no storage issue?"\n\nIt seems to me that multipathing would be a better solution, but my knowledge of this is slim-to-none. As such I'd like some guidance on what to suggest. \n\nI understand there's not a lot of data in this, and if you need me to pull some stuff out of sosreports, I can certainly do this. Just add a comment and flip it back to waiting on me, and I'll provide whatever information you need.\n\nThanks for your help. \n\n/// Internal Comment</text>, <text>Hi Sonya,\n\nI just saw this case in the queue; did you still have questions on this after todays call?  Or were your current questions on this case covered in 01952296?\n\nMultipathing should be used with no bonding, and replacement_timeout should be dropped down.\n\nBest Regards,\n\nJohn Pittman\nCustomer Engagement and Experience\nRed Hat Inc.</text>, <text>I am trying to understand why the storage redundancy does not work with the current setup and hence the question about the timers. Is that the root cause? Regardless if it's the right setup or not, but is that the reason? I need to know the reason why the VMs lose access to storage when one of the switch reboots in the first place and if there's any temporary workaround that can be applied in the interim before we get to multipathing? Even with multipathing, we will have to test that redundancy is going to work fine with that option.</text>, <text>Hi Sonya,\n\nJon Magrini mentioned earlier in an internal note (relayed by Sara): 'iSCSI is single session per tcp connection and does not round-robin or failover w/ lacp.  If loosing a nic due to switch reboot, etc, iSCSI would need to re-establish the session on the next active slave which seems to be occurring here.'\n\nSo, this means that iSCSI will not follow the bond.  It will need to disconnect and recover on the next active slave.  Reducing your replacement timer can assist in the speed at which it does this, essentially helping your 'storage redundancy' and acting as a workaround until you can implement multipathing. You can knock that as far down as 5 seconds.  That is the level that is mentioned in the iSCSI/dm-multipath article posted in the other ticket. \n\nBut in any case, you will see the logout and login of the iscsi session when there is a nic failure.  The transition is not as seamless as it would be if you were using dm-multipath to handle failover.\n\nBest Regards,\n\nJohn Pittman\nCustomer Engagement and Experience\nRed Hat Inc.</text>, <text>I was wondering why we don't see the same behaviour in the lab and we have bonding configured there same as production and have not adjusted any timers as far as I know. Where and how do I check those timer settings again? Attached are the sosreports from my lab environment. If you can take a look to see what is the reason why the same thing is not observed there or if we can schedule a bomgar to do the same test in lab and see if in fact we do see the session logging out and logging in, but maybe it's fast enough that it doesn't cause an issue for the VMs?\n\nThanks,\nSonya</text>, <text>Hi Sonya,\n\nMy apologies, as I was not in the office on Friday due to the holiday. \n\nI am currently working to get this case transitioned over to the storage team for further assistance. I appreciate your time and patience. \n\nThank you,\nSara Ferguson\nTechnical Support Engineer\nRed Hat\n\nRHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration</text>, <text>/// Internal Comment\n\nChanging SBR to Storage, as this seems to have been about the iSCSI failover, and her questions have all been based on iSCSI + bonding (LACP) failover. \n\nJohn suggested they use multipath, but this is not something they can setup right now.\n\nJmag informed them that iSCSI doesn't have failover with LACP due to iSCSI's single session per tcp connection nature. \n\nI am NNO-ing for now. Feel free to send it back if it's goes back into Networking issues/questions. Or, if you feel it should stay in Networking, please let me know. \n\n## Issue\n\nWhen the switch is rebooted, there is no failover/redundancy to the secondary slave. Connections to storage (iSCSI) is then lost.\n\n## Environment\n\nRHEL 7.3, LACP bond\n\n## Diagnostic Steps\n\nI've reviewed the case, and it appears that the LACP bond should failover. However, as discussed with jmag, iSCSI is single session per tcp connection, and will not follow the bond. \n\n## Current Summary\n\nSonya has tested this in the lab, and has not seen the same behavior. She has seen that iSCSI does not have any interruptions when LACP fails over there. \n\n## Action Plan\n\n- Review Sonya's latest update and see how we can assist further.\n\n/// Internal Comment</text>, <text>Thank you Sonya for your time in the case,\n\nMy name is Taylor and I am with the Storage team here at Red Hat. I have been looking through the information in the case and see your last update;\n\n"I was wondering why we don't see the same behaviour in the lab and we have bonding configured there same as production and have not adjusted any timers as far as I know."\n\nIf this message means that this is working in a test lab then that is great. Unfortunately again, we cannot support this being used in a Production environment due to its unstable nature. \n\nI can definitely provide some timeout tuning parameters to be entered into /etc/iscsi/iscsid.conf, but again, using networking as a load balancer for failover is not recommended, regardless if it works or not. \n\nSara did state that you have to get through a switch upgrade before you can enable multipathing and I can understand this is critical but there is no way and unfortunately no configurations we can provide to SUPPORT failover using only iscsi even with LACP. Please understand that if something does happen during the time you are upgrading the switch and there is data loss we would be held accountable. With this in mind we have to stick with what is supported and best recommended practices. \n\nWe do have numerous people watching over this case and we have collaborated internally on this. We have even had Engineering team members make notes on this specific case here. \n\nPlease let us know if there are any questions.</text>, <text>Hi, I understand your comments below. Just one follow-up question. The LACP bonding is applied using similar servers and storage in a RedHat Openstack environment and they don't see an issue. Is it possible that the solidfire driver with cinder is masking this issue? How is it that in the openstack environments it works? Are you aware of how iSCSI cinder block storage works in a RedHat Openstack environment? Does it do an iSCSI logout/login command there when the NIC fails and are the default timers/bonding mode setup differently? Thanks.</text>, <text>Sonya,\n\nI appreciate your update. The fact of the matter is that this may work in your test environment which is great. Unfortunately, due to there being no control protocols or communication between the LACP bonding mode and the iscsi daemon, we cannot recommend or approve the use of teaming/bonding mode 4 LACP when dealing with iscsi connections for use for performance or failover. It is recommended to have the connection on binded ifaces and let our multipath daemon which is more responsible for determining what path to send IO down. \n\nEthernet bonding with Mode 4 LACP, is not stable enough even if it does work for you. The code for LACP and iscsi is just not there yet. \n\nAs for the cinder behavior and what it does during a network switch port issue, I am unaware of what the coding for cinder does in response to iscsi connection issues.  The failover may happen but we cannot guarantee in-order packets. \n\nPlease see the following article:\n\nhttps://access.redhat.com/solutions/41899\nIs using bonded nic interfaces recommended with iscsi?\n\n\nI understand that this may be confusing that Cinder with Openstack may be handling this okay but I would assume there would be the same issues in "out-of-order" packets so there fore we cannot support that either. There has to be control of packet sequencing and connection monitoring which is what Multipath was created for. \n\nI hope this helps.</text>, <text>Hello,\n\nI also was able to check in with OpenStack specialty group on this as an added information for you. They agreed that the Cinder is just a API that helps control the storage. Either way, the underlying configuration of having iscsi over bonded/teamed nics is not supported or recommended due to issues that could cause data corruption. \n\nIf you do this it would be considered unsupported and would be self-support.\n\nThanks.</text>, <text>You can close this case. Thanks.</text>, <text>Sonya,\n\nThank you for the update. I appreciate you letting us know.\n\nAs requested I will be closing this case. We look foward to assisting you in the future. \n\nThanks again.</text>]
ham	[[<description>What problem/issue/behavior are you having trouble with?  What do you expect to see?\n\nAttempting to delete a VM snapshot fails. Found the following in the vdsm logs\n\n\njsonrpc.Executor/7::DEBUG::2017-10-26 20:58:56,509::__init__::530::jsonrpc.JsonRpcServer::(_handle_request) Calling 'VM.merge' in bridge with {u'topVolUUID': u'be781fc6-af4d-4a83-bfa0-78264eb7ed20', u'vmID': u'534cd202-9aaa-44d5-90ab-28ea7b27a89f', u'drive': {u'poolID': u'78a89f5e-e88a-40dc-9467-9ccbb77f59d6', u'volumeID': u'be781fc6-af4d-4a83-bfa0-78264eb7ed20', u'domainID': u'c2ba9087-43d9-44b5-a862-32f3a4dafb95', u'imageID': u'b0e6436e-36c2-412a-b664-ff7e4e6369a8'}, u'bandwidth': u'0', u'jobUUID': u'd89c6f90-b699-4541-8f4a-8abca4126764', u'baseVolUUID': u'eae65ee2-7828-485b-aa2d-49089c7593cd'}\njsonrpc.Executor/7::ERROR::2017-10-26 20:58:56,527::__init__::550::jsonrpc.JsonRpcServer::(_handle_request) Internal server error\nTraceback (most recent call last):\n  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 545, in _handle_request\n    res = method(**params)\n  File "/usr/lib/python2.7/site-packages/vdsm/rpc/Bridge.py", line 195, in _dynamicMethod\n    result = fn(*methodArgs)\n  File "/usr/share/vdsm/API.py", line 810, in merge\n    return v.merge(drive, baseVolUUID, topVolUUID, bandwidth, jobUUID)\n  File "/usr/share/vdsm/virt/vm.py", line 4603, in merge\n    chains = self._driveGetActualVolumeChain([drive])\n  File "/usr/share/vdsm/virt/vm.py", line 4778, in _driveGetActualVolumeChain\n    volChain = self._diskXMLGetVolumeChainInfo(diskXML, drive)\n  File "/usr/share/vdsm/virt/vm.py", line 4761, in _diskXMLGetVolumeChainInfo\n    entry = VolumeChainEntry(pathToVolID(drive, path), path, alloc)\n  File "/usr/share/vdsm/virt/vm.py", line 4742, in pathToVolID\n    raise LookupError("Unable to find VolumeID for path '%s'", path)\nLookupError: ("Unable to find VolumeID for path '%s'", '')</description>], <text>Hi Marcus,\n\nThis is Jason from the North America office. I will be taking ownership of the case and have escalated this to our Engineering team for investigation. Can you please confirm the vm_name is:\n\nxfer02.c01.archive.gen.clt1.prod.mlbam.net \n\nand the host is:\n\nhyp80.clt1.prod.mlbam.net\n\nAdditionally is the vm presently up? If so Please DO NOT reboot/restart/power off for any reason.\n\n\nI see we have a sosreport from the host hyp80.clt1.prod.mlbam.net so I thank you for that as well.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>WIP:\n\nSite is running an unsupported Guest OS:\n\n-[ RECORD 1]\nvm_name                        | xfer02.c01.archive.gen.clt1.prod.mlbam.net\nmem_size_mb                    | 4096\nnum_of_io_threads              | 0\nnice_level                     | 0\ncpu_shares                     | 0\nvmt_guid                       | 82fcd601-e004-447a-b741-521dcb6c8a71\nos                             | 24\nstorage_pool_id                | 78a89f5e-e88a-40dc-9467-9ccbb77f59d6\nstorage_pool_name              | CLT1-LD\ncluster_description            | Application VMs on local storage (non-infra VMs)\ncluster_spice_proxy            | \nvmt_name                       | CentOS-7.2-Baseline-CLT1\nvmt_mem_size_mb                | 16384\nvmt_os                         | 24\nvmt_creation_date              | 2017-02-06 06:02:20+00\nvmt_child_count                | 422\nvmt_num_of_sockets             | 4\nvmt_cpu_per_socket             | 1\nvmt_threads_per_cpu            | 1\nvmt_num_of_cpus                | 4\nrun_on_vds                     | e1948acd-dd7e-46b3-98ff-ad1ee2662a15\nmigrating_to_vds               | \napp_list                       | kernel-3.10.0-327.el7,ovirt-guest-agent-common-1.0.11-1.el7\nvm_pool_name                   | \nvm_pool_id                     | \nvm_guid                        | 534cd202-9aaa-44d5-90ab-28ea7b27a89f\noriginal_template_id           | 82fcd601-e004-447a-b741-521dcb6c8a71\noriginal_template_name         | CentOS-7.2-Baseline-CLT1\nlast_stop_time                 | 2017-03-20 07:20:25.706+00\nguestos_codename               | Core\nguestos_distribution           | CentOS Linux\nguestos_kernel_version         | 3.10.0-327.el7.x86_64\nguestos_type                   | Linux\nguestos_version                | 7.2.1511\n\n\n\n\n# sh vm40p.sh xfer02.c01.archive.gen.clt1.prod.mlbam.net\n \nvm_static:                                                                                                              (0 = ok, 1 = locked)\n               vm_guid                |                  vm_name                   | entity_type | os | description |              cluster_id              | template_status |               vmt_guid               |         original_template_id         \n--------------------------------------+--------------------------------------------+-------------+----+-------------+--------------------------------------+-----------------+--------------------------------------+--------------------------------------\n 534cd202-9aaa-44d5-90ab-28ea7b27a89f | xfer02.c01.archive.gen.clt1.prod.mlbam.net | VM          | 24 |             | 6583532b-c88b-42ec-8c64-48437e657bee |                 | 82fcd601-e004-447a-b741-521dcb6c8a71 | 82fcd601-e004-447a-b741-521dcb6c8a71\n(1 row)\n\n *** Cluster = Compute-LocalDisk  ;  DC = CLT1-LD ***\n \n                                                                                                                                              Template =  CentOS-7.2-Baseline-CLT1\n \n                                                                                                                                              Original Template =  CentOS-7.2-Baseline-CLT1\n \n \nvm_dynamic:                           (0 = down, 1 = up, 4 = paused, 5/6 = migrating, 7 = unknown, 8 = notresp, 9 = wfl, 15 = locked)\n               vm_guid                | status |              run_on_vds              |       guest_os        \n--------------------------------------+--------+--------------------------------------+-----------------------\n 534cd202-9aaa-44d5-90ab-28ea7b27a89f |      1 | e1948acd-dd7e-46b3-98ff-ad1ee2662a15 | 3.10.0-327.el7.x86_64\n(1 row)\n\n                                                  *** Running on host hyp80.clt1.prod.mlbam.net ***\n \n \nvm_device:\n              device_id               |                vm_id                 | boot_order |         _create_date          |         _update_date         |    \n                       address                            \n--------------------------------------+--------------------------------------+------------+-------------------------------+------------------------------+--------------------------------------------------------------\n b0e6436e-36c2-412a-b664-ff7e4e6369a8 | 534cd202-9aaa-44d5-90ab-28ea7b27a89f |          1 | 2017-03-20 07:20:08.248309+00 | 2017-10-26 20:34:41.69904+00 | {slot=0x07, bus=0x00, domain=0x0000, type=pci, function=0x0}\n(1 row)\n\n \nimages:           (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | i\nmagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n be781fc6-af4d-4a83-bfa0-78264eb7ed20 | b0e6436e-36c2-412a-b664-ff7e4e6369a8 | 2bd90e89-3cc6-41b8-b9ed-32f396bd7007 | eae65ee2-7828-485b-aa2d-49089c7593cd |           1 | 2017-10-26 20:34:35+00 |           2 |             4 | t\n eae65ee2-7828-485b-aa2d-49089c7593cd | b0e6436e-36c2-412a-b664-ff7e4e6369a8 | c5a48c3c-3ceb-4d07-8e77-3040edd100b3 | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-03-20 07:20:20+00 |           2 |             4 | f\n(2 rows)\n\n    For image_guid = be781fc6-af4d-4a83-bfa0-78264eb7ed20 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 70067712\n    For image_guid = eae65ee2-7828-485b-aa2d-49089c7593cd , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date    \n    \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------\n c5a48c3c-3ceb-4d07-8e77-3040edd100b3 | 534cd202-9aaa-44d5-90ab-28ea7b27a89f | REGULAR       | OK     | Backup__2017-10-26__20.34.32 | 2017-10-26 20:34:35.615+00\n 2bd90e89-3cc6-41b8-b9ed-32f396bd7007 | 534cd202-9aaa-44d5-90ab-28ea7b27a89f | ACTIVE        | OK     | Active VM                    | 2017-03-20 07:20:06.477+00\n(2 rows)\n\n \nbase_disks:\n               disk_id                | wipe_after_delete | propagate_errors | disk_alias | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n b0e6436e-36c2-412a-b664-ff7e4e6369a8 | f                 | Off              | rootdisk   |                  | f         |      |         0 |                 \n    |                 0 | \n(1 row)\n\n \ndisk_vm_element:\n               disk_id                |                vm_id                 | is_boot | disk_interface \n--------------------------------------+--------------------------------------+---------+----------------\n b0e6436e-36c2-412a-b664-ff7e4e6369a8 | 534cd202-9aaa-44d5-90ab-28ea7b27a89f | t       | VirtIO\n(1 row)</text>, <text>yes that is the VM and the proper host.</text>, <text>and yes the VM is online</text>, <text>======COLLABORATION======\n\nMight be related to https://access.redhat.com/solutions/542243 which has a lot of good notes on what we should be looking for in here. Let's get audit log and engine logs from what happened to start with as well as find the VDSM log that has "2017-10-26 20:58:56,509" in it.\n\n======COLLABORATION======</text>, <text>Hi, I'm Allan. I'm also assisting with this case.\n\nWe're working to put together a list of any additional data we might need. We'll also engage engineering for this issue.\n\nThanks,\nAllan Voss, RHCE, RHCVA\nSenior Technical Support Engineer\nRed Hat, Inc.\n\nIf your systems are registered on Red Hat Network (RHN), visit https://access.redhat.com/products/red-hat-subscription-management/  to learn why you need to migrate to the Red Hat Subscription Management (RHSM) interface by July 2017.\n\nRHEV 3.x series was retired on September 30, 2017:\nhttps://access.redhat.com/support/policy/updates/rhev\n\nPreparing to upgrade? Try the RHEV Upgrade Helper tool:\nhttps://access.redhat.com/labs/rhevupgradehelper/\n\nHave you configured regular database backups? \nhttps://access.redhat.com/solutions/61993  (RHEV 3.0 to 3.2)\nhttps://access.redhat.com/solutions/797463 (RHEV 3.3 to 3.6)\n\nIf you're using Self Hosted Engine, perform you database backups as per the Hosted Engine Guide:\nhttps://access.redhat.com/documentation/en-us/red_hat_virtualization/4.1/html/self-hosted_engine_guide/chap-backing_up_and_restoring_a_rhel-based_self-hosted_environment</text>, <text>Hi,\nWhy was this case closed ? I\u2019ve been waiting for an update for quite some time now\n\nMarcus Torres\nInfrastructure Architect\nBAMTECH Media\n75 9th Ave, New York NY 10020\ntele: 646-866-2609\n________________________________\nFrom: Red Hat Support &lt;support@redhat.com&gt;\nSent: Monday, November 13, 2017 6:05:35 AM\nTo: Torres, Marcus\nSubject: (Closed) (SEV 3) Case #01961808 (can't delete snapshot) ref:_00DA0HxWH._500A0YfnPO:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01961808\nCase Title       : can't delete snapshot\nCase Number      : 01961808\nCase Open Date   : 2017-10-27 10:49:54\nSeverity         : 3 (Normal)\nProblem Type     : Defect / Bug\n\nMost recent comment: On 2017-10-27 17:22:50, Voss, Allan commented:\n"Hi, I'm Allan. I'm also assisting with this case.\n\nWe're working to put together a list of any additional data we might need. We'll also engage engineering for this issue.\n\nThanks,\nAllan Voss, RHCE, RHCVA\nSenior Technical Support Engineer\nRed Hat, Inc.\n\nIf your systems are registered on Red Hat Network (RHN), visit https://access.redhat.com/products/red-hat-subscription-management/  to learn why you need to migrate to the Red Hat Subscription Management (RHSM) interface by July 2017.\n\nRHEV 3.x series was retired on September 30, 2017:\nhttps://access.redhat.com/support/policy/updates/rhev\n\nPreparing to upgrade? Try the RHEV Upgrade Helper tool:\nhttps://access.redhat.com/labs/rhevupgradehelper/\n\nHave you configured regular database backups?\nhttps://access.redhat.com/solutions/61993  (RHEV 3.0 to 3.2)\nhttps://access.redhat.com/solutions/797463 (RHEV 3.3 to 3.6)\n\nIf you're using Self Hosted Engine, perform you database backups as per the Hosted Engine Guide:\nhttps://access.redhat.com/documentation/en-us/red_hat_virtualization/4.1/html/self-hosted_engine_guide/chap-backing_up_and_restoring_a_rhel-based_self-hosted_environment"\n\n---------------------------------------\n\nHello Marcus Torres,\n\nIt has been 16 days since we requested additional information on your inquiry.  As we have not received a response, we are unable to continue resolving your inquiry further at this time.  Your case will be archived, but will be reopened if/when you contact us or provide the requested information via case comments.\n\nIn the next few days, you may receive an email request to complete a survey regarding your support experience.\n\nPlease take this opportunity to let us know what we did well and where we can improve.\n\nIn case you don't receive the email request but would like to share your feedback, please feel free to reply back to this email or update the case.\n\nYour comments help us continually refine the customer experience to provide you with better service and quicker resolution.\n\nThank you for taking the time.\n\nWe hope your issue is resolved and hope you had a pleasant support experience.\n\nRegards,\nRed Hat Customer Experience and Engagement\n\nNote: This is an automated response.\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/\nRed Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0YfnPO:ref</text>, <text>Hi Marcus,\n\nI apologize the case appears to have accidentally auto-closed. I see it is now re-opened and I am re-engaging with Allan about his last update a couple weeks prior.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Hello\n\nOn the host where the VM is running can you provide the output of the following:\nSeems like there was a live merge (snapshot) operation that seems to have failed in the past.\n\nCan you provide the following from the host where the VM is running.\n\n~~~\ncd /rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8\nsu vdsm -s /bin/sh -c 'cat be781fc6-af4d-4a83-bfa0-78264eb7ed20.meta'\nsu vdsm -s /bin/sh -c 'cat eae65ee2-7828-485b-aa2d-49089c7593cd.meta'\nsu vdsm -s /bin/sh -c 'cat fa048414-a3e0-44cf-818d-42688a995004.meta'\n\nsu vdsm -s /bin/sh -c 'qemu-img info be781fc6-af4d-4a83-bfa0-78264eb7ed20'\nsu vdsm -s /bin/sh -c 'qemu-img info eae65ee2-7828-485b-aa2d-49089c7593cd'\nsu vdsm -s /bin/sh -c 'qemu-img info fa048414-a3e0-44cf-818d-42688a995004'\n\nvirsh -r dumpxml xfer02.c01.archive.gen.clt1.prod.mlbam.net &gt; /tmp/xfer02.c01.archive.gen.clt1.prod.mlbam.net.xml ##Attach the /tmp/xfer02.c01.archive.gen.clt1.prod.mlbam.net.xml to the case.\n~~~\n\nAlso please do not restart the VM as the DB contains incorrect information.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc\nGSS\n\n\nINVESTIGATION:\n~~~~~~~~~~~~~~~\n// Working Notes - these notes are not intended as a meaningful communication\n// but rather an indicator of current thought processes and reference.\n// Please feel free to comment and ask questions concerning them.\n\n\n[I1]\tVM info in the DB\n\n\n~~~\nvm_static:                                                                                                              (0 = ok, 1 = locked)\n               vm_guid                |                  vm_name                   | entity_type | os | description |              cluster_id              | template_status |               vmt_guid               |         original_template_id         \n--------------------------------------+--------------------------------------------+-------------+----+-------------+--------------------------------------+-----------------+--------------------------------------+--------------------------------------\n 534cd202-9aaa-44d5-90ab-28ea7b27a89f | xfer02.c01.archive.gen.clt1.prod.mlbam.net | VM          | 24 |             | 6583532b-c88b-42ec-8c64-48437e657bee |                 | 82fcd601-e004-447a-b741-521dcb6c8a71 | 82fcd601-e004-447a-b741-521dcb6c8a71\n(1 row)\n                                                                                       *** Cluster = Compute-LocalDisk  ;  DC = CLT1-LD ***\n                                                                                                                                              Template =  CentOS-7.2-Baseline-CLT1\n                                                                                                                                              Original Template =  CentOS-7.2-Baseline-CLT1\n \n \nvm_dynamic:                           (0 = down, 1 = up, 4 = paused, 5/6 = migrating, 7 = unknown, 8 = notresp, 9 = wfl, 15 = locked)\n               vm_guid                | status |              run_on_vds              |       guest_os        \n--------------------------------------+--------+--------------------------------------+-----------------------\n 534cd202-9aaa-44d5-90ab-28ea7b27a89f |      1 | e1948acd-dd7e-46b3-98ff-ad1ee2662a15 | 3.10.0-327.el7.x86_64\n(1 row)\n\n                                                  *** Running on host hyp80.clt1.prod.mlbam.net ***\n \n \nvm_device:\n              device_id               |                vm_id                 | boot_order |         _create_date          |         _update_date         |                           address                            \n--------------------------------------+--------------------------------------+------------+-------------------------------+------------------------------+--------------------------------------------------------------\n b0e6436e-36c2-412a-b664-ff7e4e6369a8 | 534cd202-9aaa-44d5-90ab-28ea7b27a89f |          1 | 2017-03-20 07:20:08.248309+00 | 2017-10-26 20:34:41.69904+00 | {slot=0x07, bus=0x00, domain=0x0000, type=pci, function=0x0}\n(1 row)\n\n \nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n eae65ee2-7828-485b-aa2d-49089c7593cd | b0e6436e-36c2-412a-b664-ff7e4e6369a8 | c5a48c3c-3ceb-4d07-8e77-3040edd100b3 | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-03-20 07:20:20+00 |           2 |             4 | f\n be781fc6-af4d-4a83-bfa0-78264eb7ed20 | b0e6436e-36c2-412a-b664-ff7e4e6369a8 | 2bd90e89-3cc6-41b8-b9ed-32f396bd7007 | eae65ee2-7828-485b-aa2d-49089c7593cd |           1 | 2017-10-26 20:34:35+00 |           2 |             4 | t\n\n(2 rows)\n\n    For image_guid = be781fc6-af4d-4a83-bfa0-78264eb7ed20 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 70067712\n    For image_guid = eae65ee2-7828-485b-aa2d-49089c7593cd , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        | memory_volume \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------+---------------\n c5a48c3c-3ceb-4d07-8e77-3040edd100b3 | 534cd202-9aaa-44d5-90ab-28ea7b27a89f | REGULAR       | OK     | Backup__2017-10-26__20.34.32 | 2017-10-26 20:34:35.615+00 | \n 2bd90e89-3cc6-41b8-b9ed-32f396bd7007 | 534cd202-9aaa-44d5-90ab-28ea7b27a89f | ACTIVE        | OK     | Active VM                    | 2017-03-20 07:20:06.477+00 | \n(2 rows)\n\n \nbase_disks:\n               disk_id                | wipe_after_delete | propagate_errors | disk_alias | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n b0e6436e-36c2-412a-b664-ff7e4e6369a8 | f                 | Off              | rootdisk   |                  | f         |      |         0 |                     |                 0 | \n(1 row)\n\n \ndisk_vm_element:\n               disk_id                |                vm_id                 | is_boot | disk_interface \n--------------------------------------+--------------------------------------+---------+----------------\n b0e6436e-36c2-412a-b664-ff7e4e6369a8 | 534cd202-9aaa-44d5-90ab-28ea7b27a89f | t       | VirtIO\n(1 row)\n~~~\n\n\n[I2]\t Images on the SD.\n\n~~~\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8:\ntotal 2433286\n-rw-rw----.   1 vdsm kvm   55771136 Oct 27 14:53 be781fc6-af4d-4a83-bfa0-78264eb7ed20\n-rw-rw----.   1 vdsm kvm    1048576 Oct 26 20:34 be781fc6-af4d-4a83-bfa0-78264eb7ed20.lease\n-rw-r--r--.   1 vdsm kvm        260 Oct 26 20:34 be781fc6-af4d-4a83-bfa0-78264eb7ed20.meta\n-rw-rw----.   1 vdsm kvm     197632 Mar 20  2017 eae65ee2-7828-485b-aa2d-49089c7593cd\n-rw-rw----.   1 vdsm kvm    1048576 Mar 20  2017 eae65ee2-7828-485b-aa2d-49089c7593cd.lease\n-rw-r--r--.   1 vdsm kvm        264 Oct 26 20:34 eae65ee2-7828-485b-aa2d-49089c7593cd.meta\n-rw-rw----. 416 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 416 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 416 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n~~~\n\n[I3]\t Only the be781fc6 is open for use.\n~~~\n$ grep be781fc6 lsof_-b_M_-n_-l \nqemu-kvm  18239            107   82u      REG               0,40    55771136 4302792729 /rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20\n\n$ grep eae65ee2 lsof_-b_M_-n_-l \n$ grep fa048414 lsof_-b_M_-n_-l\n~~~\n\n[I4]\tImage not found.\n\n~~~\n2017-10-26 21:02:09,846 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.MergeVDSCommand] (pool-5-thread-7) [5d054a26] START, MergeVDSCommand(HostName = hyp80.clt1.prod.mlbam.net, MergeVDSCommandParameters:{runAsync='true', hostId='e1948acd-dd7e-46b3-98ff-ad1ee2662a15', vmId='534cd202-9aaa-44d5-90ab-28ea7b27a89f', storagePoolId='78a89f5e-e88a-40dc-9467-9ccbb77f59d6', storageDomainId='c2ba9087-43d9-44b5-a862-32f3a4dafb95', imageGroupId='b0e6436e-36c2-412a-b664-ff7e4e6369a8', imageId='be781fc6-af4d-4a83-bfa0-78264eb7ed20', baseImageId='eae65ee2-7828-485b-aa2d-49089c7593cd', topImageId='be781fc6-af4d-4a83-bfa0-78264eb7ed20', bandwidth='0'}), log id: 243c2303\n2017-10-26 21:02:09,875 WARN  [org.ovirt.engine.core.vdsbroker.vdsbroker.MergeVDSCommand] (pool-5-thread-7) [5d054a26] Unexpected return value: StatusForXmlRpc [code=-32603, message=("Unable to find VolumeID for path '%s'", '')]\n2017-10-26 21:02:09,875 ERROR [org.ovirt.engine.core.vdsbroker.vdsbroker.MergeVDSCommand] (pool-5-thread-7) [5d054a26] Failed in 'MergeVDS' method\n2017-10-26 21:02:09,875 WARN  [org.ovirt.engine.core.vdsbroker.vdsbroker.MergeVDSCommand] (pool-5-thread-7) [5d054a26] Unexpected return value: StatusForXmlRpc [code=-32603, message=("Unable to find VolumeID for path '%s'", '')]\n2017-10-26 21:02:09,877 ERROR [org.ovirt.engine.core.dal.dbbroker.auditloghandling.AuditLogDirector] (pool-5-thread-7) [5d054a26] Correlation ID: null, Call Stack: null, Custom Event ID: -1, Message: VDSM hyp80.clt1.prod.mlbam.net command failed: ("Unable to find VolumeID for path '%s'", '')\n2017-10-26 21:02:09,877 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.MergeVDSCommand] (pool-5-thread-7) [5d054a26] Command 'org.ovirt.engine.core.vdsbroker.vdsbroker.MergeVDSCommand' return value 'StatusOnlyReturnForXmlRpc [status=StatusForXmlRpc [code=-32603, message=("Unable to find VolumeID for path '%s'", '')]]'\n2017-10-26 21:02:09,877 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.MergeVDSCommand] (pool-5-thread-7) [5d054a26] HostName = hyp80.clt1.prod.mlbam.net\n2017-10-26 21:02:09,877 ERROR [org.ovirt.engine.core.vdsbroker.vdsbroker.MergeVDSCommand] (pool-5-thread-7) [5d054a26] Command 'MergeVDSCommand(HostName = hyp80.clt1.prod.mlbam.net, MergeVDSCommandParameters:{runAsync='true', hostId='e1948acd-dd7e-46b3-98ff-ad1ee2662a15', vmId='534cd202-9aaa-44d5-90ab-28ea7b27a89f', storagePoolId='78a89f5e-e88a-40dc-9467-9ccbb77f59d6', storageDomainId='c2ba9087-43d9-44b5-a862-32f3a4dafb95', imageGroupId='b0e6436e-36c2-412a-b664-ff7e4e6369a8', imageId='be781fc6-af4d-4a83-bfa0-78264eb7ed20', baseImageId='eae65ee2-7828-485b-aa2d-49089c7593cd', topImageId='be781fc6-af4d-4a83-bfa0-78264eb7ed20', bandwidth='0'})' execution failed: VDSGenericException: VDSErrorException: Failed to MergeVDS, error = ("Unable to find VolumeID for path '%s'", ''), code = -32603\n~~~\n\n~~~\njsonrpc.Executor/1::DEBUG::2017-10-26 21:02:09,848::__init__::530::jsonrpc.JsonRpcServer::(_handle_request) Calling 'VM.merge' in bridge with {u'topVolUUID': u'be781fc6-af4d-4a83-bfa0-78264eb7ed20', u'vmID': u'534cd202-9aaa-44d5-90ab-28ea7b27a89f', u'drive': {u'poolID': u'78a89f5e-e88a-40dc-9467-9ccbb77f59d6', u'volumeID': u'be781fc6-af4d-4a83-bfa0-78264eb7ed20', u'domainID': u'c2ba9087-43d9-44b5-a862-32f3a4dafb95', u'imageID': u'b0e6436e-36c2-412a-b664-ff7e4e6369a8'}, u'bandwidth': u'0', u'jobUUID': u'cfea3353-b993-402e-aa83-6f9795bf123e', u'baseVolUUID': u'eae65ee2-7828-485b-aa2d-49089c7593cd'}\njsonrpc.Executor/1::ERROR::2017-10-26 21:02:09,866::__init__::550::jsonrpc.JsonRpcServer::(_handle_request) Internal server error\nTraceback (most recent call last):\n  File "/usr/lib/python2.7/site-packages/yajsonrpc/__init__.py", line 545, in _handle_request\n    res = method(**params)\n  File "/usr/lib/python2.7/site-packages/vdsm/rpc/Bridge.py", line 195, in _dynamicMethod\n    result = fn(*methodArgs)\n  File "/usr/share/vdsm/API.py", line 810, in merge\n    return v.merge(drive, baseVolUUID, topVolUUID, bandwidth, jobUUID)\n  File "/usr/share/vdsm/virt/vm.py", line 4603, in merge\n    chains = self._driveGetActualVolumeChain([drive])\n  File "/usr/share/vdsm/virt/vm.py", line 4778, in _driveGetActualVolumeChain\n    volChain = self._diskXMLGetVolumeChainInfo(diskXML, drive)\n  File "/usr/share/vdsm/virt/vm.py", line 4761, in _diskXMLGetVolumeChainInfo\n    entry = VolumeChainEntry(pathToVolID(drive, path), path, alloc)\n  File "/usr/share/vdsm/virt/vm.py", line 4742, in pathToVolID\n    raise LookupError("Unable to find VolumeID for path '%s'", path)\nLookupError: ("Unable to find VolumeID for path '%s'", '')\njsonrpc.Executor/1::INFO::2017-10-26 21:02:09,868::__init__::513::jsonrpc.JsonRpcServer::(_serveRequest) RPC call VM.merge failed (error -32603) in 0.02 seconds\n~~~</text>, <text>I\u2019ll get this info over to you ASAP. Please note that these particular vms utilize the localdisk vdsm hook that red hat provided us on an exception basis. The disk for the vm lies in an Lvm volume at the os level living on local disk storage attached to the hypervisor</text>, <text>Hi,\nIn addition to this, we had a backup application attempt to create snapshots for a large number of VMs through the RHEV API but that seems to have failed. The snapshots were created, but the tasks remain in the db. Please see attached screenshot. THere are 67 open tasks. Snapshot deletion is not working for these as well.\n\nAgain, these are 'localdisk' VM's which utilize a vdsm hook to put the root disk on local disk storage in an LVM volume at the OS level.</text>, <text>In regards to my last update, i have a process documented for clearing up old hung tasks from the db ...\n\n1. make sure no active tasks are running on the SPM\n\n[root@hyp42 ~]# vdsClient -s 0 getAllTasksStatuses\n{'status': {'message': 'OK', 'code': 0}, 'allTasksStatus': {}}\n\n\n2. verify there are no jobs in the async tasks \n\n[root@rhvm-engine01 ~]# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "select * from async_tasks;"\n task_id | action_type | status | result | step_id | command_id | started_at | storage_pool_id | task_type | vdsm_task_id | root_command_id | user_id\n---------+-------------+--------+--------+---------+------------+------------+-----------------+-----------+--------------+-----------------+---------\n(0 rows)\n\n3. clear the task...\n\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "delete from job where correlation_id = '486ee79f'";\n\n\n\n- - - - - - - - - - -\n\nShould i go ahead with this operation for all 65 open hung tasks here ?</text>, <text>Your requested files are attached.\n\nSince this is an exception based custom vdsm hook environment, if you would prefer to jump on the phone and do a blue jeans i can accomodate that.</text>, <text>Hi Marcus,\n\nThank you for the update. I am presently working with Bimal and my Software Maintenance Engineers on this. We will provide an update as soon as possible. For the moment please hold on any task killing so we can ensure they are not still attempting to actively complete.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>We've confirmed that some of these VM's when rebooted are lacking the proper rootdisk definition/path. We have an unrelated failure bring down a hypervisor (memory errors), when restarting the VMs some of the VMs failed to start due to the error shown in the screenshot. We're concerned this might be for a large set of VMs. Please advise</text>, <text>Requesting FTS support on this issue. We currently have production VM's unable to boot and potentially over 100 VMs affected that we're concerned that if rebooted, might not come up</text>, <text>// int\n\n\nI've spoken with the customer. This is the result of an errant RHV backup system run. It targeted a datacenter with local disk systems.\n\nDue to how the local disk workaround is architected, snapshots are not supported. Issues are to be expected if attempted, the back-end storage mechanism is not set up for snapshots. \n\nThe offending backup job has been stopped forever from targeting any more localdisk datacenters.\n\n*** BEFORE WE START DOING ANYTHING DESTRUCTIVE ***\n\nI understand the architecture of this workaround pretty well.\n\nA VM starts up, the hook analyzes the intended disk information, and attempts to connect an LV of the same name to the VM in place of the actual disk.\nThe snapshots changed the 'name' of the target disk image in RHV\nERGO\nIf we make RHV forget the snapshot exists, it should go back to pointing to the old disk, and everything should be honky dory.\n\nThe snapshots *are not real* - they were created in metadata. My hope is that clearing the metadata of the snapshots should instantly resolve all of this.\n\nThis is being raised to SEV 1 as I type this.</text>, <text>A bit more context, I left out a detail.\n\nA VM is created. \n"Dummy" disks are created on a simple NFS share, thin-provisioned. 20MB overhead total. NO DATA IS EVER STORED HERE.\nWhen the VM is started, the VDSM hook analyzes the disk metadata, takes the disk image hash, and creates or attached a logical volume to it, depending on whether that volume exists or not.\n\nThe snapshot changed the disk metadata information. The original disk image still exists. It's important that we retain that original disk image, as its hash *must not change*. If it does, this gets a bit messier, as we'll have to rename LVs.\n\nThe operation we want to do is to completely undo the snapshot being taken, restoring the metadata to how it was before the snapshot took effect. The snapshot itself only affected the disks on the NFS share.\n\nAs discussed on IRC, please call me when you are able and I will walk you through how this works.\n\n+1 619 512 3517</text>, <text>Bimal and I are currently on Bluejeans with Ashton https://bluejeans.com/u/asdavis/</text>, <text>Hello,\n\nThank you for contacting Red Hat Support! My name is Robert McSwain and I am the North American virtualization engineer who will be assisting you on this case moving forward at this time. My coworker Bimal Chollera and I just got off a phone call with Ashton Davis to discuss this scenario. \n\nAction Items:\n\n1) Could you please provide a fresh logcollector that's complete with host sosreports?\n\nHow to collect logs in RHEV 3 and RHV 4 \nhttps://access.redhat.com/knowledge/solutions/61546\n\nRHV 4:\n# ovirt-log-collector\n\nOnce we have this we'll be able to begin investigating what's happened with each VM and begin pointing the database back at the right LVs for each VM.\n\n2) Please confirm which VMs are in this state which are \na) Non-critical - Ashton mentioned you should have a VM we can test with before we do anything with VMs which need to go back into production. \nb) Most critical - We can focus on these once we have a proof of concept with the test VM\nc) Critical - All the leftover VMs in this state which we'll tackle last.\n\nIf you have any questions or concerns about this case, please either update the case at access.redhat.com or feel free to give us a call at 1-888-467-3342, referencing this case number and someone will get you in touch with me if I'm available.\n\nRegards,\nRobert McSwain, RHCE, RHCVA\nSenior Technical Support Engineer\nCustomer Experience and Engagement, North America\nRed Hat, Inc.</text>, <text>there are near 96 hypervisors in this cluster, a complete ovirt-log-collection might not be possible. it will fill up the disk. \n\nat this time we believe we have 111 VM's affected by this. This is detailed in scope.txt\n\nAnything with .prod. in the hostname is Most Critical, followed by .qa. being 'critical' and 'dev' being non-critical.</text>, <text>and correct i have a test host we can mess around with</text>, <text>ovirt-log-collection without hosts (--no-hypervisors) uploaded.</text>, <text>Marcus,\n\nThat's fine to get individual host sosreports from the hosts that we're working on then. \n\nCould you please provide the following then\n\n1. The name of the VM we can test with. Ashton said there might be one of these, so let's get a test environment dedicated where we can break/fix this VM in test then we can move onto the production VMs.\n\n2. A sosreport from the host where that Test VM runs.\n\n3. Pick one .prod VM to start with and provide the sosreport from the host it runs on. We'll continue our investigation after the Test VM and first .prod VM are up and running with any other .prod level VMs that are on this host. \n\nFuture Plans. When we complete work with VMs on a single host then please pick another .prod level VM on a different host and provide a sosreport from that host. \n\nRegards,  \nRobert McSwain, RHCE, RHCVA \nSenior Technical Support Engineer\nCustomer Experience &amp; Engagement, Red Hat, Inc.\n\n~~~~~~~~~~~~~~~~~~~~IMPORTANT~~~~~~~~~~~~~~~~~~~~\nRHEV 3.x series has reached Product Retirement as of September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev\n\nPlease upgrade as soon as possible to retain supportability of your environment. Ready to upgrade to RHV 4.0 from RHEV 3.6? Double check our RHV Upgrade Helper!\nhttps://access.redhat.com/labs/rhevupgradehelper/\n\nHave you backed up your RHEV database recently? \nhttps://access.redhat.com/solutions/61993</text>, <text>our test VM is 'depot01.clt1.util.mlbam.net' which resides on hyp96.clt1.prod.mlbam.net . It's currently in a failed state and is powered off after a failed reboot. \nThis VM has 3 disks so i think it's a good use case. \n\ni've recreated a ovirt-log-collection to include info from hyp96 and attached to the case</text>, <text>for the first 'prod' test VM we'll do, we can use aspera01.c01.inf.datg.clt1.prod.mlbam.net\n\nwhich is on hypervisor hyp103.clt1.prod.mlbam.net\n\nthe ovirt-log-collection with data from hyp103 is uploaded as file:  sosreport-LogCollector-20171114233106.tar.xz.</text>, <text>Marcus,\n\nThanks, we're getting the data loaded into our database viewers and beginning analysis now. We'll update you whenever we have action items or once an hour to let you know what we're working on, whichever is most appropriate at that time.\n\nRegards,  \nRobert McSwain, RHCE, RHCVA \nSenior Technical Support Engineer\nCustomer Experience &amp; Engagement, Red Hat, Inc.\n\n~~~~~~~~~~~~~~~~~~~~IMPORTANT~~~~~~~~~~~~~~~~~~~~\nRHEV 3.x series has reached Product Retirement as of September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev\n\nPlease upgrade as soon as possible to retain supportability of your environment. Ready to upgrade to RHV 4.0 from RHEV 3.6? Double check our RHV Upgrade Helper!\nhttps://access.redhat.com/labs/rhevupgradehelper/\n\nHave you backed up your RHEV database recently? \nhttps://access.redhat.com/solutions/61993</text>, <text>Thanks Robert, talk to you in an hour</text>, <text>======INTERNAL ONLY - DO NOT PUBLISH======\n\nCurrent Notes:\n\nhttps://dbview3.rhev.gsslab.rdu.redhat.com:31087/ovirt-engine/webadmin     \n# ssh -p 31636 root@dbview3.rhev.gsslab.rdu.redhat.com\n\nTest VM to experiment on: \n'depot01.clt1.util.mlbam.net' which resides on hyp96.clt1.prod.mlbam.net - 3 disks\nsosreport-LogCollector-20171114231805.tar.xz has the host sosreport for this VM\n\nFirst Prod VM to resolve after Test VM has been fixed and we have a plan for other VMs moving forward:\n'aspera01.c01.inf.datg.clt1.prod.mlbam.net' which resides on hyp103.clt1.prod.mlbam.net\nsosreport-LogCollector-20171114233106.tar.xz has the host sosreport for this VM\n\nThe vdsm hook intercepts mounting the NFS storage that the manager knows about and instead points the storage to a logical volume on the local filesystem. The issue currently is that a backup script was used which created snapshots. Unfortunately the snapshots point to non-existent logical volumes as snapshots do not work in this environment with this VDSM hook. See https://bluejeans.com/s/i6FHw/ for more information from where this was discussed by Ashton Davis with myself and Bimal Chollera. The hook, as well as how we expect to resolve this, is in the bluejeans and will provide a bit more context than this update can provide. \n\nFull list of affected VMs from "scope.txt"\nactor01.c01.razcp.gen.clt1.dev.mlbam.net:hyp14.clt1.prod.mlbam.net\t181days\tlocaldisk=\nactor02.c01.razcp.gen.clt1.dev.mlbam.net:hyp21.clt1.prod.mlbam.net\t1day\tlocaldisk=\napi01.c01.razcp.gen.clt1.dev.mlbam.net:hyp27.clt1.prod.mlbam.net\t92days\tlocaldisk=\napi02.c01.razcp.gen.clt1.dev.mlbam.net:hyp45.clt1.prod.mlbam.net\t179days\tlocaldisk=\naspera01.c01.inf.datg.clt1.prod.mlbam.net:hyp103.clt1.prod.mlbam.net\t53days\tlocaldisk=\naspera01.c01.inf.wwe.clt1.prod.mlbam.net:hyp103.clt1.prod.mlbam.net\t51days\tlocaldisk=\naspera01.c01.jobmgr.gen.clt1.prod.mlbam.net:hyp07.clt1.prod.mlbam.net\t271days\tlocaldisk=\naspera02.c01.inf.datg.clt1.prod.mlbam.net:hyp105.clt1.prod.mlbam.net\t54days\tlocaldisk=\naspera02.c01.jobmgr.gen.clt1.prod.mlbam.net:hyp70.clt1.prod.mlbam.net\t271days\tlocaldisk=\nasperaproxy01.c01.inf.datg.clt1.prod.mlbam.net:hyp101.clt1.prod.mlbam.net\t54days\tlocaldisk=\ncbdb01.ingest01.hls.hulu.clt1.prod.mlbam.net:hyp06.clt1.prod.mlbam.net\t105days\tlocaldisk=\ncbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net:hyp77.clt1.prod.mlbam.net\t66days\tlocaldisk=\ncbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net:hyp93.clt1.prod.mlbam.net\t93days\tlocaldisk=\ncbdb01.ingest07.hls.hulu.clt1.prod.mlbam.net:hyp29.clt1.prod.mlbam.net\t94days\tlocaldisk=\ncbdb01.ingest08.hls.hulu.clt1.prod.mlbam.net:hyp90.clt1.prod.mlbam.net\t11days\tlocaldisk=\ncbdb01.mlbtv01.db.mlb.clt1.prod.bamtech.co:hyp16.clt1.prod.mlbam.net\t5days\tlocaldisk=\ncbdb02.ingest01.hls.hulu.clt1.prod.mlbam.net:hyp07.clt1.prod.mlbam.net\t89days\tlocaldisk=\ncbdb02.ingest02.hls.hulu.clt1.prod.mlbam.net:hyp75.clt1.prod.mlbam.net\t116days\tlocaldisk=\ncbdb02.ingest03.hls.hulu.clt1.prod.mlbam.net:hyp78.clt1.prod.mlbam.net\t67days\tlocaldisk=\ncbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net:hyp77.clt1.prod.mlbam.net\t89days\tlocaldisk=\ncbdb02.ingest05.hls.hulu.clt1.prod.mlbam.net:hyp85.clt1.prod.mlbam.net\t9days\tlocaldisk=\ncbdb02.ingest06.hls.hulu.clt1.prod.mlbam.net:hyp22.clt1.prod.mlbam.net\t93days\tlocaldisk=\ncbdb02.ingest07.hls.hulu.clt1.prod.mlbam.net:hyp83.clt1.prod.mlbam.net\t66days\tlocaldisk=\ncbdb03.ingest01.hls.hulu.clt1.prod.mlbam.net:hyp10.clt1.prod.mlbam.net\t97days\tlocaldisk=\ncbdb03.ingest02.hls.hulu.clt1.prod.mlbam.net:hyp76.clt1.prod.mlbam.net\t115days\tlocaldisk=\ncbdb03.ingest03.hls.hulu.clt1.prod.mlbam.net:hyp79.clt1.prod.mlbam.net\t69days\tlocaldisk=\ncbdb03.ingest04.hls.hulu.clt1.prod.mlbam.net:hyp78.clt1.prod.mlbam.net\t90days\tlocaldisk=\ncbdb03.ingest05.hls.hulu.clt1.prod.mlbam.net:hyp89.clt1.prod.mlbam.net\t77days\tlocaldisk=\ncbdb03.ingest06.hls.hulu.clt1.prod.mlbam.net:hyp26.clt1.prod.mlbam.net\t92days\tlocaldisk=\ncbdb03.ingest08.hls.hulu.clt1.prod.mlbam.net:hyp27.clt1.prod.mlbam.net\t12days\tlocaldisk=\ndepot01.clt1.util.mlbam.net:hyp96.clt1.prod.mlbam.net\t265days\tlocaldisk=\nharmonic01.c01.bc.gen.clt1.prod.mlbam.net:hyp80.clt1.prod.mlbam.net\t173days\tlocaldisk=\nharmonic02.c01.bc.gen.clt1.prod.mlbam.net:hyp92.clt1.prod.mlbam.net\t172days\tlocaldisk=\ninflux01.c01.inf.hulu.clt1.prod.mlbam.net:hyp89.clt1.prod.mlbam.net\t195days\tlocaldisk=\njava01.ad01.hls.hulu.clt1.prod.mlbam.net:hyp80.clt1.prod.mlbam.net\t264days\tlocaldisk=\njava02.ad01.hls.hulu.clt1.prod.mlbam.net:hyp29.clt1.prod.mlbam.net\t261days\tlocaldisk=\njava02.tool01.postprod.gen.clt1.qa.mlbam.net:hyp74.clt1.prod.mlbam.net\t45days\tlocaldisk=\njava03.ad01.hls.hulu.clt1.prod.mlbam.net:hyp82.clt1.prod.mlbam.net\t258days\tlocaldisk=\njava05.ad01.hls.hulu.clt1.prod.mlbam.net:hyp24.clt1.prod.mlbam.net\t192days\tlocaldisk=\njava06.ad01.hls.hulu.clt1.prod.mlbam.net:hyp28.clt1.prod.mlbam.net\t192days\tlocaldisk=\njava09.ad01.hls.hulu.clt1.prod.mlbam.net:hyp86.clt1.prod.mlbam.net\t192days\tlocaldisk=\njava10.ad01.hls.hulu.clt1.prod.mlbam.net:hyp91.clt1.prod.mlbam.net\t193days\tlocaldisk=\njenkins01.c01.razcp.gen.clt1.prod.mlbam.net:hyp24.clt1.prod.mlbam.net\t87days\tlocaldisk=\njetty01.archive01.jobmgr.gen.clt1.prod.mlbam.net:hyp30.clt1.prod.mlbam.net\t117days\tlocaldisk=\njetty01.ingest01.ams.gen.clt1.dev.mlbam.net:hyp87.clt1.prod.mlbam.net\t33days\tlocaldisk=\njetty02.archive01.jobmgr.gen.clt1.prod.mlbam.net:hyp80.clt1.prod.mlbam.net\t125days\tlocaldisk=\njetty02.ingest01.ams.gen.clt1.dev.mlbam.net:hyp95.clt1.prod.mlbam.net\t173days\tlocaldisk=\njetty02.web01.jobmgr.gen.clt1.prod.mlbam.net:hyp67.clt1.prod.mlbam.net\t272days\tlocaldisk=\nkafka01.c01.razcp.gen.clt1.dev.mlbam.net:hyp29.clt1.prod.mlbam.net\t180days\tlocaldisk=\nkafka02.c01.razcp.gen.clt1.dev.mlbam.net:hyp83.clt1.prod.mlbam.net\t180days\tlocaldisk=\nmdtools01.c01.bpa.hulu.clt1.prod.mlbam.net:hyp95.clt1.prod.mlbam.net\t256days\tlocaldisk=\nmdtools02.c01.bpa.hulu.clt1.prod.mlbam.net:hyp25.clt1.prod.mlbam.net\t111days\tlocaldisk=\nmemcached02.c01.lprovis.gen.clt1.qa.mlbam.net:hyp66.clt1.prod.mlbam.net\t261days\tlocaldisk=\nmemcached02.c01.lprovis.hulu.clt1.prod.mlbam.net:hyp75.clt1.prod.mlbam.net\t262days\tlocaldisk=\nmf01.c01.web.wwe.clt1.prod.bamtech.co:hyp102.clt1.prod.mlbam.net\t22days\tlocaldisk=\nmf02.c01.web.gen.clt1.prod.bamtech.co:hyp26.clt1.prod.mlbam.net\t30days\tlocaldisk=\nmf02.c01.web.wwe.clt1.prod.bamtech.co:hyp78.clt1.prod.mlbam.net\t24days\tlocaldisk=\nmf03.c01.web.gen.clt1.prod.bamtech.co:hyp85.clt1.prod.mlbam.net\t30days\tlocaldisk=\nmf03.c01.web.wwe.clt1.prod.bamtech.co:hyp81.clt1.prod.mlbam.net\t24days\tlocaldisk=\nmf04.c01.web.gen.clt1.prod.bamtech.co:hyp103.clt1.prod.mlbam.net\t22days\tlocaldisk=\nmf04.c01.web.wwe.clt1.prod.bamtech.co:hyp89.clt1.prod.mlbam.net\t23days\tlocaldisk=\nmf05.c01.web.gen.clt1.prod.bamtech.co:hyp102.clt1.prod.mlbam.net\t22days\tlocaldisk=\nmf05.c01.web.wwe.clt1.prod.bamtech.co:hyp32.clt1.prod.mlbam.net\t23days\tlocaldisk=\nmf06.c01.web.gen.clt1.prod.bamtech.co:hyp104.clt1.prod.mlbam.net\t23days\tlocaldisk=\nmf06.c01.web.wwe.clt1.prod.bamtech.co:hyp41.clt1.prod.mlbam.net\t23days\tlocaldisk=\nmf07.c01.web.gen.clt1.prod.bamtech.co:hyp31.clt1.prod.mlbam.net\t24days\tlocaldisk=\nmf07.c01.web.wwe.clt1.prod.bamtech.co:hyp77.clt1.prod.mlbam.net\t23days\tlocaldisk=\nmf08.c01.web.gen.clt1.prod.bamtech.co:hyp31.clt1.prod.mlbam.net\t24days\tlocaldisk=\nmf08.c01.web.wwe.clt1.prod.bamtech.co:hyp90.clt1.prod.mlbam.net\t23days\tlocaldisk=\nmf09.c01.web.wwe.clt1.prod.bamtech.co:hyp102.clt1.prod.mlbam.net\t18days\tlocaldisk=\nmf10.c01.web.wwe.clt1.prod.bamtech.co:hyp103.clt1.prod.mlbam.net\t18days\tlocaldisk=\nmf11.c01.web.wwe.clt1.prod.bamtech.co:hyp104.clt1.prod.mlbam.net\t19days\tlocaldisk=\nmf12.c01.web.wwe.clt1.prod.bamtech.co:hyp105.clt1.prod.mlbam.net\t19days\tlocaldisk=\nmonyog01.c01.noc.gen.clt1.util.mlbam.net:hyp87.clt1.prod.mlbam.net\t33days\tlocaldisk=\nmysql01.c01.jobmgr.gen.clt1.prod.mlbam.net:hyp08.clt1.prod.mlbam.net\t265days\tlocaldisk=\nmysql01.control01.provis.gen.clt1.prod.mlbam.net:hyp11.clt1.prod.mlbam.net\t237days\tlocaldisk=\nmysql02.c01.jobmgr.gen.clt1.prod.mlbam.net:hyp71.clt1.prod.mlbam.net\t249days\tlocaldisk=\nmysql02.control01.provis.gen.clt1.prod.mlbam.net:hyp71.clt1.prod.mlbam.net\t238days\tlocaldisk=\nmysqldb01.ingest01.ams.gen.clt1.prod.mlbam.net:hyp88.clt1.prod.mlbam.net\t171days\tlocaldisk=\nmysqldb01.ingest01.ams.gen.clt1.qa.mlbam.net:hyp28.clt1.prod.mlbam.net\t171days\tlocaldisk=\nmysqldb02.ingest01.ams.gen.clt1.prod.mlbam.net:hyp18.clt1.prod.mlbam.net\t8days\tlocaldisk=\nnginx01.c01.medtrans.gen.clt1.prod.mlbam.net:hyp29.clt1.prod.mlbam.net\t47days\tlocaldisk=\nnginx01.c01.medtrans.gen.clt1.qa.mlbam.net:hyp104.clt1.prod.mlbam.net\t49days\tlocaldisk=\nnginx01.event01.hls.wwe.clt1.prod.mlbam.net:hyp28.clt1.prod.mlbam.net\t118days\tlocaldisk=\nnginx01.feed01.hls.mls.clt1.prod.mlbam.net:hyp04.clt1.prod.mlbam.net\t263days\tlocaldisk=\nnginx01.ingest01.hls.hulu.clt1.prod.mlbam.net:hyp13.clt1.prod.mlbam.net\t263days\tlocaldisk=\nnginx01.linear01.hls.wwe.clt1.prod.mlbam.net:hyp72.clt1.prod.mlbam.net\t117days\tlocaldisk=\nnginx01.live01.hls.gen.clt1.dev.mlbam.net:hyp94.clt1.prod.mlbam.net\t87days\tlocaldisk=\nnginx01.live01.hls.gen.clt1.prod.mlbam.net:hyp24.clt1.prod.mlbam.net\t230days\tlocaldisk=\nnginx01.live01.hls.mls.clt1.prod.mlbam.net:hyp11.clt1.prod.mlbam.net\t272days\tlocaldisk=\nnginx01.live01.hls.riot.clt1.prod.mlbam.net:hyp27.clt1.prod.mlbam.net\t187days\tlocaldisk=\nnginx01.proxy01.hls.gen.clt1.prod.mlbam.net:hyp98.clt1.prod.mlbam.net\t49days\tlocaldisk=\nnginx01.proxy01.hls.gen.clt1.qa.mlbam.net:hyp99.clt1.prod.mlbam.net\t48days\tlocaldisk=\nproxy01.c01.infra.gen.clt1.util.mlbam.net:hyp11.clt1.prod.mlbam.net\t19days\tlocaldisk=\nproxy01.c01.streaming.gen.clt1.dev.mlbam.net:hyp03.clt1.prod.mlbam.net\t61days\tlocaldisk=\nproxy01.linear02.infra.hulu.clt1.prod.mlbam.net:hyp23.clt1.prod.mlbam.net\t270days\tlocaldisk=\nredis02.c01.inf.gen.clt1.util.mlbam.net:hyp91.clt1.prod.mlbam.net\t163days\tlocaldisk=\nsmtp01.clt1.util.mlbam.net:hyp20.clt1.prod.mlbam.net\t264days\tlocaldisk=\nsmtp02.clt1.util.mlbam.net:hyp23.clt1.prod.mlbam.net\t253days\tlocaldisk=\nsplunkfwd01.c01.noc.gen.clt1.util.mlbam.net:hyp78.clt1.prod.mlbam.net\t265days\tlocaldisk=\nsplunkfwd02.c01.noc.gen.clt1.util.mlbam.net:hyp79.clt1.prod.mlbam.net\t264days\tlocaldisk=\ntomcat01.c01.sxc.gen.clt1.prod.mlbam.net:hyp80.clt1.prod.mlbam.net\t237days\tlocaldisk=\ntomcat01.toc01.lprovis.hulu.clt1.prod.mlbam.net:hyp12.clt1.prod.mlbam.net\t258days\tlocaldisk=\ntomcat02.toc01.lprovis.hulu.clt1.prod.mlbam.net:hyp67.clt1.prod.mlbam.net\t261days\tlocaldisk=\nxcoder01.c01.streaming.gen.clt1.dev.mlbam.net:hyp93.clt1.prod.mlbam.net\t115days\tlocaldisk=\nxcoder01.c02.streaming.gen.clt1.dev.mlbam.net:hyp101.clt1.prod.mlbam.net\t47days\tlocaldisk=\nxcoder01.c03.streaming.gen.clt1.dev.mlbam.net:hyp25.clt1.prod.mlbam.net\t47days\tlocaldisk=\nxcoder02.c02.streaming.gen.clt1.dev.mlbam.net:hyp14.clt1.prod.mlbam.net\t33days\tlocaldisk=\nxcoder02.c03.streaming.gen.clt1.dev.mlbam.net:hyp99.clt1.prod.mlbam.net\t46days\tlocaldisk=\nxfer01.c01.archive.gen.clt1.prod.mlbam.net:hyp79.clt1.prod.mlbam.net\t237days\tlocaldisk=\nxfer02.c01.archive.gen.clt1.prod.mlbam.net:hyp80.clt1.prod.mlbam.net\t238days\tlocaldisk=\n\n\n======INTERNAL ONLY - DO NOT PUBLISH======</text>, <text>[root@dbview scripts]# ./vm40p depot01.clt1.util.mlbam.net\n \nvm_static:                                                                                                              (0 = ok, 1 = locked)\n               vm_guid                |           vm_name           | entity_type | os | description |              cluster_id              | template_status |               vmt_guid               |         original_template_id         \n--------------------------------------+-----------------------------+-------------+----+-------------+--------------------------------------+-----------------+--------------------------------------+--------------------------------------\n b54f9aa2-c060-4fc6-9879-d33ad87accaa | depot01.clt1.util.mlbam.net | VM          | 24 |             | 6583532b-c88b-42ec-8c64-48437e657bee |                 | 82fcd601-e004-447a-b741-521dcb6c8a71 | 82fcd601-e004-447a-b741-521dcb6c8a71\n(1 row)\n\n...                                                                                      \n \nvm_device:\n              device_id               |                vm_id                 | boot_order |         _create_date          |         _update_date          |                           address                            \n--------------------------------------+--------------------------------------+------------+-------------------------------+-------------------------------+--------------------------------------------------------------\n 1c27de0d-a56c-4bd0-96ed-4e54843b0b4b | b54f9aa2-c060-4fc6-9879-d33ad87accaa |          1 | 2017-02-20 08:16:00.6716+00   | 2017-11-12 04:02:38.694528+00 | {slot=0x07, bus=0x00, domain=0x0000, type=pci, function=0x0}\n a3f0a071-6be7-4ca7-8140-8e7ec50332f8 | b54f9aa2-c060-4fc6-9879-d33ad87accaa |          0 | 2017-02-20 08:17:46.973502+00 | 2017-11-12 04:02:38.694528+00 | {slot=0x08, bus=0x00, domain=0x0000, type=pci, function=0x0}\n be60b10a-e3f2-4b91-8297-619f019cd150 | b54f9aa2-c060-4fc6-9879-d33ad87accaa |          0 | 2017-02-20 08:17:51.469074+00 | 2017-11-12 04:02:38.694528+00 | {slot=0x09, bus=0x00, domain=0x0000, type=pci, function=0x0}\n(3 rows)\n\n \nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n ca99bf43-daf9-440a-9469-37cbe8abb76d | 1c27de0d-a56c-4bd0-96ed-4e54843b0b4b | 53c264cb-eafa-4bd9-bbc2-0b1d163cb1cd | ae207ffa-ad36-4927-b634-5ec5c6c284b0 |           1 | 2017-11-12 04:02:20+00 |           2 |             4 | t\n ae207ffa-ad36-4927-b634-5ec5c6c284b0 | 1c27de0d-a56c-4bd0-96ed-4e54843b0b4b | c21c956e-0c8a-476b-81ab-c1cadf0a0906 | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-02-20 08:16:01+00 |           2 |             4 | f\n(2 rows)\n\n    For image_guid = ca99bf43-daf9-440a-9469-37cbe8abb76d , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 )\n    For image_guid = ae207ffa-ad36-4927-b634-5ec5c6c284b0 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 )\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 20e7dfbe-0252-4eae-af76-f045e88b7cbe | a3f0a071-6be7-4ca7-8140-8e7ec50332f8 | 53c264cb-eafa-4bd9-bbc2-0b1d163cb1cd | 5698128d-687b-4822-9cd8-a5bad6444458 |           1 | 2017-11-12 04:02:21+00 |           2 |             4 | t\n 5698128d-687b-4822-9cd8-a5bad6444458 | a3f0a071-6be7-4ca7-8140-8e7ec50332f8 | c21c956e-0c8a-476b-81ab-c1cadf0a0906 | 00000000-0000-0000-0000-000000000000 |           4 | 2017-02-20 08:17:46+00 |           2 |             5 | f\n(2 rows)\n\n    For image_guid = 20e7dfbe-0252-4eae-af76-f045e88b7cbe , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 214748364800 ,  actual size = 1082880\n    For image_guid = 5698128d-687b-4822-9cd8-a5bad6444458 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 214748364800 ,  actual size = 1536\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 306d6e11-2dee-47ef-bc80-f83830d953d8 | be60b10a-e3f2-4b91-8297-619f019cd150 | 53c264cb-eafa-4bd9-bbc2-0b1d163cb1cd | f0b93d1b-1e59-4050-983d-9be80e2454e8 |           1 | 2017-11-12 04:02:22+00 |           2 |             4 | t\n f0b93d1b-1e59-4050-983d-9be80e2454e8 | be60b10a-e3f2-4b91-8297-619f019cd150 | c21c956e-0c8a-476b-81ab-c1cadf0a0906 | 00000000-0000-0000-0000-000000000000 |           4 | 2017-02-20 08:17:52+00 |           2 |             5 | f\n(2 rows)\n\n    For image_guid = 306d6e11-2dee-47ef-bc80-f83830d953d8 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 214748364800 ,  actual size = 624128\n    For image_guid = f0b93d1b-1e59-4050-983d-9be80e2454e8 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 214748364800 ,  actual size = 1536\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------\n c21c956e-0c8a-476b-81ab-c1cadf0a0906 | b54f9aa2-c060-4fc6-9879-d33ad87accaa | REGULAR       | OK     | Backup__2017-11-12__04.02.16 | 2017-11-12 04:02:23.563+00\n 53c264cb-eafa-4bd9-bbc2-0b1d163cb1cd | b54f9aa2-c060-4fc6-9879-d33ad87accaa | ACTIVE        | OK     | Active VM                    | 2017-02-20 08:15:58.394+00\n(2 rows)\n\n \nbase_disks:\n               disk_id                | wipe_after_delete | propagate_errors | disk_alias | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n 1c27de0d-a56c-4bd0-96ed-4e54843b0b4b | f                 | Off              | rootdisk   |                  | f         |      |         0 |                     |                 0 | \n(1 row)\n\n               disk_id                | wipe_after_delete | propagate_errors |            disk_alias             | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+-----------------------------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n a3f0a071-6be7-4ca7-8140-8e7ec50332f8 | f                 | Off              | depot01.clt1.util.mlbam.net_Disk1 |                  | f         |      |         0 |                     |                 0 | \n(1 row)\n\n               disk_id                | wipe_after_delete | propagate_errors |            disk_alias             | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+-----------------------------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n be60b10a-e3f2-4b91-8297-619f019cd150 | f                 | Off              | depot01.clt1.util.mlbam.net_Disk2 |                  | f         |      |         0 |                     |                 0 | \n(1 row)</text>, <text>Marcus,\n\nWe're still investigating but here's the VM information that we're looking into now:\n\nvm_static:                                                                                                              (0 = ok, 1 = locked)\n               vm_guid                |           vm_name           | entity_type | os | description |              cluster_id              | template_status |               vmt_guid               |         original_template_id         \n--------------------------------------+-----------------------------+-------------+----+-------------+--------------------------------------+-----------------+--------------------------------------+--------------------------------------\n b54f9aa2-c060-4fc6-9879-d33ad87accaa | depot01.clt1.util.mlbam.net | VM          | 24 |             | 6583532b-c88b-42ec-8c64-48437e657bee |                 | 82fcd601-e004-447a-b741-521dcb6c8a71 | 82fcd601-e004-447a-b741-521dcb6c8a71\n\nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------\n c21c956e-0c8a-476b-81ab-c1cadf0a0906 | b54f9aa2-c060-4fc6-9879-d33ad87accaa | REGULAR       | OK     | Backup__2017-11-12__04.02.16 | 2017-11-12 04:02:23.563+00\n 53c264cb-eafa-4bd9-bbc2-0b1d163cb1cd | b54f9aa2-c060-4fc6-9879-d33ad87accaa | ACTIVE        | OK     | Active VM                    | 2017-02-20 08:15:58.394+00\n\nbase_disks:\n               disk_id                | wipe_after_delete | propagate_errors | disk_alias | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n 1c27de0d-a56c-4bd0-96ed-4e54843b0b4b | f                 | Off              | rootdisk   |                  | f         |      |         0 |                     |                 0 | \n\n               disk_id                | wipe_after_delete | propagate_errors |            disk_alias             | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+-----------------------------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n a3f0a071-6be7-4ca7-8140-8e7ec50332f8 | f                 | Off              | depot01.clt1.util.mlbam.net_Disk1 |                  | f         |      |         0 |                     |                 0 | \n\n               disk_id                | wipe_after_delete | propagate_errors |            disk_alias             | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+-----------------------------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n be60b10a-e3f2-4b91-8297-619f019cd150 | f                 | Off              | depot01.clt1.util.mlbam.net_Disk2 |                  | f         |      |         0 |                     |                 0 |</text>, <text>Hello,\n\nThis is regarding the depot01.clt1.util.mlbam.net that doesn't boot.\n\nAccording to the RHV-M DB, the VM contains 3 disks each with a couple images under it.\n\n~~~\nrootdisk (1c27de0d-a56c-4bd0-96ed-4e54843b0b4b)\n  Images: ae207ffa-ad36-4927-b634-5ec5c6c284b0 \n\t  ca99bf43-daf9-440a-9469-37cbe8abb76d\n\t   \ndepot01.clt1.util.mlbam.net_Disk1 (a3f0a071-6be7-4ca7-8140-8e7ec50332f8)\n  Images: 5698128d-687b-4822-9cd8-a5bad6444458 \n  \t  20e7dfbe-0252-4eae-af76-f045e88b7cbe \n\ndepot01.clt1.util.mlbam.net_Disk2 (be60b10a-e3f2-4b91-8297-619f019cd150)\n  Images: f0b93d1b-1e59-4050-983d-9be80e2454e8\n       \t  306d6e11-2dee-47ef-bc80-f83830d953d8\n~~~\n\nWe also the base images on the local host as lvm images.\n\n\n~~~\n  ae207ffa-ad36-4927-b634-5ec5c6c284b0 ovirt-local    1 -wi-a-----  50.00g  -1  -1  253    9  UPILnQ-btR0-mUf2-fU13-BV0I-ZWSp-uiZZcw          IU_1c27de0d-a56c-4bd0-96ed-4e54843b0b4b,PU_00000000-0000-0000-0000-000000000000,VM_b54f9aa2-c060-4fc6-9879-d33ad87accaa\n\n  5698128d-687b-4822-9cd8-a5bad6444458 ovirt-local    1 -wi-a----- 200.00g  -1  -1  253   10  uGcI3s-Q7Fx-bqdc-eazD-qreP-UkHr-d1cFxH          IU_a3f0a071-6be7-4ca7-8140-8e7ec50332f8,PU_00000000-0000-0000-0000-000000000000,VM_b54f9aa2-c060-4fc6-9879-d33ad87accaa\n\n  f0b93d1b-1e59-4050-983d-9be80e2454e8 ovirt-local    1 -wi-a----- 200.00g  -1  -1  253   11  Zs7HRW-CvQ8-q2Rn-rf8g-oA3U-b9v0-GftON2          IU_be60b10a-e3f2-4b91-8297-619f019cd150,PU_00000000-0000-0000-0000-000000000000,VM_b54f9aa2-c060-4fc6-9879-d33ad87accaa\n~~~\n\nAccording to the host sos_report, there are snapshot images used by this VM on the NFS SD.\nThe leaf or active images have new date timestamp.\n\n~~~    \n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c27de0d-a56c-4bd0-96ed-4e54843b0b4b:\ntotal 2465590\n-rw-rw----.   1 vdsm kvm     197632 Feb 20  2017 ae207ffa-ad36-4927-b634-5ec5c6c284b0\n-rw-rw----.   1 vdsm kvm    1048576 Feb 20  2017 ae207ffa-ad36-4927-b634-5ec5c6c284b0.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 12 04:02 ae207ffa-ad36-4927-b634-5ec5c6c284b0.meta\n\n-rw-rw----.   1 vdsm kvm   82313216 Nov 14 21:18 ca99bf43-daf9-440a-9469-37cbe8abb76d   &lt;&lt;===========\n-rw-rw----.   1 vdsm kvm    1048576 Nov 12 04:02 ca99bf43-daf9-440a-9469-37cbe8abb76d.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 04:02 ca99bf43-daf9-440a-9469-37cbe8abb76d.meta\n\n-rw-rw----. 416 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 416 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 416 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a3f0a071-6be7-4ca7-8140-8e7ec50332f8:\ntotal 3721\n-rw-rw----. 1 vdsm kvm       851968 Nov 14 21:18 20e7dfbe-0252-4eae-af76-f045e88b7cbe  &lt;&lt;============\n-rw-rw----. 1 vdsm kvm      1048576 Nov 12 04:02 20e7dfbe-0252-4eae-af76-f045e88b7cbe.lease\n-rw-r--r--. 1 vdsm kvm          260 Nov 12 04:02 20e7dfbe-0252-4eae-af76-f045e88b7cbe.meta\n\n-rw-rw----. 1 vdsm kvm 214748364800 Feb 20  2017 5698128d-687b-4822-9cd8-a5bad6444458\n-rw-rw----. 1 vdsm kvm      1048576 Feb 20  2017 5698128d-687b-4822-9cd8-a5bad6444458.lease\n-rw-r--r--. 1 vdsm kvm          334 Nov 12 04:02 5698128d-687b-4822-9cd8-a5bad6444458.meta\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/be60b10a-e3f2-4b91-8297-619f019cd150:\ntotal 3273\n-rw-rw----. 1 vdsm kvm       393216 Nov 14 20:46 306d6e11-2dee-47ef-bc80-f83830d953d8  &lt;&lt;============\n-rw-rw----. 1 vdsm kvm      1048576 Nov 12 04:02 306d6e11-2dee-47ef-bc80-f83830d953d8.lease\n-rw-r--r--. 1 vdsm kvm          260 Nov 12 04:02 306d6e11-2dee-47ef-bc80-f83830d953d8.meta\n\n-rw-rw----. 1 vdsm kvm 214748364800 Feb 20  2017 f0b93d1b-1e59-4050-983d-9be80e2454e8\n-rw-rw----. 1 vdsm kvm      1048576 Feb 20  2017 f0b93d1b-1e59-4050-983d-9be80e2454e8.lease\n-rw-r--r--. 1 vdsm kvm          334 Nov 12 04:02 f0b93d1b-1e59-4050-983d-9be80e2454e8.meta\n~~~\n\n\nDuring our brief conversation with Ashton, he was wanting to adjust the RHV-M DB to point to the lvm image on the host.\nFrom what we see, the leaf/active images on the NFS SD appear to be have been in use by the VM as they have newer date.\n\nWe see this also from the engine logs when the VM failed when the leaf/active ca99bf43-daf9-440a-9469-37cbe8abb76d getting converted to lvm image on the host but failed due to insufficient space.\n\n~~~\n2017-11-14 21:21:11,688 ERROR [org.ovirt.engine.core.dal.dbbroker.auditloghandling.AuditLogDirector] (ForkJoinPool-1-worker-4) [] Correlation ID: null, Call Stack: null, Custom Event ID: -1, Message: VM depot01.clt1.util.mlbam.net is down with error. \nExit message: Hook Error: ('localdisk-hook: local disk not found, creating logical volume (name=ca99bf43-daf9-440a-9469-37cbe8abb76d)\\nlocaldisk-hook: creating logical volume (name=ca99bf43-daf9-440a-9469-37cbe8abb76d, size=53687091200)\\n\nTraceback (most recent call last):\\n  \nFile "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 172, in &lt;module&gt;\\n    \nmain()\\n  \nFile "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 69, in main\\n    \nreplace_disk(disk)\\n  \nFile "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 85, in replace_disk\\n    \ncreate_local_disk(orig_path, lv_name, vm_id, img_id, src_format)\\n  \nFile "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 104, in create_local_disk\\n    \ncreate_lv(lv_name, size, vm_id, img_id)\\n  \nFile "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 160, in create_lv\\n    \nlv_name)\\n  \nFile "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 168, in helper\\n    \nraise cmdutils.Error(cmd=HELPER, rc=rc, out=out, err=err)\\n\nvdsm.cmdutils.Error: Command /usr/libexec/vdsm/localdisk-helper failed with rc=1 out=\\'\\' err=\\'Traceback (most recent call last):\\\\n  \nFile "/usr/libexec/vdsm/localdisk-helper", line 111, in &lt;module&gt;\\\\n    \nargs.command(args)\\\\n  \nFile "/usr/libexec/vdsm/localdisk-helper", line 49, in create\\\\n    \nlvm(*cmd)\\\\n  \nFile "/usr/libexec/vdsm/localdisk-helper", line 73, in lvm\\\\n    \nraise cmdutils.Error(cmd=cmd, rc=rc, out=out, err=err)\\\\n\nvdsm.cmdutils.Error: Command [\\\\\\'/usr/sbin/lvm\\\\\\', \\\\\\'lvcreate\\\\\\', \\\\\\'-L\\\\\\', \\\\\\'53687091200b\\\\\\', \\\\\\'-n\\\\\\', \\\\\\'ca99bf43-daf9-440a-9469-37cbe8abb76d\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'UPDATING\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'VM_b54f9aa2-c060-4fc6-9879-d33ad87accaa\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'IU_1c27de0d-a56c-4bd0-96ed-4e54843b0b4b\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'PU_00000000-0000-0000-0000-000000000000\\\\\\', \\\\\\'ovirt-local\\\\\\'] failed with rc=5 out=\\\\\\'\\\\\\' err=\\\\\\'  Volume group "ovirt-local" has insufficient free space (4911 extents): 12800 required.\\\\\\\n\\n\\\\\\'\\\n\\n\\'\n\\n',).\n~~~\n\nIf we adjust the RHV-M DB to point to lvm image that is on the host as the ACTIVE image, you might get older data (Nov 12).\n\nIs this the approach you wish to take or merge the data from leaf/active images from the NFS SD?\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Hook used:  https://github.com/oVirt/vdsm/tree/master/vdsm_hooks/localdisk\n\nClearly states:\n\nMost operations are not supported on a VM using the localdisk hook.  For\nexample, the VM cannot be migrated to another host, cannot create\nsnapshots, copy the local disk to other storage, etc.</text>, <text>// internal\n\nThe customer was aware snapshots are not supported. This was a mistake by whomever was operating the backup system.\n\nThe backup system has been prevented from reaching these hosts again.</text>, <text>Ideally, we would prefer to have the latest data if possible. Can we test a merge somehow down to the LVM volume ? \nI was unaware that creating a snapshot turns the active image to start using that snapshot.\n\n\n(In reply to Chollera, Bimal)\n&gt; Hello,\n&gt; \n&gt; This is regarding the depot01.clt1.util.mlbam.net that doesn't boot.\n&gt; \n&gt; According to the RHV-M DB, the VM contains 3 disks each with a couple images under it.\n&gt; \n&gt; ~~~\n&gt; rootdisk (1c27de0d-a56c-4bd0-96ed-4e54843b0b4b)\n&gt;   Images: ae207ffa-ad36-4927-b634-5ec5c6c284b0 \n&gt; \t  ca99bf43-daf9-440a-9469-37cbe8abb76d\n&gt; \t   \n&gt; depot01.clt1.util.mlbam.net_Disk1 (a3f0a071-6be7-4ca7-8140-8e7ec50332f8)\n&gt;   Images: 5698128d-687b-4822-9cd8-a5bad6444458 \n&gt;   \t  20e7dfbe-0252-4eae-af76-f045e88b7cbe \n&gt; \n&gt; depot01.clt1.util.mlbam.net_Disk2 (be60b10a-e3f2-4b91-8297-619f019cd150)\n&gt;   Images: f0b93d1b-1e59-4050-983d-9be80e2454e8\n&gt;        \t  306d6e11-2dee-47ef-bc80-f83830d953d8\n&gt; ~~~\n&gt; \n&gt; We also the base images on the local host as lvm images.\n&gt; \n&gt; \n&gt; ~~~\n&gt;   ae207ffa-ad36-4927-b634-5ec5c6c284b0 ovirt-local    1 -wi-a-----  50.00g  -1  -1  253    9  UPILnQ-btR0-mUf2-fU13-BV0I-ZWSp-uiZZcw          IU_1c27de0d-a56c-4bd0-96ed-4e54843b0b4b,PU_00000000-0000-0000-0000-000000000000,VM_b54f9aa2-c060-4fc6-9879-d33ad87accaa\n&gt; \n&gt;   5698128d-687b-4822-9cd8-</text>, <text>The way I understand the vdsm hook, it shouldn't allow a VM to run on NFS at all, even if the lv creation failed. The newer timestamp is likely due to an attempted boot. It shouldn't be possible at all for the VM to run a snapshot in NFS and base image on lvm.\n\nIn other words, rolling back is probably not older data, there shouldn't be able to be new data</text>, <text>@Ashton,\n\nIt seems it does, all moved to point to the NFS SD when the image was created.\n\n~~~\njsonrpc.Executor/4::DEBUG::2017-11-12 04:02:37,169::storage::315::virt.vm::(path) vmId=`b54f9aa2-c060-4fc6-9879-d33ad87accaa`::Drive vda moved from u'/dev/ovirt-local/ae207ffa-ad36-4927-b634-5ec5c6c284b0' to u'/rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c27de0d-a56c-4bd0-96ed-4e54843b0b4b/ca99bf43-daf9-440a-9469-37cbe8abb76d'\n\njsonrpc.Executor/4::DEBUG::2017-11-12 04:02:37,200::storage::315::virt.vm::(path) vmId=`b54f9aa2-c060-4fc6-9879-d33ad87accaa`::Drive vdb moved from u'/dev/ovirt-local/5698128d-687b-4822-9cd8-a5bad6444458' to u'/rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a3f0a071-6be7-4ca7-8140-8e7ec50332f8/20e7dfbe-0252-4eae-af76-f045e88b7cbe'\n\njsonrpc.Executor/4::DEBUG::2017-11-12 04:02:37,184::storage::315::virt.vm::(path) vmId=`b54f9aa2-c060-4fc6-9879-d33ad87accaa`::Drive vdc moved from u'/dev/ovirt-local/f0b93d1b-1e59-4050-983d-9be80e2454e8' to u'/rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/be60b10a-e3f2-4b91-8297-619f019cd150/306d6e11-2dee-47ef-bc80-f83830d953d8'\n~~~</text>, <text>// int\n\nThat makes this situation much worse. Marcus confirmed there was data written to NFS. I'm.not even sure how this is possible.\n\nIs there a way to reconcile the data?</text>, <text>Hello,\n\nI am Kumar from Virtualization team and I will be assisting you during APAC hours.\n\nAllow us sometime to go through the details and get back to you. Please feel free to get back to us, if you have any concerns meanwhile.\n\nRegards,\nKumar Mashalkar,\nRed Hat Global Support Services</text>, <text>//Internal//\n\nI have briefed Germano from APAC geo on this case as my shift has ended.\n\nThe situation is that the environment is configured to use a 'localdisk' hook.\n\nhttps://github.com/oVirt/vdsm/blob/master/vdsm_hooks/localdisk/README\n\nThe caveat for this hook is that the VM cannot be migrated to another host, cannot create\nsnapshots, copy the local disk to other storage, etc.\n\nThe base image resides on the local disk of the host as lvm image and a dummy image is created on the NFS SD.\nHowever, in this case some backend operation by the customer created snapshots for the VM's.\nThese snapshot images were created on the NFS SD as leaf/active images and they were in-use by the VM but the base image (lvm) image remains on the host local disk.\nNow VM is restarted the hook creates a lvm image for the VM but in this case for the leaf/active image but it fails due to lack of space.\n\nWe see this for depot01.clt1.util.mlbam.net and most probably others is the same (96 VM's).\n\nFew thoughts ...\nCopy the base lvm image from local disk to NFS SD, disable the hook start the VM (will need to address hook, snapshot and lvm image on the local disk later).\nMerge the ae207ffa(LVM) &lt;&lt;-- ca99bf43(NFS)</text>, <text>Hello Marcus,\n\nMy name is Germano from the Virtualization team.\n\nI would like to attempt the recovery of the test VM (depot01.clt1.util.mlbam.net) via a Remote Session (Bomgar). Are you available to do it now?\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>Hello ,\nYes I am available. Please provide link\n\nMarcus Torres\nInfrastructure Architect\nBAMTECH Media\n75 9th Ave, New York NY 10020\ntele: 646-866-2609\n________________________________\nFrom: Red Hat Support &lt;support@redhat.com&gt;\nSent: Tuesday, November 14, 2017 10:14:27 PM\nTo: Merali, Imran; Torres, Marcus\nSubject: (WoC) (SEV 1) Case #01961808 (can't delete snapshot) ref:_00DA0HxWH._500A0YfnPO:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01961808\nCase Title       : can't delete snapshot\nCase Number      : 01961808\nCase Open Date   : 2017-10-27 10:49:54\nSeverity         : 1 (Urgent)\nProblem Type     : Defect / Bug\n\nMost recent comment: On 2017-11-14 22:14:19, Veit Michel, Germano commented:\n"Hello Marcus,\n\nMy name is Germano from the Virtualization team.\n\nI would like to attempt the recovery of the test VM (depot01.clt1.util.mlbam.net) via a Remote Session (Bomgar). Are you available to do it now?\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia"\n\n---------------------------------------\n\nThank you for your latest interaction with Red Hat Global Support Services. We are currently working to resolve your case.\n\nYour case has transitioned to "Waiting On Customer" status. This means that the Red Hat associate working on your case needs information or action from you to proceed. To help us resolve your case as quickly as possible, please update your case online on the Customer Portal at https://access.redhat.com.\n\nOnce you update the case to provide the requested information, we can continue working to resolve your issue.\n\nIf you wish to contact Red Hat, visit the Customer Portal at https://access.redhat.com to find phone and web contact information relevant to your region and support contract.\n\n\nThank you,\n\nRed Hat Global Support Services\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/\nRed Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0YfnPO:ref</text>, <text>Remote Support Invitation\n\nClick on the link below and follow the directions.\n\nhttps://remotesupport.redhat.com/?ak=5ec5eeb53ce690bdc73e8f5413ae7813\n\nBomgar(TM) enables a support representative to view your screen in order to assist you.\nSession traffic is fully encrypted to protect your system's data.\nOnce a session has begun, you will be able to end it at any time</text>, <text>Hello Marcus,\n\nDuring the remote session we recovered the test VM (depot01.clt1.util.mlbam.net) successfully.\nI will now work on documenting what needs to be done and writing a script in an attempt to automatically generate the commands that need to be run for each VM.\n\nThanks for your patience during the remote session. The case is set to Waiting on Red Hat.\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>Hello Marcus and team,\n\nThe Problem:\n============\n1) VMs use the localdisk hook. This creates a local LV on each host (VMs are pinned) for each image the VM uses under /dev/ovirt-local/vol_uuid when the VM runs.\n   https://github.com/oVirt/vdsm/tree/master/vdsm_hooks/localdisk\n2) This hook does nothing on snapshot creation.\n3) Snapshots were created, VMs switched active layer to the shared storage (NFS share) while using the local storage for the base volume\n4) Data was written to the shared storage volumes\n5) Status:\n   * Some VMs were shutdown \n     -&gt; wont start as there is no space in VG for the hook to create leaf LV for the snapshot.  \n     -&gt; This is a very good thing, as if it had space it would suddenly start using a fresh overlay from the local storage instead of the existing one - after snapshot - on the storage domain. \n     -&gt; Would rollback disk data to snapshot time by switching to a fresh layer, spreading after snapshot writes to 2 volumes.\n   * Most VMs are up\n     -&gt; Keep them that way until we have a plan for them.\n\nThe Solution (so far):\n======================\nMoving forward: this is what we did for the recovered test VM. It also provides a way of fixing the remaining 111 VMs.\n\nAssumptions:\n   Volume AAA is the raw  , local to the host, image (i.e.: /dev/ovirt-local/...)\n   Volume BBB is the qcow2, on nfs storage   , image (i.e.: /rhev/data-center/...)\n   Chain is: [BBB - leaf]\n             [AAA - base]\nSteps:\n1) VM shutdown (!!!)\n2) unsafe rebase of image BBB, make it point to AAA's path (so that step 3 can succeed, otherwise qemu-img commit fails to build the chain)\n   qemu-img rebase -f qcow2 -t none -u -b AAA -F raw BBB\n3) image commit, commit BBB into AAA\n   qemu-img commit -f qcow2 -t none -b AAA -d -p BBB\n4) metadata fix for AAA\n   sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' AAA.meta\n5) Remove BBB (optional, probably safer to do later)\n   # rm -rf BBB*\n6) Adjust images and snapshots in the DB to make VM with no snapshots and use A to start. Example for 3 disks at same time:\n   DELETE FROM images WHERE image_group_id in ('1c27de0d-a56c-4bd0-96ed-4e54843b0b4b','a3f0a071-6be7-4ca7-8140-8e7ec50332f8','be60b10a-e3f2-4b91-8297-619f019cd150') and active = 't';\n   UPDATE images SET active='t', imagestatus = '1' WHERE image_group_id in ('1c27de0d-a56c-4bd0-96ed-4e54843b0b4b','a3f0a071-6be7-4ca7-8140-8e7ec50332f8','be60b10a-e3f2-4b91-8297-619f019cd150') and active = 'f';\n   DELETE FROM snapshots WHERE vm_id = 'b54f9aa2-c060-4fc6-9879-d33ad87accaa' and description != 'Active VM';\n   UPDATE snapshots SET snapshot_id = 'c21c956e-0c8a-476b-81ab-c1cadf0a0906' WHERE vm_id = 'b54f9aa2-c060-4fc6-9879-d33ad87accaa' and description = 'Active VM';\n7) Start the VM\n\nSolution for the remaining 111 VMs:\n===================================\n\nAs there are 111 VMs affected, I have made an initial attempt to script the generation of the commands that need to be run on the Host and also on RHV-M. \nThis script uses the Database as input.\n#############################################################################################################\n###### PLEASE DO NOT ATTEMPT THE BELOW, THIS IS VERY EARLY VERSION AND NEEDS TO BE MANUALLY CHECKED. ########\n###### MAY CONTAIN LOGIC AND SYNTAX ISSUES                                                           ########\n###### ONLY SUPPORTS 1 SNAPSHOT AS OF NOW                                                            ########\n#############################################################################################################\n# ./mlb.py --vm-name nginx01.live01.hls.mls.clt1.prod.mlbam.net\n--&gt; DISK a11e5029-2eea-4a15-a493-f3ec24d3780c\n----&gt; RUN ON HOST\n# su vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/0903ad06-7821-4fa3-8261-8bc2f4560fd5 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162'\n# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/0903ad06-7821-4fa3-8261-8bc2f4560fd5 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162'\n# su vdsm -s /bin/sh -c 'sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/0903ad06-7821-4fa3-8261-8bc2f4560fd5.meta'\n# su vdsm -s /bin/sh -c 'rm -rf /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162*'\n----&gt; RUN ON RHV-M\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE from images where image_guid ='9ba3b527-596e-40d8-9aaa-28c324fd4162';"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t' where image_guid ='0903ad06-7821-4fa3-8261-8bc2f4560fd5';"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '8a4c0055-4813-4163-a8b8-fe9e92bf3020' and description != 'Active VM';"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE snapshots SET snapshot_id = 'faacf17c-c7c7-4fbf-9ded-98fb490c5de4' WHERE vm_id = '8a4c0055-4813-4163-a8b8-fe9e92bf3020';"\n\nAlso, even after manually checked, I would like to have one of my peers to review these steps in case we cannot backup all the LVs and NFS files before starting the procedure.\n\nNext Steps:\n===========\nThere are 5 VMs down with this problem on Data-Center CLT1-LD which are our next steps.\n  harmonic05.c01.bc.gen.clt1.prod.mlbam.net\n  memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net\n  mysql01.control01.provis.gen.clt1.prod.mlbam.net\n  nginx01.live01.hls.mls.clt1.prod.mlbam.net\n  proxy01.c01.infra.gen.clt1.util.mlbam.net (dont use script on this one, this has 2 snapshots, script only supports 1 for now)\n-&gt; As discussed on Bomgar, these are not super urgent, so we should take our time to improve our procedure and test it well to make the recovery of the other 106 (currently up) successful:\n\nSo:\n* Confirm with Marcus which of the currently down VMs is the next one to be fixed (least important first).\n* Attempt to fix it using script, improving script on problems.\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>Script on:\n\nssh -p 31636 root@dbview3.rhev.gsslab.rdu.redhat.com\nroot/mlb.py\n\nDatabase is restore there too.</text>, <text>Handover Note :=\n\nGermano confirmed with customer as its not urgent and this case can be transferred to Bimal as he is already aware about this case. But this needs to be keep in 24*7 for now. \n\nAction Plan is shared by Geramno in comment number #55 \n\n&lt;germano|mlb&gt; vpagar wrote a lengthy note... I would recommend to let this reach Bimal (who is very aware of the case) in NA to continue.... then back to me tomorrow. Unless someone from europe is eager to join. From what I talked to Marcus on Bomgar this is not super urgent, most VMs are up.\n\nMoving it to waiting on collaboration with NEP set on the case.\n\n\n\nBest Regards,\n\nVaibhav Pagar\nGSS Red Hat Inc.</text>, <text>Hello Marcus,\n\nThank you for your patience, I tried to call you on +1 6468662609 but there was no reply from your end.\n\nI just want to confirm with you that is it ok if we work on this case during NA and APAC hours, as Germano and Bimal is already aware about this case so they can continue with you during APAC and NA hours. as its a snapshot issue which needs good amount of time to troubleshoot but if you want 24*7 support on this case then we can handover it to our next shift engineers.\n\nFor now I have set appropriate time for the case so Bimal can start working once he is available. \n\nLet us know your opinion about the same.  \n\nAwaiting your reply.\n\n\nBest Regards,\n\nVaibhav Pagar\nGSS Red Hat Inc.</text>, <text>Hi Marcus,\n\nThank you very much for your patience. I am working with our Software Maintenance Engineers on thi. We will update the case once more informaiton is available.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Working on this.</text>, <text>Marcus,\n\n\nFor aspera01.c01.inf.datg.clt1.prod.mlbam.net.\n\nCan you run the following commands and try to see if the  VM starts.\n\n--&gt; DISK 8146f3fc-b54f-4aaa-877f-90607898cadf\n\nMAKESURE THE VM IS SHUTDOWN!!\n\n----&gt; RUN ON HOST\n# su vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/4f268962-db9c-4fa7-bada-dc1ff0a909f6 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8146f3fc-b54f-4aaa-877f-90607898cadf/ded51448-742d-4907-8fcb-b3eaedace7fa'\n# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/4f268962-db9c-4fa7-bada-dc1ff0a909f6 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8146f3fc-b54f-4aaa-877f-90607898cadf/ded51448-742d-4907-8fcb-b3eaedace7fa'\n# su vdsm -s /bin/sh -c 'sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8146f3fc-b54f-4aaa-877f-90607898cadf/4f268962-db9c-4fa7-bada-dc1ff0a909f6.meta'\n\n# cd /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8146f3fc-b54f-4aaa-877f-90607898cadf\n# su vdsm -s /bin/sh -c 'mv ded51448-742d-4907-8fcb-b3eaedace7fa ded51448-742d-4907-8fcb-b3eaedace7fa.orig'\n# su vdsm -s /bin/sh -c 'mv ded51448-742d-4907-8fcb-b3eaedace7fa.lease ded51448-742d-4907-8fcb-b3eaedace7fa.lease.orig'\n# su vdsm -s /bin/sh -c 'mv ded51448-742d-4907-8fcb-b3eaedace7fa.meta ded51448-742d-4907-8fcb-b3eaedace7fa.meta.orig'\n\n----&gt; RUN ON RHV-M\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE from images where image_guid ='ded51448-742d-4907-8fcb-b3eaedace7fa';"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t' where image_guid ='4f268962-db9c-4fa7-bada-dc1ff0a909f6';"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '29f5fe36-f034-4b71-809b-4e9882c9e399' and description != 'Active VM';"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE snapshots SET snapshot_id = '24f091aa-f189-4dc8-8368-8409b3318155' WHERE vm_id = '29f5fe36-f034-4b71-809b-4e9882c9e399';"\n\n\nThanks and best regards\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Will do, ill try for that VM and report back\n\n\n\n\nOn 11/15/17, 12:51 PM, "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>Following VM's have multiple disks:\n=====================================\n\ncbdb01.ingest01.hls.hulu.clt1.prod.mlbam.net\ncbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net\ncbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net\ncbdb01.ingest07.hls.hulu.clt1.prod.mlbam.net\ncbdb01.ingest08.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest01.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest02.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest03.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest06.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest07.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest01.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest02.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest03.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest04.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest05.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest06.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest08.hls.hulu.clt1.prod.mlbam.net\ndepot01.clt1.util.mlbam.net\n\ninflux01.c01.inf.hulu.clt1.prod.mlbam.net\njava01.ad01.hls.hulu.clt1.prod.mlbam.net\njava02.ad01.hls.hulu.clt1.prod.mlbam.net\njava02.tool01.postprod.gen.clt1.qa.mlbam.net\njava03.ad01.hls.hulu.clt1.prod.mlbam.net\njava05.ad01.hls.hulu.clt1.prod.mlbam.net\njava06.ad01.hls.hulu.clt1.prod.mlbam.net\njava09.ad01.hls.hulu.clt1.prod.mlbam.net\njava10.ad01.hls.hulu.clt1.prod.mlbam.net\njenkins01.c01.razcp.gen.clt1.prod.mlbam.net\nmdtools01.c01.bpa.hulu.clt1.prod.mlbam.net\nmdtools02.c01.bpa.hulu.clt1.prod.mlbam.net\nmonyog01.c01.noc.gen.clt1.util.mlbam.net\nmysql01.c01.jobmgr.gen.clt1.prod.mlbam.net\nmysql01.control01.provis.gen.clt1.prod.mlbam.net\nmysql02.c01.jobmgr.gen.clt1.prod.mlbam.net\nmysql02.control01.provis.gen.clt1.prod.mlbam.net\nmysqldb01.ingest01.ams.gen.clt1.prod.mlbam.net\nmysqldb01.ingest01.ams.gen.clt1.qa.mlbam.net\nmysqldb02.ingest01.ams.gen.clt1.prod.mlbam.net</text>, <text>Hi Bimal,\nI ran through your set of commands and all looks well! The snapshot is deleted fro the VM's profile , i no longer see the snapshot. and the VM was able to come online OK and i verified the data integrity. \n\n\n[root@hyp103 8146f3fc-b54f-4aaa-877f-90607898cadf]#\n[root@hyp103 8146f3fc-b54f-4aaa-877f-90607898cadf]#\n[root@hyp103 8146f3fc-b54f-4aaa-877f-90607898cadf]# virsh -r dumpxml aspera01.c01.inf.datg.clt1.prod.mlbam.net |grep ovirt-local\n      &lt;source dev='/dev/ovirt-local/4f268962-db9c-4fa7-bada-dc1ff0a909f6'/&gt;\n[root@hyp103 8146f3fc-b54f-4aaa-877f-90607898cadf]#\n[root@hyp103 8146f3fc-b54f-4aaa-877f-90607898cadf]# virsh -r list\n Id    Name                           State\n----------------------------------------------------\n 6     aspera01.c01.inf.wwe.clt1.prod.mlbam.net running\n 12    nginx02.proxy01.hls.gen.clt1.qa.mlbam.net running\n 16    mf04.c01.web.gen.clt1.prod.bamtech.co running\n 18    mf10.c01.web.wwe.clt1.prod.bamtech.co running\n 19    aspera01.c01.inf.datg.clt1.prod.mlbam.net running\n\n[root@hyp103 8146f3fc-b54f-4aaa-877f-90607898cadf]#\n[root@hyp103 8146f3fc-b54f-4aaa-877f-90607898cadf]# ping aspera01.c01.inf.datg.clt1.prod.mlbam.net\nPING aspera01.c01.inf.datg.clt1.prod.mlbam.net (10.34.116.126) 56(84) bytes of data.\n64 bytes from aspera01.c01.inf.datg.clt1.prod.mlbam.net (10.34.116.126): icmp_seq=1 ttl=63 time=0.231 ms\n64 bytes from aspera01.c01.inf.datg.clt1.prod.mlbam.net (10.34.116.126): icmp_seq=2 ttl=63 time=0.199 ms\n^C\n--- aspera01.c01.inf.datg.clt1.prod.mlbam.net ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1001ms\nrtt min/avg/max/mdev = 0.199/0.215/0.231/0.016 ms</text>, <text>MLB ADVANCED MEDIA (Account 5255390) has a support exception for this hook.\n\nThe hook does not support snapshots but they snapshot 111 VMs by mistake through an automated backup job. It "worked" because there is no mechanism to actually prevent the VMs from being snapshot.\n\nNow CEE now needs to fix 111 VMs in support case 01961808.\n\nWe have to improve this hook in order to make it more supportable. The way it is now we have a support exception for something that can cause huge damage. It's also putting a lot of pressure on CEE, using a lot of resources.</text>, <text>Missing from initial comment:\n[1] https://github.com/oVirt/vdsm/tree/master/vdsm_hooks/localdisk</text>, <text>Marcus,\n\nGlad to hear we got that VM up.\n\nA script has been prepared that provides command syntax that needs to be executed on the Host and the RHV-M host for each of the VM.\nBasically the command that were provided to get the previous VM up.\nWe would like to run a sanity check on the script with another VM.\nCan you provide another least important VM name?\n\nWill provide you steps to execute like before.\nIf all looks well, will release the script that you can run in your environment.\n\nLet me know if you have any questions.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Created attachment 1353151\ntentative script  to generate solution for each VM</text>, <text>low impact VM ..... let's do sysdig-poc-temp.c01.mon.gen.clt1.util.mlbam.net \n\n\nafter that i would like to try a production VM (there are 4 that have been down for a couple days now)... they all reside on hyp11.clt1.prod.mlbam.net\n\n1. memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net\n2.memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net\n3.nginx01.live01.hls.mls.clt1.prod.mlbam.net\n4.proxy01.c01.infra.gen.clt1.util.mlbam.net</text>, <text>Current fix is to basically collapse the chain down to the base image on the local host, then adjust the DB.\n\nExample:\n\n   Volume AAA is the raw  , local to the host, image (i.e.: /dev/ovirt-local/...)\n   Volume BBB is the qcow2, on nfs storage   , image (i.e.: /rhev/data-center/...)\n   Chain is: [BBB - leaf]\n             [AAA - base]\nSteps:\n1) VM shutdown (!!!)\n2) unsafe rebase of image BBB, make it point to AAA's path (so that step 3 can succeed, otherwise qemu-img commit fails to build the chain)\n   qemu-img rebase -f qcow2 -t none -u -b AAA -F raw BBB\n3) image commit, commit BBB into AAA\n   qemu-img commit -f qcow2 -t none -b AAA -d -p BBB\n4) metadata fix for AAA\n   sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' AAA.meta\n5) Remove BBB (optional, probably safer to do later)\n   # rm -rf BBB*\n6) Adjust images and snapshots in the DB to make VM with no snapshots and use AAA to start.</text>, <text>Hi Marcus,\n\nsysdig-poc-temp.c01.mon.gen.clt1.util.mlbam.net sucessfully deleted its snapshot. This is a completely different case. The snapshot was deleted when the VM was down, this succeeds but I doubt it does the correct thing. I will investigate what can be done.\n\nNotes:\n* Please don't attempt to remove the snapshots from the VMs that are down.\n* Please don't power up or down a VM that was snapshot.\n\nCould you please you confirm if the other 100+ VMs that are currently up were not shutdown after being snapshot? This greatly complicates the resolution, for these ones we will need to go full manual and may lose some data (after power up).\n\nAs we have few VMs down, do you prefer to investigate sysdig-poc-temp.c01.mon.gen.clt1.util.mlbam.net (and possibly recover it with a custom solution) or jump to another VM?\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>perhaps sysdig VM was a bad test VM. That VM has been offline for some time now and is probably not a good use case (as it's a poc VM and most likely being decommissioned.). \n\nAnd yes, with the exception of perhaps 1 VM (actor02.c01.razcp.gen.clt1.dev.mlbam.net), i dont believe any VMs have been shutdown/restarted although there is no way to know for sure at the moment. Most have a long uptime. We'll run a new scan to determine if any of these VMs have recent uptimes of less than X amount of days. \n\nFor now we can use the next aspera VM to test on: aspera02.c01.inf.datg.clt1.prod.mlbam.net\n\n\n\n*** Just in case there was a VM rebooted and the ovirt-local VG had enough space to allow the rollback, would we be able to document those remediation steps as well ?</text>, <text>Hi Marcus,\n\n&gt; perhaps sysdig VM was a bad test VM. That VM has been offline for some time now and is probably not a good use case (as it's a poc VM and most likely being decommissioned.). \nOK. We can fix it later if you find necessary.\n\n&gt; And yes, with the exception of perhaps 1 VM (actor02.c01.razcp.gen.clt1.dev.mlbam.net), i dont believe any VMs have been shutdown/restarted\nThat is good news.\n\n&gt; *** Just in case there was a VM rebooted and the ovirt-local VG had enough space to allow the rollback, would we be able to document those remediation steps as well ?\nOf course, we will help. In this case we dont have many choices. The data is branched, so we need to pick one of the branches. I assume [1] is more correct even if it rolls back a bit (power off date) because [2] has a hole in time.\nPRE SNAPSHOT -&gt; SNAPSHOT [1]\n             -&gt; Power UP [2]\n\nMoving forward, lets fix aspera02.c01.inf.datg.clt1.prod.mlbam.net\n\nThe the vm MUST(!!) be SHUTDOWN before starting the procedure. Here are the steps for aspera02.c01.inf.datg.clt1.prod.mlbam.net. \n\n# ./mlb.py --vm-name aspera02.c01.inf.datg.clt1.prod.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '6aa3b964-c214-44ad-b059-518991e2a69d' AND snapshot_id != '796be474-672a-415f-8a66-20b4e0385752';"\n\nFIXING DISK fc2d66f9-f6b1-402a-8d20-b27ec9b65e50, CHAIN ['05637aaa-2420-410c-9964-f2c32fd20bc2', 'b745b82b-7133-4b1e-a203-cf708ac4a357']\n\nHost Commands:\n==============\n# su vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/05637aaa-2420-410c-9964-f2c32fd20bc2 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357'\n# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/05637aaa-2420-410c-9964-f2c32fd20bc2 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357'\n# su vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/05637aaa-2420-410c-9964-f2c32fd20bc2.meta"\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.meta.orig'\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.lease.orig'\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.orig'\n\nRHVM Commands:\n==============\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('b745b82b-7133-4b1e-a203-cf708ac4a357');"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='796be474-672a-415f-8a66-20b4e0385752' WHERE image_guid ='05637aaa-2420-410c-9964-f2c32fd20bc2';"\n\nPlease let me know how it goes. I did some basic checks manually and the script output looks good. Please report back any syntax errors or issues you may find, I already fixed a few. "Global RHVM Commands" can be run at the end with the last disk "RHVM Command" as this VM has just 1 disk.\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>Hi Germano\nCan you double check the mv commands, they all seem to be the same command x3</text>, <text>Actually, the mv commands attempt to move the same file 3 times. the source is the same, destination differs between meta|.lease|file</text>, <text>Hi Marcus,\n\nYou are right. Oops, small bug. The first parameter was missing the extension.\n\nHere they are again:\n\nGlobal RHVM Commands:\n=====================\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '6aa3b964-c214-44ad-b059-518991e2a69d' AND snapshot_id != '796be474-672a-415f-8a66-20b4e0385752';"\n\nFIXING DISK fc2d66f9-f6b1-402a-8d20-b27ec9b65e50, CHAIN ['05637aaa-2420-410c-9964-f2c32fd20bc2', 'b745b82b-7133-4b1e-a203-cf708ac4a357']\n\nHost Commands:\n==============\n# su vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/05637aaa-2420-410c-9964-f2c32fd20bc2 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357'\n# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/05637aaa-2420-410c-9964-f2c32fd20bc2 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357'\n# su vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/05637aaa-2420-410c-9964-f2c32fd20bc2.meta"\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.meta.orig'\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.lease.orig'\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.orig'\n\nRHVM Commands:\n==============\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('b745b82b-7133-4b1e-a203-cf708ac4a357');"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='796be474-672a-415f-8a66-20b4e0385752' WHERE image_guid ='05637aaa-2420-410c-9964-f2c32fd20bc2';"\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>no problemo. I corrected it on my end as well. thanks for the updated set of commands. I finished the test and the VM has powered on successfully and i'm able to see the VM's config returned back to the lvm commands in my dumpxml output......\n\n\n[root@hyp105 ~]#\n[root@hyp105 ~]#\n[root@hyp105 ~]# virsh -r list\n Id    Name                           State\n----------------------------------------------------\n 2     seg04.c03.streaming.gen.clt1.dev.mlbam.net running\n 7     aspera02.c01.inf.datg.clt1.prod.mlbam.net running\n 8     aspera02.c01.inf.wwe.clt1.prod.mlbam.net running\n 10    mf12.c01.web.wwe.clt1.prod.bamtech.co running\n\n[root@hyp105 ~]#\n[root@hyp105 ~]# virsh -r dumpxml aspera02.c01.inf.datg.clt1.prod.mlbam.net |grep ovirt-local\n[root@hyp105 ~]# virsh -r list\n Id    Name                           State\n----------------------------------------------------\n 2     seg04.c03.streaming.gen.clt1.dev.mlbam.net running\n 8     aspera02.c01.inf.wwe.clt1.prod.mlbam.net running\n 10    mf12.c01.web.wwe.clt1.prod.bamtech.co running\n\n[root@hyp105 ~]# su vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/05637aaa-2420-410c-9964-f2c32fd20bc2 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357'\n[root@hyp105 ~]#  su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/05637aaa-2420-410c-9964-f2c32fd20bc2 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357'\n    (100.00/100%)\nImage committed.\n[root@hyp105 ~]#\n[root@hyp105 ~]# su vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/05637aaa-2420-410c-9964-f2c32fd20bc2.meta"\n[root@hyp105 ~]#\n[root@hyp105 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.meta.orig'\n[root@hyp105 ~]#\n[root@hyp105 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.lease.orig'\nmv: cannot stat \u2018/rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357\u2019: No such file or directory\n[root@hyp105 ~]#\n[root@hyp105 ~]#\n[root@hyp105 ~]#\n[root@hyp105 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.lease.orig'\n[root@hyp105 ~]#\n[root@hyp105 ~]#\n[root@hyp105 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.meta.orig /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.orig'\n[root@hyp105 ~]#\n[root@hyp105 ~]#\n[root@hyp105 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc2d66f9-f6b1-402a-8d20-b27ec9b65e50/b745b82b-7133-4b1e-a203-cf708ac4a357.meta.orig'\n[root@hyp105 ~]#\n[root@hyp105 ~]#\n[root@hyp105 ~]# virsh -r list\n Id    Name                           State\n----------------------------------------------------\n 2     seg04.c03.streaming.gen.clt1.dev.mlbam.net running\n 8     aspera02.c01.inf.wwe.clt1.prod.mlbam.net running\n 10    mf12.c01.web.wwe.clt1.prod.bamtech.co running\n 11    aspera02.c01.inf.datg.clt1.prod.mlbam.net running\n\n[root@hyp105 ~]# virsh -r dumpxml aspera02.c01.inf.datg.clt1.prod.mlbam.net |grep ovirt-local\n      &lt;source dev='/dev/ovirt-local/05637aaa-2420-410c-9964-f2c32fd20bc2'/&gt;\n[root@hyp105 ~]#</text>, <text>Hi Marcus,\n\nThats good. Could you please confirm the data inside the VM is sane and the VM is healthy?\n\nThese 3 VMs below seem to have gone through the snapshot -&gt; stop -&gt; start cycle. If it's fine to roll their disks back to Nov 12-13 (power off date) we can fix them now.\nmemcached01.c01.lprovis.hulu.clt1.prod.mlbam.net\nnginx01.live01.hls.mls.clt1.prod.mlbam.net\nproxy01.c01.infra.gen.clt1.util.mlbam.net\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>The aspera VM looks good. \n\nAs for the 3 other VMs, I believe the actual series of events is that they were put into a 'paused' state when the datastore filled up to 100% on the 12th, then when we increased the size of the datastore, the VMs were 'unpaused'. Then on the morning of the 14th it just so happens that the hypervisor (hyp11) went down due to memory errors, thus taking the VMs offline. Then the same day the memory was fixed and the VMs were attempted to be online'd but failed. (see screenshot). As far as I know these VMs have been offline since the morning of the 14th when the hypervisor went down due to uncorrectable memory errors</text>, <text>I submitted my last comment too early...\n\nSince we can't online these 3 vm's at all right now, we should go ahead with whatever remediation steps you can provide. Getting them online a couple days behind is better than not getting them online at all :) thanks Germano!</text>, <text>the memcache and mysql VMs are the priority at the moment if possible</text>, <text>Hi Marcus,\n\n&gt; Then on the morning of the 14th it just so happens that the hypervisor (hyp11) went down due to memory errors\nYou are right, it was on the 14th. The timezone of my restored DB from your LogCollector is off.\n\n&gt; the memcache and mysql VMs are the priority at the moment if possible\nSure, please find both below. I checked the Events and these two seem to be exact same case of the previous VMs, so I'm trusting the script to do the right thing as it did right for the last few VMs. If these are critical please let me know and I'll manually check them (~30min each).\n\n# ./mlb.py --vm-name memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'eef6a68a-22a2-4933-b864-5da5947a312e' AND snapshot_id != '2a42f44d-da57-479a-a2c4-9f16d38a3066';"\n\nFIXING DISK e73090d4-55f9-4cd0-b539-749ad930c3b3, CHAIN ['1ce7d37a-4c1f-43e0-99c7-589c978ba1c5', 'f908a77e-fe37-45e3-9bb3-3d84b230d61a']\n\nHost Commands:\n==============\n# su vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/1ce7d37a-4c1f-43e0-99c7-589c978ba1c5 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/f908a77e-fe37-45e3-9bb3-3d84b230d61a'\n# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/1ce7d37a-4c1f-43e0-99c7-589c978ba1c5 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/f908a77e-fe37-45e3-9bb3-3d84b230d61a'\n# su vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/1ce7d37a-4c1f-43e0-99c7-589c978ba1c5.meta"\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/f908a77e-fe37-45e3-9bb3-3d84b230d61a.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/f908a77e-fe37-45e3-9bb3-3d84b230d61a.meta.orig'\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/f908a77e-fe37-45e3-9bb3-3d84b230d61a.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/f908a77e-fe37-45e3-9bb3-3d84b230d61a.lease.orig'\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/f908a77e-fe37-45e3-9bb3-3d84b230d61a /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/f908a77e-fe37-45e3-9bb3-3d84b230d61a.orig'\n\nRHVM Commands:\n==============\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('f908a77e-fe37-45e3-9bb3-3d84b230d61a');"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='2a42f44d-da57-479a-a2c4-9f16d38a3066' WHERE image_guid ='1ce7d37a-4c1f-43e0-99c7-589c978ba1c5';"\n\n\n\n\n# ./mlb.py --vm-name mysql01.control01.provis.gen.clt1.prod.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'e008d640-57b5-4381-a0da-9339a8871eeb' AND snapshot_id != 'f0ad7ace-fbef-4877-9506-af99de82f6aa';"\n\nFIXING DISK 7b2c5b66-0224-42a5-9995-2beb362e96ca, CHAIN ['03f8dd9b-fccf-46ae-ab61-af0f27aaca0a', 'c18a4a51-9890-4767-8cff-86b885d0d73b']\n\nHost Commands:\n==============\n# su vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/03f8dd9b-fccf-46ae-ab61-af0f27aaca0a -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7b2c5b66-0224-42a5-9995-2beb362e96ca/c18a4a51-9890-4767-8cff-86b885d0d73b'\n# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/03f8dd9b-fccf-46ae-ab61-af0f27aaca0a -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7b2c5b66-0224-42a5-9995-2beb362e96ca/c18a4a51-9890-4767-8cff-86b885d0d73b'\n# su vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7b2c5b66-0224-42a5-9995-2beb362e96ca/03f8dd9b-fccf-46ae-ab61-af0f27aaca0a.meta"\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7b2c5b66-0224-42a5-9995-2beb362e96ca/c18a4a51-9890-4767-8cff-86b885d0d73b.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7b2c5b66-0224-42a5-9995-2beb362e96ca/c18a4a51-9890-4767-8cff-86b885d0d73b.meta.orig'\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7b2c5b66-0224-42a5-9995-2beb362e96ca/c18a4a51-9890-4767-8cff-86b885d0d73b.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7b2c5b66-0224-42a5-9995-2beb362e96ca/c18a4a51-9890-4767-8cff-86b885d0d73b.lease.orig'\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7b2c5b66-0224-42a5-9995-2beb362e96ca/c18a4a51-9890-4767-8cff-86b885d0d73b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7b2c5b66-0224-42a5-9995-2beb362e96ca/c18a4a51-9890-4767-8cff-86b885d0d73b.orig'\n\nRHVM Commands:\n==============\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('c18a4a51-9890-4767-8cff-86b885d0d73b');"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='f0ad7ace-fbef-4877-9506-af99de82f6aa' WHERE image_guid ='03f8dd9b-fccf-46ae-ab61-af0f27aaca0a';"\n\nFIXING DISK ed1f2841-373d-42ac-a902-9fc09eed0d73, CHAIN ['a7fa7d07-d96b-4e15-af25-3ff9b1ff531c', '618861bb-3173-4458-b957-fb9d01c7d525']\n\nHost Commands:\n==============\n# su vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/a7fa7d07-d96b-4e15-af25-3ff9b1ff531c -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed1f2841-373d-42ac-a902-9fc09eed0d73/618861bb-3173-4458-b957-fb9d01c7d525'\n# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/a7fa7d07-d96b-4e15-af25-3ff9b1ff531c -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed1f2841-373d-42ac-a902-9fc09eed0d73/618861bb-3173-4458-b957-fb9d01c7d525'\n# su vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed1f2841-373d-42ac-a902-9fc09eed0d73/a7fa7d07-d96b-4e15-af25-3ff9b1ff531c.meta"\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed1f2841-373d-42ac-a902-9fc09eed0d73/618861bb-3173-4458-b957-fb9d01c7d525.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed1f2841-373d-42ac-a902-9fc09eed0d73/618861bb-3173-4458-b957-fb9d01c7d525.meta.orig'\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed1f2841-373d-42ac-a902-9fc09eed0d73/618861bb-3173-4458-b957-fb9d01c7d525.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed1f2841-373d-42ac-a902-9fc09eed0d73/618861bb-3173-4458-b957-fb9d01c7d525.lease.orig'\n# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed1f2841-373d-42ac-a902-9fc09eed0d73/618861bb-3173-4458-b957-fb9d01c7d525 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed1f2841-373d-42ac-a902-9fc09eed0d73/618861bb-3173-4458-b957-fb9d01c7d525.orig'\n\nRHVM Commands:\n==============\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('618861bb-3173-4458-b957-fb9d01c7d525');"\n# /usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='f0ad7ace-fbef-4877-9506-af99de82f6aa' WHERE image_guid ='a7fa7d07-d96b-4e15-af25-3ff9b1ff531c';"\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>2nd command failed with the below error...doesnt seem like the volume is active ?\n\n\n[root@hyp11 ~]#  su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/1ce7d37a-4c1f-43e0-99c7-589c978ba1c5 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/f908a77e-fe37-45e3-9bb3-3d84b230d61a'\nqemu-img: Could not open '/rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e73090d4-55f9-4cd0-b539-749ad930c3b3/f908a77e-fe37-45e3-9bb3-3d84b230d61a': Could not open backing file: Could not open '/dev/ovirt-local/1ce7d37a-4c1f-43e0-99c7-589c978ba1c5': No such file or directory\n[root@hyp11 ~]# lvs\n  LV                                   VG          Attr       LSize   Pool   Origin                Data%  Meta%  Move Log Cpy%Sync Convert\n  03f8dd9b-fccf-46ae-ab61-af0f27aaca0a ovirt-local -wi-------  50.00g\n  0903ad06-7821-4fa3-8261-8bc2f4560fd5 ovirt-local -wi-------  50.00g\n  17532e14-6085-47c4-af48-7794c5b4742b ovirt-local -wi-ao----  50.00g\n  1ce7d37a-4c1f-43e0-99c7-589c978ba1c5 ovirt-local -wi-------  50.00g\n  57f5b0fa-f7a3-488a-b7e6-939ac0bcd533 ovirt-local -wi-ao----  50.00g\n  58c45e9e-b0ce-4238-bdb1-e1e4f27e57e5 ovirt-local -wi-------  50.00g\n  a7fa7d07-d96b-4e15-af25-3ff9b1ff531c ovirt-local -wi------- 100.00g\n  bbb98b6e-8e2d-43fc-8003-1170d694953f ovirt-local -wi-ao----  50.00g\n  c18a4a51-9890-4767-8cff-86b885d0d73b ovirt-local -wi-a-----  50.00g\n  d76fa0e0-f312-49b1-991b-893b8b7082f1 ovirt-local -wi-ao----  50.00g\n  ddc56090-6167-4229-ac88-1e14ad0e297f ovirt-local -wi-------  50.00g\n  f908a77e-fe37-45e3-9bb3-3d84b230d61a ovirt-local -wi-a-----  50.00g\n  pool00                               rhvh_hyp11  twi-aotz-- 202.02g                              2.54   0.91\n  rhvh-4.0-0.20160919.0                rhvh_hyp11  Vwi---tz-k 186.98g pool00 root\n  rhvh-4.0-0.20160919.0+1              rhvh_hyp11  Vwi-aotz-- 186.98g pool00 rhvh-4.0-0.20160919.0 1.27\n  root                                 rhvh_hyp11  Vwi---tz-- 186.98g pool00\n  swap                                 rhvh_hyp11  -wi-ao----   4.00g\n  var                                  rhvh_hyp11  Vwi-aotz--  15.00g pool00                       11.09\n[root@hyp11 ~]#</text>, <text>Hi Marcus,\n\nJust a correction:\n\n&gt; I checked the Events and these two seem to be exact same case of the previous VMs\nWhat I mean is we will apply the same solution. The case is in fact a bit different.\n\nI think this is the best solution, pick a contiguous branch of the data with no time holes in between, even if the VM rolls back a couple days. Probably more data would be lost or it would be inconsistent if we picked the other branch (snapshot on local disk).\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>Hi Marcus,\n\n&gt; 2nd command failed with the below error...doesnt seem like the volume is active ?\n\nYes, you need to activate the volume (I will add this to the scripts).\n\nlvchange -ay /dev/ovirt-local/1ce7d37a-4c1f-43e0-99c7-589c978ba1c5\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>Created attachment 1353232\ntentative script  to generate solution for each VM</text>, <text>Both the mysql and memcached VMs looks OK and booted fine! I currently have the application owner verifying data integrity/stability on the VM. Perhaps this is a good time to stop for the moment and reconvene tomorrow ? Verifying these last 2 prod VMS could take a little bit.\n\n\nFOR MEMCACHED VM.............................................\n\n[root@hyp11 ~]# virsh -r list\n Id    Name                           State\n----------------------------------------------------\n 2     proxy02.linear01.infra.hulu.clt1.prod.mlbam.net running\n 3     control02.c03.streaming.hulu.clt1.prod.mlbam.net running\n 4     control02.c11.streaming.hulu.clt1.prod.mlbam.net running\n 6     seg07.event01.streaming.riot.clt1.prod.mlbam.net running\n 7     memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net running\n\n[root@hyp11 ~]# virsh -r dumpxml memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net |grep ovirt-local\n      &lt;source dev='/dev/ovirt-local/1ce7d37a-4c1f-43e0-99c7-589c978ba1c5'/&gt;\n[root@hyp11 ~]#\n[root@hyp11 ~]# ping memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net\nPING memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net (10.34.116.27) 56(84) bytes of data.\n64 bytes from memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net (10.34.116.27): icmp_seq=1 ttl=63 time=0.100 ms\n64 bytes from memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net (10.34.116.27): icmp_seq=2 ttl=63 time=0.163 ms\n64 bytes from memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net (10.34.116.27): icmp_seq=3 ttl=63 time=0.090 ms\n64 bytes from memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net (10.34.116.27): icmp_seq=4 ttl=63 time=0.149 ms\n\n\nFOR MYSQL VM --------------------------------------\n\n[root@hyp11 ~]# virsh -r list\n Id    Name                           State\n----------------------------------------------------\n 2     proxy02.linear01.infra.hulu.clt1.prod.mlbam.net running\n 3     control02.c03.streaming.hulu.clt1.prod.mlbam.net running\n 4     control02.c11.streaming.hulu.clt1.prod.mlbam.net running\n 6     seg07.event01.streaming.riot.clt1.prod.mlbam.net running\n 7     memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net running\n 8     mysql01.control01.provis.gen.clt1.prod.mlbam.net running\n\n[root@hyp11 ~]# virsh -r dumpxml mysql01.control01.provis.gen.clt1.prod.mlbam.net |grep ovirt-local\n      &lt;source dev='/dev/ovirt-local/03f8dd9b-fccf-46ae-ab61-af0f27aaca0a'/&gt;\n      &lt;source dev='/dev/ovirt-local/a7fa7d07-d96b-4e15-af25-3ff9b1ff531c'/&gt;\n[root@hyp11 ~]#\n[root@hyp11 ~]# ping mysql01.control01.provis.gen.clt1.prod.mlbam.net\nPING mysql01.control01.provis.gen.clt1.prod.mlbam.net (10.34.116.77) 56(84) bytes of data.\n64 bytes from mysql01.control01.provis.gen.clt1.prod.mlbam.net (10.34.116.77): icmp_seq=1 ttl=63 time=0.106 ms\n64 bytes from mysql01.control01.provis.gen.clt1.prod.mlbam.net (10.34.116.77): icmp_seq=2 ttl=63 time=0.117 ms</text>, <text>Hi Marcus,\n\nGreat! Just one little thing, please verify the VMs disks using this command below necause grepping for ovirt-local may hide some other image being accessed (which we don't want).\n\n# virsh -r domblklist &lt;VM NAME&gt;\nTarget     Source\n------------------------------------------------\nhdc        -\nvda        /rhev/data-center/00000002-0002-0002-0002-000000000010/aa546f2d-5afa-4ebc-ac5a-b9e67da12665/images/9df59814-dd12-419f-9254-523ff85ec968/ec6b441b-8e70-4b74-b7bb-7257755aa40d\n\nI agree, let's stop so you can verify your VMs to ensure we are on the correct path. I'm setting the case to Waiting on Customer, switch it back when you want to continue.\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>Looks good! i prefer domblklist as well. I'll use that going forward. \nThanks for the great help tonight. We'll reconnect tomorrow Thank you!\n\n\n[root@hyp11 ~]# virsh -r domblklist mysql01.control01.provis.gen.clt1.prod.mlbam.net\nTarget     Source\n------------------------------------------------\nhdc        -\nvda        /dev/ovirt-local/03f8dd9b-fccf-46ae-ab61-af0f27aaca0a\nvdb        /dev/ovirt-local/a7fa7d07-d96b-4e15-af25-3ff9b1ff531c\n\n[root@hyp11 ~]#\n[root@hyp11 ~]# virsh -r domblklist memcached01.c01.lprovis.hulu.clt1.prod.mlbam.net\nTarget     Source\n------------------------------------------------\nhdc        -\nvda        /dev/ovirt-local/1ce7d37a-4c1f-43e0-99c7-589c978ba1c5\n\n[root@hyp11 ~]#</text>, <text>Hi Marcus,\n\nNice, the outputs look perfect.\n\nPlease reply back when you want to continue. Have a good night!\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>Bug tickets must have version flags set prior to targeting them to a release. Please ask maintainer to set the correct version flags and only then set the target milestone.</text>, <text>### Handover Ready ####\n\nNo update during India hours.</text>, <text>We've confirmed the latest tested VM's are OK. Please let me know how you want to continue. Do you think we're ready to starting using the script on our own at this point ? i have another 2 vms on that same hypervisor (hyp11) that are still offline.  We can test those next if you want some more tests..\n\n\nnginx01.live01.hls.mls.clt1.prod.mlbam.net\nnginx01.live01.hls.mls.clt1.prod.mlbam.net</text>, <text>Marcus,\n\nThank you for the update. \nLet me review the info on the other 2 VM's and get back to you.\n\nThanks\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Hello Marcus,\n\nThe following VM's appear to be the same.\n\n~~~\nnginx01.live01.hls.mls.clt1.prod.mlbam.net\nnginx01.live01.hls.mls.clt1.prod.mlbam.net\n~~~\n\nWhat are the other production VM's that are down and need to be started?\n\n\nThanks \nBimal.</text>, <text>my mistake , bad copy and paste, the 2nd VM is supposed to be :  proxy01.c01.infra.gen.clt1.util.mlbam.net\n\nThen there are about 200 additional VMs affected by this , which are powered on. We'll have to coordinate downtime windows with application owners to take care of those.</text>, <text>Hi Marcus,\n\nOk Thank you please keep us posted to this.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Both VM's appear don't appear to have been 'shutdown' and restarted after the snapshot opearation.\n\n~~~\n 2017-11-12 02:52:24.884+00    | 6631fab7       | Snapshot 'Backup__2017-11-12__02.52.13' creation for VM 'nginx01.live01.hls.mls.clt1.prod.mlbam.net' was initiated by arichardson@mlbam.net.\n 2017-11-12 02:52:43.539+00    | 6631fab7       | Snapshot 'Backup__2017-11-12__02.52.13' creation for VM 'nginx01.live01.hls.mls.clt1.prod.mlbam.net' has been completed.\n\n 2017-11-12 02:54:14.251+00    | 3c7e4fa4       | Snapshot 'Backup__2017-11-12__02.52.13' deletion for VM 'nginx01.live01.hls.mls.clt1.prod.mlbam.net' was initiated by arichardson@mlbam.net.\n 2017-11-12 03:17:26.114+00    | 3c7e4fa4       | Failed to delete snapshot 'Backup__2017-11-12__02.52.13' for VM 'nginx01.live01.hls.mls.clt1.prod.mlbam.net'.\n\n 2017-11-12 13:15:05.707+00    |                | VM nginx01.live01.hls.mls.clt1.prod.mlbam.net has been paused.\n 2017-11-12 13:15:05.715+00    |                | VM nginx01.live01.hls.mls.clt1.prod.mlbam.net has been paused due to no Storage space error.\n 2017-11-12 14:08:57.36+00     | 5a95788a       | VM nginx01.live01.hls.mls.clt1.prod.mlbam.net was resumed by fseifts@mlbam.net (Host: hyp11.clt1.prod.mlbam.net).\n 2017-11-12 14:09:04.101+00    |                | VM nginx01.live01.hls.mls.clt1.prod.mlbam.net has recovered from paused back to up.\n 2017-11-12 14:09:04.125+00    | 5a95788a       | VM nginx01.live01.hls.mls.clt1.prod.mlbam.net started on Host hyp11.clt1.prod.mlbam.net\n\n 2017-11-14 20:21:35.402+00    | 2ca6ad58       | VM nginx01.live01.hls.mls.clt1.prod.mlbam.net was started by nreisbeck@mlbam.net (Host: hyp11.clt1.prod.mlbam.net).\n 2017-11-14 20:21:38.126+00    |                | VM nginx01.live01.hls.mls.clt1.prod.mlbam.net is down with error. Exit message: Hook Error: \n~~~\n\n~~~\n 2017-11-12 13:14:35.681+00    |                | VM proxy01.c01.infra.gen.clt1.util.mlbam.net has been paused.\n 2017-11-12 13:14:35.737+00    |                | VM proxy01.c01.infra.gen.clt1.util.mlbam.net has been paused due to no Storage space error.\n 2017-11-12 14:37:35.458+00    | 65a2a2fb       | VM proxy01.c01.infra.gen.clt1.util.mlbam.net was resumed by fseifts@mlbam.net (Host: hyp11.clt1.prod.mlbam.net).\n 2017-11-12 14:37:39.004+00    |                | VM proxy01.c01.infra.gen.clt1.util.mlbam.net has recovered from paused back to up.\n 2017-11-12 14:37:39.078+00    | 65a2a2fb       | VM proxy01.c01.infra.gen.clt1.util.mlbam.net started on Host hyp11.clt1.prod.mlbam.net\n 2017-11-14 20:22:33.543+00    | 106bee06       | VM proxy01.c01.infra.gen.clt1.util.mlbam.net was started by nreisbeck@mlbam.net (Host: hyp11.clt1.prod.mlbam.net).\n 2017-11-14 20:22:36.669+00    |                | VM proxy01.c01.infra.gen.clt1.util.mlbam.net is down with error. Exit message: Hook Error: \n~~~</text>, <text>Hello Marcus,\n\n\nBoth nginx01.live01.hls.mls.clt1.prod.mlbam.net and proxy01.c01.infra.gen.clt1.util.mlbam.net, don't appear to have been restarted after the snapshot operation.\nYou can issue the commands below to get those two VM back into operational state.\nRegarding the other VM's that have been impacted by this, we may need to check manually to see if they have been restarted after the snapshot operation and the state of the images on the NFS share and the local.\nWe are also discussing this internally as well.  But for now our recommendation is please do not restart the VM or perform any sort of snapshot operation.\n\nLet me know if you have further questions.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\n\n\n\n~~~\n# ./mlb.py --vm-name nginx01.live01.hls.mls.clt1.prod.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '8a4c0055-4813-4163-a8b8-fe9e92bf3020' AND snapshot_id != 'b851e260-2c31-472c-8d8f-749404f103f6';"\n\nFIXING DISK a11e5029-2eea-4a15-a493-f3ec24d3780c, CHAIN ['0903ad06-7821-4fa3-8261-8bc2f4560fd5', '9ba3b527-596e-40d8-9aaa-28c324fd4162']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/0903ad06-7821-4fa3-8261-8bc2f4560fd5 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162'\nlvchange -ay /dev/ovirt-local/0903ad06-7821-4fa3-8261-8bc2f4560fd5\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/0903ad06-7821-4fa3-8261-8bc2f4560fd5 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/0903ad06-7821-4fa3-8261-8bc2f4560fd5.meta"\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a11e5029-2eea-4a15-a493-f3ec24d3780c/9ba3b527-596e-40d8-9aaa-28c324fd4162.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('9ba3b527-596e-40d8-9aaa-28c324fd4162');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='b851e260-2c31-472c-8d8f-749404f103f6' WHERE image_guid ='0903ad06-7821-4fa3-8261-8bc2f4560fd5';"\n~~~\n\n\n\n~~~\n# ./mlb.py --vm-name proxy01.c01.infra.gen.clt1.util.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '347ce5c5-2d21-4667-a6ca-2addf5820727' AND snapshot_id != 'e7cc76d9-d176-4a82-88ea-d5b47aa253d7';"\n\nFIXING DISK ad84d488-37c9-4f8b-ac34-51ce182e86a8, CHAIN ['ddc56090-6167-4229-ac88-1e14ad0e297f', '13fc005a-48a4-4fa0-9366-705a156bba07', '89583c66-83cd-4ea3-b4be-2e0aa12931b9']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/89583c66-83cd-4ea3-b4be-2e0aa12931b9'\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/89583c66-83cd-4ea3-b4be-2e0aa12931b9'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.meta"\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/89583c66-83cd-4ea3-b4be-2e0aa12931b9.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/89583c66-83cd-4ea3-b4be-2e0aa12931b9.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/89583c66-83cd-4ea3-b4be-2e0aa12931b9.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/89583c66-83cd-4ea3-b4be-2e0aa12931b9.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/89583c66-83cd-4ea3-b4be-2e0aa12931b9 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/89583c66-83cd-4ea3-b4be-2e0aa12931b9.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/ddc56090-6167-4229-ac88-1e14ad0e297f -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07'\nlvchange -ay /dev/ovirt-local/ddc56090-6167-4229-ac88-1e14ad0e297f\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ddc56090-6167-4229-ac88-1e14ad0e297f -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('13fc005a-48a4-4fa0-9366-705a156bba07','89583c66-83cd-4ea3-b4be-2e0aa12931b9');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='e7cc76d9-d176-4a82-88ea-d5b47aa253d7' WHERE image_guid ='ddc56090-6167-4229-ac88-1e14ad0e297f';"\n~~~</text>, <text>for VM: proxy01 , after the lvchange command is the following command...\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ddc56090-6167-4229-ac88-1e14ad0e297f -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07'\n\n\nThis command resulted in an error:....\n\n[root@hyp11 ~]# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ddc56090-6167-4229-ac88-1e14ad0e297f -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07'\nqcow2: Marking image as corrupt: Data cluster offset 0x12c00 unaligned (L2 offset: 0x100000, L2 index: 0x1); further corruption events will be suppressed\n    (0.00/100%)</text>, <text>There is a bug in the script,\n\nThis VM had 2 snapshots. The script handled it wrongly, this should not have been done (first command):\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/89583c66-83cd-4ea3-b4be-2e0aa12931b9'\n\nWe changed the backing file fmt from qcow2 to raw, the first commit mangled the image, the second commit spit the error.\n\nI'll fix it. This is just proof the script is not mature yet, we need to continue manually checking the steps the script produces. \nThe VM is probably lost. But these should be least important VMs. We can attempt to start with the base only, commit stopped at 0%, probably not much damage done (hopefully).</text>, <text>Hello Marcus\n\nHaving this error occur is unfortunate.  This VM contained 2 snapshots whereas the one's we recovered only had 1 snapshot. \nThe number of snapshot variation created for these impacted VM's are different.  This is why we wanted to test with least important VM's first.\nWe can attempt to start with the base only, commit stopped at 0%, probably not much damage done (hopefully).\nIf the VM starts up successfully, you will have older data as we are going back to the base image.\n\nHOST.\n\n~~~\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/13fc005a-48a4-4fa0-9366-705a156bba07.orig'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ad84d488-37c9-4f8b-ac34-51ce182e86a8/ddc56090-6167-4229-ac88-1e14ad0e297f.meta"\n~~~\n\nRHV-M\n\n~~~\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '347ce5c5-2d21-4667-a6ca-2addf5820727' AND snapshot_id != 'e7cc76d9-d176-4a82-88ea-d5b47aa253d7';"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('13fc005a-48a4-4fa0-9366-705a156bba07','89583c66-83cd-4ea3-b4be-2e0aa12931b9');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='e7cc76d9-d176-4a82-88ea-d5b47aa253d7' WHERE image_guid ='ddc56090-6167-4229-ac88-1e14ad0e297f';"\n~~~\n\nLet me know if you have any questions.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc\nGSS</text>, <text>Hello Bimal,\nThe VM (proxy01) started successfully and the disk devices point to the local ovirt-local logical volume as wanted. I think we're good with this VM. I was able to login and verify disk looks good. Thank you for the recovery procedure.\n\nI've uplaoded the list of the 140 additional VM's we will need to coordinate with application owners to shutdown and run the fix on. Please feel free to inspect the list comment where necessary. Thank you</text>, <text>Hi Marcus,\n\nI'm glad proxy01 wasn't damaged. Did you run fsck on it?\n\nWe identified the bug in the script and fixed it, there were 2 flaws for VMs with more than 1 snapshot. This VM had 2 snapshots. You have 38 VMs with 2+ snapshots (6 with 3 snapshots), these we will handle with more care, they need to be analyzed individually. If second (or third) snapshot may have been created after a reboot with the first snapshot, the script can't handle this case. But if the VM was never shutdown and the snapshots were created in sequence then the script can provide the correct solution.\n\nI don't feel confident yet in handing the script to you, I think we still need to thread carefully while manually checking the commands. I would like to see how the script behaves on some other slightly different VMs before handing it over to you.\n\nWhat VMs would you like to do next? Remember we should handle the least important ones first.\n\nAlso, when doing this for critical VMs we should backup the images after shutting down the VM, in case something goes wrong.\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>Thank you Marcus for the update.\n\nSince we got the necessary VM's up, can we remove the 7x24 flag from the case and lower to Sev 2?\nWe still need to cleanup the other VM's and we are towards that.\n\nThanks\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Hello,\nWe have a VM (harmonic01.c01.bc.gen.clt1.prod.mlbam.net) that was accidentally rebooted by the application itself. This VM is one of the VM's affected by the snapshot issue. \n\nCan we please have the fix commands to roll back this VM ? \n\nThank you</text>, <text>Please hold off on the last request, i believe this VM is unrelated to this issue and that it was always running on NFS (by accident when first deployed a year ago).</text>, <text>Hello Marcus,\n\nThank you for your reply on the case.\n\nFrom your last update it seems that you are facing issue with one VM (harmonic01.c01.bc.gen.clt1.prod.mlbam.net)\n\nThis VM is accidentally rebooted by application and now there is some snapshot issue,but as you mentioned that its not related to the issue which we are handling in this service request. \n\nI would like to inform you that we need to open a new support case for the new issue so we can keep both issues isolated from each other.\n\nCan you please open a new case from your end and attach fresh-log-collector along with sosreport from the host on which it was running earlier.\n\nLet me know if you have any query.\n\nAwaiting your reply.\n\n\nBest Regards,\n\nVaibhav Pagar\nGSS Red Hat Inc.</text>, <text>Hello,\n\nGreetings from Red Hat Support. My name is Gajanan, I am from the Virtualization team. I was be monitoring this case and assisting you during EMEA business hours.\n\nFrom the last update we have asked you to open a new case for the issue with other VM.  Also let us know whether we can drop the 24x7 support and lower the severity of this case. \n\nRegards,\nGajanan Chakkarwar, RHCE, RHCSA-RHOS,RHCVA\nVirtualization &amp; containerization Team\nGlobal Support Services\nRed Hat India.</text>, <text>Im working on getting 5 low impact VMs we can continue testing with. \n\nPlease go ahead and move this to SEV 2</text>, <text>Hi Marcus,\n\nSounds great, I will lower to sev2, do we still need to leave the 24x7 coverage at the moment or can we remove this for the moment.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>next VMs we can test with are:\n\napi02.c01.razcp.gen.clt1.dev.mlbam.net\nxfer01.c01.archive.gen.clt1.prod.mlbam.net\nxfer02.c01.archive.gen.clt1.prod.mlbam.net</text>, <text>Bryan,\nI would prefer to leave the 24x7 coverage on only because at the moment we're at a point where if any production VM's power off, we would need to contact Red hat Immediately in order to get the recovery commands for the 'fix'</text>, <text>one more VM added to the list of our next vms to test with.. Updated list is...\n\napi02.c01.razcp.gen.clt1.dev.mlbam.net\nxfer01.c01.archive.gen.clt1.prod.mlbam.net\nxfer02.c01.archive.gen.clt1.prod.mlbam.net\naperaproxy01.c01.inf.datg.clt1.prod.mlbam.net</text>, <text>I have more VMs to test with. the updated list is below of all our test hosts. The first  6 VMs are actually to be decommisioned, so we can mess those up all we want. \n\n1. actor01.c01.razcp.gen.clt1.dev.mlbam.net\n2. actor02.c01.razcp.gen.clt1.dev.mlbam.net\n3. api01.c01.razcp.gen.clt1.dev.mlbam.net\n4. api02.c01.razcp.gen.clt1.dev.mlbam.net\n5. kafka01.c01.razcp.gen.clt1.dev.mlbam.net\n6. kafka02.c01.razcp.gen.clt1.dev.mlbam.net\n7. xfer01.c01.archive.gen.clt1.prod.mlbam.net\n8.  xfer02.c01.archive.gen.clt1.prod.mlbam.net</text>, <text>Hi Marcus,\n\nSounds good. I personally am here today through 5pm eastern and am the Case owner so any updates you provide I will be immediately notified. I also will be working this weekend so if you plan to carry this over through to Sat/Sun I will be in the office from 9am-9pm both days. Also if you do not plan any weekend work please let us know so we can adjust the 24x7 coverage accordingly.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Hello Marcus\n\nactor01.c01.razcp.gen.clt1.dev.mlbam.net\n\nThis VM only has 1 snapshot created, so command below should work.\n\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '32993b85-e02d-40e7-919f-0d2cc546646b' AND snapshot_id != '04e5da50-83b1-43b4-837b-7a2c2fb4a062';"\n\nFIXING DISK 0c71427b-37b3-4d39-a3ea-74d3f31150ca, CHAIN ['bc536eb8-43cb-483c-ac67-efbd7a1fe4dd', '9a1f6da8-802f-464b-ba8d-8509ea077625']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/bc536eb8-43cb-483c-ac67-efbd7a1fe4dd -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0c71427b-37b3-4d39-a3ea-74d3f31150ca/9a1f6da8-802f-464b-ba8d-8509ea077625'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0c71427b-37b3-4d39-a3ea-74d3f31150ca/bc536eb8-43cb-483c-ac67-efbd7a1fe4dd.meta"\nlvchange -ay /dev/ovirt-local/bc536eb8-43cb-483c-ac67-efbd7a1fe4dd\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/bc536eb8-43cb-483c-ac67-efbd7a1fe4dd -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0c71427b-37b3-4d39-a3ea-74d3f31150ca/9a1f6da8-802f-464b-ba8d-8509ea077625'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0c71427b-37b3-4d39-a3ea-74d3f31150ca/9a1f6da8-802f-464b-ba8d-8509ea077625.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0c71427b-37b3-4d39-a3ea-74d3f31150ca/9a1f6da8-802f-464b-ba8d-8509ea077625.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0c71427b-37b3-4d39-a3ea-74d3f31150ca/9a1f6da8-802f-464b-ba8d-8509ea077625.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0c71427b-37b3-4d39-a3ea-74d3f31150ca/9a1f6da8-802f-464b-ba8d-8509ea077625.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0c71427b-37b3-4d39-a3ea-74d3f31150ca/9a1f6da8-802f-464b-ba8d-8509ea077625 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0c71427b-37b3-4d39-a3ea-74d3f31150ca/9a1f6da8-802f-464b-ba8d-8509ea077625.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('9a1f6da8-802f-464b-ba8d-8509ea077625');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='04e5da50-83b1-43b4-837b-7a2c2fb4a062' WHERE image_guid ='bc536eb8-43cb-483c-ac67-efbd7a1fe4dd';"\n~~~\n\n\nAlso the 8 VM list you provide all contain the single snapshot created.\nWe are looking for the VM with more the 1 snapshots\n\nFrom the BAD2.txt list, were found the following VM with more the 1 snapshot.\nAre any of them least important?\n\n2 snapshot created.\n~~~\ncbdb01.ingest07.hls.hulu.clt1.prod.mlbam.net\ncbdb01.ingest08.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest01.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest03.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest05.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest01.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest02.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest03.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest05.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest06.hls.hulu.clt1.prod.mlbam.net\njava01.ad01.hls.hulu.clt1.prod.mlbam.net\njetty01.ingest01.ams.gen.clt1.prod.mlbam.net\njetty02.ingest01.ams.gen.clt1.prod.mlbam.net\njetty02.msu01.ams.gen.clt1.prod.mlbam.net\njetty03.worker01.jobmgr.gen.clt1.prod.mlbam.net\nmdtools02.c01.bpa.hulu.clt1.prod.mlbam.net\nmemcached02.c01.lprovis.gen.clt1.qa.mlbam.net\nmf02.c01.web.wwe.clt1.prod.bamtech.co\nmf06.c01.web.gen.clt1.prod.bamtech.co\nmf06.c01.web.wwe.clt1.prod.bamtech.co\nmysql01.core01.lprovis.hulu.clt1.prod.mlbam.net\nnginx01.c01.medtrans.gen.clt1.qa.mlbam.net\nnginx01.ingest01.hls.hulu.clt1.prod.mlbam.net\nnginx01.manifest01.streaming.hulu.clt1.prod.mlbam.net\nnginx01.proxy01.hls.gen.clt1.prod.mlbam.net\nproxy01.c01.infra.gen.clt1.util.mlbam.net\nsmtp01.clt1.util.mlbam.net\nsmtp02.clt1.util.mlbam.net\ntomcat01.c01.sxc.gen.clt1.prod.mlbam.net\ntomcat01.core01.lprovis.hulu.clt1.prod.mlbam.net\ntomcat01.sweeper01.lprovis.hulu.clt1.prod.mlbam.net\n~~~\n\n3 snapshot created.\n~~~\ncbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net\ncbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest02.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest06.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest07.hls.hulu.clt1.prod.mlbam.net\ncbdb03.ingest04.hls.hulu.clt1.prod.mlbam.net\n~~~\n\nThanks \nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Let me check on that and ill get back to you thanks. I ll try and get some of these hosts powered-off with permission from the owners\n\n\n\n\nOn 11/17/17, 1:28 PM, "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>We are free to work on the following 2-snapshot VMs....\n\n\nmf02.c01.web.wwe.clt1.prod.bamtech.co\nmf06.c01.web.gen.clt1.prod.bamtech.co\nmf06.c01.web.wwe.clt1.prod.bamtech.co</text>, <text>Hi Marcus,\n\nThanks for the list.\nWere you able to get actor01.c01.razcp.gen.clt1.dev.mlbam.net up?\n\n- Bimal.</text>, <text>update: actor01.c01.razcp.gen.clt1.dev.mlbam.net is fixed OK and running well. Thanks!</text>, <text>Hi Marcus,\n\nThank you for the update, I will pass this with Bimal.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Marcus, \n\nBelow are commands for the following VMs;\n\n~~~\n2. actor02.c01.razcp.gen.clt1.dev.mlbam.net\n3. api01.c01.razcp.gen.clt1.dev.mlbam.net\n4. api02.c01.razcp.gen.clt1.dev.mlbam.net\n5. kafka01.c01.razcp.gen.clt1.dev.mlbam.net\n6. kafka02.c01.razcp.gen.clt1.dev.mlbam.net\n~~~\n\nPlease for *** PLEASE SHUT THE VM DOWN **** before running the command.\nAlso please run the commands one at time and start the VM and check its health before moving to fix the next VM.\n\nAlso this case is marked requiring 7x24 support, do you wish to continue receiving 7x24 support over the weekend or resume back on Monday?\n\nLet me know if you have any questions.\n\nThanks and best regards\nBimal Chollera\nRed Hat, Inc.\nGSS\n\n\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n# ./mlb.py --vm-name actor02.c01.razcp.gen.clt1.dev.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '82572a0b-9bc9-48f1-ba1a-b7486db041f0' AND snapshot_id != '872b1164-904d-4ef9-80f8-93057355be93';"\n\nFIXING DISK 94764e43-52d0-4cc9-96b5-1f6c7b7cd07e, CHAIN ['37adbce7-68c6-4cc1-a119-96d48d727261', '9942d944-adbe-49ab-9078-3eb283e5ed3d']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/37adbce7-68c6-4cc1-a119-96d48d727261 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/94764e43-52d0-4cc9-96b5-1f6c7b7cd07e/9942d944-adbe-49ab-9078-3eb283e5ed3d'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/94764e43-52d0-4cc9-96b5-1f6c7b7cd07e/37adbce7-68c6-4cc1-a119-96d48d727261.meta"\nlvchange -ay /dev/ovirt-local/37adbce7-68c6-4cc1-a119-96d48d727261\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/37adbce7-68c6-4cc1-a119-96d48d727261 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/94764e43-52d0-4cc9-96b5-1f6c7b7cd07e/9942d944-adbe-49ab-9078-3eb283e5ed3d'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/94764e43-52d0-4cc9-96b5-1f6c7b7cd07e/9942d944-adbe-49ab-9078-3eb283e5ed3d.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/94764e43-52d0-4cc9-96b5-1f6c7b7cd07e/9942d944-adbe-49ab-9078-3eb283e5ed3d.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/94764e43-52d0-4cc9-96b5-1f6c7b7cd07e/9942d944-adbe-49ab-9078-3eb283e5ed3d.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/94764e43-52d0-4cc9-96b5-1f6c7b7cd07e/9942d944-adbe-49ab-9078-3eb283e5ed3d.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/94764e43-52d0-4cc9-96b5-1f6c7b7cd07e/9942d944-adbe-49ab-9078-3eb283e5ed3d /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/94764e43-52d0-4cc9-96b5-1f6c7b7cd07e/9942d944-adbe-49ab-9078-3eb283e5ed3d.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('9942d944-adbe-49ab-9078-3eb283e5ed3d');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='872b1164-904d-4ef9-80f8-93057355be93' WHERE image_guid ='37adbce7-68c6-4cc1-a119-96d48d727261';"\n~~~\n\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n# ./mlb.py --vm-name api01.c01.razcp.gen.clt1.dev.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '8bb9a740-8b1d-41f3-88ee-08f537bb7bc5' AND snapshot_id != '22278dd7-fb16-4892-9052-7279c632d0ab';"\n\nFIXING DISK 615557c4-6145-467a-8f4a-ad467c7987e4, CHAIN ['e15c1a30-bb38-4c2a-b6fd-8c3c17568dc6', '9b616e44-d5cc-403b-9ef0-9fc2e038c58a']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/e15c1a30-bb38-4c2a-b6fd-8c3c17568dc6 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/615557c4-6145-467a-8f4a-ad467c7987e4/9b616e44-d5cc-403b-9ef0-9fc2e038c58a'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/615557c4-6145-467a-8f4a-ad467c7987e4/e15c1a30-bb38-4c2a-b6fd-8c3c17568dc6.meta"\nlvchange -ay /dev/ovirt-local/e15c1a30-bb38-4c2a-b6fd-8c3c17568dc6\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/e15c1a30-bb38-4c2a-b6fd-8c3c17568dc6 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/615557c4-6145-467a-8f4a-ad467c7987e4/9b616e44-d5cc-403b-9ef0-9fc2e038c58a'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/615557c4-6145-467a-8f4a-ad467c7987e4/9b616e44-d5cc-403b-9ef0-9fc2e038c58a.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/615557c4-6145-467a-8f4a-ad467c7987e4/9b616e44-d5cc-403b-9ef0-9fc2e038c58a.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/615557c4-6145-467a-8f4a-ad467c7987e4/9b616e44-d5cc-403b-9ef0-9fc2e038c58a.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/615557c4-6145-467a-8f4a-ad467c7987e4/9b616e44-d5cc-403b-9ef0-9fc2e038c58a.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/615557c4-6145-467a-8f4a-ad467c7987e4/9b616e44-d5cc-403b-9ef0-9fc2e038c58a /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/615557c4-6145-467a-8f4a-ad467c7987e4/9b616e44-d5cc-403b-9ef0-9fc2e038c58a.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('9b616e44-d5cc-403b-9ef0-9fc2e038c58a');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='22278dd7-fb16-4892-9052-7279c632d0ab' WHERE image_guid ='e15c1a30-bb38-4c2a-b6fd-8c3c17568dc6';"\n[root@dbview ~]# \n~~~\n\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n# ./mlb.py --vm-name api02.c01.razcp.gen.clt1.dev.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '970bdfff-170d-40c0-9690-cf8ea17c52eb' AND snapshot_id != 'fb926c72-1c75-4891-8d69-888abd8b9947';"\n\nFIXING DISK 2dc8dd85-d4e4-4123-831b-cfd39668175e, CHAIN ['c9db64b0-8f44-420a-a134-c5fa19a5edfd', '6d474ac6-aa26-41b7-94b3-4675a0472017']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c9db64b0-8f44-420a-a134-c5fa19a5edfd -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2dc8dd85-d4e4-4123-831b-cfd39668175e/6d474ac6-aa26-41b7-94b3-4675a0472017'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2dc8dd85-d4e4-4123-831b-cfd39668175e/c9db64b0-8f44-420a-a134-c5fa19a5edfd.meta"\nlvchange -ay /dev/ovirt-local/c9db64b0-8f44-420a-a134-c5fa19a5edfd\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c9db64b0-8f44-420a-a134-c5fa19a5edfd -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2dc8dd85-d4e4-4123-831b-cfd39668175e/6d474ac6-aa26-41b7-94b3-4675a0472017'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2dc8dd85-d4e4-4123-831b-cfd39668175e/6d474ac6-aa26-41b7-94b3-4675a0472017.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2dc8dd85-d4e4-4123-831b-cfd39668175e/6d474ac6-aa26-41b7-94b3-4675a0472017.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2dc8dd85-d4e4-4123-831b-cfd39668175e/6d474ac6-aa26-41b7-94b3-4675a0472017.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2dc8dd85-d4e4-4123-831b-cfd39668175e/6d474ac6-aa26-41b7-94b3-4675a0472017.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2dc8dd85-d4e4-4123-831b-cfd39668175e/6d474ac6-aa26-41b7-94b3-4675a0472017 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2dc8dd85-d4e4-4123-831b-cfd39668175e/6d474ac6-aa26-41b7-94b3-4675a0472017.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('6d474ac6-aa26-41b7-94b3-4675a0472017');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='fb926c72-1c75-4891-8d69-888abd8b9947' WHERE image_guid ='c9db64b0-8f44-420a-a134-c5fa19a5edfd';"\n[root@dbview ~]# \n~~~\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n# ./mlb.py --vm-name kafka01.c01.razcp.gen.clt1.dev.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '946f5b71-2b6c-4e79-90d1-d4182a86885f' AND snapshot_id != 'f848f446-16ed-451b-80b3-e5a979e2a207';"\n\nFIXING DISK d2b03ff0-9603-4076-9e84-08a566b2cf52, CHAIN ['4a8160ec-2c00-4443-a5d1-2b708dc8493a', 'b1dcf513-7999-426b-bd00-74ea3bc6a1b0']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/4a8160ec-2c00-4443-a5d1-2b708dc8493a -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d2b03ff0-9603-4076-9e84-08a566b2cf52/b1dcf513-7999-426b-bd00-74ea3bc6a1b0'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d2b03ff0-9603-4076-9e84-08a566b2cf52/4a8160ec-2c00-4443-a5d1-2b708dc8493a.meta"\nlvchange -ay /dev/ovirt-local/4a8160ec-2c00-4443-a5d1-2b708dc8493a\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/4a8160ec-2c00-4443-a5d1-2b708dc8493a -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d2b03ff0-9603-4076-9e84-08a566b2cf52/b1dcf513-7999-426b-bd00-74ea3bc6a1b0'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d2b03ff0-9603-4076-9e84-08a566b2cf52/b1dcf513-7999-426b-bd00-74ea3bc6a1b0.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d2b03ff0-9603-4076-9e84-08a566b2cf52/b1dcf513-7999-426b-bd00-74ea3bc6a1b0.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d2b03ff0-9603-4076-9e84-08a566b2cf52/b1dcf513-7999-426b-bd00-74ea3bc6a1b0.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d2b03ff0-9603-4076-9e84-08a566b2cf52/b1dcf513-7999-426b-bd00-74ea3bc6a1b0.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d2b03ff0-9603-4076-9e84-08a566b2cf52/b1dcf513-7999-426b-bd00-74ea3bc6a1b0 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d2b03ff0-9603-4076-9e84-08a566b2cf52/b1dcf513-7999-426b-bd00-74ea3bc6a1b0.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('b1dcf513-7999-426b-bd00-74ea3bc6a1b0');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='f848f446-16ed-451b-80b3-e5a979e2a207' WHERE image_guid ='4a8160ec-2c00-4443-a5d1-2b708dc8493a';"\n[root@dbview ~]# \n~~~\n\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n# ./mlb.py --vm-name kafka02.c01.razcp.gen.clt1.dev.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'e1eff785-257a-4fa3-a08d-8ec94bc46e4f' AND snapshot_id != '1b7715e0-7292-47a9-bd51-6e5d27d22677';"\n\nFIXING DISK b421a6c9-79ed-4245-b7ee-948441c9445e, CHAIN ['14ef9a41-b6a9-4917-912f-8c7ac5acae06', 'c75d97fe-915f-443e-9373-5e63699b8e6e']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/14ef9a41-b6a9-4917-912f-8c7ac5acae06 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b421a6c9-79ed-4245-b7ee-948441c9445e/c75d97fe-915f-443e-9373-5e63699b8e6e'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b421a6c9-79ed-4245-b7ee-948441c9445e/14ef9a41-b6a9-4917-912f-8c7ac5acae06.meta"\nlvchange -ay /dev/ovirt-local/14ef9a41-b6a9-4917-912f-8c7ac5acae06\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/14ef9a41-b6a9-4917-912f-8c7ac5acae06 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b421a6c9-79ed-4245-b7ee-948441c9445e/c75d97fe-915f-443e-9373-5e63699b8e6e'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b421a6c9-79ed-4245-b7ee-948441c9445e/c75d97fe-915f-443e-9373-5e63699b8e6e.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b421a6c9-79ed-4245-b7ee-948441c9445e/c75d97fe-915f-443e-9373-5e63699b8e6e.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b421a6c9-79ed-4245-b7ee-948441c9445e/c75d97fe-915f-443e-9373-5e63699b8e6e.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b421a6c9-79ed-4245-b7ee-948441c9445e/c75d97fe-915f-443e-9373-5e63699b8e6e.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b421a6c9-79ed-4245-b7ee-948441c9445e/c75d97fe-915f-443e-9373-5e63699b8e6e /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b421a6c9-79ed-4245-b7ee-948441c9445e/c75d97fe-915f-443e-9373-5e63699b8e6e.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('c75d97fe-915f-443e-9373-5e63699b8e6e');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1b7715e0-7292-47a9-bd51-6e5d27d22677' WHERE image_guid ='14ef9a41-b6a9-4917-912f-8c7ac5acae06';"\n~~~</text>, <text>All the VM's are up but there over 100 VM's impacted by this problem.\n\nHave provided commands to fix some of the VM's in comment 125.\n\nBelow are for the 2 VM's.\n\nPlease  *** PLEASE SHUT THE VM DOWN **** before running the command.\nAlso please run the commands one at time and start the VM and check its health before moving to fix the next VM.\n\n~~~\n\n*** PLEASE SHUT THE VM DOWN ****\n\n# ./mlb.py --vm-name xfer01.c01.archive.gen.clt1.prod.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '5d675133-6c5b-441d-b674-a2e75f4652fc' AND snapshot_id != '2f2e0c01-a22a-420c-9b7d-d0d2695a4e8c';"\n\nFIXING DISK ea2eb323-28c3-4b0d-a8f5-a7121e8eb356, CHAIN ['47602084-ba6a-4fa6-b57c-8e2d84f3868c', '0c856fe5-9f9c-4a76-98fa-ce60d64bad4a']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/47602084-ba6a-4fa6-b57c-8e2d84f3868c -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/47602084-ba6a-4fa6-b57c-8e2d84f3868c.meta"\nlvchange -ay /dev/ovirt-local/47602084-ba6a-4fa6-b57c-8e2d84f3868c\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/47602084-ba6a-4fa6-b57c-8e2d84f3868c -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0c856fe5-9f9c-4a76-98fa-ce60d64bad4a');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='2f2e0c01-a22a-420c-9b7d-d0d2695a4e8c' WHERE image_guid ='47602084-ba6a-4fa6-b57c-8e2d84f3868c';"\n~~~\n\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n\n# ./mlb.py --vm-name xfer02.c01.archive.gen.clt1.prod.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '534cd202-9aaa-44d5-90ab-28ea7b27a89f' AND snapshot_id != 'd9895c9f-f4c4-4c0e-9588-d9732a9be102';"\n\nFIXING DISK b0e6436e-36c2-412a-b664-ff7e4e6369a8, CHAIN ['eae65ee2-7828-485b-aa2d-49089c7593cd', 'be781fc6-af4d-4a83-bfa0-78264eb7ed20']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/eae65ee2-7828-485b-aa2d-49089c7593cd -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/eae65ee2-7828-485b-aa2d-49089c7593cd.meta"\nlvchange -ay /dev/ovirt-local/eae65ee2-7828-485b-aa2d-49089c7593cd\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/eae65ee2-7828-485b-aa2d-49089c7593cd -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('be781fc6-af4d-4a83-bfa0-78264eb7ed20');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d9895c9f-f4c4-4c0e-9588-d9732a9be102' WHERE image_guid ='eae65ee2-7828-485b-aa2d-49089c7593cd';"\n~~~</text>, <text>Marcus,\n\nAs a precaution, can you take back up the engine as we are updating the RHV-M DB.\n\nBelow is the command, you will need to specify the &lt;backup-location&gt; dir path.\n\n~~~\n/usr/bin/engine-backup --mode=backup --scope=all --file="/&lt;backup-location&gt;/engine-backup-$(date +%Y%m%d%H%M%S).tar.bz2" --log=/var/log/ovirt-engine-backups.log)\n~~~\n\nThanks \nBimal</text>, <text>Thank Bimal.\n\nWe have daily RHEVM backups for all our environments via Tower/Ansible. I can kick off additional backups before running through these VMs.\n\nAs for your 24x7 question, i'd like to keep the 24x7 flag on just in case any production VMs happen to offline during the weekend, but I dont anticipate any more queries this weekend to you guys. It's more of just a precaution in case we put ourselves in a spot that we can't boot a production VM. but we've alreadytold all our teams NOT to purposely reboot/poweroff any VMs in that environment.  have a good weekend.</text>, <text>Hi Marcus,\n\nSounds good, please let us know the outcome on the commands provided earlier the fix the following VM's.\n\n~~~\n2. actor02.c01.razcp.gen.clt1.dev.mlbam.net\n3. api01.c01.razcp.gen.clt1.dev.mlbam.net\n4. api02.c01.razcp.gen.clt1.dev.mlbam.net\n5. kafka01.c01.razcp.gen.clt1.dev.mlbam.net\n6. kafka02.c01.razcp.gen.clt1.dev.mlbam.net\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc\nGSS</text>, <text>Hello,\n\nMy name is Chetan. I am member of virtualization team.\n\nI was monitoring and assisting you over this case during EMEA business hours today. \t`\n\nAs there is no response since last 12 hours, I am dropping 24x7 support on this case. This means the case will be updated during business hours for your region's support location.\n\nIf you want 24x7 support enabled on this case, please feel free to  give us a call on the numbers listed on our contact page or update the case with latest requested information .\n\n  &gt;&gt; Refer : https://access.redhat.com/support/contact/technicalSupport/\n\nFeel free to reach us if you have any queries.\n\nRegards,\nChetan Nagarkar,\nRed Hat Global Support Services</text>, <text>Chetan,\n\nMarcus specifically requested that 24x7 NOT be removed from this case. Your doing so is unauthorized. As it happens, we need assistance with a PRODUCTION VM which kernel panic'd, went down, and now is unable to be booted due to the issue described in this case:\n\ncbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net\n\nPlease restore 24x7 support to this case. \n\nThanks,\n\nImran Merali\nDirector, Core Infrastructure\nBAMTECH Media</text>, <text>Request Management Escalation: We've had a production VM become unavailable over the weekend and our on-call requires assistance getting the VM back online. The VM in question is cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net. It is also worth noting that in spite of Marcus having asked that 24x7 support be kept through the weekend, it was dropped by one of your engineers because someone hadn't responded in 12 hours. Please restore it. Thanks.</text>, <text>Hello,\n\nMy name is Martin Klika and I am Manager EMEA Suppor team, working as Duty Manager this weekend.\n\nLet me apologize for inconvenience. I've restored the 24x7 handling and I'll have Virtualization Engineer with you shortly\n\n\nWith Kind Regards\n\nMartin Klika\nManager - Technical Support\nCustomer Experience and Engagement</text>, <text>Hello,\n\nSincere apologies on behalf of my collegue who dropped the 24x7 support on the case, he must have missed to see the note about the same. \n\nRegrading the issue you are facing I will be involving senior team member to check the issue but to proceed with the same can you provide us with the fresh logs from the system: \n\n1) Provide us with the fresh logs from the rhevm machine along with rhev db dump, SPM host sosreport and the host on which you tried to start the affected VM: \n\nCollect the logs with Ovirt Log Collector command which should be run as root user on the rhevm machine. Provide the host address or hostname of the hypervisor to gather data from it.\n\n# ovirt-log-collector  --hosts=&lt;SPM-host-ip&gt;,&lt;Other-host-ip&gt;\n\n2) Also collect a screenshot of the console of the VM which is unable to Boot, we need to check what is logged there or what errors are reported there.  \n\nRegards,\nGajanan Chakkarwar, RHCE, RHCSA-RHOS,RHCVA\nVirtualization &amp; containerization Team\nGlobal Support Services\nRed Hat India.</text>, <text>// FTS - 24x7 HANDOVER NOTE // \n\n///// DO NOT DROP 24x7 For this case without customer consent! //////\n\nCurrent Issue: Customer needs assistance with a PRODUCTION VM : "cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net"  which kernel panic'd, went down, and now is unable to be booted due to the issue described in this case. \n\nAction Plan: Asked for latest logs from the environment to check the issue further.</text>, <text>Attaching SOS Report and snapshot.</text>, <text>I will go through the logs. Is this a VM that has not yet been fixed following the steps Bimal has been providing? Was it shutdown or did it panic while running?\n\nRegards,\n\nFrank</text>, <text>Hi Frank,\n\nThis VM has not been fixed yet using the steps Bimal had been providing. From what our ops crew told me, it kernel panic'd while running. Our ops team subsequently decided to shut it down and try and boot it again without success, and that's where we are now.\n\nMy understanding is that Bimal's steps are still in somewhat of a testing phase, and that there's a script that's being worked on to generate the remediation steps per VM (extracting the proper Disk UUID's, cobbling together the commands, etc). \n\nThanks for taking a look. \n\nRegards,\n\nImran</text>, <text>WIP:\n\n# sh vm40p.sh cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net\n \nvm_static:                                                                                                              (0 = ok, 1 = locked)\n               vm_guid                |                   vm_name                    | entity_type | os | description |              cluster_id  \n            | template_status |               vmt_guid               |         original_template_id         \n--------------------------------------+----------------------------------------------+-------------+----+-------------+--------------------------\n------------+-----------------+--------------------------------------+--------------------------------------\n ce42843f-48da-4a18-a6f8-4c2b7fd07718 | cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net | VM          | 24 |             | 6583532b-c88b-42ec-8c64-4\n8437e657bee |                 | 82fcd601-e004-447a-b741-521dcb6c8a71 | 82fcd601-e004-447a-b741-521dcb6c8a71\n(1 row)\n\n                                                                                       *** Cluster = Compute-LocalDisk  ;  DC = CLT1-LD ***\n \n                                                                                                                                              Template =  CentOS-7.2-Baseline-CLT1\n \n                                                                                                                                              Original Template =  CentOS-7.2-Baseline-CLT1\n \n \nvm_dynamic:                           (0 = down, 1 = up, 4 = paused, 5/6 = migrating, 7 = unknown, 8 = notresp, 9 = wfl, 15 = locked)\n               vm_guid                | status | run_on_vds | guest_os \n--------------------------------------+--------+------------+----------\n ce42843f-48da-4a18-a6f8-4c2b7fd07718 |      0 |            | \n(1 row)\n\n \nvm_device:\n              device_id               |                vm_id                 | boot_order |         _create_date          |         _update_date \n         |                           address                            \n--------------------------------------+--------------------------------------+------------+-------------------------------+----------------------\n---------+--------------------------------------------------------------\n ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0 | ce42843f-48da-4a18-a6f8-4c2b7fd07718 |          0 | 2017-02-22 11:01:35.252949+00 | 2017-11-19 10:45:28.0\n59981+00 | {function=0x0, bus=0x00, domain=0x0000, type=pci, slot=0x08}\n db06154a-1191-45ac-9a79-0f03d5131ec5 | ce42843f-48da-4a18-a6f8-4c2b7fd07718 |          1 | 2017-02-15 10:13:03.927285+00 | 2017-11-19 10:45:28.0\n59981+00 | {function=0x0, bus=0x00, domain=0x0000, type=pci, slot=0x07}\n 517749d7-0e48-48cf-be9c-1c3480b6b87a | ce42843f-48da-4a18-a6f8-4c2b7fd07718 |          0 | 2017-11-07 18:17:51.146585+00 | 2017-11-19 10:45:28.0\n59981+00 | {function=0x0, bus=0x00, domain=0x0000, type=pci, slot=0x09}\n(3 rows)\n\n \nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid     \n          | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+----------------------------\n----------+-------------+------------------------+-------------+---------------+--------\n c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 | ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0 | fe1b6775-1e1e-41a3-865b-be3cb35592c0 | 00000000-0000-0000-0000-000\n000000000 |           1 | 2017-02-22 11:01:37+00 |           2 |             5 | f\n ec5581c6-a7b1-432c-8192-eb3002587285 | ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0 | a35549c2-e9d1-4c98-b367-17dffd3d51c0 | c5f7dcf4-a6e3-4995-b6e0-048\n1e7203d19 |           1 | 2017-11-12 08:23:00+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 1536\n    For image_guid = ec5581c6-a7b1-432c-8192-eb3002587285 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 53930182144\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid     \n          | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+----------------------------\n----------+-------------+------------------------+-------------+---------------+--------\n c1180dc5-4c63-4b8c-b337-714988d0aefc | db06154a-1191-45ac-9a79-0f03d5131ec5 | fe1b6775-1e1e-41a3-865b-be3cb35592c0 | fa048414-a3e0-44cf-818d-426\n88a995004 |           1 | 2017-02-15 10:13:03+00 |           2 |             4 | f\n cec2e225-84a8-4f37-aa21-4b412a5a66ec | db06154a-1191-45ac-9a79-0f03d5131ec5 | a35549c2-e9d1-4c98-b367-17dffd3d51c0 | c1180dc5-4c63-4b8c-b337-714\n988d0aefc |           1 | 2017-11-12 08:22:57+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = c1180dc5-4c63-4b8c-b337-714988d0aefc , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n    For image_guid = cec2e225-84a8-4f37-aa21-4b412a5a66ec , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 1365034496\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid     \n          | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+----------------------------\n----------+-------------+------------------------+-------------+---------------+--------\n f7013031-8f4a-43b4-aef2-918c839676a3 | 517749d7-0e48-48cf-be9c-1c3480b6b87a | fe1b6775-1e1e-41a3-865b-be3cb35592c0 | 00000000-0000-0000-0000-000\n000000000 |           1 | 2017-11-07 18:17:50+00 |           2 |             5 | f\n bbe651c0-f080-45c2-92b0-082b14cfc6ce | 517749d7-0e48-48cf-be9c-1c3480b6b87a | a35549c2-e9d1-4c98-b367-17dffd3d51c0 | f7013031-8f4a-43b4-aef2-918\nc839676a3 |           1 | 2017-11-12 08:22:58+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = f7013031-8f4a-43b4-aef2-918c839676a3 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 35343230464\n    For image_guid = bbe651c0-f080-45c2-92b0-082b14cfc6ce , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 53794235904\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       crea\ntion_date        \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+-----------\n-----------------\n a35549c2-e9d1-4c98-b367-17dffd3d51c0 | ce42843f-48da-4a18-a6f8-4c2b7fd07718 | ACTIVE        | OK     | Active VM                    | 2017-02-15\n 10:13:02.939+00\n fe1b6775-1e1e-41a3-865b-be3cb35592c0 | ce42843f-48da-4a18-a6f8-4c2b7fd07718 | REGULAR       | OK     | Backup__2017-11-12__08.22.50 | 2017-11-12\n 08:23:00.377+00\n(2 rows)\n\n \nbase_disks:\n               disk_id                | wipe_after_delete | propagate_errors |                     disk_alias                     | disk_descript\nion | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+----------------------------------------------------+--------------\n----+-----------+------+-----------+---------------------+-------------------+--------------------\n ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0 | f                 | Off              | cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net_Disk1 |              \n    | f         |      |         0 |                     |                 0 | \n(1 row)\n\n               disk_id                | wipe_after_delete | propagate_errors | disk_alias | disk_description | shareable | sgio | alignment | las\nt_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+------------+------------------+-----------+------+-----------+----\n-----------------+-------------------+--------------------\n db06154a-1191-45ac-9a79-0f03d5131ec5 | f                 | Off              | rootdisk   |                  | f         |      |         0 |    \n                 |                 0 | \n(1 row)\n\n               disk_id                | wipe_after_delete | propagate_errors |                     disk_alias                     | disk_descript\nion | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+----------------------------------------------------+--------------\n----+-----------+------+-----------+---------------------+-------------------+--------------------\n 517749d7-0e48-48cf-be9c-1c3480b6b87a | f                 | Off              | cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net_Disk2 |              \n    | f         |      |         0 |                     |                 0 | \n(1 row)\n\n \ndisk_vm_element:\n               disk_id                |                vm_id                 | is_boot | disk_interface \n--------------------------------------+--------------------------------------+---------+----------------\n ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0 | ce42843f-48da-4a18-a6f8-4c2b7fd07718 | f       | VirtIO\n(1 row)\n\n               disk_id                |                vm_id                 | is_boot | disk_interface \n--------------------------------------+--------------------------------------+---------+----------------\n db06154a-1191-45ac-9a79-0f03d5131ec5 | ce42843f-48da-4a18-a6f8-4c2b7fd07718 | t       | VirtIO\n(1 row)\n\n               disk_id                |                vm_id                 | is_boot | disk_interface \n--------------------------------------+--------------------------------------+---------+----------------\n 517749d7-0e48-48cf-be9c-1c3480b6b87a | ce42843f-48da-4a18-a6f8-4c2b7fd07718 | f       | VirtIO\n(1 row)</text>, <text>I believe this will fix the VM but I really want to verify this with Bimal as I have not used the MLB script previously.\n\n[root@dbview ~]# ./mlb.py --vm-name  cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ce42843f-48da-4a18-a6f8-4c2b7fd07718' AND snapshot_id != 'a35549c2-e9d1-4c98-b367-17dffd3d51c0';"\n\nFIXING DISK 517749d7-0e48-48cf-be9c-1c3480b6b87a, CHAIN ['f7013031-8f4a-43b4-aef2-918c839676a3', 'bbe651c0-f080-45c2-92b0-082b14cfc6ce']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f7013031-8f4a-43b4-aef2-918c839676a3 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/f7013031-8f4a-43b4-aef2-918c839676a3.meta"\nlvchange -ay /dev/ovirt-local/f7013031-8f4a-43b4-aef2-918c839676a3\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f7013031-8f4a-43b4-aef2-918c839676a3 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('bbe651c0-f080-45c2-92b0-082b14cfc6ce');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a35549c2-e9d1-4c98-b367-17dffd3d51c0' WHERE image_guid ='f7013031-8f4a-43b4-aef2-918c839676a3';"\n\nFIXING DISK db06154a-1191-45ac-9a79-0f03d5131ec5, CHAIN ['c1180dc5-4c63-4b8c-b337-714988d0aefc', 'cec2e225-84a8-4f37-aa21-4b412a5a66ec']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/c1180dc5-4c63-4b8c-b337-714988d0aefc.meta"\nlvchange -ay /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('cec2e225-84a8-4f37-aa21-4b412a5a66ec');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a35549c2-e9d1-4c98-b367-17dffd3d51c0' WHERE image_guid ='c1180dc5-4c63-4b8c-b337-714988d0aefc';"\n\nFIXING DISK ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0, CHAIN ['c5f7dcf4-a6e3-4995-b6e0-0481e7203d19', 'ec5581c6-a7b1-432c-8192-eb3002587285']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19.meta"\nlvchange -ay /dev/ovirt-local/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('ec5581c6-a7b1-432c-8192-eb3002587285');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a35549c2-e9d1-4c98-b367-17dffd3d51c0' WHERE image_guid ='c5f7dcf4-a6e3-4995-b6e0-0481e7203d19';"</text>, <text>INVESTIGATION: \n~~~~~~~~~~~~~~\n// Working Notes - these notes are not intended as a meaningful communication\n// but rather an indicator of current thought processes and reference.\n// Please feel free to comment and ask questions concerning them.\n\n\n[I1]\tThis VM has 3 disks and single snapshot\n\n~~~\nvm_device:\n              device_id               |                vm_id                 | boot_order |         _create_date          |         _update_date          |                           address                            \n--------------------------------------+--------------------------------------+------------+-------------------------------+-------------------------------+--------------------------------------------------------------\n ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0 | ce42843f-48da-4a18-a6f8-4c2b7fd07718 |          0 | 2017-02-22 11:01:35.252949+00 | 2017-11-19 10:45:28.059981+00 | {function=0x0, bus=0x00, domain=0x0000, type=pci, slot=0x08}\n db06154a-1191-45ac-9a79-0f03d5131ec5 | ce42843f-48da-4a18-a6f8-4c2b7fd07718 |          1 | 2017-02-15 10:13:03.927285+00 | 2017-11-19 10:45:28.059981+00 | {function=0x0, bus=0x00, domain=0x0000, type=pci, slot=0x07}\n 517749d7-0e48-48cf-be9c-1c3480b6b87a | ce42843f-48da-4a18-a6f8-4c2b7fd07718 |          0 | 2017-11-07 18:17:51.146585+00 | 2017-11-19 10:45:28.059981+00 | {function=0x0, bus=0x00, domain=0x0000, type=pci, slot=0x09}\n(3 rows)\n\nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 | ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0 | fe1b6775-1e1e-41a3-865b-be3cb35592c0 | 00000000-0000-0000-0000-000000000000 |           1 | 2017-02-22 11:01:37+00 |           2 |             5 | f\n ec5581c6-a7b1-432c-8192-eb3002587285 | ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0 | a35549c2-e9d1-4c98-b367-17dffd3d51c0 | c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 |           1 | 2017-11-12 08:23:00+00 |           2 |             4 | t\n(2 rows)\n\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n c1180dc5-4c63-4b8c-b337-714988d0aefc | db06154a-1191-45ac-9a79-0f03d5131ec5 | fe1b6775-1e1e-41a3-865b-be3cb35592c0 | fa048414-a3e0-44cf-818d-42688a995004 |           1 | 2017-02-15 10:13:03+00 |           2 |             4 | f\n cec2e225-84a8-4f37-aa21-4b412a5a66ec | db06154a-1191-45ac-9a79-0f03d5131ec5 | a35549c2-e9d1-4c98-b367-17dffd3d51c0 | c1180dc5-4c63-4b8c-b337-714988d0aefc |           1 | 2017-11-12 08:22:57+00 |           2 |             4 | t\n(2 rows)\n\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n f7013031-8f4a-43b4-aef2-918c839676a3 | 517749d7-0e48-48cf-be9c-1c3480b6b87a | fe1b6775-1e1e-41a3-865b-be3cb35592c0 | 00000000-0000-0000-0000-000000000000 |           1 | 2017-11-07 18:17:50+00 |           2 |             5 | f\n bbe651c0-f080-45c2-92b0-082b14cfc6ce | 517749d7-0e48-48cf-be9c-1c3480b6b87a | a35549c2-e9d1-4c98-b367-17dffd3d51c0 | f7013031-8f4a-43b4-aef2-918c839676a3 |           1 | 2017-11-12 08:22:58+00 |           2 |             4 | t\n(2 rows)\n\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------\n a35549c2-e9d1-4c98-b367-17dffd3d51c0 | ce42843f-48da-4a18-a6f8-4c2b7fd07718 | ACTIVE        | OK     | Active VM                    | 2017-02-15 10:13:02.939+00\n fe1b6775-1e1e-41a3-865b-be3cb35592c0 | ce42843f-48da-4a18-a6f8-4c2b7fd07718 | REGULAR       | OK     | Backup__2017-11-12__08.22.50 | 2017-11-12 08:23:00.377+00\n(2 rows)\n~~~\n\n[I2]\tBase disks on the lvm on the host:\n\n~~~\n  c1180dc5-4c63-4b8c-b337-714988d0aefc ovirt-local    1 -wi-a----- 50.00g  IU_db06154a-1191-45ac-9a79-0f03d5131ec5,PU_00000000-0000-0000-0000-000000000000,VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718\n  c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 ovirt-local    1 -wi-a----- 50.00g  IU_ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0,PU_00000000-0000-0000-0000-000000000000,VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718\n  cec2e225-84a8-4f37-aa21-4b412a5a66ec ovirt-local    1 -wi-a----- 50.00g  IU_db06154a-1191-45ac-9a79-0f03d5131ec5,PU_00000000-0000-0000-0000-000000000000,VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718\n~~~\n\n[I3]\tImages on the SD.\n\n~~~\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0:\ntotal 52668857\n-rw-rw----. 1 vdsm kvm 53687091200 Feb 22  2017 c5f7dcf4-a6e3-4995-b6e0-0481e7203d19\n-rw-rw----. 1 vdsm kvm     1048576 Feb 22  2017 c5f7dcf4-a6e3-4995-b6e0-0481e7203d19.lease\n-rw-r--r--. 1 vdsm kvm         351 Nov 12 08:22 c5f7dcf4-a6e3-4995-b6e0-0481e7203d19.meta\n\n-rw-rw----. 1 vdsm kvm 43032117248 Nov 19 10:45 ec5581c6-a7b1-432c-8192-eb3002587285\n-rw-rw----. 1 vdsm kvm     1048576 Nov 12 08:23 ec5581c6-a7b1-432c-8192-eb3002587285.lease\n-rw-r--r--. 1 vdsm kvm         260 Nov 12 08:23 ec5581c6-a7b1-432c-8192-eb3002587285.meta\n\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5:\ntotal 3697966\n-rw-rw----.   1 vdsm kvm     197632 Feb 15  2017 c1180dc5-4c63-4b8c-b337-714988d0aefc\n-rw-rw----.   1 vdsm kvm    1048576 Feb 15  2017 c1180dc5-4c63-4b8c-b337-714988d0aefc.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 12 08:22 c1180dc5-4c63-4b8c-b337-714988d0aefc.meta\n\n-rw-rw----.   1 vdsm kvm 1089798144 Nov 19 10:44 cec2e225-84a8-4f37-aa21-4b412a5a66ec\n-rw-rw----.   1 vdsm kvm    1048576 Nov 12 08:22 cec2e225-84a8-4f37-aa21-4b412a5a66ec.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 08:22 cec2e225-84a8-4f37-aa21-4b412a5a66ec.meta\n\n-rw-rw----. 416 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 416 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 416 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a:\ntotal 87854641\n-rw-rw----. 1 vdsm kvm 42940891136 Nov 19 10:45 bbe651c0-f080-45c2-92b0-082b14cfc6ce\n-rw-rw----. 1 vdsm kvm     1048576 Nov 12 08:22 bbe651c0-f080-45c2-92b0-082b14cfc6ce.lease\n-rw-r--r--. 1 vdsm kvm         260 Nov 12 08:22 bbe651c0-f080-45c2-92b0-082b14cfc6ce.meta\n\n-rw-rw----. 1 vdsm kvm 53687091200 Nov 12 13:06 f7013031-8f4a-43b4-aef2-918c839676a3\n-rw-rw----. 1 vdsm kvm     1048576 Nov  7 18:17 f7013031-8f4a-43b4-aef2-918c839676a3.lease\n-rw-r--r--. 1 vdsm kvm         351 Nov 12 08:22 f7013031-8f4a-43b4-aef2-918c839676a3.meta\n~~~\n\n[I4]\tVM doesn't appear to be restarted after the Snapshot creation on Nov 12.\n\n~~~\n 2017-11-12 08:23:01.274+00    | 17b96eae       | Snapshot 'Backup__2017-11-12__08.22.50' creation for VM 'cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net' was initiated by arichardson@mlbam.net.\n 2017-11-12 13:06:25.049+00    | 17b96eae       | Snapshot 'Backup__2017-11-12__08.22.50' creation for VM 'cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net' has been completed.\n\n 2017-11-12 13:13:38.376+00    |                | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net has been paused.\n 2017-11-12 13:13:38.395+00    |                | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net has been paused due to no Storage space error.\n 2017-11-12 14:06:29.338+00    | 2619cd54       | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net was resumed by fseifts@mlbam.net (Host: hyp77.clt1.prod.mlbam.net).\n 2017-11-12 14:06:31.193+00    |                | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net has recovered from paused back to up.\n 2017-11-12 14:06:31.211+00    | 2619cd54       | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net started on Host hyp77.clt1.prod.mlbam.net\n 2017-11-14 20:32:39.437+00    |                | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net is not responding.\n 2017-11-14 20:35:27.608+00    |                | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net is not responding.\n 2017-11-19 10:39:47.818+00    |                | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net is not responding.\n 2017-11-19 10:42:19.688+00    |                | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net is not responding.\n\n 2017-11-19 10:44:35.554+00    | 2ca7230b       | User nthrower@mlbam.net initiated console session for VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net\n 2017-11-19 10:44:57.49+00     |                | User nthrower@mlbam.net is connected to VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net.\n 2017-11-19 10:45:12.618+00    |                | User nthrower@mlbam.net got disconnected from VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net.\n\n 2017-11-19 10:45:17.297+00    | 7389dbf6       | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net powered off by nthrower@mlbam.net (Host: hyp77.clt1.prod.mlbam.net).\n 2017-11-19 10:45:17.513+00    |                | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net is down. Exit message: Admin shut down from the engine\n 2017-11-19 10:45:18.717+00    | 760cc216       | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net configuration was updated by system.\n 2017-11-19 10:45:27.853+00    | 52beacba       | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net was started by nthrower@mlbam.net (Host: hyp77.clt1.prod.mlbam.net).\n\n 2017-11-19 10:52:58.706+00    | 5b0ced50       | Failed to power off VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net (Host: hyp77.clt1.prod.mlbam.net, User: nthrower@mlbam.net).\n 2017-11-19 10:55:59.106+00    | 20fbc101       | Failed to power off VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net (Host: hyp77.clt1.prod.mlbam.net, User: nthrower@mlbam.net).\n 2017-11-19 10:59:41.636+00    |                | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net has been paused.\n 2017-11-19 11:02:31.12+00     | 3cc60096       | Failed to resume VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net (Host: hyp77.clt1.prod.mlbam.net, User: inascimento@mlbam.net).\n 2017-11-19 11:05:48.9+00      | 579faf16       | Failed to power off VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net (Host: hyp77.clt1.prod.mlbam.net, User: inascimento@mlbam.net).\n 2017-11-19 11:06:47.107+00    |                | VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net is down with error. Exit message: _path.\n~~~</text>, <text>Hello Marcus / Imran,\n\n\nFirst, sincere apologies on the case being from removed from the 24x7 coverage.\nThe 24x7 has been re-enabled and case notes reflecting not to drop the coverage.\n\nGetting back to the VM.  This VM has 3 disks with a single snapshot. \n\nBelow are the commands that needs to executed on the RHVM host, these are referenced with 'GLOBAL RHVM Commands and "RHV Commands.\nThe commands that need to be executed on the Host are referenced with 'Host Commands'.\n\nAfter the VM starts, please check the health of the VM\n\nPlease let us know if you have any questions.\n\nThanks and best regards.\n\nBimal Chollera\nRed Hat, Inc.\nGSS\n\n\n==================================================================================\n\n# ./mlb.py --vm-name cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ce42843f-48da-4a18-a6f8-4c2b7fd07718' AND snapshot_id != 'a35549c2-e9d1-4c98-b367-17dffd3d51c0';"\n\nFIXING DISK 517749d7-0e48-48cf-be9c-1c3480b6b87a, CHAIN ['f7013031-8f4a-43b4-aef2-918c839676a3', 'bbe651c0-f080-45c2-92b0-082b14cfc6ce']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f7013031-8f4a-43b4-aef2-918c839676a3 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/f7013031-8f4a-43b4-aef2-918c839676a3.meta"\nlvchange -ay /dev/ovirt-local/f7013031-8f4a-43b4-aef2-918c839676a3\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f7013031-8f4a-43b4-aef2-918c839676a3 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('bbe651c0-f080-45c2-92b0-082b14cfc6ce');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a35549c2-e9d1-4c98-b367-17dffd3d51c0' WHERE image_guid ='f7013031-8f4a-43b4-aef2-918c839676a3';"\n\nFIXING DISK db06154a-1191-45ac-9a79-0f03d5131ec5, CHAIN ['c1180dc5-4c63-4b8c-b337-714988d0aefc', 'cec2e225-84a8-4f37-aa21-4b412a5a66ec']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/c1180dc5-4c63-4b8c-b337-714988d0aefc.meta"\nlvchange -ay /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('cec2e225-84a8-4f37-aa21-4b412a5a66ec');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a35549c2-e9d1-4c98-b367-17dffd3d51c0' WHERE image_guid ='c1180dc5-4c63-4b8c-b337-714988d0aefc';"\n\nFIXING DISK ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0, CHAIN ['c5f7dcf4-a6e3-4995-b6e0-0481e7203d19', 'ec5581c6-a7b1-432c-8192-eb3002587285']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19.meta"\nlvchange -ay /dev/ovirt-local/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('ec5581c6-a7b1-432c-8192-eb3002587285');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a35549c2-e9d1-4c98-b367-17dffd3d51c0' WHERE image_guid ='c5f7dcf4-a6e3-4995-b6e0-0481e7203d19';"\n\n==================================================================================\n\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc\nGSS</text>, <text>HI Bimal,\nAre we sure that's the right LV ? the first lvchange commands results in a 'volume not found' error. Here are the volumes on the host...\n\n\n[root@hyp77 ~]# lvs\n  LV                                   VG          Attr       LSize   Pool   Origin                Data%  Meta%  Move Log Cpy%Sync Convert\n  05fa416c-07ba-49de-8ec4-6a98e1036f96 ovirt-local -wi-ao----  21.00g\n  0e7564f2-4a4a-4794-9cba-768261cc8144 ovirt-local -wi-ao----  50.00g\n  470e5160-d1d4-4707-b953-572760b69c70 ovirt-local -wi-a-----  50.00g\n  704a3981-c062-4441-b3bb-5cd40dd93dd8 ovirt-local -wi-ao----  50.00g\n  74808296-db3b-44b7-8a45-2b193f546683 ovirt-local -wi-ao----  50.00g\n  927ed8a5-1698-46d2-a216-414b1c5e475d ovirt-local -wi-ao----  50.00g\n  c1180dc5-4c63-4b8c-b337-714988d0aefc ovirt-local -wi-------  50.00g\n  c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 ovirt-local -wi-------  50.00g\n  cec2e225-84a8-4f37-aa21-4b412a5a66ec ovirt-local -wi-------  50.00g\n  f04df6f6-e2c6-48a3-b777-dde60817a64b ovirt-local -wi-ao----  50.00g\n  pool00                               rhvh_hyp77  twi-aotz-- 202.06g                              3.43   1.35\n  rhvh-4.0-0.20170104.0                rhvh_hyp77  Vwi---tz-k 187.06g pool00 root\n  rhvh-4.0-0.20170104.0+1              rhvh_hyp77  Vwi-aotz-- 187.06g pool00 rhvh-4.0-0.20170104.0 2.70\n  root                                 rhvh_hyp77  Vwi-a-tz-- 187.06g pool00                       2.63\n  swap                                 rhvh_hyp77  -wi-ao----   4.00g\n  var                                  rhvh_hyp77  Vwi-aotz--  15.00g pool00                       10.82</text>, <text>in addition, please note that i've already run the first 2 commands (the rebase and sed)</text>, <text>Hello Marcus,\n\nThis seems to be very different from the other VM's we have fixed.\n\nTo confirm, you only ran the following 4 commands command but the lvchange failed?\n\n~~~\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ce42843f-48da-4a18-a6f8-4c2b7fd07718' AND snapshot_id != 'a35549c2-e9d1-4c98-b367-17dffd3d51c0';"\n\nFIXING DISK 517749d7-0e48-48cf-be9c-1c3480b6b87a, CHAIN ['f7013031-8f4a-43b4-aef2-918c839676a3', 'bbe651c0-f080-45c2-92b0-082b14cfc6ce']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f7013031-8f4a-43b4-aef2-918c839676a3 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/f7013031-8f4a-43b4-aef2-918c839676a3.meta"\n\nlvchange -ay /dev/ovirt-local/f7013031-8f4a-43b4-aef2-918c839676a3\n~~~\n\nFor now hold of running the rest of the commands.\n\n\nBelow are base images on the ovirt-local for this VM\n\n~~~\n  c1180dc5-4c63-4b8c-b337-714988d0aefc ovirt-local    1 -wi-a----- 50.00g  IU_db06154a-1191-45ac-9a79-0f03d5131ec5,PU_00000000-0000-0000-0000-000000000000,VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718\n  cec2e225-84a8-4f37-aa21-4b412a5a66ec ovirt-local    1 -wi-a----- 50.00g  IU_db06154a-1191-45ac-9a79-0f03d5131ec5,PU_00000000-0000-0000-0000-000000000000,VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718\n\n  c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 ovirt-local    1 -wi-a----- 50.00g  IU_ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0,PU_00000000-0000-0000-0000-000000000000,VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718\n~~~\n\nFirst issue, the images any volumes/images (bbe651c0 and f7013031) from cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net_Disk2 are NOT on ovirt-local.\nBut they are on the NFS storage.  This disk was added on Nov 7th and there was a snapshot on the Nov 12th.\n\n~~~\n 2017-11-07 18:17:51.218+00    | 1607863c       | Add-Disk operation of cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net_Disk2 was initiated on VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net by mvelez@mlbam.net.\n 2017-11-07 18:18:09.693+00    | 1607863c       | The disk cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net_Disk2 was successfully added to VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net.\n~~~\n\n~~~\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a:\ntotal 87854641\n-rw-rw----. 1 vdsm kvm 42940891136 Nov 19 10:45 bbe651c0-f080-45c2-92b0-082b14cfc6ce\n-rw-rw----. 1 vdsm kvm     1048576 Nov 12 08:22 bbe651c0-f080-45c2-92b0-082b14cfc6ce.lease\n-rw-r--r--. 1 vdsm kvm         260 Nov 12 08:22 bbe651c0-f080-45c2-92b0-082b14cfc6ce.meta\n\n-rw-rw----. 1 vdsm kvm 53687091200 Nov 12 13:06 f7013031-8f4a-43b4-aef2-918c839676a3\n-rw-rw----. 1 vdsm kvm     1048576 Nov  7 18:17 f7013031-8f4a-43b4-aef2-918c839676a3.lease\n-rw-r--r--. 1 vdsm kvm         351 Nov 12 08:22 f7013031-8f4a-43b4-aef2-918c839676a3.meta\n~~~\n\nThe 'qemu-img rebase' command completed because we are just pointing the backing file /dev/ovirt-local/f7013031-8f4a-43b4-aef2-918c839676a3 to the volume/images /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce that is on the NFS.  The 'sed' command only changes the metadata enty.  We should be able to revert the changes.\nBut what is puzzling is that why weren't the volumes/images of this disk create the lvm image on the ovirt-local.\n  \n\nSecond issue, the first 2 images (c1180dc5 and cec2e225) are from same disk rootdisk (db06154a).\nYou should only have 1 base image for the each disk, like we have seen before.\nWhat seems to be unknown, were both of these volumes/images on the ovirt-local open when the VM was up and why we have 2 images.\n\n\nThe last image (c5f7dcf4) is from disk cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net_Disk1.  This seems to be correct.\n\n\nCan you provide the following for the 2 issues.\n\n~~~\nsu vdsm -s /bin/sh -c 'qemu-img info /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce'\nsu vdsm -s /bin/sh -c 'qemu-img info /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/f7013031-8f4a-43b4-aef2-918c839676a3'\n\nsu vdsm -s /bin/sh -c 'qemu-img info /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc'\nsu vdsm -s /bin/sh -c 'qemu-img info /dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec'\n\nsu vdsm -s /bin/sh -c 'qemu-img map /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc' &gt; /tmp/c1180dc5.out\nsu vdsm -s /bin/sh -c 'qemu-img map /dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec' &gt; /tmp/cec2e225.out\n\n## Attach the /tmp out files to the case\n\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>The  cec2e225-84a8-4f37-aa21-4b412a5a66ec appears to have been created today.\n\n~~~\nThread-137178::DEBUG::2017-11-19 11:06:44,587::commands::86::root::(execCmd) SUCCESS: &lt;err&gt; = 'localdisk-hook: local disk not found, creating logical volume (name=cec2e225-84a8-4f37-aa21-4b412a5a66ec)\\nlocaldisk-hook: creating logical volume (name=cec2e225-84a8-4f37-aa21-4b412a5a66ec, size=53687091200)\\nlocaldisk-hook: copying image /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec to cec2e225-84a8-4f37-aa21-4b412a5a66ec\\nlocaldisk-hook: copy completed in 1267.457 seconds\\n'; &lt;rc&gt; = 0\n\nThread-137178::INFO::2017-11-19 11:06:44,588::hooks::108::root::(_runHooksDir) localdisk-hook: local disk not found, creating logical volume (name=cec2e225-84a8-4f37-aa21-4b412a5a66ec)\nlocaldisk-hook: creating logical volume (name=cec2e225-84a8-4f37-aa21-4b412a5a66ec, size=53687091200)\nlocaldisk-hook: copying image /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec to cec2e225-84a8-4f37-aa21-4b412a5a66ec\n~~~</text>, <text>To clarify, i had not run the global RHVM command yet. I typically run that after the other RHVM commands. \nWe were unaware that one of these disks was always on NFS, it seems the user might have attempted to add the disk hot instead of cold as we need to.\n\nHere's the output you requested.... Please note the failed paths for the last commands.\n\n[root@hyp77 ~]# su vdsm -s /bin/sh -c 'qemu-img info /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce'\nimage: /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce\nfile format: qcow2\nvirtual size: 50G (53687091200 bytes)\ndisk size: 50G\ncluster_size: 65536\nbacking file: /dev/ovirt-local/f7013031-8f4a-43b4-aef2-918c839676a3\nbacking file format: raw\nFormat specific information:\n    compat: 0.10\n    refcount bits: 16\n[root@hyp77 ~]#\n[root@hyp77 ~]# su vdsm -s /bin/sh -c 'qemu-img info /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/f7013031-8f4a-43b4-aef2-918c839676a3'\nimage: /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/f7013031-8f4a-43b4-aef2-918c839676a3\nfile format: raw\nvirtual size: 50G (53687091200 bytes)\ndisk size: 34G\n\n[root@hyp77 ~]# su vdsm -s /bin/sh -c 'qemu-img info /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc'\nqemu-img: Could not open '/dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc': Could not open '/dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc': No such file or directory\n[root@hyp77 ~]# su vdsm -s /bin/sh -c 'qemu-img info /dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec'\nqemu-img: Could not open '/dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec': Could not open '/dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec': No such file or directory\n[root@hyp77 ~]#\n\n\n[root@hyp77 ~]# su vdsm -s /bin/sh -c 'qemu-img map /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc' &gt; /tmp/c1180dc5.out\nqemu-img: Could not open '/dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc': Could not open '/dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc': No such file or directory\n[root@hyp77 ~]# su vdsm -s /bin/sh -c 'qemu-img map /dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec' &gt; /tmp/cec2e225.out\nqemu-img: Could not open '/dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec': Could not open '/dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec': No such file or directory\n[root@hyp77 ~]#</text>, <text>Bimal,\nThis particular instance might not be worth the trouble of recovering since it's a very complicated setup with the accidental disk on NFS.\nWe're OK with deleting this VM entirely and rebuilding. Can you please let me know which LV's in ovirt-local belong to this VM ? I'll go ahead and delete the VM and delete the LV's for it. \n\nThat should be all that's needed to delete a VM correct ?  I'm guessing deleting the VM from the GUI will automatically remove the NFS based disks that might exist for it ?</text>, <text>Hello Marcus,\n\nThanks for the update.\n\nWe can try to salvage this VM with the following commands below.\nI have provided some info on removal of the VM at the end.\n\nThe cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net_Disk1 is ok but the other 2 disk will require some changes to the command.\n\n\n======================================================\nFor rootdisk:\n\nPlan is to quarantine the cec2e225-84a8-4f37-aa21-4b412a5a66ec image on the /dev/ovirt-local\n\n~~~\n  c1180dc5-4c63-4b8c-b337-714988d0aefc ovirt-local    1 -wi-a----- 50.00g  IU_db06154a-1191-45ac-9a79-0f03d5131ec5,PU_00000000-0000-0000-0000-000000000000,VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718\n  cec2e225-84a8-4f37-aa21-4b412a5a66ec ovirt-local    1 -wi-a----- 50.00g  IU_db06154a-1191-45ac-9a79-0f03d5131ec5,PU_00000000-0000-0000-0000-000000000000,VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718\n~~~\n\nOn the Host:\n\n~~~\nlvchange --deltag IU_db06154a-1191-45ac-9a79-0f03d5131ec5 /dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec \nlvchange --deltag PU_00000000-0000-0000-0000-000000000000 /dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec \nlvchange --deltag VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718 /dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec \nlvrename /dev/ovirt-local/cec2e225-84a8-4f37-aa21-4b412a5a66ec /dev/ovirt-local/_temp_cec2e225-84a8-4f37-aa21-4b412a5a66ec\n~~~\n\nNow we will have the base c1180dc5 on the ovirt-local and the cec2e225 on the NFS share.\n\nFIXING DISK db06154a-1191-45ac-9a79-0f03d5131ec5, CHAIN ['c1180dc5-4c63-4b8c-b337-714988d0aefc', 'cec2e225-84a8-4f37-aa21-4b412a5a66ec']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/c1180dc5-4c63-4b8c-b337-714988d0aefc.meta"\nlvchange -ay /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c1180dc5-4c63-4b8c-b337-714988d0aefc -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/cec2e225-84a8-4f37-aa21-4b412a5a66ec.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('cec2e225-84a8-4f37-aa21-4b412a5a66ec');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a35549c2-e9d1-4c98-b367-17dffd3d51c0' WHERE image_guid ='c1180dc5-4c63-4b8c-b337-714988d0aefc';"\n\n======================================================\n\n\n======================================================\nFor cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net_Disk2.\n\nWill need to rebase bbe651c0-f080-45c2-92b0-082b14cfc6ce backing file point to f7013031-8f4a-43b4-aef2-918c839676a3\n\n\nFIXING DISK 517749d7-0e48-48cf-be9c-1c3480b6b87a, CHAIN ['f7013031-8f4a-43b4-aef2-918c839676a3', 'bbe651c0-f080-45c2-92b0-082b14cfc6ce']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/f7013031-8f4a-43b4-aef2-918c839676a3 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce'\n\nsu vdsm -s /bin/sh -c 'qemu-img info /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce'\n## The output should show the backing_file as /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/f7013031-8f4a-43b4-aef2-918c839676a3\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/f7013031-8f4a-43b4-aef2-918c839676a3.meta"\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/f7013031-8f4a-43b4-aef2-918c839676a3 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/bbe651c0-f080-45c2-92b0-082b14cfc6ce.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('bbe651c0-f080-45c2-92b0-082b14cfc6ce');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a35549c2-e9d1-4c98-b367-17dffd3d51c0' WHERE image_guid ='f7013031-8f4a-43b4-aef2-918c839676a3';"\n\n======================================================\n\n======================================================\nFor cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net_Disk1\n\n~~~\nFIXING DISK ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0, CHAIN ['c5f7dcf4-a6e3-4995-b6e0-0481e7203d19', 'ec5581c6-a7b1-432c-8192-eb3002587285']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19.meta"\nlvchange -ay /dev/ovirt-local/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c5f7dcf4-a6e3-4995-b6e0-0481e7203d19 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/ec5581c6-a7b1-432c-8192-eb3002587285.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('ec5581c6-a7b1-432c-8192-eb3002587285');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a35549c2-e9d1-4c98-b367-17dffd3d51c0' WHERE image_guid ='c5f7dcf4-a6e3-4995-b6e0-0481e7203d19';"\n~~~\n\n======================================================\n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ce42843f-48da-4a18-a6f8-4c2b7fd07718' AND snapshot_id != 'a35549c2-e9d1-4c98-b367-17dffd3d51c0';"\n\n======================================================\n\nStart the VM.\n\n\nIf you wish to remove the VM, you can delete the VM via the GUI including the disks.\n\nCheck for any lvm images with:\n\n~~~\nlvm lvs -o +tags |grep VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718\n~~~\n\nYou can use the lvremove to remove the image from the /dev/ovirt-local for the images just for VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>THanks Bimal, i'm running through these commands now.</text>, <text>Hi Bimal,\nIt doesn't seem like i'm able to run the VM after running the commands,  please see screen shot. I think at this point we should just start over with this VM and rebuild ?</text>, <text>That would probably be the quickest and safest path at this point.\n\nRegards,\n\nFrank</text>, <text>If you wish to remove the VM, you can delete the VM via the GUI including the disks.\n\nCheck for any lvm images with:\n\n~~~\nlvm lvs -o +tags |grep VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718\n~~~\n\nYou can use the lvremove to remove the image from the /dev/ovirt-local for the images just for VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718.</text>, <text>Thanks. I\u2019ll move forward with this option then. \n\n\n\n\nOn 11/19/17, 4:12 PM, "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>Please let us know when you want to move forward with the mf* VMs. They all have 2 snapshots and would be a good test as you wanted...\n\nmf02.c01.web.wwe.clt1.prod.bamtech.co\nmf06.c01.web.gen.clt1.prod.bamtech.co\nmf06.c01.web.wwe.clt1.prod.bamtech.co</text>, <text>Hello Marcus,\n\nLet me check the mf* VM's and will revert back on it.\n\nThe cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net from yesterday, were you able to remove it and the lv associated to it?\n\nAlso I had provided commands earlier the fix the following VM's, were you able get them going?\n\n~~~\n2. actor02.c01.razcp.gen.clt1.dev.mlbam.net\n3. api01.c01.razcp.gen.clt1.dev.mlbam.net\n4. api02.c01.razcp.gen.clt1.dev.mlbam.net\n5. kafka01.c01.razcp.gen.clt1.dev.mlbam.net\n6. kafka02.c01.razcp.gen.clt1.dev.mlbam.net\n~~~\n\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc\nGS</text>, <text>Hello Marcus,\n\nFor the mf* VM's can you provide the lvm lvs output to see if there any additional images created for these VM's.\n\n\nhyp78.clt1.prod.mlbam.net (for VM mf02.c01.web.wwe.clt1.prod.bamtech.co).\n\n~~~\nlvm lvs -o +tags |grep VM_c418cd27-c44a-4cf8-9dfb-1ddef4c3d707\n~~~\n\nhyp104.clt1.prod.mlbam.net (for VM mf06.c01.web.gen.clt1.prod.bamtech.co).\n\n~~~\nlvm lvs -o +tags |grep VM_063f28f5-b510-4c33-aaef-606b79426792\n~~~\n\nhyp41.clt1.prod.mlbam.net (for VM mf06.c01.web.wwe.clt1.prod.bamtech.co).\n\n~~~\nlvm lvs -o +tags |grep VM_33a9a88d-cea8-42bb-90e3-dc70d2b0abb1\n~~~\n\nThanks \nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Just wanted to provide an update on the following VMs that you gave me the commands for...\n\n2. actor02.c01.razcp.gen.clt1.dev.mlbam.net\n3. api01.c01.razcp.gen.clt1.dev.mlbam.net\n4. api02.c01.razcp.gen.clt1.dev.mlbam.net\n5. kafka01.c01.razcp.gen.clt1.dev.mlbam.net\n6. kafka02.c01.razcp.gen.clt1.dev.mlbam.net\n\nAll worked OK except api01, the rebase command hung at 0% completed. I ended up killing the shell and leaving the VM offline. That VM will be decommisioned anyways so I'm scrapping it for now. thanks!</text>, <text>For the MF host data you requested.....\n\n[root@hyp78 ~]# lvm lvs -o +tags |grep VM_c418cd27-c44a-4cf8-9dfb-1ddef4c3d707\n  c5d87c3e-8059-4c7d-b48e-a653e34c8d0e ovirt-local -wi-ao----  50.00g                                                                      IU_7cdcc7df-59df-48dd-b453-d85161df41c9,PU_00000000-0000-0000-0000-000000000000,VM_c418cd27-c44a-4cf8-9dfb-1ddef4c3d707\n\n\n\n[root@hyp104 ~]# lvm lvs -o +tags |grep VM_063f28f5-b510-4c33-aaef-606b79426792\n  2e715f5b-e805-4a01-ae5a-02e0a9da6474 ovirt-local -wi-ao----  50.00g                                                                      IU_2c57b84d-3966-4626-9309-fbf6b9d1df5d,PU_00000000-0000-0000-0000-000000000000,VM_063f28f5-b510-4c33-aaef-606b79426792\n\n\n\n[root@hyp41 ~]# lvm lvs -o +tags |grep VM_33a9a88d-cea8-42bb-90e3-dc70d2b0abb1\n  bbc50212-86ec-407f-9b0e-89db0a2fc786 ovirt-local -wi-ao----  50.00g                                                                      IU_c8b33473-7ee5-4179-b708-c5c1bdd9e6ab,PU_00000000-0000-0000-0000-000000000000,VM_33a9a88d-cea8-42bb-90e3-dc70d2b0abb1\n[root@hyp41 ~]#</text>, <text>Hello Marcus,\n\n\nBelow are the commands to address the following VM's.\n\n~~~\nxfer01.c01.archive.gen.clt1.prod.mlbam.net\nxfer02.c01.archive.gen.clt1.prod.mlbam.net\n\nmf02.c01.web.wwe.clt1.prod.bamtech.co\nmf06.c01.web.gen.clt1.prod.bamtech.co\nmf06.c01.web.wwe.clt1.prod.bamtech.co\n~~~\n\nPlease  *** PLEASE SHUT THE VM DOWN **** before running the command.\nAlso please run the commands one at time and start the VM and check its health before moving to fix the next VM.\n\n\nThanks \nBimal Chollera\nRed Hat, Inc.\nGSS\n\n\n~~~\n\n*** PLEASE SHUT THE VM DOWN ****\n\n# ./mlb.py --vm-name xfer01.c01.archive.gen.clt1.prod.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '5d675133-6c5b-441d-b674-a2e75f4652fc' AND snapshot_id != '2f2e0c01-a22a-420c-9b7d-d0d2695a4e8c';"\n\nFIXING DISK ea2eb323-28c3-4b0d-a8f5-a7121e8eb356, CHAIN ['47602084-ba6a-4fa6-b57c-8e2d84f3868c', '0c856fe5-9f9c-4a76-98fa-ce60d64bad4a']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/47602084-ba6a-4fa6-b57c-8e2d84f3868c -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/47602084-ba6a-4fa6-b57c-8e2d84f3868c.meta"\nlvchange -ay /dev/ovirt-local/47602084-ba6a-4fa6-b57c-8e2d84f3868c\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/47602084-ba6a-4fa6-b57c-8e2d84f3868c -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea2eb323-28c3-4b0d-a8f5-a7121e8eb356/0c856fe5-9f9c-4a76-98fa-ce60d64bad4a.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0c856fe5-9f9c-4a76-98fa-ce60d64bad4a');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='2f2e0c01-a22a-420c-9b7d-d0d2695a4e8c' WHERE image_guid ='47602084-ba6a-4fa6-b57c-8e2d84f3868c';"\n~~~\n\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n\n# ./mlb.py --vm-name xfer02.c01.archive.gen.clt1.prod.mlbam.net\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '534cd202-9aaa-44d5-90ab-28ea7b27a89f' AND snapshot_id != 'd9895c9f-f4c4-4c0e-9588-d9732a9be102';"\n\nFIXING DISK b0e6436e-36c2-412a-b664-ff7e4e6369a8, CHAIN ['eae65ee2-7828-485b-aa2d-49089c7593cd', 'be781fc6-af4d-4a83-bfa0-78264eb7ed20']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/eae65ee2-7828-485b-aa2d-49089c7593cd -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/eae65ee2-7828-485b-aa2d-49089c7593cd.meta"\nlvchange -ay /dev/ovirt-local/eae65ee2-7828-485b-aa2d-49089c7593cd\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/eae65ee2-7828-485b-aa2d-49089c7593cd -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/b0e6436e-36c2-412a-b664-ff7e4e6369a8/be781fc6-af4d-4a83-bfa0-78264eb7ed20.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('be781fc6-af4d-4a83-bfa0-78264eb7ed20');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d9895c9f-f4c4-4c0e-9588-d9732a9be102' WHERE image_guid ='eae65ee2-7828-485b-aa2d-49089c7593cd';"\n~~~\n\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n[root@dbview ~]# ./mlb.py --vm-name mf02.c01.web.wwe.clt1.prod.bamtech.co\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'c418cd27-c44a-4cf8-9dfb-1ddef4c3d707' AND snapshot_id != 'e1c105b6-3bda-4b60-9011-340a6c59bf94';"\n\nFIXING DISK 7cdcc7df-59df-48dd-b453-d85161df41c9, CHAIN ['c5d87c3e-8059-4c7d-b48e-a653e34c8d0e', '85a97d23-4db8-49d1-97be-3de61b68f0aa', 'd32942ad-5961-45af-b7dc-0aa476a27070']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/85a97d23-4db8-49d1-97be-3de61b68f0aa -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/d32942ad-5961-45af-b7dc-0aa476a27070'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/d32942ad-5961-45af-b7dc-0aa476a27070.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/d32942ad-5961-45af-b7dc-0aa476a27070.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/d32942ad-5961-45af-b7dc-0aa476a27070.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/d32942ad-5961-45af-b7dc-0aa476a27070.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/d32942ad-5961-45af-b7dc-0aa476a27070 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/d32942ad-5961-45af-b7dc-0aa476a27070.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c5d87c3e-8059-4c7d-b48e-a653e34c8d0e -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/85a97d23-4db8-49d1-97be-3de61b68f0aa'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/c5d87c3e-8059-4c7d-b48e-a653e34c8d0e.meta"\nlvchange -ay /dev/ovirt-local/c5d87c3e-8059-4c7d-b48e-a653e34c8d0e\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c5d87c3e-8059-4c7d-b48e-a653e34c8d0e -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/85a97d23-4db8-49d1-97be-3de61b68f0aa'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/85a97d23-4db8-49d1-97be-3de61b68f0aa.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/85a97d23-4db8-49d1-97be-3de61b68f0aa.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/85a97d23-4db8-49d1-97be-3de61b68f0aa.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/85a97d23-4db8-49d1-97be-3de61b68f0aa.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/85a97d23-4db8-49d1-97be-3de61b68f0aa /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7cdcc7df-59df-48dd-b453-d85161df41c9/85a97d23-4db8-49d1-97be-3de61b68f0aa.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('85a97d23-4db8-49d1-97be-3de61b68f0aa','d32942ad-5961-45af-b7dc-0aa476a27070');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='e1c105b6-3bda-4b60-9011-340a6c59bf94' WHERE image_guid ='c5d87c3e-8059-4c7d-b48e-a653e34c8d0e';"\n~~~\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n[root@dbview ~]# ./mlb.py --vm-name mf06.c01.web.gen.clt1.prod.bamtech.co\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '063f28f5-b510-4c33-aaef-606b79426792' AND snapshot_id != '8b68a0c2-827c-4b15-aab8-30cd210a139b';"\n\nFIXING DISK 2c57b84d-3966-4626-9309-fbf6b9d1df5d, CHAIN ['2e715f5b-e805-4a01-ae5a-02e0a9da6474', '87a10d5d-5105-4858-82d2-3a5a5d21b89a', 'a2dda0a0-255e-4c56-86a1-499d078ace61']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/87a10d5d-5105-4858-82d2-3a5a5d21b89a -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/a2dda0a0-255e-4c56-86a1-499d078ace61'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/a2dda0a0-255e-4c56-86a1-499d078ace61.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/a2dda0a0-255e-4c56-86a1-499d078ace61.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/a2dda0a0-255e-4c56-86a1-499d078ace61.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/a2dda0a0-255e-4c56-86a1-499d078ace61.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/a2dda0a0-255e-4c56-86a1-499d078ace61 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/a2dda0a0-255e-4c56-86a1-499d078ace61.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/2e715f5b-e805-4a01-ae5a-02e0a9da6474 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/87a10d5d-5105-4858-82d2-3a5a5d21b89a'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/2e715f5b-e805-4a01-ae5a-02e0a9da6474.meta"\nlvchange -ay /dev/ovirt-local/2e715f5b-e805-4a01-ae5a-02e0a9da6474\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/2e715f5b-e805-4a01-ae5a-02e0a9da6474 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/87a10d5d-5105-4858-82d2-3a5a5d21b89a'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/87a10d5d-5105-4858-82d2-3a5a5d21b89a.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/87a10d5d-5105-4858-82d2-3a5a5d21b89a.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/87a10d5d-5105-4858-82d2-3a5a5d21b89a.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/87a10d5d-5105-4858-82d2-3a5a5d21b89a.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/87a10d5d-5105-4858-82d2-3a5a5d21b89a /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c57b84d-3966-4626-9309-fbf6b9d1df5d/87a10d5d-5105-4858-82d2-3a5a5d21b89a.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('87a10d5d-5105-4858-82d2-3a5a5d21b89a','a2dda0a0-255e-4c56-86a1-499d078ace61');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='8b68a0c2-827c-4b15-aab8-30cd210a139b' WHERE image_guid ='2e715f5b-e805-4a01-ae5a-02e0a9da6474';"\n~~~\n\n\n~~~\n*** PLEASE SHUT THE VM DOWN ****\n\n[root@dbview ~]# ./mlb.py --vm-name mf06.c01.web.wwe.clt1.prod.bamtech.co\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '33a9a88d-cea8-42bb-90e3-dc70d2b0abb1' AND snapshot_id != '54a86488-c2da-4c83-86fc-86497e24cdca';"\n\nFIXING DISK c8b33473-7ee5-4179-b708-c5c1bdd9e6ab, CHAIN ['bbc50212-86ec-407f-9b0e-89db0a2fc786', 'e6c93d78-fc04-497d-95be-ef9e8189ee16', '30246e70-b220-475d-92b8-f6de05c5bbfa']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/e6c93d78-fc04-497d-95be-ef9e8189ee16 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/30246e70-b220-475d-92b8-f6de05c5bbfa'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/30246e70-b220-475d-92b8-f6de05c5bbfa.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/30246e70-b220-475d-92b8-f6de05c5bbfa.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/30246e70-b220-475d-92b8-f6de05c5bbfa.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/30246e70-b220-475d-92b8-f6de05c5bbfa.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/30246e70-b220-475d-92b8-f6de05c5bbfa /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/30246e70-b220-475d-92b8-f6de05c5bbfa.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/bbc50212-86ec-407f-9b0e-89db0a2fc786 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/e6c93d78-fc04-497d-95be-ef9e8189ee16'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/bbc50212-86ec-407f-9b0e-89db0a2fc786.meta"\nlvchange -ay /dev/ovirt-local/bbc50212-86ec-407f-9b0e-89db0a2fc786\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/bbc50212-86ec-407f-9b0e-89db0a2fc786 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/e6c93d78-fc04-497d-95be-ef9e8189ee16'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/e6c93d78-fc04-497d-95be-ef9e8189ee16.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/e6c93d78-fc04-497d-95be-ef9e8189ee16.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/e6c93d78-fc04-497d-95be-ef9e8189ee16.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/e6c93d78-fc04-497d-95be-ef9e8189ee16.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/e6c93d78-fc04-497d-95be-ef9e8189ee16 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b33473-7ee5-4179-b708-c5c1bdd9e6ab/e6c93d78-fc04-497d-95be-ef9e8189ee16.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('e6c93d78-fc04-497d-95be-ef9e8189ee16','30246e70-b220-475d-92b8-f6de05c5bbfa');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='54a86488-c2da-4c83-86fc-86497e24cdca' WHERE image_guid ='bbc50212-86ec-407f-9b0e-89db0a2fc786';"\n[root@dbview ~]# \n~~~~</text>, <text>Latest update. The following VMs applied the fix OK and seem fine. thank you!\n\nxfer01.c01.archive.gen.clt1.prod.mlbam.net\nxfer02.c01.archive.gen.clt1.prod.mlbam.net\nmf02.c01.web.wwe.clt1.prod.bamtech.co\nmf06.c01.web.gen.clt1.prod.bamtech.co\nmf06.c01.web.wwe.clt1.prod.bamtech.co\n\n\non a separate note, i noticed something different between a 'fixed' VM that we have run the fix commands on and a local disk VM that was NOT snapshot'd (a localdisk vm that has nothing wrong with it)....\n\nIf i do a domblklist on an existing VM that has not been affected by a snapshot, i see a 'hdd' device pointing to an .img.\nFor a VM that we have applied the 'fix' to, that entry is not there.\nIs this a concern ?\n\nEXample......\n\nVM that has not been affected by snapshot:.\n--------------------------------------------------------\n[root@hyp98 ~]# virsh -r domblklist seg02.c03.streaming.gen.clt1.dev.mlbam.net\nTarget     Source\n------------------------------------------------\nhdc        -\nhdd        /var/run/vdsm/payload/c308c4c7-b9a0-4501-b837-db55f06a254f.ec7da6ac71f1aac154633a61e00659e0.img\nvda        /dev/ovirt-local/fef6cfa2-494b-4345-a015-ee24c3817a16\n\n\n\n\nVM that has been 'fixed'\n--------------------------------------------------------\n[root@hyp41 ~]# virsh -r domblklist mf06.c01.web.wwe.clt1.prod.bamtech.co\nTarget     Source\n------------------------------------------------\nhdc        -\nvda        /dev/ovirt-local/bbc50212-86ec-407f-9b0e-89db0a2fc786</text>, <text>Hello Marcus,\n\nGlad to see the VM started up.\n\nThe /var/run/vdsm/payload/ directory is created by vdsmd on a host the first time the cloud-init/sysprep option is used to start a VM on that host.\nThe hdd path could be a cdrom pointing to the image.  The 'virsh -r dumpxml VM_NAME' might show more details on it.\n\nSo far we have been successfully using the script with 1 disk with multiple snapshots to get some of the VM's up.\nWe would still like verify VM's with multiple disks and with multiple snapshots.\n\nBelow is a list that we have parsed using the RHV-M DB with the vm_names from BAD2.txt you provided.\nThe second column is the number of disks, third is number of snapshots and last is the last_start_date pulled from the RHV-M DB.\nAre there VM's that are least important from the 2 and 3 disks that can be used to verify?\n\n~~~\nmysql02.core01.lprovis.hulu.clt1.prod.mlbam.net 2 2 2017-02-12 18:59:05.48+00\nmysql01.c01.jobmgr.gen.clt1.prod.mlbam.net 2 2 2017-02-20 15:00:04.15+00\njava03.ad01.hls.hulu.clt1.prod.mlbam.net 2 2 2017-02-22 15:02:59.211+00\njava02.ad01.hls.hulu.clt1.prod.mlbam.net 2 2 2017-02-24 13:45:43.015+00\nmdtools01.c01.bpa.hulu.clt1.prod.mlbam.net 2 2 2017-03-01 08:38:26.188+00\nmysql02.c01.jobmgr.gen.clt1.prod.mlbam.net 2 2 2017-03-09 10:07:40.655+00\nmysql02.control01.provis.gen.clt1.prod.mlbam.net 2 2 2017-03-20 11:43:27.421+00\ninflux01.c01.inf.hulu.clt1.prod.mlbam.net 2 2 2017-05-01 12:45:51.109+00\njava05.ad01.hls.hulu.clt1.prod.mlbam.net 2 2 2017-05-04 08:03:12.572+00\njava09.ad01.hls.hulu.clt1.prod.mlbam.net 2 2 2017-05-04 08:06:04.75+00\njava10.ad01.hls.hulu.clt1.prod.mlbam.net 2 2 2017-05-04 08:06:05.055+00\njava06.ad01.hls.hulu.clt1.prod.mlbam.net 2 2 2017-05-04 08:06:05.603+00\nmysqldb01.ingest01.ams.gen.clt1.qa.mlbam.net 2 2 2017-05-25 13:53:35.23+00\nmysqldb01.ingest01.ams.gen.clt1.prod.mlbam.net 2 2 2017-05-25 13:56:31.563+00\njenkins01.c01.razcp.gen.clt1.prod.mlbam.net 2 2 2017-08-17 12:52:56.411+00\nmonyog01.c01.noc.gen.clt1.util.mlbam.net 2 2 2017-10-10 07:54:42.589+00\nmysqldb02.ingest01.ams.gen.clt1.prod.mlbam.net 2 2 2017-11-03 17:11:31.547+00\ncbdb01.mlbtv01.db.mlb.clt1.prod.bamtech.co 2 2 2017-11-07 18:50:02.267+00\nmysql01.core01.lprovis.hulu.clt1.prod.mlbam.net 2 3 2017-02-11 14:22:13.534+00\njava01.ad01.hls.hulu.clt1.prod.mlbam.net 2 3 2017-02-22 14:21:36.998+00\nmdtools02.c01.bpa.hulu.clt1.prod.mlbam.net 2 3 2017-02-28 14:13:48.865+00\n~~~\n\n~~~\ncbdb03.ingest08.hls.hulu.clt1.prod.mlbam.net 3 2 2017-10-31 20:03:50.968+00\ncbdb03.ingest02.hls.hulu.clt1.prod.mlbam.net 3 3 2017-06-08 15:41:54.967+00\ncbdb03.ingest01.hls.hulu.clt1.prod.mlbam.net 3 3 2017-08-07 12:53:17.354+00\ncbdb01.ingest07.hls.hulu.clt1.prod.mlbam.net 3 3 2017-08-09 17:49:33.351+00\ncbdb03.ingest06.hls.hulu.clt1.prod.mlbam.net 3 3 2017-08-11 18:10:36.415+00\ncbdb02.ingest01.hls.hulu.clt1.prod.mlbam.net 3 3 2017-08-15 11:44:01.141+00\ncbdb03.ingest05.hls.hulu.clt1.prod.mlbam.net 3 3 2017-08-27 00:42:41.137+00\ncbdb03.ingest03.hls.hulu.clt1.prod.mlbam.net 3 3 2017-09-04 01:57:55.912+00\ncbdb02.ingest03.hls.hulu.clt1.prod.mlbam.net 3 3 2017-09-06 20:55:20.206+00\ncbdb01.ingest08.hls.hulu.clt1.prod.mlbam.net 3 3 2017-11-01 00:45:47.747+00\ncbdb02.ingest05.hls.hulu.clt1.prod.mlbam.net 3 3 2017-11-03 00:40:41.025+00\ncbdb02.ingest06.hls.hulu.clt1.prod.mlbam.net 3 4 2017-05-23 13:29:40.05+00\ncbdb02.ingest02.hls.hulu.clt1.prod.mlbam.net 3 4 2017-06-08 17:15:38.973+00\ncbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net 3 4 2017-07-03 09:14:51.284+00\ncbdb01.ingest01.hls.hulu.clt1.prod.mlbam.net 3 4 2017-07-30 17:02:27.732+00\ncbdb03.ingest04.hls.hulu.clt1.prod.mlbam.net 3 4 2017-08-15 12:57:34.18+00\ncbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net 3 4 2017-09-06 17:02:07.176+00\ncbdb02.ingest07.hls.hulu.clt1.prod.mlbam.net 3 4 2017-09-07 10:11:14.72+00\n~~~\n\nThanks\nBimal.</text>, <text>Thanks Bimal for the clarification not the payload. I should have noticed the word payload in the path, since we do in fact use cloud-init first boot so that makes sense.\n\nAs for the the VM list you provided, let me check which of these we can test on and ill get back to you. Thanks!\n\n\n\n\nOn 11/21/17, 2:34 PM, "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>Thanks for the update Marcus.\n- Bimal.</text>, <text>Hello,\n\n   My name is sachin from virtualization team. I am monitoring this case during APAC hours. \n\nCan you please update us the current status of this issue ?\n\nIf you have any questions or concerns about this case, please either update the case at access.redhat.com or For any urgent requirement feel free to call Red Hat Technical Support in the below listed numbers.\nhttps://access.redhat.com/support/contact/technicalSupport.html\n\nBest Regards,\nSachin</text>, <text>Hello,\n\nDue to inactivity on this case, the 24x7 flag is no longer set on this case. This means we will be assisting you primarily during the support hours of your region when you are available. This can be readjusted at any time, depending on your availability. Please feel free to give us a call on the numbers listed on our contact page, or update the case if you have any questions or concerns.\n\n"Contacting Technical Support"\n\nhttps://access.redhat.com/support/contact/technicalSupport.html\n\nBest Regards,\nSachin</text>, <text>Still working on this. Will have some test VM's from that list to work on soon</text>, <text>Hi Marcus,\n\nSounds good please keep us posted.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Hi,\nCan we have the fix commands for VM: jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net\n\nThe VM went down on it's own and we would like to run the fix commands before booting it back up.</text>, <text>Hello Marcus,\n\nIt seems like this VM was restarted after the snapshot issue on Nov 11.\nThere were attempts to restart on couple different servers but failed due the space issue and finally was started on hyp102 as it contained enough space to create a lvm image for the snapshot that was on the NFS SD.\nI believe there will be an multiple lvm images for this VM (VM id 02a9965e-e1fd-460f-931e-f9e7d6e22aa2) on hyp102\n\nCan you upload the logs for hyp102?\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc\nGSS\n\n\n~~~\n 2017-11-11 02:15:21.879+00    | 1c46fecd       | Snapshot 'Backup__2017-11-11__02.15.19' creation for VM 'jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net' was initiated by arichardson@mlbam.net.\n 2017-11-11 02:15:29.321+00    | 1c46fecd       | Snapshot 'Backup__2017-11-11__02.15.19' creation for VM 'jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net' has been completed.\n\n 2017-11-11 02:16:31.794+00    | 2feb92cc       | Snapshot 'Backup__2017-11-11__02.15.19' deletion for VM 'jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net' was initiated by arichardson@mlbam.net.\n 2017-11-11 02:16:39.124+00    | 2feb92cc       | Failed to delete snapshot 'Backup__2017-11-11__02.15.19' for VM 'jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net'.\n\n 2017-11-12 07:48:27.171+00    | 987babd        | Snapshot 'Backup__2017-11-12__07.48.22' creation for VM 'jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net' was initiated by arichardson@mlbam.net.\n 2017-11-12 13:05:58.059+00    | 987babd        | Snapshot 'Backup__2017-11-12__07.48.22' creation for VM 'jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net' has been completed.\n 2017-11-12 13:14:55.254+00    |                | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net has been paused.\n 2017-11-12 13:14:55.313+00    |                | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net has been paused due to no Storage space error.\n\n 2017-11-12 14:29:07.366+00    | 405b6bc1       | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net was resumed by fseifts@mlbam.net (Host: hyp11.clt1.prod.mlbam.net).\n 2017-11-12 14:29:15.859+00    |                | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net has recovered from paused back to up.\n 2017-11-12 14:29:15.884+00    | 405b6bc1       | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net started on Host hyp11.clt1.prod.mlbam.net\n\n 2017-11-14 20:21:22.929+00    | 37d7124e       | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net was started by nreisbeck@mlbam.net (Host: hyp11.clt1.prod.mlbam.net).\n\n 2017-11-14 20:21:24.738+00    |                | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net is down with error. Exit message: Hook Error: ('localdisk-hook: local disk not found, creating logical volume (name=fe6e6a8e-bedd-4a09-ad60-eadb4684fb9c)\\nlocaldisk-hook: creating logical volume (name=fe6e6a8e-bedd-4a09-ad60-eadb4684fb9c, size=53687091200)\\nTraceback (most recent call last):\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 172, in &lt;module&gt;\\n    main()\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 69, in main\\n    replace_disk(disk)\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 85, in replace_disk\\n    create_local_disk(orig_path, lv_name, vm_id, img_id, src_format)\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 104, in create_local_disk\\n    create_lv(lv_name, size, vm_id, img_id)\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 160, in create_lv\\n    lv_name)\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 168, in helper\\n    raise cmdutils.Error(cmd=HELPER, rc=rc, out=out, err=err)\\nvdsm.cmdutils.Error: Command /usr/libexec/vdsm/localdisk-helper failed with rc=1 out=\\'\\' err=\\'Traceback (most recent call last):\\\\n  File "/usr/libexec/vdsm/localdisk-helper", line 111, in &lt;module&gt;\\\\n    args.command(args)\\\\n  File "/usr/libexec/vdsm/localdisk-helper", line 49, in create\\\\n    lvm(*cmd)\\\\n  File "/usr/libexec/vdsm/localdisk-helper", line 73, in lvm\\\\n    raise cmdutils.Error(cmd=cmd, rc=rc, out=out, err=err)\\\\nvdsm.cmdutils.Error: Command [\\\\\\'/usr/sbin/lvm\\\\\\', \\\\\\'lvcreate\\\\\\', \\\\\\'-L\\\\\\', \\\\\\'53687091200b\\\\\\', \\\\\\'-n\\\\\\', \\\\\\'fe6e6a8e-bedd-4a09-ad60-eadb4684fb9c\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'UPDATING\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'VM_02a9965e-e1fd-460f-931e-f9e7d6e22aa2\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'IU_fc68df70-cada-41e0-8c03-6fa8c4d14a63\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'PU_00000000-0000-0000-0000-000000000000\\\\\\', \\\\\\'ovirt-local\\\\\\'] failed with rc=5 out=\\\\\\'\\\\\\' err=\\\\\\'  Volume group "ovirt-local" has insufficient free space (4911 extents): 12800 required.\\\\\\\\n\\\\\\'\\\\n\\'\\n',).\n\n 2017-11-14 20:21:24.856+00    | 37d7124e       | Failed to run VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net on Host hyp11.clt1.prod.mlbam.net.\n\n 2017-11-14 20:21:25.948+00    | 37d7124e       | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net was started by nreisbeck@mlbam.net (Host: hyp96.clt1.prod.mlbam.net).\n\n 2017-11-14 20:21:33.698+00    |                | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net is down with error. Exit message: Hook Error: ('localdisk-hook: local disk not found, creating logical volume (name=fe6e6a8e-bedd-4a09-ad60-eadb4684fb9c)\\nlocaldisk-hook: creating logical volume (name=fe6e6a8e-bedd-4a09-ad60-eadb4684fb9c, size=53687091200)\\nTraceback (most recent call last):\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 172, in &lt;module&gt;\\n    main()\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 69, in main\\n    replace_disk(disk)\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 85, in replace_disk\\n    create_local_disk(orig_path, lv_name, vm_id, img_id, src_format)\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 104, in create_local_disk\\n    create_lv(lv_name, size, vm_id, img_id)\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 160, in create_lv\\n    lv_name)\\n  File "/usr/libexec/vdsm/hooks/after_disk_prepare/localdisk", line 168, in helper\\n    raise cmdutils.Error(cmd=HELPER, rc=rc, out=out, err=err)\\nvdsm.cmdutils.Error: Command /usr/libexec/vdsm/localdisk-helper failed with rc=1 out=\\'\\' err=\\'Traceback (most recent call last):\\\\n  File "/usr/libexec/vdsm/localdisk-helper", line 111, in &lt;module&gt;\\\\n    args.command(args)\\\\n  File "/usr/libexec/vdsm/localdisk-helper", line 49, in create\\\\n    lvm(*cmd)\\\\n  File "/usr/libexec/vdsm/localdisk-helper", line 73, in lvm\\\\n    raise cmdutils.Error(cmd=cmd, rc=rc, out=out, err=err)\\\\nvdsm.cmdutils.Error: Command [\\\\\\'/usr/sbin/lvm\\\\\\', \\\\\\'lvcreate\\\\\\', \\\\\\'-L\\\\\\', \\\\\\'53687091200b\\\\\\', \\\\\\'-n\\\\\\', \\\\\\'fe6e6a8e-bedd-4a09-ad60-eadb4684fb9c\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'UPDATING\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'VM_02a9965e-e1fd-460f-931e-f9e7d6e22aa2\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'IU_fc68df70-cada-41e0-8c03-6fa8c4d14a63\\\\\\', \\\\\\'--addtag\\\\\\', \\\\\\'PU_00000000-0000-0000-0000-000000000000\\\\\\', \\\\\\'ovirt-local\\\\\\'] failed with rc=5 out=\\\\\\'\\\\\\' err=\\\\\\'  Volume group "ovirt-local" has insufficient free space (4911 extents): 12800 required.\\\\\\\\n\\\\\\'\\\\n\\'\\n',).\n\n 2017-11-14 20:21:33.822+00    | 37d7124e       | Failed to run VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net on Host hyp96.clt1.prod.mlbam.net.\n\n 2017-11-14 20:21:35.344+00    | 37d7124e       | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net was started by nreisbeck@mlbam.net (Host: hyp102.clt1.prod.mlbam.net).\n\n 2017-11-14 20:24:47.306+00    | 37d7124e       | Guest jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net started on Host hyp102.clt1.prod.mlbam.net. (Default Host parameter was ignored - assigned Host was not available).\n~~~</text>, <text>Hi Marcus,\n\nFor this VM, there are 3 images. \n\n~~~\nimages:\n              image_guid              \n--------------------------------------\n 58c45e9e-b0ce-4238-bdb1-e1e4f27e57e5  &lt;&lt;--- BASE\n 61b028a5-495d-4072-9cbe-ee9ad0a2277f  &lt;&lt;--- Generated from snapshot operation.\n fe6e6a8e-bedd-4a09-ad60-eadb4684fb9c  &lt;&lt;--- Generated from snapshot operation.\n~~~\n\nSince this VM was restarted on hyp102.clt1.prod.mlbam.net, the snapshot image has been created as lvm image on the host.\n\n~~~\n  fe6e6a8e-bedd-4a09-ad60-eadb4684fb9c ovirt-local    1 -wi-a----- 50.00g  IU_fc68df70-cada-41e0-8c03-6fa8c4d14a63,PU_00000000-0000-0000-0000-000000000000,VM_02a9965e-e1fd-460f-931e-f9e7d6e22aa2\n~~~\n\nNormally we have the base image (58c45e9e-b0ce-4238-bdb1-e1e4f27e57e5) as the lvm on the host.\n\nAlso the fe6e6a8e image was last written back on Nov 14 when the VM was running on hyp11.clt1.prod.mlbam.net.\n~~~\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc68df70-cada-41e0-8c03-6fa8c4d14a63:\ntotal 2625778\n\n-rw-rw----.   1 vdsm kvm  111607808 Nov 14 12:32 fe6e6a8e-bedd-4a09-ad60-eadb4684fb9c\n~~~\n\nSo the situation we are currently is the snapshot image was created as lvm image on the hyp102.clt1.prod.mlbam.net host while the base image might still be on hyp11.clt1.prod.mlbam.net host.\n\nCan you run the following on hyp11.clt1.prod.mlbam.net to see if the lvm image 58c45e9e-b0ce-4238-bdb1-e1e4f27e57e5 is on the host\n\n~~~\nlvm lvs -o +tags |grep VM_02a9965e-e1fd-460f-931e-f9e7d6e22aa2\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Let's forget about VM 'jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net'\n\nThe application team was unable to wait for a workaround/fix and decided to redeploy the VM entirely asap. Thank you</text>, <text>Hello Marcus,\n\nThanks for the update.\n\nThe situation of 'jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net' was quite different as it was restarted on a different host.\n\nThis VM contained the following three images:\n\n~~~\n58c45e9e-b0ce-4238-bdb1-e1e4f27e57e5 - Base LVM image on the host\n61b028a5-495d-4072-9cbe-ee9ad0a2277f - Generate from snapshot operation\nfe6e6a8e-bedd-4a09-ad60-eadb4684fb9c - Generate from snapshot operation.\n~~~\n\nThe VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net was up and running on hyp11.clt1.prod.mlbam.net on Nov 14 around 20:21.\nThe hyp11.clt1.prod.mlbam.net host had lvm base image of 58c45e9e-b0ce-4238-bdb1-e1e4f27e57e5.\nNow a few mins later the VM was shutdown and fails to start due the storage space problem. \n\n~~~\n 2017-11-14 20:21:22.929+00    | 37d7124e       | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net was started by nreisbeck@mlbam.net (Host: hyp11.clt1.prod.mlbam.net).\n 2017-11-14 20:21:24.738+00    |                | VM jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net is down with error. Exit message: Hook Error: \n~~~\n\nNext we see the VM starting up on hyp102.clt1.prod.mlbam.net.\n\n~~~\n 2017-11-14 20:24:47.306+00    | 37d7124e       | Guest jetty01.core01.jobmgr.gen.clt1.prod.mlbam.net started on Host hyp102.clt1.prod.mlbam.net.\n~~~\n\nSince this VM was never ran on this host, the 'hook' went and created a base image.\nThe base lvm image created on hyp102 was from the snapshot image from Nov12\n\n~~~\n  fe6e6a8e-bedd-4a09-ad60-eadb4684fb9c ovirt-local    1 -wi-a----- 50.00g  IU_fc68df70-cada-41e0-8c03-6fa8c4d14a63,PU_00000000-0000-0000-0000-000000000000,VM_02a9965e-e1fd-460f-931e-f9e7d6e22aa2\n~~~\n\nSo we now we have 2 different images for this VM.\nOriginal base on hyp11 that contained data up to Nove 14 20:21, then on hyp102 with new snapshot based lvm image from data from Nov 14 20:24 and on.\nThis VM could have started on hyp102 but you data would have been from Nov 14 20:24 and onwards.\n\nThe script only fixes VM's that haven't been restarted. \nAny VM's that have been restarted will contain multiple lvm images on the host (if one got created with enough space).\n\nSince this VM's was re-deployed, you may want to remove the old/stale base lvm images on the host to get some space back.\n\nThanks and best regards\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Merged for oVirt 4.2.1 (will be part of RHV 4.2 GA).\n\nFred/Tal - the patch seems simple enough - let's please clone this and provide a fix for 4.1.9 too.</text>, <text>Hi,\nFrom the list of VMs you would like to continue with next (the ones with dual disks etc),, we can start with the following VM's. \n\njava01.ad01.hls.hulu.clt1.prod.mlbam.net 2 3 2017-02-22 14:21:36.998+00\njava02.ad01.hls.hulu.clt1.prod.mlbam.net 2 2 2017-02-24 13:45:43.015+00\njava03.ad01.hls.hulu.clt1.prod.mlbam.net 2 2 2017-02-22 15:02:59.211+00</text>, <text>Hi Marcus,\n\nI have let Bimal know, I apologize he is presently working a couple other cases at the moment but should be able to update you later today. Also I was wondering if it would be ok to consider reducing this case to a severity to a 3 unless we are seeing a raised priority again on the case?\n\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>That sounds ok. Thank you\n\n\n\n\nOn 12/8/17, 2:13 PM, "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>Hello Marcus,\n\nCan you run the following on the RHV-M engine host to makesure the VM's haven't been restarted.\nI have the RHV-M DB from 11/19.\n\n\n~~~\ncd /var/log/ovirt-engine\ngrep CreateVmVDSCommand engine.log |grep java01.ad01.hls.hulu.clt1.prod.mlbam.net\ngrep CreateVmVDSCommand engine.log |grep java02.ad01.hls.hulu.clt1.prod.mlbam.net\ngrep CreateVmVDSCommand engine.log |grep java03.ad01.hls.hulu.clt1.prod.mlbam.net\n\nzgrep CreateVmVDSCommand engine.log*gz |grep java01.ad01.hls.hulu.clt1.prod.mlbam.net\nzgrep CreateVmVDSCommand engine.log*gz |grep java02.ad01.hls.hulu.clt1.prod.mlbam.net\nzgrep CreateVmVDSCommand engine.log*gz |grep java03.ad01.hls.hulu.clt1.prod.mlbam.net\n~~~\n\nThanks\nBimal Chollera\nRed Hat, Inc\nGSS</text>, <text>// Internal //\n// Awaiting to see if the VM is not started, the following commands can be run to fix the problem //\n\n\njava01.ad01.hls.hulu.clt1.prod.mlbam.net \n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '37eaec98-4f90-4e29-90d8-f1361f22f0a0' AND snapshot_id != 'b9890c71-7798-4057-bd5f-8b768c3f28f6';"\n\nFIXING DISK f8462fa2-d5a2-41e4-9f24-238b2742e6b6, CHAIN ['8452f811-61be-4c31-b736-f3421606d344', '9af43d68-d490-4b4e-ab62-2094c1d25999', '1cfcbafd-6544-42be-b8ee-a913ee6a6739']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/8452f811-61be-4c31-b736-f3421606d344 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/8452f811-61be-4c31-b736-f3421606d344.meta"\nlvchange -ay /dev/ovirt-local/8452f811-61be-4c31-b736-f3421606d344\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/8452f811-61be-4c31-b736-f3421606d344 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('9af43d68-d490-4b4e-ab62-2094c1d25999','1cfcbafd-6544-42be-b8ee-a913ee6a6739');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='b9890c71-7798-4057-bd5f-8b768c3f28f6' WHERE image_guid ='8452f811-61be-4c31-b736-f3421606d344';"\n\nFIXING DISK f7bf5639-f544-4f23-a632-0558f4c6cade, CHAIN ['ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59', '2f94eb3e-54a4-441e-acce-be016ee84bc4', '09435254-92bc-4a61-9f3a-2cdb6efd68b2']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59.meta"\nlvchange -ay /dev/ovirt-local/ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('2f94eb3e-54a4-441e-acce-be016ee84bc4','09435254-92bc-4a61-9f3a-2cdb6efd68b2');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='b9890c71-7798-4057-bd5f-8b768c3f28f6' WHERE image_guid ='ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59';"\n~~~\n\n\njava02.ad01.hls.hulu.clt1.prod.mlbam.net\n\n~~~\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '265f505b-df72-4ff6-a5c8-73bd1f892a11' AND snapshot_id != '67acf63d-1ff1-4e38-9aad-f2e66b384db5';"\n\nFIXING DISK fb5ddab9-9646-4e3b-baa6-4f3e8a01aec2, CHAIN ['46753c49-b1b4-44e7-a47a-313290b2bf3a']\n\nHost Commands:\n==============\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='67acf63d-1ff1-4e38-9aad-f2e66b384db5' WHERE image_guid ='46753c49-b1b4-44e7-a47a-313290b2bf3a';"\n\nFIXING DISK d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c, CHAIN ['457f2207-3b3f-41ab-bd25-1a8f7902afb0', '0a5b2b4f-3402-462e-8adc-dd7642e9cdfe']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/457f2207-3b3f-41ab-bd25-1a8f7902afb0 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/457f2207-3b3f-41ab-bd25-1a8f7902afb0.meta"\nlvchange -ay /dev/ovirt-local/457f2207-3b3f-41ab-bd25-1a8f7902afb0\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/457f2207-3b3f-41ab-bd25-1a8f7902afb0 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0a5b2b4f-3402-462e-8adc-dd7642e9cdfe');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='67acf63d-1ff1-4e38-9aad-f2e66b384db5' WHERE image_guid ='457f2207-3b3f-41ab-bd25-1a8f7902afb0';"\n~~~\n\n\njava03.ad01.hls.hulu.clt1.prod.mlbam.net \n\n~~~\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'f01b3cb1-f07d-4361-8cbf-53d0605f8729' AND snapshot_id != 'f6c9355c-4813-4561-83f9-dde75e1cf3f5';"\n\nFIXING DISK ffac4983-cf72-4db9-92f4-f91a4116ed85, CHAIN ['f71e340a-c9ef-4740-83b2-2ad4b97a02d7', 'f15cdb5e-3666-4155-9c51-634a5ae09708']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f71e340a-c9ef-4740-83b2-2ad4b97a02d7 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f71e340a-c9ef-4740-83b2-2ad4b97a02d7.meta"\nlvchange -ay /dev/ovirt-local/f71e340a-c9ef-4740-83b2-2ad4b97a02d7\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f71e340a-c9ef-4740-83b2-2ad4b97a02d7 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('f15cdb5e-3666-4155-9c51-634a5ae09708');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='f6c9355c-4813-4561-83f9-dde75e1cf3f5' WHERE image_guid ='f71e340a-c9ef-4740-83b2-2ad4b97a02d7';"\n\nFIXING DISK 1f67883f-1183-480d-b83a-ab931d5ce6cb, CHAIN ['ab67ec1b-9413-4e03-b2d8-0a40260d8748', 'cd0e6e10-7de2-4b32-b21f-e3a462bc0f60']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/ab67ec1b-9413-4e03-b2d8-0a40260d8748 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/ab67ec1b-9413-4e03-b2d8-0a40260d8748.meta"\nlvchange -ay /dev/ovirt-local/ab67ec1b-9413-4e03-b2d8-0a40260d8748\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ab67ec1b-9413-4e03-b2d8-0a40260d8748 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('cd0e6e10-7de2-4b32-b21f-e3a462bc0f60');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='f6c9355c-4813-4561-83f9-dde75e1cf3f5' WHERE image_guid ='ab67ec1b-9413-4e03-b2d8-0a40260d8748';"\n~~~</text>, <text>This bug has been cloned to the z-stream bug#1524424</text>, <text>Hi Bimal,\nNo matches on those greps....\n\n\n[root@rhvm-engine01 ~]# cd /var/log/ovirt-engine\n[root@rhvm-engine01 ovirt-engine]# grep CreateVmVDSCommand engine.log |grep java01.ad01.hls.hulu.clt1.prod.mlbam.net\n[root@rhvm-engine01 ovirt-engine]# grep CreateVmVDSCommand engine.log |grep java02.ad01.hls.hulu.clt1.prod.mlbam.net\n[root@rhvm-engine01 ovirt-engine]# grep CreateVmVDSCommand engine.log |grep java03.ad01.hls.hulu.clt1.prod.mlbam.net\n[root@rhvm-engine01 ovirt-engine]#\n[root@rhvm-engine01 ovirt-engine]# zgrep CreateVmVDSCommand engine.log*gz |grep java01.ad01.hls.hulu.clt1.prod.mlbam.net\n[root@rhvm-engine01 ovirt-engine]# zgrep CreateVmVDSCommand engine.log*gz |grep java02.ad01.hls.hulu.clt1.prod.mlbam.net\n[root@rhvm-engine01 ovirt-engine]# zgrep CreateVmVDSCommand engine.log*gz |grep java03.ad01.hls.hulu.clt1.prod.mlbam.net\n[root@rhvm-engine01 ovirt-engine]#</text>, <text>Hello Marcus,\n\nJust in case - besure to have a backup of this VM.\n\njava01.ad01.hls.hulu.clt1.prod.mlbam.net \n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '37eaec98-4f90-4e29-90d8-f1361f22f0a0' AND snapshot_id != 'b9890c71-7798-4057-bd5f-8b768c3f28f6';"\n\nFIXING DISK f8462fa2-d5a2-41e4-9f24-238b2742e6b6, CHAIN ['8452f811-61be-4c31-b736-f3421606d344', '9af43d68-d490-4b4e-ab62-2094c1d25999', '1cfcbafd-6544-42be-b8ee-a913ee6a6739']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/1cfcbafd-6544-42be-b8ee-a913ee6a6739.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/8452f811-61be-4c31-b736-f3421606d344 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/8452f811-61be-4c31-b736-f3421606d344.meta"\nlvchange -ay /dev/ovirt-local/8452f811-61be-4c31-b736-f3421606d344\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/8452f811-61be-4c31-b736-f3421606d344 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8462fa2-d5a2-41e4-9f24-238b2742e6b6/9af43d68-d490-4b4e-ab62-2094c1d25999.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('9af43d68-d490-4b4e-ab62-2094c1d25999','1cfcbafd-6544-42be-b8ee-a913ee6a6739');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='b9890c71-7798-4057-bd5f-8b768c3f28f6' WHERE image_guid ='8452f811-61be-4c31-b736-f3421606d344';"\n\nFIXING DISK f7bf5639-f544-4f23-a632-0558f4c6cade, CHAIN ['ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59', '2f94eb3e-54a4-441e-acce-be016ee84bc4', '09435254-92bc-4a61-9f3a-2cdb6efd68b2']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/09435254-92bc-4a61-9f3a-2cdb6efd68b2.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59.meta"\nlvchange -ay /dev/ovirt-local/ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f7bf5639-f544-4f23-a632-0558f4c6cade/2f94eb3e-54a4-441e-acce-be016ee84bc4.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('2f94eb3e-54a4-441e-acce-be016ee84bc4','09435254-92bc-4a61-9f3a-2cdb6efd68b2');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='b9890c71-7798-4057-bd5f-8b768c3f28f6' WHERE image_guid ='ec237ba9-7e63-45a7-b0c8-a6c46fbb5e59';"\n~~~</text>, <text>Hi Bimal,\nAre the commands you listed just for java01 ? Thanks</text>, <text>Hello Marcus,\n\nYes it was just for java01. Did the commands complete successfully?\nBelow are for Java02 and Java03.  \n\nFor  java02.ad01.hls.hulu.clt1.prod.mlbam.net\n\n~~~\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '265f505b-df72-4ff6-a5c8-73bd1f892a11' AND snapshot_id != '67acf63d-1ff1-4e38-9aad-f2e66b384db5';"\n\nFIXING DISK fb5ddab9-9646-4e3b-baa6-4f3e8a01aec2, CHAIN ['46753c49-b1b4-44e7-a47a-313290b2bf3a']\n\nHost Commands:\n==============\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='67acf63d-1ff1-4e38-9aad-f2e66b384db5' WHERE image_guid ='46753c49-b1b4-44e7-a47a-313290b2bf3a';"\n\nFIXING DISK d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c, CHAIN ['457f2207-3b3f-41ab-bd25-1a8f7902afb0', '0a5b2b4f-3402-462e-8adc-dd7642e9cdfe']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/457f2207-3b3f-41ab-bd25-1a8f7902afb0 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/457f2207-3b3f-41ab-bd25-1a8f7902afb0.meta"\nlvchange -ay /dev/ovirt-local/457f2207-3b3f-41ab-bd25-1a8f7902afb0\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/457f2207-3b3f-41ab-bd25-1a8f7902afb0 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d47a9f9c-ca6d-40b2-9ced-fbeb5976d35c/0a5b2b4f-3402-462e-8adc-dd7642e9cdfe.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0a5b2b4f-3402-462e-8adc-dd7642e9cdfe');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='67acf63d-1ff1-4e38-9aad-f2e66b384db5' WHERE image_guid ='457f2207-3b3f-41ab-bd25-1a8f7902afb0';"\n~~~\n\n\njava03.ad01.hls.hulu.clt1.prod.mlbam.net \n\n~~~\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'f01b3cb1-f07d-4361-8cbf-53d0605f8729' AND snapshot_id != 'f6c9355c-4813-4561-83f9-dde75e1cf3f5';"\n\nFIXING DISK ffac4983-cf72-4db9-92f4-f91a4116ed85, CHAIN ['f71e340a-c9ef-4740-83b2-2ad4b97a02d7', 'f15cdb5e-3666-4155-9c51-634a5ae09708']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f71e340a-c9ef-4740-83b2-2ad4b97a02d7 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f71e340a-c9ef-4740-83b2-2ad4b97a02d7.meta"\nlvchange -ay /dev/ovirt-local/f71e340a-c9ef-4740-83b2-2ad4b97a02d7\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f71e340a-c9ef-4740-83b2-2ad4b97a02d7 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ffac4983-cf72-4db9-92f4-f91a4116ed85/f15cdb5e-3666-4155-9c51-634a5ae09708.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('f15cdb5e-3666-4155-9c51-634a5ae09708');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='f6c9355c-4813-4561-83f9-dde75e1cf3f5' WHERE image_guid ='f71e340a-c9ef-4740-83b2-2ad4b97a02d7';"\n\nFIXING DISK 1f67883f-1183-480d-b83a-ab931d5ce6cb, CHAIN ['ab67ec1b-9413-4e03-b2d8-0a40260d8748', 'cd0e6e10-7de2-4b32-b21f-e3a462bc0f60']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/ab67ec1b-9413-4e03-b2d8-0a40260d8748 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/ab67ec1b-9413-4e03-b2d8-0a40260d8748.meta"\nlvchange -ay /dev/ovirt-local/ab67ec1b-9413-4e03-b2d8-0a40260d8748\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ab67ec1b-9413-4e03-b2d8-0a40260d8748 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1f67883f-1183-480d-b83a-ab931d5ce6cb/cd0e6e10-7de2-4b32-b21f-e3a462bc0f60.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('cd0e6e10-7de2-4b32-b21f-e3a462bc0f60');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='f6c9355c-4813-4561-83f9-dde75e1cf3f5' WHERE image_guid ='ab67ec1b-9413-4e03-b2d8-0a40260d8748';"\n~~~</text>, <text>Bug report changed to ON_QA status by Errata System.\nA QE request has been submitted for advisory RHEA-2017:29661-02\nhttps://errata.devel.redhat.com/advisory/29661</text>, <text>Verified with the following code:\n-----------------------------------------\novirt-engine-4.2.0.2-0.1.el7.noarch\nvdsm-4.20.9.2-1.el7ev.x86_64\n\nVerified with the following scenario:\n-----------------------------------------\n1. Install he hook\n2. Snapshot the VM &gt;&gt;&gt;&gt;&gt; Error is displayed informing the user that snapshots cant be created as the hook is configured on the system.\n\n\nMoving to VERIFY</text>, <text>HI Bimal,\nI'm still waiting on downtime window for java01/02/03. \n\nTurns out today a hypervisor went down because of hard crash. I have two VM's that are currently powered off. Would we be able to get the fix commands for these two ?...\n\n\njenkins01.c01.razcp.gen.clt1.prod.mlbam.net - hyp24.clt1.prod.mlbam.net (Need to boot)\njava05.ad01.hls.hulu.clt1.prod.mlbam.net - hyp24.clt1.prod.mlbam.net (Need to boot)</text>, <text>I will be out of the office with limited access to email until January 8, 2017. I will respond to your email message when I return.\n\n\nKind Regards,\n\n\nImran Merali\n\nDirector, Core Infrastructure\n\nBAMTECH Media</text>, <text>Hello Marcus,\n\nHappy New Year!!.\nI was out of the office during the holidays.\n\nCan you send you the logs from  hyp24.clt1.prod.mlbam.net to check the image information for the VM's.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Current layout of the VM per the DB.\n\n~~~\nvm_static:                                                                                                              (0 = ok, 1 = locked)\n               vm_guid                |                   vm_name                   | entity_type | os | description |              cluster_id              | template_status |               vmt_guid               |         original_template_id         \n--------------------------------------+---------------------------------------------+-------------+----+-------------+--------------------------------------+-----------------+--------------------------------------+--------------------------------------\n de6a5784-32fa-4057-adce-f9faa277498c | jenkins01.c01.razcp.gen.clt1.prod.mlbam.net | VM          | 24 |             | 6583532b-c88b-42ec-8c64-48437e657bee |                 | 82fcd601-e004-447a-b741-521dcb6c8a71 | 82fcd601-e004-447a-b741-521dcb6c8a71\n(1 row)\n\n                                   *** Cluster = Compute-LocalDisk  ;  DC = CLT1-LD ***   Template =  CentOS-7.2-Baseline-CLT1        Original Template =  CentOS-7.2-Baseline-CLT1\n \n \nvm_dynamic:                           (0 = down, 1 = up, 4 = paused, 5/6 = migrating, 7 = unknown, 8 = notresp, 9 = wfl, 15 = locked)\n               vm_guid                | status |              run_on_vds              |         guest_os          \n--------------------------------------+--------+--------------------------------------+---------------------------\n de6a5784-32fa-4057-adce-f9faa277498c |      1 | 5a6a4a2b-d15a-4f31-9ed7-e480e047f84f | 3.10.0-693.2.2.el7.x86_64\n(1 row)\n\n                                                  *** Running on host hyp24.clt1.prod.mlbam.net ***\n \n \nvm_device:\n              device_id               |                vm_id                 | boot_order |         _create_date          |         _update_date          |                           address                            \n--------------------------------------+--------------------------------------+------------+-------------------------------+-------------------------------+--------------------------------------------------------------\n 2fd43e49-f4e1-40c2-8f47-9b8999552d97 | de6a5784-32fa-4057-adce-f9faa277498c |          0 | 2017-09-06 19:28:43.363577+00 | 2017-11-17 14:20:04.758225+00 | {slot=0x08, bus=0x00, domain=0x0000, type=pci, function=0x0}\n cb00d3ef-7203-4542-bfaa-87780e0922d6 | de6a5784-32fa-4057-adce-f9faa277498c |          1 | 2017-05-22 19:47:06.217542+00 | 2017-11-17 14:20:04.758225+00 | {slot=0x07, bus=0x00, domain=0x0000, type=pci, function=0x0}\n(2 rows)\n\n \nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 97e6210e-8657-4704-b838-583ce466ac67 | 2fd43e49-f4e1-40c2-8f47-9b8999552d97 | 934e7b94-187b-445a-bca9-8e5d9120d91b | 00000000-0000-0000-0000-000000000000 |           1 | 2017-09-06 19:28:42+00 |           2 |             5 | t\n(1 row)\n\n    For image_guid = 97e6210e-8657-4704-b838-583ce466ac67 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 63798691328\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n e04de039-88ce-4e2a-8895-508419876e6e | cb00d3ef-7203-4542-bfaa-87780e0922d6 | f35d207e-e948-495a-b93c-e066d2414a7f | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-05-22 19:47:05+00 |           2 |             4 | f\n 47317f72-0760-488b-b214-6363997cc7f2 | cb00d3ef-7203-4542-bfaa-87780e0922d6 | 934e7b94-187b-445a-bca9-8e5d9120d91b | e04de039-88ce-4e2a-8895-508419876e6e |           1 | 2017-11-12 04:35:32+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = e04de039-88ce-4e2a-8895-508419876e6e , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n    For image_guid = 47317f72-0760-488b-b214-6363997cc7f2 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 31621285376\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------\n 934e7b94-187b-445a-bca9-8e5d9120d91b | de6a5784-32fa-4057-adce-f9faa277498c | ACTIVE        | OK     | Active VM                    | 2017-05-22 19:47:04.496+00\n f35d207e-e948-495a-b93c-e066d2414a7f | de6a5784-32fa-4057-adce-f9faa277498c | REGULAR       | OK     | Backup__2017-11-12__04.35.28 | 2017-11-12 04:35:46.78+00\n(2 rows)\n\n \nbase_disks:\n               disk_id                | wipe_after_delete | propagate_errors |                    disk_alias                     | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+---------------------------------------------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n 2fd43e49-f4e1-40c2-8f47-9b8999552d97 | f                 | Off              | jenkins01.c01.razcp.gen.clt1.prod.mlbam.net_Disk1 |                  | f         |      |         0 |                     |                 0 | \n(1 row)\n\n               disk_id                | wipe_after_delete | propagate_errors | disk_alias | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n cb00d3ef-7203-4542-bfaa-87780e0922d6 | f                 | Off              | rootdisk   |                  | f         |      |         0 |                     |                 0 | \n(1 row)\n\n \ndisk_vm_element:\n               disk_id                |                vm_id                 | is_boot | disk_interface \n--------------------------------------+--------------------------------------+---------+----------------\n 2fd43e49-f4e1-40c2-8f47-9b8999552d97 | de6a5784-32fa-4057-adce-f9faa277498c | f       | VirtIO\n(1 row)\n\n               disk_id                |                vm_id                 | is_boot | disk_interface \n--------------------------------------+--------------------------------------+---------+----------------\n cb00d3ef-7203-4542-bfaa-87780e0922d6 | de6a5784-32fa-4057-adce-f9faa277498c | t       | VirtIO\n(1 row)\n~~~\n\nTo fix the VM:\n  jenkins01.c01.razcp.gen.clt1.prod.mlbam.net\n\n~~~\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'de6a5784-32fa-4057-adce-f9faa277498c' AND snapshot_id != '934e7b94-187b-445a-bca9-8e5d9120d91b';"\n\nFIXING DISK cb00d3ef-7203-4542-bfaa-87780e0922d6, CHAIN ['e04de039-88ce-4e2a-8895-508419876e6e', '47317f72-0760-488b-b214-6363997cc7f2']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/e04de039-88ce-4e2a-8895-508419876e6e -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/e04de039-88ce-4e2a-8895-508419876e6e.meta"\nlvchange -ay /dev/ovirt-local/e04de039-88ce-4e2a-8895-508419876e6e\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/e04de039-88ce-4e2a-8895-508419876e6e -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('47317f72-0760-488b-b214-6363997cc7f2');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='934e7b94-187b-445a-bca9-8e5d9120d91b' WHERE image_guid ='e04de039-88ce-4e2a-8895-508419876e6e';"\n~~~</text>, <text>Current layout of the VM\n\n\n~~~\nvm_static:                                                                                                              (0 = ok, 1 = locked)\n               vm_guid                |                 vm_name                  | entity_type | os | description |              cluster_id              | template_status |               vmt_guid               |         original_template_id         \n--------------------------------------+------------------------------------------+-------------+----+-------------+--------------------------------------+-----------------+--------------------------------------+--------------------------------------\n 0392d189-510d-4f56-9cf9-166c3d2a9ae6 | java05.ad01.hls.hulu.clt1.prod.mlbam.net | VM          | 24 |             | 6583532b-c88b-42ec-8c64-48437e657bee |                 | 82fcd601-e004-447a-b741-521dcb6c8a71 | 82fcd601-e004-447a-b741-521dcb6c8a71\n(1 row)\n\n                    *** Cluster = Compute-LocalDisk  ;  DC = CLT1-LD *** Template =  CentOS-7.2-Baseline-CLT1   Original Template =  CentOS-7.2-Baseline-CLT1\n \n \nvm_dynamic:                           (0 = down, 1 = up, 4 = paused, 5/6 = migrating, 7 = unknown, 8 = notresp, 9 = wfl, 15 = locked)\n               vm_guid                | status |              run_on_vds              |       guest_os        \n--------------------------------------+--------+--------------------------------------+-----------------------\n 0392d189-510d-4f56-9cf9-166c3d2a9ae6 |      1 | 5a6a4a2b-d15a-4f31-9ed7-e480e047f84f | 3.10.0-327.el7.x86_64\n(1 row)\n\n                                                  *** Running on host hyp24.clt1.prod.mlbam.net ***\n \n \nvm_device:\n              device_id               |                vm_id                 | boot_order |         _create_date          |         _update_date          |                           address                            \n--------------------------------------+--------------------------------------+------------+-------------------------------+-------------------------------+--------------------------------------------------------------\n a6695490-092c-420a-b2a5-dac2641787e0 | 0392d189-510d-4f56-9cf9-166c3d2a9ae6 |          0 | 2017-05-04 07:54:57.888075+00 | 2017-11-12 13:12:35.610111+00 | {slot=0x09, bus=0x00, domain=0x0000, type=pci, function=0x0}\n cc9b8799-28e0-416e-937e-772d3b8f7aba | 0392d189-510d-4f56-9cf9-166c3d2a9ae6 |          1 | 2017-05-04 07:42:14.627883+00 | 2017-11-12 13:12:35.610111+00 | {slot=0x07, bus=0x00, domain=0x0000, type=pci, function=0x0}\n(2 rows)\n\n \nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 78e036d3-d28a-4ffc-bbbc-af8051c3de9d | a6695490-092c-420a-b2a5-dac2641787e0 | 3f707d5d-66d5-40bc-b171-5d3e2caaf6ee | 00000000-0000-0000-0000-000000000000 |           1 | 2017-05-04 15:52:41+00 |           2 |             5 | f\n c765b42b-4bc6-4e40-b3d4-da99d218524f | a6695490-092c-420a-b2a5-dac2641787e0 | d8b987d5-2c6f-495b-804b-a947891904ae | 78e036d3-d28a-4ffc-bbbc-af8051c3de9d |           1 | 2017-11-12 13:12:12+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = 78e036d3-d28a-4ffc-bbbc-af8051c3de9d , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 22548578304 ,  actual size = 1536\n    For image_guid = c765b42b-4bc6-4e40-b3d4-da99d218524f , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 22548578304 ,  actual size = 886272\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269 | cc9b8799-28e0-416e-937e-772d3b8f7aba | 3f707d5d-66d5-40bc-b171-5d3e2caaf6ee | fa048414-a3e0-44cf-818d-42688a995004 |           1 | 2017-05-04 15:40:00+00 |           2 |             4 | f\n 1956e6be-bcd1-4542-b575-b06fe4c06172 | cc9b8799-28e0-416e-937e-772d3b8f7aba | d8b987d5-2c6f-495b-804b-a947891904ae | 4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269 |           1 | 2017-11-12 13:12:13+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = 4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n    For image_guid = 1956e6be-bcd1-4542-b575-b06fe4c06172 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 203908608\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------\n d8b987d5-2c6f-495b-804b-a947891904ae | 0392d189-510d-4f56-9cf9-166c3d2a9ae6 | ACTIVE        | OK     | Active VM                    | 2017-05-04 07:42:12.671+00\n 3f707d5d-66d5-40bc-b171-5d3e2caaf6ee | 0392d189-510d-4f56-9cf9-166c3d2a9ae6 | REGULAR       | OK     | Backup__2017-11-12__13.12.07 | 2017-11-12 13:12:13.058+00\n(2 rows)\n\n \nbase_disks:\n               disk_id                | wipe_after_delete | propagate_errors |                   disk_alias                   | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+------------------------------------------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n a6695490-092c-420a-b2a5-dac2641787e0 | f                 | Off              | java05.ad01.hls.hulu.clt1.prod.mlbam.net_Disk1 |                  | f         |      |         0 |                     |                 0 | \n(1 row)\n\n               disk_id                | wipe_after_delete | propagate_errors | disk_alias | disk_description | shareable | sgio | alignment | last_alignment_scan | disk_storage_type | cinder_volume_type \n--------------------------------------+-------------------+------------------+------------+------------------+-----------+------+-----------+---------------------+-------------------+--------------------\n cc9b8799-28e0-416e-937e-772d3b8f7aba | f                 | Off              | rootdisk   |                  | f         |      |         0 |                     |                 0 | \n(1 row)\n\n \ndisk_vm_element:\n               disk_id                |                vm_id                 | is_boot | disk_interface \n--------------------------------------+--------------------------------------+---------+----------------\n a6695490-092c-420a-b2a5-dac2641787e0 | 0392d189-510d-4f56-9cf9-166c3d2a9ae6 | f       | VirtIO\n(1 row)\n\n               disk_id                |                vm_id                 | is_boot | disk_interface \n--------------------------------------+--------------------------------------+---------+----------------\n cc9b8799-28e0-416e-937e-772d3b8f7aba | 0392d189-510d-4f56-9cf9-166c3d2a9ae6 | t       | VirtIO\n(1 row)\n~~~\n\njava05.ad01.hls.hulu.clt1.prod.mlbam.net\n\n\n~~~\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0392d189-510d-4f56-9cf9-166c3d2a9ae6' AND snapshot_id != 'd8b987d5-2c6f-495b-804b-a947891904ae';"\n\nFIXING DISK a6695490-092c-420a-b2a5-dac2641787e0, CHAIN ['78e036d3-d28a-4ffc-bbbc-af8051c3de9d', 'c765b42b-4bc6-4e40-b3d4-da99d218524f']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/78e036d3-d28a-4ffc-bbbc-af8051c3de9d -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/78e036d3-d28a-4ffc-bbbc-af8051c3de9d.meta"\nlvchange -ay /dev/ovirt-local/78e036d3-d28a-4ffc-bbbc-af8051c3de9d\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/78e036d3-d28a-4ffc-bbbc-af8051c3de9d -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('c765b42b-4bc6-4e40-b3d4-da99d218524f');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d8b987d5-2c6f-495b-804b-a947891904ae' WHERE image_guid ='78e036d3-d28a-4ffc-bbbc-af8051c3de9d';"\n\nFIXING DISK cc9b8799-28e0-416e-937e-772d3b8f7aba, CHAIN ['4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269', '1956e6be-bcd1-4542-b575-b06fe4c06172']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269.meta"\nlvchange -ay /dev/ovirt-local/4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('1956e6be-bcd1-4542-b575-b06fe4c06172');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d8b987d5-2c6f-495b-804b-a947891904ae' WHERE image_guid ='4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269';"\n~~~</text>, <text>logs uploaded. Happy New year to you as well Bimal!\n\nPlease consider this high priority at the moment, the application on the VM needs to be up asap. Thank you</text>, <text>jenkins01.c01.razcp.gen.clt1.prod.mlbam.net\n============================================\n\nThis VM has two disks:\n\tjenkins01.c01.razcp.gen.clt1.prod.mlbam.net_Disk1 (2fd43e49-f4e1-40c2-8f47-9b8999552d97).\n\trootdisk (cb00d3ef-7203-4542-bfaa-87780e0922d6)\n\nThe jenkins01.c01.razcp.gen.clt1.prod.mlbam.net_Disk1 contains single image:\n\t97e6210e-8657-4704-b838-583ce466ac67\nThis is currently writting to NFS share\n\nThe rootdisk (2fd43e49-f4e1-40c2-8f47-9b8999552d97) contains two images:\n\te04de039-88ce-4e2a-8895-508419876e6e (base ovirt-local image)\n\t47317f72-0760-488b-b214-6363997cc7f2 (image on the NFS image)\n\t\nSo will need to fix just the rootdisk images:\n\n~~~\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'de6a5784-32fa-4057-adce-f9faa277498c' AND snapshot_id != '934e7b94-187b-445a-bca9-8e5d9120d91b';"\n\nFIXING DISK cb00d3ef-7203-4542-bfaa-87780e0922d6, CHAIN ['e04de039-88ce-4e2a-8895-508419876e6e', '47317f72-0760-488b-b214-6363997cc7f2']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/e04de039-88ce-4e2a-8895-508419876e6e -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/e04de039-88ce-4e2a-8895-508419876e6e.meta"\nlvchange -ay /dev/ovirt-local/e04de039-88ce-4e2a-8895-508419876e6e\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/e04de039-88ce-4e2a-8895-508419876e6e -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cb00d3ef-7203-4542-bfaa-87780e0922d6/47317f72-0760-488b-b214-6363997cc7f2.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('47317f72-0760-488b-b214-6363997cc7f2');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='934e7b94-187b-445a-bca9-8e5d9120d91b' WHERE image_guid ='e04de039-88ce-4e2a-8895-508419876e6e';"\n~~~\n\n\n\n\n\n\n\njava05.ad01.hls.hulu.clt1.prod.mlbam.net\n==========================================\n\nThis VM has two disks:\n\tjava05.ad01.hls.hulu.clt1.prod.mlbam.net_Disk1 (a6695490-092c-420a-b2a5-dac2641787e0).\n\trootdisk (cc9b8799-28e0-416e-937e-772d3b8f7aba)\n\nThe jenkins01.c01.razcp.gen.clt1.prod.mlbam.net_Disk1 contains two images:\n\t78e036d3-d28a-4ffc-bbbc-af8051c3de9d (base ovirt-local image)\n\tc765b42b-4bc6-4e40-b3d4-da99d218524f (image on the NFS)\n\nThe rootdisk (cc9b8799-28e0-416e-937e-772d3b8f7aba) contains two images:\n\t4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269 (base ovirt-local image)\n\t1956e6be-bcd1-4542-b575-b06fe4c06172 (image on the NFS)\n\nSo will need to fix both disks.\n\n~~~\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0392d189-510d-4f56-9cf9-166c3d2a9ae6' AND snapshot_id != 'd8b987d5-2c6f-495b-804b-a947891904ae';"\n\nFIXING DISK a6695490-092c-420a-b2a5-dac2641787e0, CHAIN ['78e036d3-d28a-4ffc-bbbc-af8051c3de9d', 'c765b42b-4bc6-4e40-b3d4-da99d218524f']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/78e036d3-d28a-4ffc-bbbc-af8051c3de9d -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/78e036d3-d28a-4ffc-bbbc-af8051c3de9d.meta"\nlvchange -ay /dev/ovirt-local/78e036d3-d28a-4ffc-bbbc-af8051c3de9d\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/78e036d3-d28a-4ffc-bbbc-af8051c3de9d -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a6695490-092c-420a-b2a5-dac2641787e0/c765b42b-4bc6-4e40-b3d4-da99d218524f.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('c765b42b-4bc6-4e40-b3d4-da99d218524f');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d8b987d5-2c6f-495b-804b-a947891904ae' WHERE image_guid ='78e036d3-d28a-4ffc-bbbc-af8051c3de9d';"\n\nFIXING DISK cc9b8799-28e0-416e-937e-772d3b8f7aba, CHAIN ['4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269', '1956e6be-bcd1-4542-b575-b06fe4c06172']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269.meta"\nlvchange -ay /dev/ovirt-local/4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/cc9b8799-28e0-416e-937e-772d3b8f7aba/1956e6be-bcd1-4542-b575-b06fe4c06172.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('1956e6be-bcd1-4542-b575-b06fe4c06172');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d8b987d5-2c6f-495b-804b-a947891904ae' WHERE image_guid ='4f9ce7e1-41ac-45f7-95bf-63bcfdfb7269';"\n~~~</text>, <text>Thanks Bimal, that worked for the jenkins Vm. Ill try the java05 VM now as well.\n\nCan we try this host next ?\ncbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net</text>, <text>Hi Marcus,\n\nCan you get me the sos report of the host where cbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net is running.\nI want to check how the images are layed out in on the ovirt-local and on the NFS.\n\nThanks\nBimal Chollera</text>, <text>update: java05.ad01.hls.hulu.clt1.prod.mlbam.net  , is also OK now. thank you!</text>, <text>Hi Marcus,\n\nCan you get me the sos report of the host where cbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net is running.\nI want to check how the images are layed out in on the ovirt-local and on the NFS.\n\nThanks\nBimal Chollera</text>, <text>Sure Bimal, i've uploaded the sosreport for hyp93 which is where cbdb01.ingest06 is set to run on. THanks</text>, <text>Hi Marcus,\n\nThe sos_report (sosreport-mtorres.01961808-20180105153955.tar.xz) appears to be from hyp24.clt1.prod.mlbam.net host and not from hyp93.\n\n~~~\n$ cat hostname\nhyp24.clt1.prod.mlbam.net\n~~~\n\n~~~\nqemu      8011  4.2  1.6 4911472 4330780 ?     Sl    2017 620:42 /usr/libexec/qemu-kvm -name guest=proxy02.linear02.infra.hulu.clt1.prod.mlbam.net,deb\nqemu      9009  9.2  0.8 4934392 2321896 ?     Sl    2017 1349:20 /usr/libexec/qemu-kvm -name guest=nginx04.event01.hls.wwe.clt1.prod.mlbam.net,debug-\nqemu      9497  0.0  0.1 4821704 476456 ?      Sl    2017   1:54 /usr/libexec/qemu-kvm -name guest=nginx01.live01.hls.gen.clt1.prod.mlbam.net,debug-th\nqemu     11621 11.4  2.4 23883308 6429492 ?    Sl    2017 1652:37 /usr/libexec/qemu-kvm -name guest=seg01.c01.streaming.wwe.clt1.prod.mlbam.net,debug-\n~~~\n\nCan you upload the hyp93 sosreport.\n\nThanks\nBimal</text>, <text>My mistake, here you go... thanks</text>, <text>Hello Marcus,\n\nThis VM is in a different state then from other ones we have fixed.\n\nIt appears the VM was restarted after snapshot was taken hence creating an additional /dev/ovirt-local image on the host.\n\nThe root disk has 2 /dev/ovirt-local images and has additional snapshot images on the NFS share\n\n~~~\nb5566cb3-ef9e-4868-ba69-c788e3de0da0 ovirt-local    1 -wi------- 50.00g  IU_d553a03e-82c2-4c0d-8133-1378a25f2b10,PU_00000000-0000-0000-0000-000000000000,VM_dcc491d1-ec89-47bd-9e09-d5e8016c7ecf\nda60253b-d9c8-47e8-9e13-b4a0a3b09558 ovirt-local    1 -wi------- 50.00g  IU_d553a03e-82c2-4c0d-8133-1378a25f2b10,PU_00000000-0000-0000-0000-000000000000,VM_dcc491d1-ec89-47bd-9e09-d5e8016c7ecf\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10:\ntotal 26701738\n\n-rw-rw----. 416 vdsm kvm  1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 416 vdsm kvm     1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 416 vdsm kvm         262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm      197632 Mar 21  2017 da60253b-d9c8-47e8-9e13-b4a0a3b09558\n-rw-rw----.   1 vdsm kvm     1048576 Mar 21  2017 da60253b-d9c8-47e8-9e13-b4a0a3b09558.lease\n-rw-r--r--.   1 vdsm kvm         264 Nov 11 08:43 da60253b-d9c8-47e8-9e13-b4a0a3b09558.meta\n\n-rw-rw----.   1 vdsm kvm  1410334720 Nov 12 13:06 db8d3c67-7aa8-48c2-b869-badc7daac777\n-rw-rw----.   1 vdsm kvm     1048576 Nov 11 08:43 db8d3c67-7aa8-48c2-b869-badc7daac777.lease\n-rw-r--r--.   1 vdsm kvm         264 Nov 12 11:43 db8d3c67-7aa8-48c2-b869-badc7daac777.meta\n\n-rw-rw----.   1 vdsm kvm 18471452672 Dec 31 22:33 b5566cb3-ef9e-4868-ba69-c788e3de0da0\n-rw-rw----.   1 vdsm kvm     1048576 Nov 12 11:43 b5566cb3-ef9e-4868-ba69-c788e3de0da0.lease\n-rw-r--r--.   1 vdsm kvm         260 Nov 12 11:43 b5566cb3-ef9e-4868-ba69-c788e3de0da0.meta\n~~~\n\nThe cbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net_Disk1 has single /dev/ovirt-local image and additional snapshot images on the NFS share.\n\n~~~\n  6914a6da-c220-427f-828d-846a227013a0 ovirt-local    1 -wi------- 50.00g IU_ed076134-4852-45f3-bd85-9542d6fa580f,PU_00000000-0000-0000-0000-000000000000,VM_dcc491d1-ec89-47bd-9e09-d5e8016c7ecf\n\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f:\ntotal 65729022\n\n-rw-rw----. 1 vdsm kvm 53687091200 Mar 21  2017 6914a6da-c220-427f-828d-846a227013a0\n-rw-rw----. 1 vdsm kvm     1048576 Mar 21  2017 6914a6da-c220-427f-828d-846a227013a0.lease\n-rw-r--r--. 1 vdsm kvm         360 Nov 11 08:43 6914a6da-c220-427f-828d-846a227013a0.meta\n\n-rw-rw----. 1 vdsm kvm     7340032 Nov 12 13:06 464889a1-d0ec-4c8a-8463-fe9670331a73\n-rw-rw----. 1 vdsm kvm     1048576 Nov 11 08:43 464889a1-d0ec-4c8a-8463-fe9670331a73.lease\n-rw-r--r--. 1 vdsm kvm         264 Nov 12 11:43 464889a1-d0ec-4c8a-8463-fe9670331a73.meta\n\n-rw-rw----. 1 vdsm kvm 53693120512 Dec 31 22:33 89d90e17-0bb8-4554-a93f-99bb6c2ec6ee\n-rw-rw----. 1 vdsm kvm     1048576 Nov 12 11:43 89d90e17-0bb8-4554-a93f-99bb6c2ec6ee.lease\n-rw-r--r--. 1 vdsm kvm         260 Nov 12 11:43 89d90e17-0bb8-4554-a93f-99bb6c2ec6ee.meta\n~~~\n\n\nThe cbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net_Disk2 doesn't have any lvm images on the /dev/ovirt-local but has base and snapshot image on the NFS.\nSeems like this disk was added while the VM was up.\n\n~~~\n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 55812faa-887d-43da-a864-cbb4ce7db604 | 89c24839-e341-47ac-88bb-b22ef228ce7e | 4a931b69-0fb9-440c-8a58-4abe8800563b | 00000000-0000-0000-0000-000000000000 |           1 | 2017-11-06 17:35:23+00 |           2 |             5 | f\n 0e9c9965-3b15-4ea8-a4a0-0d6902c2a021 | 89c24839-e341-47ac-88bb-b22ef228ce7e | 23f163e7-4066-49fa-9245-c22eea0861dc | 55812faa-887d-43da-a864-cbb4ce7db604 |           1 | 2017-11-12 11:43:23+00 |           2 |             4 | t\n(2 rows)\n\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e:\ntotal 130982065\n\n-rw-rw----. 1 vdsm kvm 107374182400 Nov  7 08:59 55812faa-887d-43da-a864-cbb4ce7db604\n-rw-rw----. 1 vdsm kvm      1048576 Nov  6 17:35 55812faa-887d-43da-a864-cbb4ce7db604.lease\n-rw-r--r--. 1 vdsm kvm          351 Nov 12 11:43 55812faa-887d-43da-a864-cbb4ce7db604.meta\n\n-rw-rw----. 1 vdsm kvm  53512503296 Nov 23 23:05 0e9c9965-3b15-4ea8-a4a0-0d6902c2a021\n-rw-rw----. 1 vdsm kvm      1048576 Nov 12 11:43 0e9c9965-3b15-4ea8-a4a0-0d6902c2a021.lease\n-rw-r--r--. 1 vdsm kvm          260 Nov 12 11:43 0e9c9965-3b15-4ea8-a4a0-0d6902c2a021.meta\n~~~\n\nWe may need to revert back to the base image for the root disk which might go back to Nov 12.\nAlso do you know if any Volume Groups were created/used in the VM that were using these Physical Volumes?\nIf we collapse or revert back to older image, data might be inconsistent in the VM if any VG existed using the PV's.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Verified with the following code:\n------------------------------------------\novirt-engine-4.1.9-0.2.el7.noarch\nvdsm-4.19.44-1.el7ev.x86_64\n\n\nVerified with the following scenario:\n------------------------------------------\n1. Create VM with local hook\n2. Create snapshot &gt;&gt;&gt;&gt;&gt; fails with message indicating that snapshots are not supported on local hook disks\n\n\nMoving to VERIFIED!</text>, <text>Thanks Bimal, Ill test this cbdb and move forward with the java01/2/3 hosts as well now.</text>, <text>Hi Marcus,\n\nThank you for the update. We will await your reply.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series was retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Advisory RHBA-2018:32164-02 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory</text>, <text>Bug report changed to RELEASE_PENDING status by Errata System.\nAdvisory RHBA-2018:32164-02 has been changed to PUSH_READY status.\nhttps://errata.devel.redhat.com/advisory/32164</text>, <text>Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHBA-2018:0135</text>, <text>Hi Bimal,\ncan we try this VM next ?  \n\ncbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net\n\n\nIt's one of the VMs that has 3 snapshots. It would be a good test for the script. \nI've uploaded a sosreport from that hypervisor</text>, <text>Hi Marcus,\n\nWas this VM shutdown and failed to restart today only?\n\nI see both base and active images on the ovirt-local for couple of the disk for this VM (VM_c1d861e1-35d4-482c-9c1b-7522115bfade)\n\nCan you run the following on the RHV-M host to check if it was restarted before?\n\n~~~\nexport PGPASSWORD=`grep ENGINE_DB_PASSWORD /etc/ovirt-engine/engine.conf.d/10-setup-database.conf | sed 's/ENGINE_DB_PASSWORD="//'| sed 's/"//'`\npsql -U engine -h localhost -c "select log_time,correlation_id, message from audit_log order by log_time asc;" |grep cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net\n~~~</text>, <text>Also from the hyp77 host:\n\nCan you provide the output of the following commands;\n\n~~~\nsu vdsm -s /bin/sh -c 'qemu-img info /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/6adfda46-2a1a-4eb6-9c52-09c88dddfd18'\nsu vdsm -s /bin/sh -c 'qemu-img info /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665'\n~~~\n\nThanks\nBimal</text>, <text>Sure here you go. I'm not sure of the power history of the host. The hypervisr actually failed and was restarted so there is a possibility the VM was attempted to be started back on the host.\n\n[root@rhvm-engine01 ~]# export PGPASSWORD=`grep ENGINE_DB_PASSWORD /etc/ovirt-engine/engine.conf.d/10-setup-database.conf | sed 's/ENGINE_DB_PASSWORD="//'| sed 's/"//'`\n[root@rhvm-engine01 ~]# psql -U engine -h localhost -c "select log_time,correlation_id, message from audit_log order by log_time asc;" |grep cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net\n 2018-01-26 10:36:45.378-05    |                | VM cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net was set to the Unknown status.\n 2018-01-26 10:38:36.457-05    | 4a51e27a       | VM cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net configuration was updated by system.\n 2018-01-26 11:17:15.131-05    | 2d8407cb       | VM cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net configuration was updated by hhack@mlbam.net.\n\n\n\n[root@hyp77 ~]# su vdsm -s /bin/sh -c 'qemu-img info /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/6adfda46-2a1a-4eb6-9c52-09c88dddfd18'\nimage: /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/6adfda46-2a1a-4eb6-9c52-09c88dddfd18\nfile format: raw\nvirtual size: 50G (53687091200 bytes)\ndisk size: 3.2G\n[root@hyp77 ~]# su vdsm -s /bin/sh -c 'qemu-img info /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665'\nimage: /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665\nfile format: qcow2\nvirtual size: 50G (53687091200 bytes)\ndisk size: 34G\ncluster_size: 65536\nbacking file: 6adfda46-2a1a-4eb6-9c52-09c88dddfd18 (actual path: /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/6adfda46-2a1a-4eb6-9c52-09c88dddfd18)\nbacking file format: raw\nFormat specific information:\n    compat: 0.10\n    refcount bits: 16\n[root@hyp77 ~]#</text>, <text>// Internal //\n// Still awaiting data from customer on the qemu-img info:\n\n\n\nFor Disk_1:   DISK 36d5a757-ed83-411e-afb5-6adc86aa9852, CHAIN ['0e7564f2-4a4a-4794-9cba-768261cc8144', 'b2922102-2201-4d6e-a10a-651f259685c2', '927ed8a5-1698-46d2-a216-414b1c5e475d']\nThere are 2 images on the /dev/ovirt-local:  0e7564f2-4a4a-4794-9cba-768261cc8144', '927ed8a5-1698-46d2-a216-414b1c5e475d\nOnly the base image 0e7564f2-4a4a-4794-9cba-768261cc8144 should be on /dev/ovirt-local.\n1.  Quarantine the /dev/ovirt-local/927ed8a5-1698-46d2-a216-414b1c5e475d.\n2.  Merge b2922102-2201-4d6e-a10a-651f259685c2 &lt;&lt;-- 927ed8a5-1698-46d2-a216-414b1c5e475d.\n3.  Rebase b2922102-2201-4d6e-a10a-651f259685c2 to have backing file of /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144.\n4.  Merge /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144 &lt;&lt;-- b2922102-2201-4d6e-a10a-651f259685c2\n5.  Fix the DB.\n\n~~~ \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 0e7564f2-4a4a-4794-9cba-768261cc8144 | 36d5a757-ed83-411e-afb5-6adc86aa9852 | b3df314a-b896-44c9-b463-6bc6a4a2a4f1 | 00000000-0000-0000-0000-000000000000 |           4 | 2017-02-22 09:01:39+00 |           2 |             5 | f\n b2922102-2201-4d6e-a10a-651f259685c2 | 36d5a757-ed83-411e-afb5-6adc86aa9852 | cb7398aa-57ca-41e7-80af-557ba0fdb831 | 0e7564f2-4a4a-4794-9cba-768261cc8144 |           1 | 2017-11-11 09:09:18+00 |           2 |             4 | f\n 927ed8a5-1698-46d2-a216-414b1c5e475d | 36d5a757-ed83-411e-afb5-6adc86aa9852 | 1427630d-48f9-4832-9568-e5f1868d10fc | b2922102-2201-4d6e-a10a-651f259685c2 |           1 | 2017-11-12 10:19:47+00 |           2 |             4 | t\n\n  0e7564f2-4a4a-4794-9cba-768261cc8144 ovirt-local    1 -wi------- 50.00g  IU_36d5a757-ed83-411e-afb5-6adc86aa9852,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade  &lt;&lt;-- Original Base\n  927ed8a5-1698-46d2-a216-414b1c5e475d ovirt-local    1 -wi------- 50.00g  IU_36d5a757-ed83-411e-afb5-6adc86aa9852,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade  &lt;&lt;==== ACTIVE / LEAF image\n\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852:\ntotal 102567510\n-rw-rw----. 1 vdsm kvm 53687091200 Feb 22  2017 0e7564f2-4a4a-4794-9cba-768261cc8144\n-rw-rw----. 1 vdsm kvm     1048576 Feb 22  2017 0e7564f2-4a4a-4794-9cba-768261cc8144.lease\n-rw-r--r--. 1 vdsm kvm         351 Nov 11 09:09 0e7564f2-4a4a-4794-9cba-768261cc8144.meta\n\n-rw-rw----. 1 vdsm kvm 36383293440 Nov 12 13:05 b2922102-2201-4d6e-a10a-651f259685c2\n-rw-rw----. 1 vdsm kvm     1048576 Nov 11 09:09 b2922102-2201-4d6e-a10a-651f259685c2.lease\n-rw-r--r--. 1 vdsm kvm         264 Nov 12 10:19 b2922102-2201-4d6e-a10a-651f259685c2.meta\n\n-rw-rw----. 1 vdsm kvm 47451144192 Jan 26 15:34 927ed8a5-1698-46d2-a216-414b1c5e475d\n-rw-rw----. 1 vdsm kvm     1048576 Nov 12 10:19 927ed8a5-1698-46d2-a216-414b1c5e475d.lease\n-rw-r--r--. 1 vdsm kvm         260 Nov 12 10:19 927ed8a5-1698-46d2-a216-414b1c5e475d.meta\n\nHost Commands:\n==============\n\nlvchange --deltag 'IU_36d5a757-ed83-411e-afb5-6adc86aa9852' /dev/ovirt-local/927ed8a5-1698-46d2-a216-414b1c5e475d\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/927ed8a5-1698-46d2-a216-414b1c5e475d\n\nlvchange --deltag 'VM_c1d861e1-35d4-482c-9c1b-7522115bfade' /dev/ovirt-local/927ed8a5-1698-46d2-a216-414b1c5e475d\n\nlvrename /dev/ovirt-local/927ed8a5-1698-46d2-a216-414b1c5e475d /dev/ovirt-local/_temp_927ed8a5-1698-46d2-a216-414b1c5e475d\n\nlvm lvs -o +tags |grep VM_c1d861e1-35d4-482c-9c1b-7522115bfade \n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/0e7564f2-4a4a-4794-9cba-768261cc8144.meta"\n\nlvchange -ay /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2.orig'\n~~~\n\n\n\nFor Disk_2:   DISK e09d2bfb-2bca-4ab9-8314-389f2337d48e, CHAIN ['6adfda46-2a1a-4eb6-9c52-09c88dddfd18', '1d0b1d5a-d6ea-47d6-b62c-5743f40a4665']\nThere are 1 images on the /dev/ovirt-local:  e09d2bfb-2bca-4ab9-8314-389f2337d48e\nThe base image is not on /dev/ovirt-local.  The LEAF/ACTIVE is 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.\n\n1.  Quarantine the /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.\n2.  Merge 6adfda46-2a1a-4eb6-9c52-09c88dddfd18 &lt;&lt;-- 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.\n3.  Fix the DB.\n\n\nImages on the DB:\n                                                                                                                                                 (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n~~~ \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 6adfda46-2a1a-4eb6-9c52-09c88dddfd18 | e09d2bfb-2bca-4ab9-8314-389f2337d48e | cb7398aa-57ca-41e7-80af-557ba0fdb831 | 00000000-0000-0000-0000-000000000000 |           1 | 2017-11-07 17:38:17+00 |           2 |             5 | f\n 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 | e09d2bfb-2bca-4ab9-8314-389f2337d48e | 1427630d-48f9-4832-9568-e5f1868d10fc | 6adfda46-2a1a-4eb6-9c52-09c88dddfd18 |           1 | 2017-11-12 10:19:45+00 |           2 |             4 | t\n(2 rows)\n\n  1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 ovirt-local    1 -wi------- 50.00g  IU_e09d2bfb-2bca-4ab9-8314-389f2337d48e,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade  &lt;&lt;==== ACTIVE / LEAF image\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e:\ntotal 39356545\n\n-rw-rw----. 1 vdsm kvm 53687091200 Nov 12 13:04 6adfda46-2a1a-4eb6-9c52-09c88dddfd18\n-rw-rw----. 1 vdsm kvm     1048576 Nov  7 17:38 6adfda46-2a1a-4eb6-9c52-09c88dddfd18.lease\n-rw-r--r--. 1 vdsm kvm         351 Nov 12 10:19 6adfda46-2a1a-4eb6-9c52-09c88dddfd18.meta\n\n-rw-rw----. 1 vdsm kvm 29467410432 Jan 26 13:00 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665\n-rw-rw----. 1 vdsm kvm     1048576 Nov 12 10:19 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.lease\n-rw-r--r--. 1 vdsm kvm         260 Nov 12 10:19 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.meta\n\nHost Commands:\n==============\n\nlvchange --deltag 'IU_e09d2bfb-2bca-4ab9-8314-389f2337d48e' /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 \n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 \n\nlvchange --deltag 'VM_c1d861e1-35d4-482c-9c1b-7522115bfade' /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 \n\nlvrename /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 /dev/ovirt-local/_temp_1d0b1d5a-d6ea-47d6-b62c-5743f40a4665\n\nlvm lvs -o +tags |grep VM_c1d861e1-35d4-482c-9c1b-7522115bfade \n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/6adfda46-2a1a-4eb6-9c52-09c88dddfd18 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/6adfda46-2a1a-4eb6-9c52-09c88dddfd18.meta"\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.orig'\n~~~\n\n\n\n\nFor root disk:   DISK a0f63bb3-e641-4248-867c-8c0c6738e8aa, CHAIN ['f04df6f6-e2c6-48a3-b777-dde60817a64b', 'dd0f51ef-39f9-40a5-aaa1-70c2e18b0947', '470e5160-d1d4-4707-b953-572760b69c70'\nThere are 2 images on the /dev/ovirt-local:  f04df6f6-e2c6-48a3-b777-dde60817a64b', '470e5160-d1d4-4707-b953-572760b69c70\nOnly the base image f04df6f6-e2c6-48a3-b777-dde60817a64b should be on /dev/ovirt-local.\n1.  Quarantine the /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70.\n2.  Merge dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 &lt;&lt;-- 470e5160-d1d4-4707-b953-572760b69c70.\n3.  Rebase dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 to have backing file of /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b.\n4.  Merge /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b &lt;&lt;-- dd0f51ef-39f9-40a5-aaa1-70c2e18b0947\n5.  Fix the DB.\n\nImages on the DB:\n                                                                                                                                                 (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n~~~ \n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n f04df6f6-e2c6-48a3-b777-dde60817a64b | a0f63bb3-e641-4248-867c-8c0c6738e8aa | b3df314a-b896-44c9-b463-6bc6a4a2a4f1 | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-02-15 09:00:32+00 |           2 |             4 | f\n dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 | a0f63bb3-e641-4248-867c-8c0c6738e8aa | cb7398aa-57ca-41e7-80af-557ba0fdb831 | f04df6f6-e2c6-48a3-b777-dde60817a64b |           1 | 2017-11-11 09:09:19+00 |           2 |             4 | f\n 470e5160-d1d4-4707-b953-572760b69c70 | a0f63bb3-e641-4248-867c-8c0c6738e8aa | 1427630d-48f9-4832-9568-e5f1868d10fc | dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 |           1 | 2017-11-12 10:19:44+00 |           2 |             4 | t\n\n\n\n  f04df6f6-e2c6-48a3-b777-dde60817a64b ovirt-local    1 -wi------- 50.00g IU_a0f63bb3-e641-4248-867c-8c0c6738e8aa,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade\t&lt;&lt;-- Original Base\n  470e5160-d1d4-4707-b953-572760b69c70 ovirt-local    1 -wi------- 50.00g IU_a0f63bb3-e641-4248-867c-8c0c6738e8aa,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade\t&lt;&lt;==== ACTIVE / LEAF image\n\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa:\ntotal 4897202\n\n-rw-rw----.   1 vdsm kvm     197632 Feb 15  2017 f04df6f6-e2c6-48a3-b777-dde60817a64b\n-rw-rw----.   1 vdsm kvm    1048576 Feb 15  2017 f04df6f6-e2c6-48a3-b777-dde60817a64b.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 11 09:09 f04df6f6-e2c6-48a3-b777-dde60817a64b.meta\n\n-rw-rw----. 416 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 416 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 416 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm  327811072 Nov 12 13:05 dd0f51ef-39f9-40a5-aaa1-70c2e18b0947\n-rw-rw----.   1 vdsm kvm    1048576 Nov 11 09:09 dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 12 10:19 dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.meta\n\n-rw-rw----.   1 vdsm kvm 1740832768 Jan 26 15:34 470e5160-d1d4-4707-b953-572760b69c70\n-rw-rw----.   1 vdsm kvm    1048576 Nov 12 10:19 470e5160-d1d4-4707-b953-572760b69c70.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 10:19 470e5160-d1d4-4707-b953-572760b69c70.meta\n\n\nHost Commands:\n==============\n\nlvchange --deltag 'IU_a0f63bb3-e641-4248-867c-8c0c6738e8aa' /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 \n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 \n\nlvchange --deltag 'VM_c1d861e1-35d4-482c-9c1b-7522115bfade' /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 \n\nlvrename /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 /dev/ovirt-local/_temp_470e5160-d1d4-4707-b953-572760b69c70\n\nlvm lvs -o +tags |grep VM_c1d861e1-35d4-482c-9c1b-7522115bfade \n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/f04df6f6-e2c6-48a3-b777-dde60817a64b.meta"\n\nlvchange -ay /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.orig'\n~~~\n\n\nRHVM Commands:\n=============\n\n~~~\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'c1d861e1-35d4-482c-9c1b-7522115bfade' AND snapshot_id != '1427630d-48f9-4832-9568-e5f1868d10fc';"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('1d0b1d5a-d6ea-47d6-b62c-5743f40a4665');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1427630d-48f9-4832-9568-e5f1868d10fc' WHERE image_guid ='6adfda46-2a1a-4eb6-9c52-09c88dddfd18';"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('b2922102-2201-4d6e-a10a-651f259685c2','927ed8a5-1698-46d2-a216-414b1c5e475d');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1427630d-48f9-4832-9568-e5f1868d10fc' WHERE image_guid ='0e7564f2-4a4a-4794-9cba-768261cc8144';"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('dd0f51ef-39f9-40a5-aaa1-70c2e18b0947','470e5160-d1d4-4707-b953-572760b69c70');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1427630d-48f9-4832-9568-e5f1868d10fc' WHERE image_guid ='f04df6f6-e2c6-48a3-b777-dde60817a64b';"\n~~~</text>, <text>Hi Marcus,\n\nI am not sure what happened with case update. \nI may have accidentally update my WIP info as public comment yesterday instead of the following below.\nSorry for any confusion.\n\n- Bimal.\n\n\n===========================================\nHi Marcus:\n\nThis VM is very different the other we address:\nI would highly suggest taking a backup of the VM data before making changes.\nI have tried to explain the situation of the VM Disk below.\n\nIf you have any questions, please revert back.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS\n\n\n  \n==================================================================\n\nFor Disk_1:   DISK 36d5a757-ed83-411e-afb5-6adc86aa9852, CHAIN ['0e7564f2-4a4a-4794-9cba-768261cc8144', 'b2922102-2201-4d6e-a10a-651f259685c2', '927ed8a5-1698-46d2-a216-414b1c5e475d']\n\nThere are 2 images on the /dev/ovirt-local:  \n0e7564f2-4a4a-4794-9cba-768261cc8144', '927ed8a5-1698-46d2-a216-414b1c5e475d\nOnly the base image 0e7564f2-4a4a-4794-9cba-768261cc8144 should be on /dev/ovirt-local.\n\nFix Plan:\n\n1.  Quarantine the /dev/ovirt-local/927ed8a5-1698-46d2-a216-414b1c5e475d.\n2.  Merge b2922102-2201-4d6e-a10a-651f259685c2 &lt;&lt;-- 927ed8a5-1698-46d2-a216-414b1c5e475d.\n3.  Rebase b2922102-2201-4d6e-a10a-651f259685c2 to have backing file of /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144.\n4.  Merge /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144 &lt;&lt;-- b2922102-2201-4d6e-a10a-651f259685c2\n5.  Fix the DB.\n\n~~~\nRHV-M DB:\n~~~~~~~~~~~\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 0e7564f2-4a4a-4794-9cba-768261cc8144 | 36d5a757-ed83-411e-afb5-6adc86aa9852 | b3df314a-b896-44c9-b463-6bc6a4a2a4f1 | 00000000-0000-0000-0000-000000000000 |           4 | 2017-02-22 09:01:39+00 |           2 |             5 | f\n b2922102-2201-4d6e-a10a-651f259685c2 | 36d5a757-ed83-411e-afb5-6adc86aa9852 | cb7398aa-57ca-41e7-80af-557ba0fdb831 | 0e7564f2-4a4a-4794-9cba-768261cc8144 |           1 | 2017-11-11 09:09:18+00 |           2 |             4 | f\n 927ed8a5-1698-46d2-a216-414b1c5e475d | 36d5a757-ed83-411e-afb5-6adc86aa9852 | 1427630d-48f9-4832-9568-e5f1868d10fc | b2922102-2201-4d6e-a10a-651f259685c2 |           1 | 2017-11-12 10:19:47+00 |           2 |             4 | t\n\nOn the ovirt-local:\n~~~~~~~~~~~~~~~~~~~~\n\n  0e7564f2-4a4a-4794-9cba-768261cc8144 ovirt-local    1 -wi------- 50.00g  IU_36d5a757-ed83-411e-afb5-6adc86aa9852,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade  &lt;&lt;-- Original Base\n  927ed8a5-1698-46d2-a216-414b1c5e475d ovirt-local    1 -wi------- 50.00g  IU_36d5a757-ed83-411e-afb5-6adc86aa9852,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade  &lt;&lt;==== ACTIVE / LEAF image\n\n\nOn the NFS share:\n~~~~~~~~~~~~~~~~~~\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852:\ntotal 102567510\n-rw-rw----. 1 vdsm kvm 53687091200 Feb 22  2017 0e7564f2-4a4a-4794-9cba-768261cc8144\n-rw-rw----. 1 vdsm kvm     1048576 Feb 22  2017 0e7564f2-4a4a-4794-9cba-768261cc8144.lease\n-rw-r--r--. 1 vdsm kvm         351 Nov 11 09:09 0e7564f2-4a4a-4794-9cba-768261cc8144.meta\n\n-rw-rw----. 1 vdsm kvm 36383293440 Nov 12 13:05 b2922102-2201-4d6e-a10a-651f259685c2\n-rw-rw----. 1 vdsm kvm     1048576 Nov 11 09:09 b2922102-2201-4d6e-a10a-651f259685c2.lease\n-rw-r--r--. 1 vdsm kvm         264 Nov 12 10:19 b2922102-2201-4d6e-a10a-651f259685c2.meta\n\n-rw-rw----. 1 vdsm kvm 47451144192 Jan 26 15:34 927ed8a5-1698-46d2-a216-414b1c5e475d\n-rw-rw----. 1 vdsm kvm     1048576 Nov 12 10:19 927ed8a5-1698-46d2-a216-414b1c5e475d.lease\n-rw-r--r--. 1 vdsm kvm         260 Nov 12 10:19 927ed8a5-1698-46d2-a216-414b1c5e475d.meta\n~~~\n\n\nHost Commands:\n==============\n\nlvchange --deltag 'IU_36d5a757-ed83-411e-afb5-6adc86aa9852' /dev/ovirt-local/927ed8a5-1698-46d2-a216-414b1c5e475d\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/927ed8a5-1698-46d2-a216-414b1c5e475d\n\nlvchange --deltag 'VM_c1d861e1-35d4-482c-9c1b-7522115bfade' /dev/ovirt-local/927ed8a5-1698-46d2-a216-414b1c5e475d\n\nlvrename /dev/ovirt-local/927ed8a5-1698-46d2-a216-414b1c5e475d /dev/ovirt-local/_temp_927ed8a5-1698-46d2-a216-414b1c5e475d\n\nlvm lvs -o +tags |grep 36d5a757\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/927ed8a5-1698-46d2-a216-414b1c5e475d.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/0e7564f2-4a4a-4794-9cba-768261cc8144.meta"\n\nlvchange -ay /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/36d5a757-ed83-411e-afb5-6adc86aa9852/b2922102-2201-4d6e-a10a-651f259685c2.orig'\n~~~\n\n==================================================================\n\n\nFor Disk_2:   DISK e09d2bfb-2bca-4ab9-8314-389f2337d48e, CHAIN ['6adfda46-2a1a-4eb6-9c52-09c88dddfd18', '1d0b1d5a-d6ea-47d6-b62c-5743f40a4665']\n\nThere are 1 images on the /dev/ovirt-local:  \ne09d2bfb-2bca-4ab9-8314-389f2337d48e\nThe base image is not on /dev/ovirt-local.  The LEAF/ACTIVE is 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.\n\nFix Plan:\n\n1.  Quarantine the /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.\n2.  Merge 6adfda46-2a1a-4eb6-9c52-09c88dddfd18 &lt;&lt;-- 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.\n3.  Fix the DB.\n\n~~~\n\nRHV-M DB:\n~~~~~~~~~\n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 6adfda46-2a1a-4eb6-9c52-09c88dddfd18 | e09d2bfb-2bca-4ab9-8314-389f2337d48e | cb7398aa-57ca-41e7-80af-557ba0fdb831 | 00000000-0000-0000-0000-000000000000 |           1 | 2017-11-07 17:38:17+00 |           2 |             5 | f\n 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 | e09d2bfb-2bca-4ab9-8314-389f2337d48e | 1427630d-48f9-4832-9568-e5f1868d10fc | 6adfda46-2a1a-4eb6-9c52-09c88dddfd18 |           1 | 2017-11-12 10:19:45+00 |           2 |             4 | t\n(2 rows)\n\n\nOn the ovirt-local:\n~~~~~~~~~~~~~~~~~~~\n\n  1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 ovirt-local    1 -wi------- 50.00g  IU_e09d2bfb-2bca-4ab9-8314-389f2337d48e,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade  &lt;&lt;==== ACTIVE / LEAF image\n\n\nOn the NFS share:\n~~~~~~~~~~~~~~~~~\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e:\ntotal 39356545\n\n-rw-rw----. 1 vdsm kvm 53687091200 Nov 12 13:04 6adfda46-2a1a-4eb6-9c52-09c88dddfd18\n-rw-rw----. 1 vdsm kvm     1048576 Nov  7 17:38 6adfda46-2a1a-4eb6-9c52-09c88dddfd18.lease\n-rw-r--r--. 1 vdsm kvm         351 Nov 12 10:19 6adfda46-2a1a-4eb6-9c52-09c88dddfd18.meta\n\n-rw-rw----. 1 vdsm kvm 29467410432 Jan 26 13:00 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665\n-rw-rw----. 1 vdsm kvm     1048576 Nov 12 10:19 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.lease\n-rw-r--r--. 1 vdsm kvm         260 Nov 12 10:19 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.meta\n~~~\n\nHost Commands:\n==============\n\nlvchange --deltag 'IU_e09d2bfb-2bca-4ab9-8314-389f2337d48e' /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 \n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 \n\nlvchange --deltag 'VM_c1d861e1-35d4-482c-9c1b-7522115bfade' /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 \n\nlvrename /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 /dev/ovirt-local/_temp_1d0b1d5a-d6ea-47d6-b62c-5743f40a4665\n\nlvm lvs -o +tags |grep e09d2bfb\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/6adfda46-2a1a-4eb6-9c52-09c88dddfd18 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/6adfda46-2a1a-4eb6-9c52-09c88dddfd18.meta"\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e09d2bfb-2bca-4ab9-8314-389f2337d48e/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665.orig'\n~~~\n\n==================================================================\n\nFor root disk:   DISK a0f63bb3-e641-4248-867c-8c0c6738e8aa, CHAIN ['f04df6f6-e2c6-48a3-b777-dde60817a64b', 'dd0f51ef-39f9-40a5-aaa1-70c2e18b0947', '470e5160-d1d4-4707-b953-572760b69c70'\n\nThere are 2 images on the /dev/ovirt-local:  \nf04df6f6-e2c6-48a3-b777-dde60817a64b', '470e5160-d1d4-4707-b953-572760b69c70\nOnly the base image f04df6f6-e2c6-48a3-b777-dde60817a64b should be on /dev/ovirt-local.\n\nFix Plan:\n\n1.  Quarantine the /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70.\n2.  Merge dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 &lt;&lt;-- 470e5160-d1d4-4707-b953-572760b69c70.\n3.  Rebase dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 to have backing file of /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b.\n4.  Merge /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b &lt;&lt;-- dd0f51ef-39f9-40a5-aaa1-70c2e18b0947\n5.  Fix the DB.\n\n~~~\nRHV-M DB:\n  \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n f04df6f6-e2c6-48a3-b777-dde60817a64b | a0f63bb3-e641-4248-867c-8c0c6738e8aa | b3df314a-b896-44c9-b463-6bc6a4a2a4f1 | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-02-15 09:00:32+00 |           2 |             4 | f\n dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 | a0f63bb3-e641-4248-867c-8c0c6738e8aa | cb7398aa-57ca-41e7-80af-557ba0fdb831 | f04df6f6-e2c6-48a3-b777-dde60817a64b |           1 | 2017-11-11 09:09:19+00 |           2 |             4 | f\n 470e5160-d1d4-4707-b953-572760b69c70 | a0f63bb3-e641-4248-867c-8c0c6738e8aa | 1427630d-48f9-4832-9568-e5f1868d10fc | dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 |           1 | 2017-11-12 10:19:44+00 |           2 |             4 | t\n\nOn the ovirt-local:\n~~~~~~~~~~~~~~~~~~~\n\n  f04df6f6-e2c6-48a3-b777-dde60817a64b ovirt-local    1 -wi------- 50.00g IU_a0f63bb3-e641-4248-867c-8c0c6738e8aa,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade\t&lt;&lt;-- Original Base\n  470e5160-d1d4-4707-b953-572760b69c70 ovirt-local    1 -wi------- 50.00g IU_a0f63bb3-e641-4248-867c-8c0c6738e8aa,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade\t&lt;&lt;==== ACTIVE / LEAF image\n\n\nOn the NFS share:\n~~~~~~~~~~~~~~~~~\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa:\ntotal 4897202\n\n-rw-rw----.   1 vdsm kvm     197632 Feb 15  2017 f04df6f6-e2c6-48a3-b777-dde60817a64b\n-rw-rw----.   1 vdsm kvm    1048576 Feb 15  2017 f04df6f6-e2c6-48a3-b777-dde60817a64b.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 11 09:09 f04df6f6-e2c6-48a3-b777-dde60817a64b.meta\n\n-rw-rw----. 416 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 416 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 416 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm  327811072 Nov 12 13:05 dd0f51ef-39f9-40a5-aaa1-70c2e18b0947\n-rw-rw----.   1 vdsm kvm    1048576 Nov 11 09:09 dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 12 10:19 dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.meta\n\n-rw-rw----.   1 vdsm kvm 1740832768 Jan 26 15:34 470e5160-d1d4-4707-b953-572760b69c70\n-rw-rw----.   1 vdsm kvm    1048576 Nov 12 10:19 470e5160-d1d4-4707-b953-572760b69c70.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 10:19 470e5160-d1d4-4707-b953-572760b69c70.meta\n~~~\n\nHost Commands:\n==============\n\nlvchange --deltag 'IU_a0f63bb3-e641-4248-867c-8c0c6738e8aa' /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 \n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 \n\nlvchange --deltag 'VM_c1d861e1-35d4-482c-9c1b-7522115bfade' /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 \n\nlvrename /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 /dev/ovirt-local/_temp_470e5160-d1d4-4707-b953-572760b69c70\n\nlvm lvs -o +tags |grep a0f63bb3 \n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/470e5160-d1d4-4707-b953-572760b69c70.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/f04df6f6-e2c6-48a3-b777-dde60817a64b.meta"\n\nlvchange -ay /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a0f63bb3-e641-4248-867c-8c0c6738e8aa/dd0f51ef-39f9-40a5-aaa1-70c2e18b0947.orig'\n~~~\n\n\nRHVM Commands:\n=============\n\n~~~\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'c1d861e1-35d4-482c-9c1b-7522115bfade' AND snapshot_id != '1427630d-48f9-4832-9568-e5f1868d10fc';"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('1d0b1d5a-d6ea-47d6-b62c-5743f40a4665');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1427630d-48f9-4832-9568-e5f1868d10fc' WHERE image_guid ='6adfda46-2a1a-4eb6-9c52-09c88dddfd18';"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('b2922102-2201-4d6e-a10a-651f259685c2','927ed8a5-1698-46d2-a216-414b1c5e475d');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1427630d-48f9-4832-9568-e5f1868d10fc' WHERE image_guid ='0e7564f2-4a4a-4794-9cba-768261cc8144';"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('dd0f51ef-39f9-40a5-aaa1-70c2e18b0947','470e5160-d1d4-4707-b953-572760b69c70');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1427630d-48f9-4832-9568-e5f1868d10fc' WHERE image_guid ='f04df6f6-e2c6-48a3-b777-dde60817a64b';"\n~~~</text>, <text>Hello Bimal!\nI've attached the output of all the commands for the cbdb VM you gave me the fix commands for. The only issue was the renaming of a volume to a temp name (search for 'not found in volume group' ). That error didn't seem to be a show stopper, it allowed me to go ahead with the other commands. At first glance all seems well with the VM.  I have the application team verifying the application now and ill let you know if they give a thumbs up!\n\n\n[root@hyp77 ~]# virsh -r list\n Id    Name                           State\n----------------------------------------------------\n 1     cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net running\n\n\n[root@hyp77 ~]# virsh -r domblklist cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net\nTarget     Source\n------------------------------------------------\nhdc        -\nvda        /dev/ovirt-local/f04df6f6-e2c6-48a3-b777-dde60817a64b\nvdb        /dev/ovirt-local/0e7564f2-4a4a-4794-9cba-768261cc8144\nvdc        /dev/ovirt-local/6adfda46-2a1a-4eb6-9c52-09c88dddfd18\n\n\n[root@hyp77 ~]# ping cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net\nPING cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net (10.34.116.47) 56(84) bytes of data.\n64 bytes from cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net (10.34.116.47): icmp_seq=1 ttl=63 time=0.145 ms\n64 bytes from cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net (10.34.116.47): icmp_seq=2 ttl=63 time=0.144 ms\n64 bytes from cbdb01.ingest03.hls.hulu.clt1.prod.mlbam.net (10.34.116.47): icmp_seq=3 ttl=63 time=0.142 ms</text>, <text>Hi Marcus,\n\nOk - I see. Typo in my cut/paste.  Sorry about that.\n\n~~~\n[root@hyp77 ~]# lvrename /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 /dev/ovirt-local/_temp_470e5160-d1d4-4707-b953-572760b69c70\n\n  Existing logical volume "1d0b1d5a-d6ea-47d6-b62c-5743f40a4665" not found in volume group "ovirt-local"\n~~~\n\nThe 1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 was already rename for the other disk:\n\n~~~\n[root@hyp77 ~]# lvrename /dev/ovirt-local/1d0b1d5a-d6ea-47d6-b62c-5743f40a4665 /dev/ovirt-local/_temp_1d0b1d5a-d6ea-47d6-b62c-5743f40a4665\n  Renamed "1d0b1d5a-d6ea-47d6-b62c-5743f40a4665" to "_temp_1d0b1d5a-d6ea-47d6-b62c-5743f40a4665" in volume group "ovirt-local"\n~~~\n\nShould have been:\n\n~~~\n# lvrename /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 /dev/ovirt-local/_temp_470e5160-d1d4-4707-b953-572760b69c70\n~~~\n\nBut no harm was done.  As you already removed the tags of /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70.\nAnd the lvm lvs only reported the base disk.\n\n\n\n~~~\n[root@hyp77 ~]# lvchange --deltag 'IU_a0f63bb3-e641-4248-867c-8c0c6738e8aa' /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70\n  Logical volume ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 changed.\n[root@hyp77 ~]# lvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70\n  Logical volume ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 changed.\n[root@hyp77 ~]# lvchange --deltag 'VM_c1d861e1-35d4-482c-9c1b-7522115bfade' /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70\n  Logical volume ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 changed.\n\n\n[root@hyp77 ~]# lvm lvs -o +tags |grep a0f63bb3\n  f04df6f6-e2c6-48a3-b777-dde60817a64b       ovirt-local -wi-------  50.00g  IU_a0f63bb3-e641-4248-867c-8c0c6738e8aa,PU_00000000-0000-0000-0000-000000000000,VM_c1d861e1-35d4-482c-9c1b-7522115bfade\n~~~\n\n\nIf you check on hyp77, with lvm command, you will probably see it there without the tags.\n\n~~~\n# lvm lvs -o +tags |grep 470e5160\n~~~\n\nThen run, to quarantine it:\n\n~~~\n# lvrename /dev/ovirt-local/470e5160-d1d4-4707-b953-572760b69c70 /dev/ovirt-local/_temp_470e5160-d1d4-4707-b953-572760b69c70\n# lvm lvs -o +tags |grep temp\n~~~\n\nWhen all is verify you can later do lvmremove on the _temp_'s.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc\nGSS</text>, <text>Great Thanks Bimal!\n\nSo now that we're comfortable that the cbdb VM fix worked ok... can we fix the remaining VM's that are living on hyp77 ? These haven't been powered on yet either....\n\ncbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net\ncbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net\njava07.ad01.hls.hulu.clt1.prod.mlbam.net\nmf07.c01.web.wwe.clt1.prod.bamtech.co\nproxy01.c01.infra.gen.clt1.util.mlbam.net\nxcoder01.c01.streaming.gen.clt1.dev.mlbam.net</text>, <text>Thanks Marcus,\n\nAllow me some time to review these VM's.\n\n- Bimal.</text>, <text>Of course. Thank yoU!\n\n\n\n\nOn 2/6/18, 12:45 PM, "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>Hi Marcus:\n\n\n\n1.\tjava07.ad01.hls.hulu.clt1.prod.mlbam.net (VM_ID f13e6be0-3deb-497e-9948-2f3344916dfa) only has 1 ACTIVE snapshot.  There are no other snapshot's for this VM\n\tIt contains 2 disk and each of the images are on the /dev/ovirt-local.\n\tSeems to be in correct state.  \n\n~~~\n74808296-db3b-44b7-8a45-2b193f546683 ovirt-local    1 -wi------- 50.00g IU_1626af4a-c25e-4506-ab9a-afdec0c58bb0,PU_00000000-0000-0000-0000-000000000000,VM_f13e6be0-3deb-497e-9948-2f3344916dfa\n\n05fa416c-07ba-49de-8ec4-6a98e1036f96 ovirt-local    1 -wi------- 21.00g IU_f1e1dc0a-a72a-4c2e-add1-48969c2a9551,PU_00000000-0000-0000-0000-000000000000,VM_f13e6be0-3deb-497e-9948-2f3344916dfa\n~~~\n\n\n\n2.\tmf07.c01.web.wwe.clt1.prod.bamtech.co:  \n\troot_Disk1 29a297e0-7ec9-49e8-b833-11be51935d1e, CHAIN ['704a3981-c062-4441-b3bb-5cd40dd93dd8', 'f564ab61-afa3-45c0-805a-6fa2349a8f15']\n\n\tThere are 2 images on the /dev/ovirt-local:  \n\t'704a3981-c062-4441-b3bb-5cd40dd93dd8', 'f564ab61-afa3-45c0-805a-6fa2349a8f15'\n\tOnly the base image 704a3981-c062-4441-b3bb-5cd40dd93dd8 should be on /dev/ovirt-local.\n\n~~~~\n  704a3981-c062-4441-b3bb-5cd40dd93dd8 ovirt-local    1 -wi------- 50.00g  IU_29a297e0-7ec9-49e8-b833-11be51935d1e,PU_00000000-0000-0000-0000-000000000000,VM_b66c3e27-da40-4433-b830-330f6498f60e\n  f564ab61-afa3-45c0-805a-6fa2349a8f15 ovirt-local    1 -wi------- 50.00g  IU_29a297e0-7ec9-49e8-b833-11be51935d1e,PU_00000000-0000-0000-0000-000000000000,VM_b66c3e27-da40-4433-b830-330f6498f60e\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e:\ntotal 2264129\n-rw-rw----. 1 vdsm kvm 1668546560 Oct 19 19:39 704a3981-c062-4441-b3bb-5cd40dd93dd8\n-rw-rw----. 1 vdsm kvm    1048576 Oct 19 19:39 704a3981-c062-4441-b3bb-5cd40dd93dd8.lease\n-rw-r--r--. 1 vdsm kvm        264 Nov 11 03:30 704a3981-c062-4441-b3bb-5cd40dd93dd8.meta\n-rw-rw----. 1 vdsm kvm  180420608 Jan 26 15:34 f564ab61-afa3-45c0-805a-6fa2349a8f15\n-rw-rw----. 1 vdsm kvm    1048576 Nov 11 03:30 f564ab61-afa3-45c0-805a-6fa2349a8f15.lease\n-rw-r--r--. 1 vdsm kvm        260 Nov 11 03:30 f564ab61-afa3-45c0-805a-6fa2349a8f15.meta\n~~~\n\n\nFix Plan:\n\n1.  Quarantine the /dev/ovirt-local/f564ab61-afa3-45c0-805a-6fa2349a8f15.\n2.  Merge 704a3981-c062-4441-b3bb-5cd40dd93dd8 &lt;&lt;-- f564ab61-afa3-45c0-805a-6fa2349a8f15.\n3.  Rebase f564ab61-afa3-45c0-805a-6fa2349a8f15 to have backing file of /dev/ovirt-local/704a3981-c062-4441-b3bb-5cd40dd93dd8.\n4.  Merge /dev/ovirt-local/704a3981-c062-4441-b3bb-5cd40dd93dd8 &lt;&lt;-- f564ab61-afa3-45c0-805a-6fa2349a8f15\n5.  Fix the DB.\n\n\n~~~\nHost Commands:\n==============\n\nlvchange --deltag 'IU_29a297e0-7ec9-49e8-b833-11be51935d1e' /dev/ovirt-local/f564ab61-afa3-45c0-805a-6fa2349a8f15\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/f564ab61-afa3-45c0-805a-6fa2349a8f15\n\nlvchange --deltag 'VM_b66c3e27-da40-4433-b830-330f6498f60e' /dev/ovirt-local/f564ab61-afa3-45c0-805a-6fa2349a8f15\n\nlvrename /dev/ovirt-local/f564ab61-afa3-45c0-805a-6fa2349a8f15 /dev/ovirt-local/_temp_f564ab61-afa3-45c0-805a-6fa2349a8f15\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/704a3981-c062-4441-b3bb-5cd40dd93dd8 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/704a3981-c062-4441-b3bb-5cd40dd93dd8.meta"\nlvchange -ay /dev/ovirt-local/704a3981-c062-4441-b3bb-5cd40dd93dd8\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/704a3981-c062-4441-b3bb-5cd40dd93dd8 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'b66c3e27-da40-4433-b830-330f6498f60e' AND snapshot_id != '5380ab1a-b6ed-45cd-a828-80941858d520';"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('f564ab61-afa3-45c0-805a-6fa2349a8f15');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='5380ab1a-b6ed-45cd-a828-80941858d520' WHERE image_guid ='704a3981-c062-4441-b3bb-5cd40dd93dd8';"\n~~~\n\n\n\n\n3.  \tcbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net (VM_ce42843f-48da-4a18-a6f8-4c2b7fd07718) is listed as down per the RHV-M DB we have.\n\n\tIt shows the VM last ran on:\n\n~~~\n2017-11-19 11:05:48.9+00 Failed to power off VM cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net (Host: hyp77.clt1.prod.mlbam.net, User: inascimento@mlbam.net)\n~~~\n\n\tThe hyp77.clt1.prod.mlbam.net logs from 2017/11/19 contain the images/volume information about this VM but the new logs from 2018/01/26 don't have any of the images listed.\n\n\tCan you run the following command on hyp77.clt1.prod.mlbam.net  host to see that it reports for this VM?\n\n~~~\n# lvm lvs -o +tags |grep ed05679c\n\n# lvm lvs -o +tags |grep db06154a\n\n# lvm lvs -o +tags |grep 517749d7\n\n# ls -ltr /rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed05679c-dfb9-4fd5-bfe1-1c2dbe4176d0/*\n\n# ls -ltr /rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/db06154a-1191-45ac-9a79-0f03d5131ec5/*\n\n# ls -ltr /rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/517749d7-0e48-48cf-be9c-1c3480b6b87a/*\n~~~\n\n\n4.\tWe don't have sos_report for the following hosts:\n\n~~~\nproxy01.c01.infra.gen.clt1.util.mlbam.net  Running on host hyp11.clt1.prod.mlbam.net\n\ncbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net Running on host hyp93.clt1.prod.mlbam.net\nxcoder01.c01.streaming.gen.clt1.dev.mlbam.net hyp93.clt1.prod.mlbam.net\n~~~\n\nLet me know if you have any questions.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc\nGSS</text>, <text>thanks Bimal, i didnt realize those last 2 hosts werent running on hyp77. I've uplaoded the sosreport for hyp93 thank you! xcoder vm is the priority for this round as they're waiting for that VM to come back online thank you!in the meantime ill work through the fixes for the hosts you provided me. thanks</text>, <text>Hi Marcus,\n\nTo fix xcoder01.c01.streaming.gen.clt1.dev.mlbam.net.\nAs always please take a backup of the VM as precaution incase of problems.\n\n- Bimal\n\n\n~~~\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '51d9930d-9c1e-4742-b4c4-c48fdae987f7' AND snapshot_id != '29a8c9a2-332c-41f1-88b8-1afadd7572ba';"\n\nFIXING DISK 084efd75-6db6-4fc9-94bf-f1820641de14, CHAIN ['30624599-8038-44a3-b22c-77f375fb96ab', '18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/30624599-8038-44a3-b22c-77f375fb96ab -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/084efd75-6db6-4fc9-94bf-f1820641de14/18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/084efd75-6db6-4fc9-94bf-f1820641de14/30624599-8038-44a3-b22c-77f375fb96ab.meta"\n\nlvchange -ay /dev/ovirt-local/30624599-8038-44a3-b22c-77f375fb96ab\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/30624599-8038-44a3-b22c-77f375fb96ab -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/084efd75-6db6-4fc9-94bf-f1820641de14/18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/084efd75-6db6-4fc9-94bf-f1820641de14/18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/084efd75-6db6-4fc9-94bf-f1820641de14/18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/084efd75-6db6-4fc9-94bf-f1820641de14/18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/084efd75-6db6-4fc9-94bf-f1820641de14/18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/084efd75-6db6-4fc9-94bf-f1820641de14/18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/084efd75-6db6-4fc9-94bf-f1820641de14/18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='29a8c9a2-332c-41f1-88b8-1afadd7572ba' WHERE image_guid ='30624599-8038-44a3-b22c-77f375fb96ab';"\n~~~\n\n\n\n\nINVESTIGATION:\n~~~~~~~~~~~~~~~\n// Working Notes - these notes are not intended as a meaningful communication\n// but rather an indicator of current thought processes and reference.\n// Please feel free to comment and ask questions concerning them.\n\n\n[I1]\tImage and snapshot info in the RHV-M DB.\n\n~~~\nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 30624599-8038-44a3-b22c-77f375fb96ab | 084efd75-6db6-4fc9-94bf-f1820641de14 | 7a666e57-4919-4ed8-864d-7c3eadf60994 | 20f9e080-c6c5-45dc-be72-27c33ba85d45 |           4 | 2017-07-06 19:49:29+00 |           2 |             4 | f\n 18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb | 084efd75-6db6-4fc9-94bf-f1820641de14 | 29a8c9a2-332c-41f1-88b8-1afadd7572ba | 30624599-8038-44a3-b22c-77f375fb96ab |           1 | 2017-11-12 00:38:57+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = 30624599-8038-44a3-b22c-77f375fb96ab , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n    For image_guid = 18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 5545428480\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        | memory_volume \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------+---------------\n 29a8c9a2-332c-41f1-88b8-1afadd7572ba | 51d9930d-9c1e-4742-b4c4-c48fdae987f7 | ACTIVE        | OK     | Active VM                    | 2017-07-06 19:49:27.305+00 | \n 7a666e57-4919-4ed8-864d-7c3eadf60994 | 51d9930d-9c1e-4742-b4c4-c48fdae987f7 | REGULAR       | OK     | Backup__2017-11-12__00.38.51 | 2017-11-12 00:38:56.639+00 | \n(2 rows)\n~~~\n\n[I2]\tOnly the base 30624599 is on /dev/ovirt-local on the host\n\n~~~\n  30624599-8038-44a3-b22c-77f375fb96ab ovirt-local    1 -wi------- 50.00g IU_084efd75-6db6-4fc9-94bf-f1820641de14,PU_00000000-0000-0000-0000-000000000000,VM_51d9930d-9c1e-4742-b4c4-c48fdae987f7\n~~~\n\n[I3]\tOn the NFS. \n\tContains the LEAF/ACTIVE images 18b3c4a8\n\n~~~\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/084efd75-6db6-4fc9-94bf-f1820641de14:\ntotal 7954878\n\n-rw-rw----. 74 vdsm kvm 1662844928 May 31  2017 20f9e080-c6c5-45dc-be72-27c33ba85d45\n-rw-rw----. 74 vdsm kvm    1048576 May 31  2017 20f9e080-c6c5-45dc-be72-27c33ba85d45.lease\n-rw-r--r--. 74 vdsm kvm        262 May 31  2017 20f9e080-c6c5-45dc-be72-27c33ba85d45.meta\n\n-rw-rw----.  1 vdsm kvm     197632 Jul  6  2017 30624599-8038-44a3-b22c-77f375fb96ab\n-rw-rw----.  1 vdsm kvm    1048576 Jul  6  2017 30624599-8038-44a3-b22c-77f375fb96ab.lease\n-rw-r--r--.  1 vdsm kvm        264 Nov 12 00:38 30624599-8038-44a3-b22c-77f375fb96ab.meta\n\n-rw-rw----.  1 vdsm kvm 4835639296 Dec 31 22:33 18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb\n-rw-rw----.  1 vdsm kvm    1048576 Nov 12 00:38 18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb.lease\n-rw-r--r--.  1 vdsm kvm        260 Nov 12 00:38 18b3c4a8-1f60-41a0-8543-c0c2bbb27ceb.meta\n~~~</text>, <text>Hi Bimal,\nFor hosts java07 and mf07 on  Feb 06 2018 at 02:26 PM -05:00, you wrote a set of commands... is that set of commands taking care of both VMs ?</text>, <text>Hi Marcus,\n\n1.\tjava07.ad01.hls.hulu.clt1.prod.mlbam.net (VM_ID f13e6be0-3deb-497e-9948-2f3344916dfa) only has 1 ACTIVE snapshot.  There are no other snapshot's for this VM\n\tIt contains 2 disk and each of the images are on the /dev/ovirt-local.\n\tSeems to be in correct state.  \n\tSo there is nothing to fix this.\n\n~~~\n74808296-db3b-44b7-8a45-2b193f546683 ovirt-local    1 -wi------- 50.00g IU_1626af4a-c25e-4506-ab9a-afdec0c58bb0,PU_00000000-0000-0000-0000-000000000000,VM_f13e6be0-3deb-497e-9948-2f3344916dfa\n\n05fa416c-07ba-49de-8ec4-6a98e1036f96 ovirt-local    1 -wi------- 21.00g IU_f1e1dc0a-a72a-4c2e-add1-48969c2a9551,PU_00000000-0000-0000-0000-000000000000,VM_f13e6be0-3deb-497e-9948-2f3344916dfa\n~~~\n\n\n2.\tmf07.c01.web.wwe.clt1.prod.bamtech.co:  \n\troot_Disk1 29a297e0-7ec9-49e8-b833-11be51935d1e, CHAIN ['704a3981-c062-4441-b3bb-5cd40dd93dd8', 'f564ab61-afa3-45c0-805a-6fa2349a8f15']\n\n\tThere are 2 images on the /dev/ovirt-local:  \n\t'704a3981-c062-4441-b3bb-5cd40dd93dd8', 'f564ab61-afa3-45c0-805a-6fa2349a8f15'\n\tOnly the base image 704a3981-c062-4441-b3bb-5cd40dd93dd8 should be on /dev/ovirt-local.\n\n~~~~\n  704a3981-c062-4441-b3bb-5cd40dd93dd8 ovirt-local    1 -wi------- 50.00g  IU_29a297e0-7ec9-49e8-b833-11be51935d1e,PU_00000000-0000-0000-0000-000000000000,VM_b66c3e27-da40-4433-b830-330f6498f60e\n  f564ab61-afa3-45c0-805a-6fa2349a8f15 ovirt-local    1 -wi------- 50.00g  IU_29a297e0-7ec9-49e8-b833-11be51935d1e,PU_00000000-0000-0000-0000-000000000000,VM_b66c3e27-da40-4433-b830-330f6498f60e\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e:\ntotal 2264129\n-rw-rw----. 1 vdsm kvm 1668546560 Oct 19 19:39 704a3981-c062-4441-b3bb-5cd40dd93dd8\n-rw-rw----. 1 vdsm kvm    1048576 Oct 19 19:39 704a3981-c062-4441-b3bb-5cd40dd93dd8.lease\n-rw-r--r--. 1 vdsm kvm        264 Nov 11 03:30 704a3981-c062-4441-b3bb-5cd40dd93dd8.meta\n-rw-rw----. 1 vdsm kvm  180420608 Jan 26 15:34 f564ab61-afa3-45c0-805a-6fa2349a8f15\n-rw-rw----. 1 vdsm kvm    1048576 Nov 11 03:30 f564ab61-afa3-45c0-805a-6fa2349a8f15.lease\n-rw-r--r--. 1 vdsm kvm        260 Nov 11 03:30 f564ab61-afa3-45c0-805a-6fa2349a8f15.meta\n~~~\n\n\nFix Plan:\n\n1.  Quarantine the /dev/ovirt-local/f564ab61-afa3-45c0-805a-6fa2349a8f15.\n2.  Merge 704a3981-c062-4441-b3bb-5cd40dd93dd8 &lt;&lt;-- f564ab61-afa3-45c0-805a-6fa2349a8f15.\n3.  Rebase f564ab61-afa3-45c0-805a-6fa2349a8f15 to have backing file of /dev/ovirt-local/704a3981-c062-4441-b3bb-5cd40dd93dd8.\n4.  Merge /dev/ovirt-local/704a3981-c062-4441-b3bb-5cd40dd93dd8 &lt;&lt;-- f564ab61-afa3-45c0-805a-6fa2349a8f15\n5.  Fix the DB.\n\n\n~~~\nHost Commands:\n==============\n\nlvchange --deltag 'IU_29a297e0-7ec9-49e8-b833-11be51935d1e' /dev/ovirt-local/f564ab61-afa3-45c0-805a-6fa2349a8f15\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/f564ab61-afa3-45c0-805a-6fa2349a8f15\n\nlvchange --deltag 'VM_b66c3e27-da40-4433-b830-330f6498f60e' /dev/ovirt-local/f564ab61-afa3-45c0-805a-6fa2349a8f15\n\nlvrename /dev/ovirt-local/f564ab61-afa3-45c0-805a-6fa2349a8f15 /dev/ovirt-local/_temp_f564ab61-afa3-45c0-805a-6fa2349a8f15\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/704a3981-c062-4441-b3bb-5cd40dd93dd8 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/704a3981-c062-4441-b3bb-5cd40dd93dd8.meta"\nlvchange -ay /dev/ovirt-local/704a3981-c062-4441-b3bb-5cd40dd93dd8\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/704a3981-c062-4441-b3bb-5cd40dd93dd8 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/29a297e0-7ec9-49e8-b833-11be51935d1e/f564ab61-afa3-45c0-805a-6fa2349a8f15.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'b66c3e27-da40-4433-b830-330f6498f60e' AND snapshot_id != '5380ab1a-b6ed-45cd-a828-80941858d520';"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('f564ab61-afa3-45c0-805a-6fa2349a8f15');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='5380ab1a-b6ed-45cd-a828-80941858d520' WHERE image_guid ='704a3981-c062-4441-b3bb-5cd40dd93dd8';"\n~~~\n\n\nThanks\n\n~ Bimal</text>, <text>java07 is good!\nmf07 is good!\nproxy01 is good as well! (this was previously fixed it seems)\nxcoder01 is good!\n\n\nRemanining for this run: (this can wait until next week , doesn't need to be today)....\n    1. cbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net last ran on hyp93 (you have the sosreport for that hypervisor)\n\n   \nAS FOR....\n   2. cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net  = turns out this host was one of the ones we tried to fix (with your help) but in the end, the host failed to rebuild and start properly, it was just too far gone. We rebuilt this VM it turns out, sorry for the confusion. I think we can safely skip this one. All disks seems OK..\n\n\n[root@hyp77 ~]# virsh -r dumpxml cbdb02.ingest04.hls.hulu.clt1.prod.mlbam.net |grep ovirt-local\n      &lt;source dev='/dev/ovirt-local/654a830c-f6b2-4be7-9288-c1e2a5bb5c6d'/&gt;\n      &lt;source dev='/dev/ovirt-local/e4ac971f-2224-496e-8fce-029ded96455b'/&gt;\n      &lt;source dev='/dev/ovirt-local/0971ffeb-9cd6-4720-8e0a-993a456e83c0'/&gt;</text>, <text>cbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net VM:\n\n~~~\nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 55812faa-887d-43da-a864-cbb4ce7db604 | 89c24839-e341-47ac-88bb-b22ef228ce7e | 4a931b69-0fb9-440c-8a58-4abe8800563b | 00000000-0000-0000-0000-000000000000 |           1 | 2017-11-06 17:35:23+00 |           2 |             5 | f\n 0e9c9965-3b15-4ea8-a4a0-0d6902c2a021 | 89c24839-e341-47ac-88bb-b22ef228ce7e | 23f163e7-4066-49fa-9245-c22eea0861dc | 55812faa-887d-43da-a864-cbb4ce7db604 |           1 | 2017-11-12 11:43:23+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = 55812faa-887d-43da-a864-cbb4ce7db604 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 107374182400 ,  actual size = 67065005568\n    For image_guid = 0e9c9965-3b15-4ea8-a4a0-0d6902c2a021 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 107374182400 ,  actual size = 67057903104\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n da60253b-d9c8-47e8-9e13-b4a0a3b09558 | d553a03e-82c2-4c0d-8133-1378a25f2b10 | e78c5b4e-dbbe-4ba5-bbcc-bf378ec972f8 | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-03-20 17:42:56+00 |           2 |             4 | f\n db8d3c67-7aa8-48c2-b869-badc7daac777 | d553a03e-82c2-4c0d-8133-1378a25f2b10 | 4a931b69-0fb9-440c-8a58-4abe8800563b | da60253b-d9c8-47e8-9e13-b4a0a3b09558 |           1 | 2017-11-11 08:43:03+00 |           2 |             4 | f\n b5566cb3-ef9e-4868-ba69-c788e3de0da0 | d553a03e-82c2-4c0d-8133-1378a25f2b10 | 23f163e7-4066-49fa-9245-c22eea0861dc | db8d3c67-7aa8-48c2-b869-badc7daac777 |           1 | 2017-11-12 11:43:25+00 |           2 |             4 | t\n(3 rows)\n\n    For image_guid = b5566cb3-ef9e-4868-ba69-c788e3de0da0 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 22990005760\n    For image_guid = db8d3c67-7aa8-48c2-b869-badc7daac777 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 1540204032\n    For image_guid = da60253b-d9c8-47e8-9e13-b4a0a3b09558 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 6914a6da-c220-427f-828d-846a227013a0 | ed076134-4852-45f3-bd85-9542d6fa580f | e78c5b4e-dbbe-4ba5-bbcc-bf378ec972f8 | 00000000-0000-0000-0000-000000000000 |           4 | 2017-03-20 17:43:50+00 |           2 |             5 | f\n 464889a1-d0ec-4c8a-8463-fe9670331a73 | ed076134-4852-45f3-bd85-9542d6fa580f | 4a931b69-0fb9-440c-8a58-4abe8800563b | 6914a6da-c220-427f-828d-846a227013a0 |           1 | 2017-11-11 08:43:02+00 |           2 |             4 | f\n 89d90e17-0bb8-4554-a93f-99bb6c2ec6ee | ed076134-4852-45f3-bd85-9542d6fa580f | 23f163e7-4066-49fa-9245-c22eea0861dc | 464889a1-d0ec-4c8a-8463-fe9670331a73 |           1 | 2017-11-12 11:43:26+00 |           2 |             4 | t\n(3 rows)\n\n    For image_guid = 89d90e17-0bb8-4554-a93f-99bb6c2ec6ee , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 67293283840\n    For image_guid = 6914a6da-c220-427f-828d-846a227013a0 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 1536\n    For image_guid = 464889a1-d0ec-4c8a-8463-fe9670331a73 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 7243264\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |           description           |       creation_date        | memory_volume \n--------------------------------------+--------------------------------------+---------------+--------+---------------------------------+----------------------------+---------------\n 23f163e7-4066-49fa-9245-c22eea0861dc | dcc491d1-ec89-47bd-9e09-d5e8016c7ecf | ACTIVE        | OK     | Active VM                       | 2017-03-20 17:42:42.204+00 | \n c42343e3-1a91-4ea2-827c-6e19d3e43c28 | dcc491d1-ec89-47bd-9e09-d5e8016c7ecf | NEXT_RUN      | OK     | Next Run configuration snapshot | 2017-11-06 19:10:57.924+00 | \n e78c5b4e-dbbe-4ba5-bbcc-bf378ec972f8 | dcc491d1-ec89-47bd-9e09-d5e8016c7ecf | REGULAR       | OK     | Backup__2017-11-11__08.42.58    | 2017-11-11 08:43:02.974+00 | \n 4a931b69-0fb9-440c-8a58-4abe8800563b | dcc491d1-ec89-47bd-9e09-d5e8016c7ecf | REGULAR       | OK     | Backup__2017-11-12__11.42.58    | 2017-11-12 11:43:27.026+00 | \n(4 rows)\n~~~\n\n~~~\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e:\ntotal 130982065\n\n-rw-rw----. 1 vdsm kvm 107374182400 Nov  7 08:59 55812faa-887d-43da-a864-cbb4ce7db604\n-rw-rw----. 1 vdsm kvm      1048576 Nov  6 17:35 55812faa-887d-43da-a864-cbb4ce7db604.lease\n-rw-r--r--. 1 vdsm kvm          351 Nov 12 11:43 55812faa-887d-43da-a864-cbb4ce7db604.meta\n\n-rw-rw----. 1 vdsm kvm  53512503296 Nov 23 23:05 0e9c9965-3b15-4ea8-a4a0-0d6902c2a021\n-rw-rw----. 1 vdsm kvm      1048576 Nov 12 11:43 0e9c9965-3b15-4ea8-a4a0-0d6902c2a021.lease\n-rw-r--r--. 1 vdsm kvm          260 Nov 12 11:43 0e9c9965-3b15-4ea8-a4a0-0d6902c2a021.meta\n~~~\n\n~~~\n  da60253b-d9c8-47e8-9e13-b4a0a3b09558 ovirt-local    1 -wi------- 50.00g  IU_d553a03e-82c2-4c0d-8133-1378a25f2b10,PU_00000000-0000-0000-0000-000000000000,VM_dcc491d1-ec89-47bd-9e09-d5e8016c7ecf\n\n  b5566cb3-ef9e-4868-ba69-c788e3de0da0 ovirt-local    1 -wi------- 50.00g  IU_d553a03e-82c2-4c0d-8133-1378a25f2b10,PU_00000000-0000-0000-0000-000000000000,VM_dcc491d1-ec89-47bd-9e09-d5e8016c7ecf\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10:\ntotal 26701738\n\n-rw-rw----. 409 vdsm kvm  1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm     1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm         262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm      197632 Mar 21  2017 da60253b-d9c8-47e8-9e13-b4a0a3b09558\n-rw-rw----.   1 vdsm kvm     1048576 Mar 21  2017 da60253b-d9c8-47e8-9e13-b4a0a3b09558.lease\n-rw-r--r--.   1 vdsm kvm         264 Nov 11 08:43 da60253b-d9c8-47e8-9e13-b4a0a3b09558.meta\n\n-rw-rw----.   1 vdsm kvm  1410334720 Nov 12 13:06 db8d3c67-7aa8-48c2-b869-badc7daac777\n-rw-rw----.   1 vdsm kvm     1048576 Nov 11 08:43 db8d3c67-7aa8-48c2-b869-badc7daac777.lease\n-rw-r--r--.   1 vdsm kvm         264 Nov 12 11:43 db8d3c67-7aa8-48c2-b869-badc7daac777.meta\n\n-rw-rw----.   1 vdsm kvm 18471452672 Dec 31 22:33 b5566cb3-ef9e-4868-ba69-c788e3de0da0\n-rw-rw----.   1 vdsm kvm     1048576 Nov 12 11:43 b5566cb3-ef9e-4868-ba69-c788e3de0da0.lease\n-rw-r--r--.   1 vdsm kvm         260 Nov 12 11:43 b5566cb3-ef9e-4868-ba69-c788e3de0da0.meta\n~~~\n\n~~~\n  6914a6da-c220-427f-828d-846a227013a0 ovirt-local    1 -wi------- 50.00g  IU_ed076134-4852-45f3-bd85-9542d6fa580f,PU_00000000-0000-0000-0000-000000000000,VM_dcc491d1-ec89-47bd-9e09-d5e8016c7ecf\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f:\ntotal 65729022\n\n-rw-rw----. 1 vdsm kvm 53687091200 Mar 21  2017 6914a6da-c220-427f-828d-846a227013a0\n-rw-rw----. 1 vdsm kvm     1048576 Mar 21  2017 6914a6da-c220-427f-828d-846a227013a0.lease\n-rw-r--r--. 1 vdsm kvm         360 Nov 11 08:43 6914a6da-c220-427f-828d-846a227013a0.meta\n\n-rw-rw----. 1 vdsm kvm     7340032 Nov 12 13:06 464889a1-d0ec-4c8a-8463-fe9670331a73\n-rw-rw----. 1 vdsm kvm     1048576 Nov 11 08:43 464889a1-d0ec-4c8a-8463-fe9670331a73.lease\n-rw-r--r--. 1 vdsm kvm         264 Nov 12 11:43 464889a1-d0ec-4c8a-8463-fe9670331a73.meta\n\n-rw-rw----. 1 vdsm kvm 53693120512 Dec 31 22:33 89d90e17-0bb8-4554-a93f-99bb6c2ec6ee\n-rw-rw----. 1 vdsm kvm     1048576 Nov 12 11:43 89d90e17-0bb8-4554-a93f-99bb6c2ec6ee.lease\n-rw-r--r--. 1 vdsm kvm         260 Nov 12 11:43 89d90e17-0bb8-4554-a93f-99bb6c2ec6ee.meta\n~~~</text>, <text>Hi Marcus,\n\n\nFor cbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net VM:\n\nAs always please take a backup of the VM as precaution incase of problems.\n\n- Bimal\n\n~~~\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'dcc491d1-ec89-47bd-9e09-d5e8016c7ecf' AND snapshot_id != '23f163e7-4066-49fa-9245-c22eea0861dc';"\n\nFIXING DISK ed076134-4852-45f3-bd85-9542d6fa580f, CHAIN ['6914a6da-c220-427f-828d-846a227013a0', '464889a1-d0ec-4c8a-8463-fe9670331a73', '89d90e17-0bb8-4554-a93f-99bb6c2ec6ee']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/464889a1-d0ec-4c8a-8463-fe9670331a73 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/89d90e17-0bb8-4554-a93f-99bb6c2ec6ee'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/89d90e17-0bb8-4554-a93f-99bb6c2ec6ee.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/89d90e17-0bb8-4554-a93f-99bb6c2ec6ee.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/89d90e17-0bb8-4554-a93f-99bb6c2ec6ee.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/89d90e17-0bb8-4554-a93f-99bb6c2ec6ee.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/89d90e17-0bb8-4554-a93f-99bb6c2ec6ee /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/89d90e17-0bb8-4554-a93f-99bb6c2ec6ee.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/6914a6da-c220-427f-828d-846a227013a0 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/464889a1-d0ec-4c8a-8463-fe9670331a73'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/6914a6da-c220-427f-828d-846a227013a0.meta"\n\nlvchange -ay /dev/ovirt-local/6914a6da-c220-427f-828d-846a227013a0\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/6914a6da-c220-427f-828d-846a227013a0 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/464889a1-d0ec-4c8a-8463-fe9670331a73'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/464889a1-d0ec-4c8a-8463-fe9670331a73.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/464889a1-d0ec-4c8a-8463-fe9670331a73.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/464889a1-d0ec-4c8a-8463-fe9670331a73.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/464889a1-d0ec-4c8a-8463-fe9670331a73.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/464889a1-d0ec-4c8a-8463-fe9670331a73 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ed076134-4852-45f3-bd85-9542d6fa580f/464889a1-d0ec-4c8a-8463-fe9670331a73.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('464889a1-d0ec-4c8a-8463-fe9670331a73','89d90e17-0bb8-4554-a93f-99bb6c2ec6ee');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='23f163e7-4066-49fa-9245-c22eea0861dc' WHERE image_guid ='6914a6da-c220-427f-828d-846a227013a0';"\n~~~\n\n\nFIXING DISK d553a03e-82c2-4c0d-8133-1378a25f2b10, CHAIN ['da60253b-d9c8-47e8-9e13-b4a0a3b09558', 'db8d3c67-7aa8-48c2-b869-badc7daac777', 'b5566cb3-ef9e-4868-ba69-c788e3de0da0']\n\n~~~\nHost Commands:\n==============\n\nlvchange --deltag 'IU_d553a03e-82c2-4c0d-8133-1378a25f2b10' /dev/ovirt-local/b5566cb3-ef9e-4868-ba69-c788e3de0da0\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/b5566cb3-ef9e-4868-ba69-c788e3de0da0\n\nlvchange --deltag 'VM_dcc491d1-ec89-47bd-9e09-d5e8016c7ecf' /dev/ovirt-local/b5566cb3-ef9e-4868-ba69-c788e3de0da0\n\nlvrename /dev/ovirt-local/b5566cb3-ef9e-4868-ba69-c788e3de0da0 /dev/ovirt-local/_temp_b5566cb3-ef9e-4868-ba69-c788e3de0da0\n\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/db8d3c67-7aa8-48c2-b869-badc7daac777 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/b5566cb3-ef9e-4868-ba69-c788e3de0da0'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/b5566cb3-ef9e-4868-ba69-c788e3de0da0.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/b5566cb3-ef9e-4868-ba69-c788e3de0da0.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/b5566cb3-ef9e-4868-ba69-c788e3de0da0.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/b5566cb3-ef9e-4868-ba69-c788e3de0da0.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/b5566cb3-ef9e-4868-ba69-c788e3de0da0 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/b5566cb3-ef9e-4868-ba69-c788e3de0da0.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/da60253b-d9c8-47e8-9e13-b4a0a3b09558 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/db8d3c67-7aa8-48c2-b869-badc7daac777'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/da60253b-d9c8-47e8-9e13-b4a0a3b09558.meta"\n\nlvchange -ay /dev/ovirt-local/da60253b-d9c8-47e8-9e13-b4a0a3b09558\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/da60253b-d9c8-47e8-9e13-b4a0a3b09558 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/db8d3c67-7aa8-48c2-b869-badc7daac777'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/db8d3c67-7aa8-48c2-b869-badc7daac777.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/db8d3c67-7aa8-48c2-b869-badc7daac777.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/db8d3c67-7aa8-48c2-b869-badc7daac777.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/db8d3c67-7aa8-48c2-b869-badc7daac777.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/db8d3c67-7aa8-48c2-b869-badc7daac777 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d553a03e-82c2-4c0d-8133-1378a25f2b10/db8d3c67-7aa8-48c2-b869-badc7daac777.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('db8d3c67-7aa8-48c2-b869-badc7daac777','b5566cb3-ef9e-4868-ba69-c788e3de0da0');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='23f163e7-4066-49fa-9245-c22eea0861dc' WHERE image_guid ='da60253b-d9c8-47e8-9e13-b4a0a3b09558';"\n~~~\n\n\nFIXING DISK 89c24839-e341-47ac-88bb-b22ef228ce7e, CHAIN ['55812faa-887d-43da-a864-cbb4ce7db604', '0e9c9965-3b15-4ea8-a4a0-0d6902c2a021']\n\n~~~\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e/55812faa-887d-43da-a864-cbb4ce7db604.meta"\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e/55812faa-887d-43da-a864-cbb4ce7db604 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e/0e9c9965-3b15-4ea8-a4a0-0d6902c2a021'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e/0e9c9965-3b15-4ea8-a4a0-0d6902c2a021.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e/0e9c9965-3b15-4ea8-a4a0-0d6902c2a021.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e/0e9c9965-3b15-4ea8-a4a0-0d6902c2a021.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e/0e9c9965-3b15-4ea8-a4a0-0d6902c2a021.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e/0e9c9965-3b15-4ea8-a4a0-0d6902c2a021 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89c24839-e341-47ac-88bb-b22ef228ce7e/0e9c9965-3b15-4ea8-a4a0-0d6902c2a021.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0e9c9965-3b15-4ea8-a4a0-0d6902c2a021');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='23f163e7-4066-49fa-9245-c22eea0861dc' WHERE image_guid ='55812faa-887d-43da-a864-cbb4ce7db604';"\n~~~</text>, <text>thanks for the Bimal.\n\nCan we do this VM next please : xcoder02.c03.streaming.gen.clt1.dev.mlbam.net\n\nruns on hyp99.clt1.prod.mlbam.net . I've attached a new sos report. thank you!</text>, <text>Hi Marcus,\n\nFrom the sosreport of hyp99, the VM xcoder02.c03.streaming.gen.clt1.dev.mlbam.net is up and running.\n\n~~~\nqemu     24474  0.0  0.4 34288248 1151608 ?    Sl    2017  59:29 /usr/libexec/qemu-kvm -name guest=xcoder02.c03.streaming.gen.clt1.dev.mlbam.net\n~~~\n\nAlso there are 2 lvm images on the /dev/ovirt-local for this VM.\nThe 5a80dfcb is the base image and the 39bff29a is the ACTIVE image from the snapshot.\n\nThe 39bff29a image that is on the NFS shows date of Dec 13.\n\n\n\n~~~\n  5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2 ovirt-local    1 -wi-a----- 50.00g  IU_15954057-a54a-4584-8f52-5ada900bd947,PU_00000000-0000-0000-0000-000000000000,VM_e965061f-7ddb-4c6c-8fe6-9acdb4c06d98\n  39bff29a-06e1-413d-bd90-38547c03e852 ovirt-local    1 -wi-ao---- 50.00g  IU_15954057-a54a-4584-8f52-5ada900bd947,PU_00000000-0000-0000-0000-000000000000,VM_e965061f-7ddb-4c6c-8fe6-9acdb4c06d98\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947:\ntotal 4758646\n-rw-rw----. 19 vdsm kvm 2639921152 Sep 21 19:03 104c2f26-584e-4f19-a104-588dd62385a9\n-rw-rw----. 19 vdsm kvm    1048576 Sep 21 18:50 104c2f26-584e-4f19-a104-588dd62385a9.lease\n-rw-r--r--. 19 vdsm kvm        262 Sep 21 19:03 104c2f26-584e-4f19-a104-588dd62385a9.meta\n\n-rw-rw----.  1 vdsm kvm     197632 Sep 27 17:54 5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2\n-rw-rw----.  1 vdsm kvm    1048576 Sep 27 17:54 5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2.lease\n-rw-r--r--.  1 vdsm kvm        264 Nov 11 02:49 5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2.meta\n\n-rw-rw----.  1 vdsm kvm 1246756864 Dec 13 22:09 39bff29a-06e1-413d-bd90-38547c03e852\n-rw-rw----.  1 vdsm kvm    1048576 Nov 11 02:49 39bff29a-06e1-413d-bd90-38547c03e852.lease\n-rw-r--r--.  1 vdsm kvm        260 Nov 11 02:49 39bff29a-06e1-413d-bd90-38547c03e852.meta\n~~~\n\nWe can fix the VM, using that method we have been but you may end up with stale data (from Dec 13th).\n\nI recall on we tried to fix a VM that had both images on /dev/ovirt-local and weren't successful.  \nI think you ended up rebuilding it.\n\nCan you run the following on the hyp99 host and attach to the case.\n\n~~~\nvirsh -r dumpxml xcoder02.c03.streaming.gen.clt1.dev.mlbam.net &gt; /tmp/xcoder02.c03.streaming.gen.clt1.dev.mlbam.net.xml\n~~~\n\nWant to see what images are shows in the XML for this VM.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Correct Bimal, the VM is currently powered on. It's experiencing issues so the application owners require a restart. \n\nTHe requested information is attached</text>, <text>Hi Marcus,\n\nThe XML shows the /dev/ovirt-local/39bff29a-06e1-413d-bd90-38547c03e852 as the boot device.\n\n~~~\n &lt;/disk&gt;\n    &lt;disk type='block' device='disk' snapshot='no'&gt;\n      &lt;driver name='qemu' type='raw' cache='none' error_policy='stop' io='native'/&gt;\n      &lt;source dev='/dev/ovirt-local/39bff29a-06e1-413d-bd90-38547c03e852'/&gt;\n      &lt;backingStore/&gt;\n      &lt;target dev='sda' bus='scsi'/&gt;\n      &lt;serial&gt;15954057-a54a-4584-8f52-5ada900bd947&lt;/serial&gt;\n      &lt;boot order='1'/&gt;\n      &lt;alias name='scsi0-0-0-0'/&gt;\n      &lt;address type='drive' controller='0' bus='0' target='0' unit='0'/&gt;\n    &lt;/disk&gt;\n~~~\n\nThe image 39bff29a-06e1-413d-bd90-38547c03e852 is already on the /dev/ovirt-local so if the VM is restarted, it should use it to boot.  As indicated earlier, the NFS image is from Dec 13th.  Fixing it will result in old stale data.  If the VM needs to be restarted, I would suggest backing up the data for restarting it.    \n\nThanks \nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Hello Bimal,\nSince the VM is currently in a frozen state, i have no issues with moving forward with attempting the fix. I'll await your fix commands. thank you</text>, <text>Hi Marcus,\n\nI think you should be able to start the VM and get it booted with have current data since the both images of the disk are on the host (/dev/ovirt-local).\nOnly thing here will be you will need to leave the VM as is with 2 lvm images on the host.\n\n~~~\n  5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2 ovirt-local    1 -wi-a----- 50.00g  IU_15954057-a54a-4584-8f52-5ada900bd947,PU_00000000-0000-0000-0000-000000000000,VM_e965061f-7ddb-4c6c-8fe6-9acdb4c06d98\n  39bff29a-06e1-413d-bd90-38547c03e852 ovirt-local    1 -wi-ao---- 50.00g  IU_15954057-a54a-4584-8f52-5ada900bd947,PU_00000000-0000-0000-0000-000000000000,VM_e965061f-7ddb-4c6c-8fe6-9acdb4c06d98\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947:\ntotal 4758646\n-rw-rw----. 19 vdsm kvm 2639921152 Sep 21 19:03 104c2f26-584e-4f19-a104-588dd62385a9\n-rw-rw----. 19 vdsm kvm    1048576 Sep 21 18:50 104c2f26-584e-4f19-a104-588dd62385a9.lease\n-rw-r--r--. 19 vdsm kvm        262 Sep 21 19:03 104c2f26-584e-4f19-a104-588dd62385a9.meta\n\n-rw-rw----.  1 vdsm kvm     197632 Sep 27 17:54 5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2\n-rw-rw----.  1 vdsm kvm    1048576 Sep 27 17:54 5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2.lease\n-rw-r--r--.  1 vdsm kvm        264 Nov 11 02:49 5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2.meta\n\n-rw-rw----.  1 vdsm kvm 1246756864 Dec 13 22:09 39bff29a-06e1-413d-bd90-38547c03e852\n-rw-rw----.  1 vdsm kvm    1048576 Nov 11 02:49 39bff29a-06e1-413d-bd90-38547c03e852.lease\n-rw-r--r--.  1 vdsm kvm        260 Nov 11 02:49 39bff29a-06e1-413d-bd90-38547c03e852.meta\n~~~\n\nIf you still want to move forward in fixing it, below are the commands.\nAgain, the data will go back to Dec 13th.  You will need to shutdown the VM before running the command.\n\n\nLet me know if you have any questions.\n\nThanks \nBimal Chollera\nRed Hat, Inc\nGSS\n\n\nVM xcoder02.c03.streaming.gen.clt1.dev.mlbam.net\n================================================\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'e965061f-7ddb-4c6c-8fe6-9acdb4c06d98' AND snapshot_id != '0068c3b9-123b-46ea-bc69-a47d8734ded9';"\n\nFIXING DISK 15954057-a54a-4584-8f52-5ada900bd947, CHAIN ['5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2', '39bff29a-06e1-413d-bd90-38547c03e852']\n\nHost Commands:\n==============\n\nlvchange --deltag 'IU_15954057-a54a-4584-8f52-5ada900bd947' /dev/ovirt-local/39bff29a-06e1-413d-bd90-38547c03e852\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/39bff29a-06e1-413d-bd90-38547c03e852\n\nlvchange --deltag 'VM_e965061f-7ddb-4c6c-8fe6-9acdb4c06d98' /dev/ovirt-local/39bff29a-06e1-413d-bd90-38547c03e852\n\nlvrename /dev/ovirt-local/39bff29a-06e1-413d-bd90-38547c03e852 /dev/ovirt-local/_temp_39bff29a-06e1-413d-bd90-38547c03e852\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947/39bff29a-06e1-413d-bd90-38547c03e852'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947/5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2.meta"\n\nlvchange -ay /dev/ovirt-local/5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947/39bff29a-06e1-413d-bd90-38547c03e852'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947/39bff29a-06e1-413d-bd90-38547c03e852.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947/39bff29a-06e1-413d-bd90-38547c03e852.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947/39bff29a-06e1-413d-bd90-38547c03e852.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947/39bff29a-06e1-413d-bd90-38547c03e852.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947/39bff29a-06e1-413d-bd90-38547c03e852 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/15954057-a54a-4584-8f52-5ada900bd947/39bff29a-06e1-413d-bd90-38547c03e852.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('39bff29a-06e1-413d-bd90-38547c03e852');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='0068c3b9-123b-46ea-bc69-a47d8734ded9' WHERE image_guid ='5a80dfcb-6b98-4df8-aa6f-9892fcaeb4d2';"</text>, <text>fyi: cbdb01.ingest06 looks ok after i applied your fix commands. thank you\n\n\n[root@hyp93 ~]# virsh -r domblklist cbdb01.ingest06.hls.hulu.clt1.prod.mlbam.net\nTarget     Source\n------------------------------------------------\nhdc        -\nvda        /dev/ovirt-local/da60253b-d9c8-47e8-9e13-b4a0a3b09558\nvdb        /dev/ovirt-local/6914a6da-c220-427f-828d-846a227013a0\nvdc        /dev/ovirt-local/55812faa-887d-43da-a864-cbb4ce7db604</text>, <text>looks like xcoder02 was beyond repair. It was stuck during switch_root at boot. easier to rebuild the VM as opposed to attempting to rescue it at this point thanky ou</text>, <text>Hi Marcus,\n\nWanted to followup on this case.\nIts been a while since we have seen any activity.  \nDo you still want it open?\n\nThanks\n\n- Bimal.</text>, <text>Yes please. we have about 100 more Vms to complete and we're setting up a project plan to tackle about 10-15 VMs per week starting next week on a consistent basis. Up until now we've been fixing one off Vms and also trying through different configurations to ensure your fix script can handle different setups.</text>, <text>Thanks Marcus,\n\nWill keep this open and await your reply.\n\nThanks\nBimal.</text>, <text>Hi Bimal\nCan we move forward with the fix commands for the following VMs please ? Thank you\n\n\njava01.tool01.postprod.gen.clt1.prod.mlbam.net\njava02.tool01.postprod.gen.clt1.prod.mlbam.net\n \njava01.tool01.postprod.mls.clt1.prod.mlbam.net\njava02.tool01.postprod.mls.clt1.prod.mlbam.net\n \njava01.tool01.postprod.gen.clt1.qa.mlbam.net\njava02.tool01.postprod.gen.clt1.qa.mlbam.net\n \njava06.ad01.hls.hulu.clt1.prod.mlbam.net\njava09.ad01.hls.hulu.clt1.prod.mlbam.net\njava10.ad01.hls.hulu.clt1.prod.mlbam.net</text>, <text>Hi Marcus,\n\nFrom the RHV-M DB we have, the VM's are shown to be running on various hosts.\n\n~~~\njava01.tool01.postprod.gen.clt1.prod.mlbam.net  - hyp80.clt1.prod.mlbam.net\njava02.tool01.postprod.gen.clt1.prod.mlbam.net  - hyp95.clt1.prod.mlbam.net\n\njava01.tool01.postprod.mls.clt1.prod.mlbam.net - hyp08.clt1.prod.mlbam.net\njava02.tool01.postprod.mls.clt1.prod.mlbam.net - hyp72.clt1.prod.mlbam.net\n\njava01.tool01.postprod.gen.clt1.qa.mlbam.net - hyp73.clt1.prod.mlbam.net\njava02.tool01.postprod.gen.clt1.qa.mlbam.net - hyp74.clt1.prod.mlbam.net\n\njava06.ad01.hls.hulu.clt1.prod.mlbam.net - hyp28.clt1.prod.mlbam.net\njava09.ad01.hls.hulu.clt1.prod.mlbam.net - hyp86.clt1.prod.mlbam.net\njava10.ad01.hls.hulu.clt1.prod.mlbam.net - hyp91.clt1.prod.mlbam.net\n~~~\n\nInstead of providing sos report for the hosts, can you run the followiong from the RHV-M.\n\n~~~\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp80.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp80-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp80.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp80-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp95.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp95-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp95.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp95-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp08.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp08-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp08.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp08-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp72.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp72-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp72.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp72-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp73.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp73-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp73.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp73-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp74.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp74-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp74.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp74-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp28.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp28-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp28.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp28-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp86.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp86-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp86.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp86-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp91.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp91-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp91.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp91-lvm_lvs'\n~~~\n\nThen from RHV-M host, sftp to get files and upload them to the case.\n\n~~~\ncd /tmp/\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp80.clt1.prod.mlbam.net \nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp95.clt1.prod.mlbam.net \nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp08.clt1.prod.mlbam.net \nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp72.clt1.prod.mlbam.net \nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp73.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp74.clt1.prod.mlbam.net \nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp28.clt1.prod.mlbam.net \nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp86.clt1.prod.mlbam.net \nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp91.clt1.prod.mlbam.net \nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n~~~\n\ntar up the /tmp/hyp* and upload to the case.\n\nThanks\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Here you go bimal thank you</text>, <text>java01.tool01.postprod.gen.clt1.prod.mlbam.net\n================================================\n\nroot_Disk1 7fae4449-5a01-4b66-889b-9f5aec0db9cd, CHAIN ['f517a683-92d2-43a5-81a4-8f9c25697af7', '80fedf06-8a23-401c-9530-f95e15b800cd']\n\nThere are 2 images on the /dev/ovirt-local:\n\nf517a683-92d2-43a5-81a4-8f9c25697af7', '80fedf06-8a23-401c-9530-f95e15b800cd'.\nOnly the base f517a683-92d2-43a5-81a4-8f9c25697af7 should be on /dev/ovirt-local.\n\n~~~\nf517a683-92d2-43a5-81a4-8f9c25697af7 ovirt-local -wi-ao----  50.00g IU_7fae4449-5a01-4b66-889b-9f5aec0db9cd,PU_00000000-0000-0000-0000-000000000000,VM_43a345a1-f5ec-48b1-916e-ef8a0f043466\n80fedf06-8a23-401c-9530-f95e15b800cd ovirt-local -wi-a-----  50.00g IU_7fae4449-5a01-4b66-889b-9f5aec0db9cd,PU_00000000-0000-0000-0000-000000000000,VM_43a345a1-f5ec-48b1-916e-ef8a0f043466\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7fae4449-5a01-4b66-889b-9f5aec0db9cd:\ntotal 4039542\n-rw-rw----. 85 vdsm kvm 1662844928 May 31  2017 20f9e080-c6c5-45dc-be72-27c33ba85d45\n-rw-rw----. 85 vdsm kvm    1048576 May 31  2017 20f9e080-c6c5-45dc-be72-27c33ba85d45.lease\n-rw-r--r--. 85 vdsm kvm        262 May 31  2017 20f9e080-c6c5-45dc-be72-27c33ba85d45.meta\n\n-rw-rw----.  1 vdsm kvm     197632 Jun 12  2017 f517a683-92d2-43a5-81a4-8f9c25697af7\n-rw-rw----.  1 vdsm kvm    1048576 Jun 12  2017 f517a683-92d2-43a5-81a4-8f9c25697af7.lease\n-rw-r--r--.  1 vdsm kvm        264 Nov 11 12:47 f517a683-92d2-43a5-81a4-8f9c25697af7.meta\n\n-rw-rw----.  1 vdsm kvm 1635123200 Mar  8 18:01 80fedf06-8a23-401c-9530-f95e15b800cd\n-rw-rw----.  1 vdsm kvm    1048576 Nov 11 12:47 80fedf06-8a23-401c-9530-f95e15b800cd.lease\n-rw-r--r--.  1 vdsm kvm        260 Nov 12 03:17 80fedf06-8a23-401c-9530-f95e15b800cd.meta\n~~~\n\nFix Plan:\n==========\n\n1.  Quarantine the /dev/ovirt-local/80fedf06-8a23-401c-9530-f95e15b800cd.\n2.  Rebase 80fedf06-8a23-401c-9530-f95e15b800cd to have backing file of /dev/ovirt-local/f517a683-92d2-43a5-81a4-8f9c25697af7.\n3.  Merge /dev/ovirt-local/f517a683-92d2-43a5-81a4-8f9c25697af7 &lt;&lt;-- 80fedf06-8a23-401c-9530-f95e15b800cd\n4.  Fix the DB.\n\n\nAction Plan:\n=============\n\n~~~\n1.  Shutdown the VM.\n\n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '43a345a1-f5ec-48b1-916e-ef8a0f043466' AND snapshot_id != 'fd2d65cf-c274-45cc-8623-4ccfe6361e98';"\n\nFIXING DISK 7fae4449-5a01-4b66-889b-9f5aec0db9cd, CHAIN ['f517a683-92d2-43a5-81a4-8f9c25697af7', '80fedf06-8a23-401c-9530-f95e15b800cd']\n\nHost Commands:\n==============\n\nlvchange -an /dev/ovirt-local/80fedf06-8a23-401c-9530-f95e15b800cd\n\nlvchange --deltag 'IU_7fae4449-5a01-4b66-889b-9f5aec0db9cd' /dev/ovirt-local/80fedf06-8a23-401c-9530-f95e15b800cd\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/80fedf06-8a23-401c-9530-f95e15b800cd\n\nlvchange --deltag 'VM_43a345a1-f5ec-48b1-916e-ef8a0f043466' /dev/ovirt-local/80fedf06-8a23-401c-9530-f95e15b800cd\n\nlvrename /dev/ovirt-local/80fedf06-8a23-401c-9530-f95e15b800cd /dev/ovirt-local/_temp_80fedf06-8a23-401c-9530-f95e15b800cd\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f517a683-92d2-43a5-81a4-8f9c25697af7 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7fae4449-5a01-4b66-889b-9f5aec0db9cd/80fedf06-8a23-401c-9530-f95e15b800cd'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7fae4449-5a01-4b66-889b-9f5aec0db9cd/f517a683-92d2-43a5-81a4-8f9c25697af7.meta"\n\nlvchange -ay /dev/ovirt-local/f517a683-92d2-43a5-81a4-8f9c25697af7\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f517a683-92d2-43a5-81a4-8f9c25697af7 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7fae4449-5a01-4b66-889b-9f5aec0db9cd/80fedf06-8a23-401c-9530-f95e15b800cd'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7fae4449-5a01-4b66-889b-9f5aec0db9cd/80fedf06-8a23-401c-9530-f95e15b800cd.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7fae4449-5a01-4b66-889b-9f5aec0db9cd/80fedf06-8a23-401c-9530-f95e15b800cd.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7fae4449-5a01-4b66-889b-9f5aec0db9cd/80fedf06-8a23-401c-9530-f95e15b800cd.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7fae4449-5a01-4b66-889b-9f5aec0db9cd/80fedf06-8a23-401c-9530-f95e15b800cd.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7fae4449-5a01-4b66-889b-9f5aec0db9cd/80fedf06-8a23-401c-9530-f95e15b800cd /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7fae4449-5a01-4b66-889b-9f5aec0db9cd/80fedf06-8a23-401c-9530-f95e15b800cd.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('80fedf06-8a23-401c-9530-f95e15b800cd');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='fd2d65cf-c274-45cc-8623-4ccfe6361e98' WHERE image_guid ='f517a683-92d2-43a5-81a4-8f9c25697af7';"\n~~~\n\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>java02.tool01.postprod.gen.clt1.prod.mlbam.net \n===============================================\n\nroot_Disk1 bead7522-f43c-4c2d-b6b2-4f88872d9a9e, CHAIN ['b566010c-1838-439a-b9a1-9606bfd94add', 'a65edbbe-5b2b-48b5-8a62-4721f313b68f']\n\nBase image on the /dev/ovirt-local\n\n~~~\n  b566010c-1838-439a-b9a1-9606bfd94add ovirt-local -wi-ao----  50.00g  IU_bead7522-f43c-4c2d-b6b2-4f88872d9a9e,PU_00000000-0000-0000-0000-000000000000,VM_335f780d-ecd3-41d5-bc99-e936ba78a60f\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bead7522-f43c-4c2d-b6b2-4f88872d9a9e:\ntotal 4046798\n-rw-rw----. 85 vdsm kvm 1662844928 May 31  2017 20f9e080-c6c5-45dc-be72-27c33ba85d45\n-rw-rw----. 85 vdsm kvm    1048576 May 31  2017 20f9e080-c6c5-45dc-be72-27c33ba85d45.lease\n-rw-r--r--. 85 vdsm kvm        262 May 31  2017 20f9e080-c6c5-45dc-be72-27c33ba85d45.meta\n\n-rw-rw----.  1 vdsm kvm     197632 Jun 12  2017 b566010c-1838-439a-b9a1-9606bfd94add\n-rw-rw----.  1 vdsm kvm    1048576 Jun 12  2017 b566010c-1838-439a-b9a1-9606bfd94add.lease\n-rw-r--r--.  1 vdsm kvm        264 Nov 12 06:17 b566010c-1838-439a-b9a1-9606bfd94add.meta\n\n-rw-rw----.  1 vdsm kvm 1641349120 Mar  8 18:01 a65edbbe-5b2b-48b5-8a62-4721f313b68f\n-rw-rw----.  1 vdsm kvm    1048576 Nov 12 06:17 a65edbbe-5b2b-48b5-8a62-4721f313b68f.lease\n-rw-r--r--.  1 vdsm kvm        260 Nov 12 06:17 a65edbbe-5b2b-48b5-8a62-4721f313b68f.meta\n~~~\n\nFix Plan:\n==========\n\n1.  Rebase a65edbbe-5b2b-48b5-8a62-4721f313b68f to have backing file of /dev/ovirt-local/b566010c-1838-439a-b9a1-9606bfd94add.\n2.  Merge /dev/ovirt-local/b566010c-1838-439a-b9a1-9606bfd94add &lt;&lt;-- a65edbbe-5b2b-48b5-8a62-4721f313b68f\n3.  Fix the DB.\n\nAction Plan:\n=============\n\n~~~\n1.  Shutdown the VM.\n\n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '335f780d-ecd3-41d5-bc99-e936ba78a60f' AND snapshot_id != 'a83b2799-36b3-43ae-8f1d-da5c48d2d90b';"\n\nFIXING DISK bead7522-f43c-4c2d-b6b2-4f88872d9a9e, CHAIN ['b566010c-1838-439a-b9a1-9606bfd94add', 'a65edbbe-5b2b-48b5-8a62-4721f313b68f']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/b566010c-1838-439a-b9a1-9606bfd94add -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bead7522-f43c-4c2d-b6b2-4f88872d9a9e/a65edbbe-5b2b-48b5-8a62-4721f313b68f'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bead7522-f43c-4c2d-b6b2-4f88872d9a9e/b566010c-1838-439a-b9a1-9606bfd94add.meta"\n\nlvchange -ay /dev/ovirt-local/b566010c-1838-439a-b9a1-9606bfd94add\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/b566010c-1838-439a-b9a1-9606bfd94add -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bead7522-f43c-4c2d-b6b2-4f88872d9a9e/a65edbbe-5b2b-48b5-8a62-4721f313b68f'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bead7522-f43c-4c2d-b6b2-4f88872d9a9e/a65edbbe-5b2b-48b5-8a62-4721f313b68f.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bead7522-f43c-4c2d-b6b2-4f88872d9a9e/a65edbbe-5b2b-48b5-8a62-4721f313b68f.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bead7522-f43c-4c2d-b6b2-4f88872d9a9e/a65edbbe-5b2b-48b5-8a62-4721f313b68f.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bead7522-f43c-4c2d-b6b2-4f88872d9a9e/a65edbbe-5b2b-48b5-8a62-4721f313b68f.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bead7522-f43c-4c2d-b6b2-4f88872d9a9e/a65edbbe-5b2b-48b5-8a62-4721f313b68f /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bead7522-f43c-4c2d-b6b2-4f88872d9a9e/a65edbbe-5b2b-48b5-8a62-4721f313b68f.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('a65edbbe-5b2b-48b5-8a62-4721f313b68f');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a83b2799-36b3-43ae-8f1d-da5c48d2d90b' WHERE image_guid ='b566010c-1838-439a-b9a1-9606bfd94add';"\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>java09.ad01.hls.hulu.clt1.prod.mlbam.net \n=========================================\n\nThis VM contains 2 disks:\n\n~~~\njava09.ad01.hls.hulu.clt1.prod.mlbam.net_Disk1 3a12105d-c530-4cd8-a441-f8864cb868e6, CHAIN ['16321508-52fb-46ba-93f5-fd1556206367', '81439d0b-36ef-4e04-8049-9a2e738b9340']\n\nrootdisk a02a9993-deac-450b-841f-258c2f749830, CHAIN ['ba2c7691-918c-4c98-abe1-510e0cca605e', 'ff3fe343-691f-4530-a0a5-35354020f799']\n~~~\n\nThe base images of each disks are on the ovirt-local:\n\n~~~\n  16321508-52fb-46ba-93f5-fd1556206367 ovirt-local -wi-ao----  21.00g   IU_3a12105d-c530-4cd8-a441-f8864cb868e6,PU_00000000-0000-0000-0000-000000000000,VM_0963fc2a-abbd-4078-93f8-dab371299007\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/3a12105d-c530-4cd8-a441-f8864cb868e6:\ntotal 4105\n-rw-rw----. 1 vdsm kvm 22548578304 May  4  2017 16321508-52fb-46ba-93f5-fd1556206367\n-rw-rw----. 1 vdsm kvm     1048576 May  4  2017 16321508-52fb-46ba-93f5-fd1556206367.lease\n-rw-r--r--. 1 vdsm kvm         346 Nov 11 07:15 16321508-52fb-46ba-93f5-fd1556206367.meta\n\n-rw-rw----. 1 vdsm kvm     1114112 Mar  8 13:37 81439d0b-36ef-4e04-8049-9a2e738b9340\n-rw-rw----. 1 vdsm kvm     1048576 Nov 11 07:15 81439d0b-36ef-4e04-8049-9a2e738b9340.lease\n-rw-r--r--. 1 vdsm kvm         259 Nov 12 00:47 81439d0b-36ef-4e04-8049-9a2e738b9340.meta\n\n\n  ba2c7691-918c-4c98-abe1-510e0cca605e ovirt-local -wi-ao----  50.00g   IU_a02a9993-deac-450b-841f-258c2f749830,PU_00000000-0000-0000-0000-000000000000,VM_0963fc2a-abbd-4078-93f8-dab371299007\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a02a9993-deac-450b-841f-258c2f749830:\ntotal 3074286\n\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 May  4  2017 ba2c7691-918c-4c98-abe1-510e0cca605e\n-rw-rw----.   1 vdsm kvm    1048576 May  4  2017 ba2c7691-918c-4c98-abe1-510e0cca605e.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 11 07:15 ba2c7691-918c-4c98-abe1-510e0cca605e.meta\n\n-rw-rw----.   1 vdsm kvm  579928064 Mar  8 18:01 ff3fe343-691f-4530-a0a5-35354020f799\n-rw-rw----.   1 vdsm kvm    1048576 Nov 11 07:15 ff3fe343-691f-4530-a0a5-35354020f799.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 00:47 ff3fe343-691f-4530-a0a5-35354020f799.meta\n~~~\n\n\nFix Plan:\n==========\n\n1.  Rebase 81439d0b-36ef-4e04-8049-9a2e738b9340 to have backing file of /dev/ovirt-local/16321508-52fb-46ba-93f5-fd1556206367.\n2.  Merge /dev/ovirt-local/16321508-52fb-46ba-93f5-fd1556206367 &lt;&lt;-- 81439d0b-36ef-4e04-8049-9a2e738b9340\n\n3.  Rebase ff3fe343-691f-4530-a0a5-35354020f799 to have backing file of /dev/ovirt-local/ba2c7691-918c-4c98-abe1-510e0cca605e.\n4.  Merge /dev/ovirt-local/ba2c7691-918c-4c98-abe1-510e0cca605e &lt;&lt;-- ff3fe343-691f-4530-a0a5-35354020f799\n\n5.  Fix the DB.\n\n\n\nAction Plan:\n=============\n\n~~~\n1.  Shutdown the VM.\n\n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0963fc2a-abbd-4078-93f8-dab371299007' AND snapshot_id != '66d4f3e0-b6a2-4ff6-b521-f95e510c9c23';"\n\nFIXING DISK 3a12105d-c530-4cd8-a441-f8864cb868e6, CHAIN ['16321508-52fb-46ba-93f5-fd1556206367', '81439d0b-36ef-4e04-8049-9a2e738b9340']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/16321508-52fb-46ba-93f5-fd1556206367 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/3a12105d-c530-4cd8-a441-f8864cb868e6/81439d0b-36ef-4e04-8049-9a2e738b9340'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/3a12105d-c530-4cd8-a441-f8864cb868e6/16321508-52fb-46ba-93f5-fd1556206367.meta"\n\nlvchange -ay /dev/ovirt-local/16321508-52fb-46ba-93f5-fd1556206367\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/16321508-52fb-46ba-93f5-fd1556206367 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/3a12105d-c530-4cd8-a441-f8864cb868e6/81439d0b-36ef-4e04-8049-9a2e738b9340'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/3a12105d-c530-4cd8-a441-f8864cb868e6/81439d0b-36ef-4e04-8049-9a2e738b9340.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/3a12105d-c530-4cd8-a441-f8864cb868e6/81439d0b-36ef-4e04-8049-9a2e738b9340.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/3a12105d-c530-4cd8-a441-f8864cb868e6/81439d0b-36ef-4e04-8049-9a2e738b9340.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/3a12105d-c530-4cd8-a441-f8864cb868e6/81439d0b-36ef-4e04-8049-9a2e738b9340.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/3a12105d-c530-4cd8-a441-f8864cb868e6/81439d0b-36ef-4e04-8049-9a2e738b9340 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/3a12105d-c530-4cd8-a441-f8864cb868e6/81439d0b-36ef-4e04-8049-9a2e738b9340.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('81439d0b-36ef-4e04-8049-9a2e738b9340');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='66d4f3e0-b6a2-4ff6-b521-f95e510c9c23' WHERE image_guid ='16321508-52fb-46ba-93f5-fd1556206367';"\n\n\n\nFIXING DISK a02a9993-deac-450b-841f-258c2f749830, CHAIN ['ba2c7691-918c-4c98-abe1-510e0cca605e', 'ff3fe343-691f-4530-a0a5-35354020f799']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/ba2c7691-918c-4c98-abe1-510e0cca605e -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a02a9993-deac-450b-841f-258c2f749830/ff3fe343-691f-4530-a0a5-35354020f799'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a02a9993-deac-450b-841f-258c2f749830/ba2c7691-918c-4c98-abe1-510e0cca605e.meta"\n\nlvchange -ay /dev/ovirt-local/ba2c7691-918c-4c98-abe1-510e0cca605e\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ba2c7691-918c-4c98-abe1-510e0cca605e -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a02a9993-deac-450b-841f-258c2f749830/ff3fe343-691f-4530-a0a5-35354020f799'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a02a9993-deac-450b-841f-258c2f749830/ff3fe343-691f-4530-a0a5-35354020f799.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a02a9993-deac-450b-841f-258c2f749830/ff3fe343-691f-4530-a0a5-35354020f799.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a02a9993-deac-450b-841f-258c2f749830/ff3fe343-691f-4530-a0a5-35354020f799.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a02a9993-deac-450b-841f-258c2f749830/ff3fe343-691f-4530-a0a5-35354020f799.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a02a9993-deac-450b-841f-258c2f749830/ff3fe343-691f-4530-a0a5-35354020f799 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a02a9993-deac-450b-841f-258c2f749830/ff3fe343-691f-4530-a0a5-35354020f799.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('ff3fe343-691f-4530-a0a5-35354020f799');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='66d4f3e0-b6a2-4ff6-b521-f95e510c9c23' WHERE image_guid ='ba2c7691-918c-4c98-abe1-510e0cca605e';"\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>java10.ad01.hls.hulu.clt1.prod.mlbam.net\n==========================================\n\nThis VM contains 2 disks:\n\n~~~\njava10.ad01.hls.hulu.clt1.prod.mlbam.net_Disk1 642a4077-adb5-4ea3-bcd8-fcf309479d4b, CHAIN ['a8eb4e7a-2e41-444e-907f-43683246b328', 'd1469074-5f8d-4671-93a7-70edfdacc770']\n\nrootdisk 03e68673-73f1-43a5-9b5c-73db336d9cf0, CHAIN ['0136a6ed-dae1-4221-a276-23b20b5e3228', 'eafed675-0545-4a3f-8a6f-912e54b12325']\n~~~\n\nThere are 2 images from each disk on the /dev/ovirt-local.\nOnly the base should be on the /dev/ovirt-local\n\n~~~\n  a8eb4e7a-2e41-444e-907f-43683246b328 ovirt-local -wi-ao----  21.00g  IU_642a4077-adb5-4ea3-bcd8-fcf309479d4b,PU_00000000-0000-0000-0000-000000000000,VM_7b96fa60-0cd2-43f5-b595-da1f78d6de88\n  d1469074-5f8d-4671-93a7-70edfdacc770 ovirt-local -wi-a-----  21.00g  IU_642a4077-adb5-4ea3-bcd8-fcf309479d4b,PU_00000000-0000-0000-0000-000000000000,VM_7b96fa60-0cd2-43f5-b595-da1f78d6de88\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/642a4077-adb5-4ea3-bcd8-fcf309479d4b:\ntotal 4297\n-rw-rw----. 1 vdsm kvm 22548578304 May  4  2017 a8eb4e7a-2e41-444e-907f-43683246b328\n-rw-rw----. 1 vdsm kvm     1048576 May  4  2017 a8eb4e7a-2e41-444e-907f-43683246b328.lease\n-rw-r--r--. 1 vdsm kvm         346 Nov 12 00:49 a8eb4e7a-2e41-444e-907f-43683246b328.meta\n\n-rw-rw----. 1 vdsm kvm     1179648 Mar  8 13:37 d1469074-5f8d-4671-93a7-70edfdacc770\n-rw-rw----. 1 vdsm kvm     1048576 Nov 12 00:49 d1469074-5f8d-4671-93a7-70edfdacc770.lease\n-rw-r--r--. 1 vdsm kvm         259 Nov 12 00:49 d1469074-5f8d-4671-93a7-70edfdacc770.meta\n\n\n\n  0136a6ed-dae1-4221-a276-23b20b5e3228 ovirt-local -wi-ao----  50.00g  IU_03e68673-73f1-43a5-9b5c-73db336d9cf0,PU_00000000-0000-0000-0000-000000000000,VM_7b96fa60-0cd2-43f5-b595-da1f78d6de88\n  eafed675-0545-4a3f-8a6f-912e54b12325 ovirt-local -wi-a-----  50.00g  IU_03e68673-73f1-43a5-9b5c-73db336d9cf0,PU_00000000-0000-0000-0000-000000000000,VM_7b96fa60-0cd2-43f5-b595-da1f78d6de88\n\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03e68673-73f1-43a5-9b5c-73db336d9cf0:\ntotal 3127590\n\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 May  4  2017 0136a6ed-dae1-4221-a276-23b20b5e3228\n-rw-rw----.   1 vdsm kvm    1048576 May  4  2017 0136a6ed-dae1-4221-a276-23b20b5e3228.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 12 00:49 0136a6ed-dae1-4221-a276-23b20b5e3228.meta\n\n-rw-rw----.   1 vdsm kvm  623312896 Mar  8 18:01 eafed675-0545-4a3f-8a6f-912e54b12325\n-rw-rw----.   1 vdsm kvm    1048576 Nov 12 00:49 eafed675-0545-4a3f-8a6f-912e54b12325.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 00:49 eafed675-0545-4a3f-8a6f-912e54b12325.meta\n~~~\n\n\nFix Plan:\n==========\n\n1.  Quarantine the /dev/ovirt-local/d1469074-5f8d-4671-93a7-70edfdacc770.\n2.  Rebase d1469074-5f8d-4671-93a7-70edfdacc770 to have backing file of /dev/ovirt-local/a8eb4e7a-2e41-444e-907f-43683246b328.\n3.  Merge /dev/ovirt-local/a8eb4e7a-2e41-444e-907f-43683246b328 &lt;&lt;-- d1469074-5f8d-4671-93a7-70edfdacc770\n\n4.  Quarantine the /dev/ovirt-local/eafed675-0545-4a3f-8a6f-912e54b12325.\n5.  Rebase eafed675-0545-4a3f-8a6f-912e54b12325 to have backing file of /dev/ovirt-local/0136a6ed-dae1-4221-a276-23b20b5e3228.\n6.  Merge /dev/ovirt-local/0136a6ed-dae1-4221-a276-23b20b5e3228 &lt;&lt;-- eafed675-0545-4a3f-8a6f-912e54b12325\n\n7.  Fix the DB.\n\n\n\nAction Plan:\n=============\n\n~~~\n1.  Shutdown the VM.\n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '7b96fa60-0cd2-43f5-b595-da1f78d6de88' AND snapshot_id != 'cb761b7a-5c08-4636-9046-b876bd682b6d';"\n\nFIXING DISK 642a4077-adb5-4ea3-bcd8-fcf309479d4b, CHAIN ['a8eb4e7a-2e41-444e-907f-43683246b328', 'd1469074-5f8d-4671-93a7-70edfdacc770']\n\nHost Commands:\n==============\n\nlvchange -an /dev/ovirt-local/d1469074-5f8d-4671-93a7-70edfdacc770\n\nlvchange --deltag 'IU_642a4077-adb5-4ea3-bcd8-fcf309479d4b' /dev/ovirt-local/d1469074-5f8d-4671-93a7-70edfdacc770\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/d1469074-5f8d-4671-93a7-70edfdacc770\n\nlvchange --deltag 'VM_7b96fa60-0cd2-43f5-b595-da1f78d6de88' /dev/ovirt-local/d1469074-5f8d-4671-93a7-70edfdacc770\n\nlvrename /dev/ovirt-local/d1469074-5f8d-4671-93a7-70edfdacc770 /dev/ovirt-local/_temp_d1469074-5f8d-4671-93a7-70edfdacc770\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/a8eb4e7a-2e41-444e-907f-43683246b328 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/642a4077-adb5-4ea3-bcd8-fcf309479d4b/d1469074-5f8d-4671-93a7-70edfdacc770'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/642a4077-adb5-4ea3-bcd8-fcf309479d4b/a8eb4e7a-2e41-444e-907f-43683246b328.meta"\n\nlvchange -ay /dev/ovirt-local/a8eb4e7a-2e41-444e-907f-43683246b328\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/a8eb4e7a-2e41-444e-907f-43683246b328 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/642a4077-adb5-4ea3-bcd8-fcf309479d4b/d1469074-5f8d-4671-93a7-70edfdacc770'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/642a4077-adb5-4ea3-bcd8-fcf309479d4b/d1469074-5f8d-4671-93a7-70edfdacc770.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/642a4077-adb5-4ea3-bcd8-fcf309479d4b/d1469074-5f8d-4671-93a7-70edfdacc770.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/642a4077-adb5-4ea3-bcd8-fcf309479d4b/d1469074-5f8d-4671-93a7-70edfdacc770.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/642a4077-adb5-4ea3-bcd8-fcf309479d4b/d1469074-5f8d-4671-93a7-70edfdacc770.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/642a4077-adb5-4ea3-bcd8-fcf309479d4b/d1469074-5f8d-4671-93a7-70edfdacc770 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/642a4077-adb5-4ea3-bcd8-fcf309479d4b/d1469074-5f8d-4671-93a7-70edfdacc770.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('d1469074-5f8d-4671-93a7-70edfdacc770');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='cb761b7a-5c08-4636-9046-b876bd682b6d' WHERE image_guid ='a8eb4e7a-2e41-444e-907f-43683246b328';"\n\n\n\nFIXING DISK 03e68673-73f1-43a5-9b5c-73db336d9cf0, CHAIN ['0136a6ed-dae1-4221-a276-23b20b5e3228', 'eafed675-0545-4a3f-8a6f-912e54b12325']\n\nHost Commands:\n==============\n\nlvchange -an /dev/ovirt-local/eafed675-0545-4a3f-8a6f-912e54b12325\n\nlvchange --deltag 'IU_03e68673-73f1-43a5-9b5c-73db336d9cf0' /dev/ovirt-local/eafed675-0545-4a3f-8a6f-912e54b12325\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/eafed675-0545-4a3f-8a6f-912e54b12325\n\nlvchange --deltag 'VM_7b96fa60-0cd2-43f5-b595-da1f78d6de88' /dev/ovirt-local/eafed675-0545-4a3f-8a6f-912e54b12325\n\nlvrename /dev/ovirt-local/eafed675-0545-4a3f-8a6f-912e54b12325 /dev/ovirt-local/_temp_eafed675-0545-4a3f-8a6f-912e54b12325\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/0136a6ed-dae1-4221-a276-23b20b5e3228 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03e68673-73f1-43a5-9b5c-73db336d9cf0/eafed675-0545-4a3f-8a6f-912e54b12325'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03e68673-73f1-43a5-9b5c-73db336d9cf0/0136a6ed-dae1-4221-a276-23b20b5e3228.meta"\n\nlvchange -ay /dev/ovirt-local/0136a6ed-dae1-4221-a276-23b20b5e3228\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/0136a6ed-dae1-4221-a276-23b20b5e3228 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03e68673-73f1-43a5-9b5c-73db336d9cf0/eafed675-0545-4a3f-8a6f-912e54b12325'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03e68673-73f1-43a5-9b5c-73db336d9cf0/eafed675-0545-4a3f-8a6f-912e54b12325.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03e68673-73f1-43a5-9b5c-73db336d9cf0/eafed675-0545-4a3f-8a6f-912e54b12325.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03e68673-73f1-43a5-9b5c-73db336d9cf0/eafed675-0545-4a3f-8a6f-912e54b12325.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03e68673-73f1-43a5-9b5c-73db336d9cf0/eafed675-0545-4a3f-8a6f-912e54b12325.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03e68673-73f1-43a5-9b5c-73db336d9cf0/eafed675-0545-4a3f-8a6f-912e54b12325 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03e68673-73f1-43a5-9b5c-73db336d9cf0/eafed675-0545-4a3f-8a6f-912e54b12325.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('eafed675-0545-4a3f-8a6f-912e54b12325');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='cb761b7a-5c08-4636-9046-b876bd682b6d' WHERE image_guid ='0136a6ed-dae1-4221-a276-23b20b5e3228';"\n\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>java06.ad01.hls.hulu.clt1.prod.mlbam.net\n=========================================\n\nThis VM contains 2 disks:\n\n~~~\njava06.ad01.hls.hulu.clt1.prod.mlbam.net_Disk1 99200509-995d-4f36-b495-42b795172b42, CHAIN ['48b3ce3b-bbca-466e-b0a8-fce61f2e63b8', '8e9eed1f-c911-4d59-8e6f-a75a9ee35d10']\n\nrootdisk 05c12051-5dba-4af0-a95c-080ba0c7de49, CHAIN ['0eb70516-0bb8-44a6-a4ad-2e3dcf50d985', 'a2697ae9-b8c5-46e2-80c8-855b9eebc33c']\n~~~\n\nThe base images of each disks are on the ovirt-local:\n\n~~~\n  48b3ce3b-bbca-466e-b0a8-fce61f2e63b8 ovirt-local -wi-ao----  21.00g  IU_99200509-995d-4f36-b495-42b795172b42,PU_00000000-0000-0000-0000-000000000000,VM_305a5aef-c391-42ed-9b66-a24949e088dd\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99200509-995d-4f36-b495-42b795172b42:\ntotal 4105\n-rw-rw----. 1 vdsm kvm 22548578304 May  4  2017 48b3ce3b-bbca-466e-b0a8-fce61f2e63b8\n-rw-rw----. 1 vdsm kvm     1048576 May  4  2017 48b3ce3b-bbca-466e-b0a8-fce61f2e63b8.lease\n-rw-r--r--. 1 vdsm kvm         346 Nov 11 10:42 48b3ce3b-bbca-466e-b0a8-fce61f2e63b8.meta\n\n-rw-rw----. 1 vdsm kvm     1114112 Mar  8 13:36 8e9eed1f-c911-4d59-8e6f-a75a9ee35d10\n-rw-rw----. 1 vdsm kvm     1048576 Nov 11 10:42 8e9eed1f-c911-4d59-8e6f-a75a9ee35d10.lease\n-rw-r--r--. 1 vdsm kvm         259 Nov 12 00:01 8e9eed1f-c911-4d59-8e6f-a75a9ee35d10.meta\n\n 0eb70516-0bb8-44a6-a4ad-2e3dcf50d985 ovirt-local -wi-ao----  50.00g  IU_05c12051-5dba-4af0-a95c-080ba0c7de49,PU_00000000-0000-0000-0000-000000000000,VM_305a5aef-c391-42ed-9b66-a24949e088dd\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/05c12051-5dba-4af0-a95c-080ba0c7de49:\ntotal 2816894\n\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 May  4  2017 0eb70516-0bb8-44a6-a4ad-2e3dcf50d985\n-rw-rw----.   1 vdsm kvm    1048576 May  4  2017 0eb70516-0bb8-44a6-a4ad-2e3dcf50d985.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 11 10:42 0eb70516-0bb8-44a6-a4ad-2e3dcf50d985.meta\n\n-rw-rw----.   1 vdsm kvm  369360896 Mar  8 18:01 a2697ae9-b8c5-46e2-80c8-855b9eebc33c\n-rw-rw----.   1 vdsm kvm    1048576 Nov 11 10:42 a2697ae9-b8c5-46e2-80c8-855b9eebc33c.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 00:01 a2697ae9-b8c5-46e2-80c8-855b9eebc33c.meta\n~~~\n\nFix Plan:\n==========\n\n1.  Rebase 8e9eed1f-c911-4d59-8e6f-a75a9ee35d10 to have backing file of /dev/ovirt-local/48b3ce3b-bbca-466e-b0a8-fce61f2e63b8.\n2.  Merge /dev/ovirt-local/48b3ce3b-bbca-466e-b0a8-fce61f2e63b8 &lt;&lt;-- 8e9eed1f-c911-4d59-8e6f-a75a9ee35d10\n\n3.  Rebase a2697ae9-b8c5-46e2-80c8-855b9eebc33c to have backing file of /dev/ovirt-local/0eb70516-0bb8-44a6-a4ad-2e3dcf50d985.\n4.  Merge /dev/ovirt-local/0eb70516-0bb8-44a6-a4ad-2e3dcf50d985 &lt;&lt;-- a2697ae9-b8c5-46e2-80c8-855b9eebc33c\n\n5.  Fix the DB.\n\n\n\nAction Plan:\n=============\n\n~~~\n1.  Shutdown the VM.\n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '305a5aef-c391-42ed-9b66-a24949e088dd' AND snapshot_id != 'a67290d0-2114-4104-9376-c7ed932b0d9d';"\n\nFIXING DISK 99200509-995d-4f36-b495-42b795172b42, CHAIN ['48b3ce3b-bbca-466e-b0a8-fce61f2e63b8', '8e9eed1f-c911-4d59-8e6f-a75a9ee35d10']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/48b3ce3b-bbca-466e-b0a8-fce61f2e63b8 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99200509-995d-4f36-b495-42b795172b42/8e9eed1f-c911-4d59-8e6f-a75a9ee35d10'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99200509-995d-4f36-b495-42b795172b42/48b3ce3b-bbca-466e-b0a8-fce61f2e63b8.meta"\n\nlvchange -ay /dev/ovirt-local/48b3ce3b-bbca-466e-b0a8-fce61f2e63b8\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/48b3ce3b-bbca-466e-b0a8-fce61f2e63b8 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99200509-995d-4f36-b495-42b795172b42/8e9eed1f-c911-4d59-8e6f-a75a9ee35d10'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99200509-995d-4f36-b495-42b795172b42/8e9eed1f-c911-4d59-8e6f-a75a9ee35d10.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99200509-995d-4f36-b495-42b795172b42/8e9eed1f-c911-4d59-8e6f-a75a9ee35d10.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99200509-995d-4f36-b495-42b795172b42/8e9eed1f-c911-4d59-8e6f-a75a9ee35d10.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99200509-995d-4f36-b495-42b795172b42/8e9eed1f-c911-4d59-8e6f-a75a9ee35d10.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99200509-995d-4f36-b495-42b795172b42/8e9eed1f-c911-4d59-8e6f-a75a9ee35d10 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99200509-995d-4f36-b495-42b795172b42/8e9eed1f-c911-4d59-8e6f-a75a9ee35d10.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('8e9eed1f-c911-4d59-8e6f-a75a9ee35d10');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a67290d0-2114-4104-9376-c7ed932b0d9d' WHERE image_guid ='48b3ce3b-bbca-466e-b0a8-fce61f2e63b8';"\n\n\n\n\nFIXING DISK 05c12051-5dba-4af0-a95c-080ba0c7de49, CHAIN ['0eb70516-0bb8-44a6-a4ad-2e3dcf50d985', 'a2697ae9-b8c5-46e2-80c8-855b9eebc33c']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/0eb70516-0bb8-44a6-a4ad-2e3dcf50d985 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/05c12051-5dba-4af0-a95c-080ba0c7de49/a2697ae9-b8c5-46e2-80c8-855b9eebc33c'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/05c12051-5dba-4af0-a95c-080ba0c7de49/0eb70516-0bb8-44a6-a4ad-2e3dcf50d985.meta"\n\nlvchange -ay /dev/ovirt-local/0eb70516-0bb8-44a6-a4ad-2e3dcf50d985\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/0eb70516-0bb8-44a6-a4ad-2e3dcf50d985 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/05c12051-5dba-4af0-a95c-080ba0c7de49/a2697ae9-b8c5-46e2-80c8-855b9eebc33c'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/05c12051-5dba-4af0-a95c-080ba0c7de49/a2697ae9-b8c5-46e2-80c8-855b9eebc33c.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/05c12051-5dba-4af0-a95c-080ba0c7de49/a2697ae9-b8c5-46e2-80c8-855b9eebc33c.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/05c12051-5dba-4af0-a95c-080ba0c7de49/a2697ae9-b8c5-46e2-80c8-855b9eebc33c.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/05c12051-5dba-4af0-a95c-080ba0c7de49/a2697ae9-b8c5-46e2-80c8-855b9eebc33c.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/05c12051-5dba-4af0-a95c-080ba0c7de49/a2697ae9-b8c5-46e2-80c8-855b9eebc33c /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/05c12051-5dba-4af0-a95c-080ba0c7de49/a2697ae9-b8c5-46e2-80c8-855b9eebc33c.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('a2697ae9-b8c5-46e2-80c8-855b9eebc33c');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a67290d0-2114-4104-9376-c7ed932b0d9d' WHERE image_guid ='0eb70516-0bb8-44a6-a4ad-2e3dcf50d985';"\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>java01.tool01.postprod.mls.clt1.prod.mlbam.net \n==============================================\n\nrootdisk 90502ca9-666e-4a51-9fee-4b7aab0c6531, CHAIN ['91e95078-3d1e-423b-9adc-2dc0e5402053', '5cb73742-da2d-4d39-8ac9-dbdaa01bf704']\n\nBase image on the /dev/ovirt-local\n\n~~~\n91e95078-3d1e-423b-9adc-2dc0e5402053 ovirt-local -wi-ao----  50.00g  IU_90502ca9-666e-4a51-9fee-4b7aab0c6531,PU_00000000-0000-0000-0000-000000000000,VM_aaeef655-72ba-4c04-9bab-299ba248d946\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/90502ca9-666e-4a51-9fee-4b7aab0c6531:\ntotal 3409630\n\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 Feb 12  2017 91e95078-3d1e-423b-9adc-2dc0e5402053\n-rw-rw----.   1 vdsm kvm    1048576 Feb 12  2017 91e95078-3d1e-423b-9adc-2dc0e5402053.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 12 05:57 91e95078-3d1e-423b-9adc-2dc0e5402053.meta\n\n-rw-rw----.   1 vdsm kvm  854130688 Mar  8 18:01 5cb73742-da2d-4d39-8ac9-dbdaa01bf704\n-rw-rw----.   1 vdsm kvm    1048576 Nov 12 05:57 5cb73742-da2d-4d39-8ac9-dbdaa01bf704.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 05:57 5cb73742-da2d-4d39-8ac9-dbdaa01bf704.meta\n~~~\n\n\nFix Plan:\n==========\n\n1.  Rebase 5cb73742-da2d-4d39-8ac9-dbdaa01bf704 to have backing file of /dev/ovirt-local/91e95078-3d1e-423b-9adc-2dc0e5402053.\n2.  Merge /dev/ovirt-local/91e95078-3d1e-423b-9adc-2dc0e5402053 &lt;&lt;-- 5cb73742-da2d-4d39-8ac9-dbdaa01bf704\n3.  Fix the DB.\n\nAction Plan:\n=============\n\n~~~\n1.  Shutdown the VM.\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'aaeef655-72ba-4c04-9bab-299ba248d946' AND snapshot_id != '7eebb824-2833-482f-83cb-a94d81111316';"\n\nFIXING DISK 90502ca9-666e-4a51-9fee-4b7aab0c6531, CHAIN ['91e95078-3d1e-423b-9adc-2dc0e5402053', '5cb73742-da2d-4d39-8ac9-dbdaa01bf704']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/91e95078-3d1e-423b-9adc-2dc0e5402053 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/90502ca9-666e-4a51-9fee-4b7aab0c6531/5cb73742-da2d-4d39-8ac9-dbdaa01bf704'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/90502ca9-666e-4a51-9fee-4b7aab0c6531/91e95078-3d1e-423b-9adc-2dc0e5402053.meta"\n\nlvchange -ay /dev/ovirt-local/91e95078-3d1e-423b-9adc-2dc0e5402053\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/91e95078-3d1e-423b-9adc-2dc0e5402053 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/90502ca9-666e-4a51-9fee-4b7aab0c6531/5cb73742-da2d-4d39-8ac9-dbdaa01bf704'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/90502ca9-666e-4a51-9fee-4b7aab0c6531/5cb73742-da2d-4d39-8ac9-dbdaa01bf704.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/90502ca9-666e-4a51-9fee-4b7aab0c6531/5cb73742-da2d-4d39-8ac9-dbdaa01bf704.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/90502ca9-666e-4a51-9fee-4b7aab0c6531/5cb73742-da2d-4d39-8ac9-dbdaa01bf704.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/90502ca9-666e-4a51-9fee-4b7aab0c6531/5cb73742-da2d-4d39-8ac9-dbdaa01bf704.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/90502ca9-666e-4a51-9fee-4b7aab0c6531/5cb73742-da2d-4d39-8ac9-dbdaa01bf704 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/90502ca9-666e-4a51-9fee-4b7aab0c6531/5cb73742-da2d-4d39-8ac9-dbdaa01bf704.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('5cb73742-da2d-4d39-8ac9-dbdaa01bf704');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='7eebb824-2833-482f-83cb-a94d81111316' WHERE image_guid ='91e95078-3d1e-423b-9adc-2dc0e5402053';"\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>java02.tool01.postprod.mls.clt1.prod.mlbam.net \n===============================================\n\nrootdisk f8e5c048-af4e-40c4-8d75-5268098102d9, CHAIN ['ab8db9ac-822d-4008-9b6e-87017726d60e', '70fe492c-c1c4-4444-ae45-f5803368115c']\n\nBase image on the /dev/ovirt-local\n\n~~~\n  ab8db9ac-822d-4008-9b6e-87017726d60e ovirt-local -wi-ao----  50.00g   IU_f8e5c048-af4e-40c4-8d75-5268098102d9,PU_00000000-0000-0000-0000-000000000000,VM_46ca0600-c069-4447-97e9-d3d1719bab5d\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8e5c048-af4e-40c4-8d75-5268098102d9:\ntotal 3427006\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 Feb 13  2017 ab8db9ac-822d-4008-9b6e-87017726d60e\n-rw-rw----.   1 vdsm kvm    1048576 Feb 13  2017 ab8db9ac-822d-4008-9b6e-87017726d60e.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 11 08:39 ab8db9ac-822d-4008-9b6e-87017726d60e.meta\n\n-rw-rw----.   1 vdsm kvm  868024320 Mar  8 18:01 70fe492c-c1c4-4444-ae45-f5803368115c\n-rw-rw----.   1 vdsm kvm    1048576 Nov 11 08:39 70fe492c-c1c4-4444-ae45-f5803368115c.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 11 08:39 70fe492c-c1c4-4444-ae45-f5803368115c.meta\n~~~\n\nFix Plan:\n==========\n\n1.  Rebase 70fe492c-c1c4-4444-ae45-f5803368115c to have backing file of /dev/ovirt-local/ab8db9ac-822d-4008-9b6e-87017726d60e.\n2.  Merge /dev/ovirt-local/ab8db9ac-822d-4008-9b6e-87017726d60e &lt;&lt;-- 70fe492c-c1c4-4444-ae45-f5803368115c\n3.  Fix the DB.\n\nAction Plan:\n=============\n\n~~~\n1.  Shutdown the VM.\n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '46ca0600-c069-4447-97e9-d3d1719bab5d' AND snapshot_id != '71c3b2c8-267a-4274-a589-fe8d8dc21415';"\n\nFIXING DISK f8e5c048-af4e-40c4-8d75-5268098102d9, CHAIN ['ab8db9ac-822d-4008-9b6e-87017726d60e', '70fe492c-c1c4-4444-ae45-f5803368115c']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/ab8db9ac-822d-4008-9b6e-87017726d60e -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8e5c048-af4e-40c4-8d75-5268098102d9/70fe492c-c1c4-4444-ae45-f5803368115c'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8e5c048-af4e-40c4-8d75-5268098102d9/ab8db9ac-822d-4008-9b6e-87017726d60e.meta"\n\nlvchange -ay /dev/ovirt-local/ab8db9ac-822d-4008-9b6e-87017726d60e\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ab8db9ac-822d-4008-9b6e-87017726d60e -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8e5c048-af4e-40c4-8d75-5268098102d9/70fe492c-c1c4-4444-ae45-f5803368115c'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8e5c048-af4e-40c4-8d75-5268098102d9/70fe492c-c1c4-4444-ae45-f5803368115c.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8e5c048-af4e-40c4-8d75-5268098102d9/70fe492c-c1c4-4444-ae45-f5803368115c.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8e5c048-af4e-40c4-8d75-5268098102d9/70fe492c-c1c4-4444-ae45-f5803368115c.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8e5c048-af4e-40c4-8d75-5268098102d9/70fe492c-c1c4-4444-ae45-f5803368115c.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8e5c048-af4e-40c4-8d75-5268098102d9/70fe492c-c1c4-4444-ae45-f5803368115c /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f8e5c048-af4e-40c4-8d75-5268098102d9/70fe492c-c1c4-4444-ae45-f5803368115c.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('70fe492c-c1c4-4444-ae45-f5803368115c');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='71c3b2c8-267a-4274-a589-fe8d8dc21415' WHERE image_guid ='ab8db9ac-822d-4008-9b6e-87017726d60e';"\n\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>java02.tool01.postprod.gen.clt1.qa.mlbam.net \n=============================================\n\nrootdisk fcbfb751-cb0a-4d1c-9890-8b376d54c5ce, CHAIN ['d961cede-f36e-4f0d-bd4d-db5b47d9168a', '9e69cf01-3a81-417a-b96e-73fc5450185d']\n\nBase image on the /dev/ovirt-local\n\n~~~\n  d961cede-f36e-4f0d-bd4d-db5b47d9168a ovirt-local -wi-ao----  50.00g   IU_fcbfb751-cb0a-4d1c-9890-8b376d54c5ce,PU_00000000-0000-0000-0000-000000000000,VM_e8bf654c-4221-4edc-bf6e-5b203d4a34a6\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fcbfb751-cb0a-4d1c-9890-8b376d54c5ce:\ntotal 2852182\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 Feb 13  2017 d961cede-f36e-4f0d-bd4d-db5b47d9168a\n-rw-rw----.   1 vdsm kvm    1048576 Feb 13  2017 d961cede-f36e-4f0d-bd4d-db5b47d9168a.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 11 11:06 d961cede-f36e-4f0d-bd4d-db5b47d9168a.meta\n\n-rw-rw----.   1 vdsm kvm  398327808 Mar  8 18:01 9e69cf01-3a81-417a-b96e-73fc5450185d\n-rw-rw----.   1 vdsm kvm    1048576 Nov 11 11:06 9e69cf01-3a81-417a-b96e-73fc5450185d.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 00:19 9e69cf01-3a81-417a-b96e-73fc5450185d.meta\n~~~\n\nFix Plan:\n==========\n\n1.  Rebase 9e69cf01-3a81-417a-b96e-73fc5450185d to have backing file of /dev/ovirt-local/d961cede-f36e-4f0d-bd4d-db5b47d9168a.\n2.  Merge /dev/ovirt-local/d961cede-f36e-4f0d-bd4d-db5b47d9168a &lt;&lt;-- 9e69cf01-3a81-417a-b96e-73fc5450185d\n3.  Fix the DB.\n\nAction Plan:\n=============\n\n~~~\n1.  Shutdown the VM.\n\n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'e8bf654c-4221-4edc-bf6e-5b203d4a34a6' AND snapshot_id != 'a9f6164d-86f7-423a-ae55-3daadec59a76';"\n\nFIXING DISK fcbfb751-cb0a-4d1c-9890-8b376d54c5ce, CHAIN ['d961cede-f36e-4f0d-bd4d-db5b47d9168a', '9e69cf01-3a81-417a-b96e-73fc5450185d']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/d961cede-f36e-4f0d-bd4d-db5b47d9168a -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fcbfb751-cb0a-4d1c-9890-8b376d54c5ce/9e69cf01-3a81-417a-b96e-73fc5450185d'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fcbfb751-cb0a-4d1c-9890-8b376d54c5ce/d961cede-f36e-4f0d-bd4d-db5b47d9168a.meta"\n\nlvchange -ay /dev/ovirt-local/d961cede-f36e-4f0d-bd4d-db5b47d9168a\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/d961cede-f36e-4f0d-bd4d-db5b47d9168a -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fcbfb751-cb0a-4d1c-9890-8b376d54c5ce/9e69cf01-3a81-417a-b96e-73fc5450185d'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fcbfb751-cb0a-4d1c-9890-8b376d54c5ce/9e69cf01-3a81-417a-b96e-73fc5450185d.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fcbfb751-cb0a-4d1c-9890-8b376d54c5ce/9e69cf01-3a81-417a-b96e-73fc5450185d.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fcbfb751-cb0a-4d1c-9890-8b376d54c5ce/9e69cf01-3a81-417a-b96e-73fc5450185d.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fcbfb751-cb0a-4d1c-9890-8b376d54c5ce/9e69cf01-3a81-417a-b96e-73fc5450185d.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fcbfb751-cb0a-4d1c-9890-8b376d54c5ce/9e69cf01-3a81-417a-b96e-73fc5450185d /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fcbfb751-cb0a-4d1c-9890-8b376d54c5ce/9e69cf01-3a81-417a-b96e-73fc5450185d.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('9e69cf01-3a81-417a-b96e-73fc5450185d');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a9f6164d-86f7-423a-ae55-3daadec59a76' WHERE image_guid ='d961cede-f36e-4f0d-bd4d-db5b47d9168a';"\n\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Hi Marcus,\n\nI have updated teh case with AP for each of the VM.\nTry to run the one at a time ensuring the VM starts up successfully.\n\nThanks\nBimal.</text>, <text>Hello Bimal,\ncan we have the fix commands for VM: java01.tool01.postprod.gen.clt1.qa.mlbam.net\n\nthnks.</text>, <text>java01.tool01.postprod.gen.clt1.qa.mlbam.net\n==============================================\n\nrootdisk ee4dff55-0654-4580-8449-e56a8b719eac, CHAIN ['ce9b8c1d-6057-4849-a1de-aac0df6dd19b', 'd1d54e66-0e5c-4471-b307-8e6576bc40c4']\n\nBase image on the /dev/ovirt-local\n\n~~~\n  ce9b8c1d-6057-4849-a1de-aac0df6dd19b ovirt-local -wi-ao----  50.00g   IU_ee4dff55-0654-4580-8449-e56a8b719eac,PU_00000000-0000-0000-0000-000000000000,VM_e9cac042-9c2c-43f1-beb9-e97028c71f95\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac:\ntotal 2906270\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 Feb 13  2017 ce9b8c1d-6057-4849-a1de-aac0df6dd19b\n-rw-rw----.   1 vdsm kvm    1048576 Feb 13  2017 ce9b8c1d-6057-4849-a1de-aac0df6dd19b.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 12 00:38 ce9b8c1d-6057-4849-a1de-aac0df6dd19b.meta\n\n-rw-rw----.   1 vdsm kvm  442499072 Mar  8 18:01 d1d54e66-0e5c-4471-b307-8e6576bc40c4\n-rw-rw----.   1 vdsm kvm    1048576 Nov 12 00:38 d1d54e66-0e5c-4471-b307-8e6576bc40c4.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 00:38 d1d54e66-0e5c-4471-b307-8e6576bc40c4.meta\n~~~\n\n\nFix Plan:\n==========\n\n1.  Rebase d1d54e66-0e5c-4471-b307-8e6576bc40c4 to have backing file of /dev/ovirt-local/ce9b8c1d-6057-4849-a1de-aac0df6dd19b.\n2.  Merge /dev/ovirt-local/ce9b8c1d-6057-4849-a1de-aac0df6dd19b &lt;&lt;-- d1d54e66-0e5c-4471-b307-8e6576bc40c4\n3.  Fix the DB.\n\nAction Plan:\n=============\n\n~~~\n1.  Shutdown the VM.\n\n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'e9cac042-9c2c-43f1-beb9-e97028c71f95' AND snapshot_id != '17feef8c-1342-4d59-9783-89254b082989';"\n\nFIXING DISK ee4dff55-0654-4580-8449-e56a8b719eac, CHAIN ['ce9b8c1d-6057-4849-a1de-aac0df6dd19b', 'd1d54e66-0e5c-4471-b307-8e6576bc40c4']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/ce9b8c1d-6057-4849-a1de-aac0df6dd19b -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac/d1d54e66-0e5c-4471-b307-8e6576bc40c4'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac/ce9b8c1d-6057-4849-a1de-aac0df6dd19b.meta"\n\nlvchange -ay /dev/ovirt-local/ce9b8c1d-6057-4849-a1de-aac0df6dd19b\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ce9b8c1d-6057-4849-a1de-aac0df6dd19b -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac/d1d54e66-0e5c-4471-b307-8e6576bc40c4'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac/d1d54e66-0e5c-4471-b307-8e6576bc40c4.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac/d1d54e66-0e5c-4471-b307-8e6576bc40c4.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac/d1d54e66-0e5c-4471-b307-8e6576bc40c4.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac/d1d54e66-0e5c-4471-b307-8e6576bc40c4.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac/d1d54e66-0e5c-4471-b307-8e6576bc40c4 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac/d1d54e66-0e5c-4471-b307-8e6576bc40c4.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('d1d54e66-0e5c-4471-b307-8e6576bc40c4');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='17feef8c-1342-4d59-9783-89254b082989' WHERE image_guid ='ce9b8c1d-6057-4849-a1de-aac0df6dd19b';"\n\n\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>HI Bimal\nlvchange command is failing.\n\n[root@hyp74 ~]# lvchange -ay /dev/ovirt-local/ce9b8c1d-6057-4849-a1de-aac0df6dd19b\n  Failed to find logical volume "ovirt-local/ce9b8c1d-6057-4849-a1de-aac0df6dd19b"\n\nThis host was shutdown and accidentally restarted today if that makes a difference. About 2 hours ago</text>, <text>The last logs showed java01.tool01.postprod.gen.clt1.qa.mlbam.net running on hyp73.clt1.prod.mlbam.net\nFrom the output, it show you are trying to run the command on hyp74.  The ovirt-local will not be on hyp74.\n\nCan you send the following from hyp73:\n\n~~~\n# lvm lvs -o +tags |grep e9cac042\n\n# cd /rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ee4dff55-0654-4580-8449-e56a8b719eac\n# ls -al\n~~~\n\nThanks\nBimal.</text>, <text>you were correct, i was on the wrong hypervisor thank you!.\n\nAll 10 VMs were fixed sussessfully thank you. We'll have a new list of 10 more Vms soon</text>, <text>Hi Marcus,\n\nThanks for the update.\n\nWill await the list for the next round.\n\nBy any chance, do you have idea how many we need to fix.\n\nThanks\nBimal</text>, <text>Hi Bimal,\nLeft to fix are about 85 Vms. (Down from 120 total)</text>, <text>Thanks Marcus,\n\nWill await the list for the next round.\n\nThanks\nBimal.</text>, <text>Hello Bimal,\nNext VMs to fix:\n\n\njetty01.control01.provis.gen.clt1.qa.mlbam.net\njetty02.control01.provis.gen.clt1.qa.mlbam.net\njetty01.ingest01.ams.gen.clt1.qa.mlbam.net\njetty02.ingest01.ams.gen.clt1.qa.mlbam.net\njetty01.ingest01.ams.gen.clt1.dev.mlbam.net\njetty02.ingest01.ams.gen.clt1.dev.mlbam.net\njetty01.ingest01.ams.gen.clt1.prod.mlbam.net\njetty02.ingest01.ams.gen.clt1.prod.mlbam.net\njetty01.msu01.ams.gen.clt1.prod.mlbam.net\njetty02.msu01.ams.gen.clt1.prod.mlbam.net\njetty01.archive01.jobmgr.gen.clt1.prod.mlbam.net\njetty02.archive01.jobmgr.gen.clt1.prod.mlbam.net\njetty01.worker01.jobmgr.gen.clt1.prod.mlbam.net\njetty02.worker01.jobmgr.gen.clt1.prod.mlbam.net\njetty03.worker01.jobmgr.gen.clt1.prod.mlbam.net\njetty04.worker01.jobmgr.gen.clt1.prod.mlbam.net\njetty01.controller01.splicer.gen.clt1.prod.mlbam.net\njetty02.core01.jobmgr.gen.clt1.prod.mlbam.net\njetty02.web01.jobmgr.gen.clt1.prod.mlbam.net</text>, <text>Hi Marcus,\n\n\nFrom the RHV-M DB we have, the VM's are shown to be running on various hosts.\n\n~~~\njetty01.control01.provis.gen.clt1.qa.mlbam.net \t\thyp65.clt1.prod.mlbam.net\njetty02.control01.provis.gen.clt1.qa.mlbam.net\t\thyp66.clt1.prod.mlbam.net\njetty01.ingest01.ams.gen.clt1.qa.mlbam.net\t\t\thyp23.clt1.prod.mlbam.net\njetty02.ingest01.ams.gen.clt1.qa.mlbam.net\t\t\thyp15.clt1.prod.mlbam.net\njetty01.ingest01.ams.gen.clt1.dev.mlbam.net\t\t\thyp87.clt1.prod.mlbam.net\njetty02.ingest01.ams.gen.clt1.dev.mlbam.net\t\t\thyp95.clt1.prod.mlbam.net\njetty01.ingest01.ams.gen.clt1.prod.mlbam.net\t\thyp85.clt1.prod.mlbam.net\njetty02.ingest01.ams.gen.clt1.prod.mlbam.net\t\thyp95.clt1.prod.mlbam.net\njetty01.msu01.ams.gen.clt1.prod.mlbam.net\t\t\thyp10.clt1.prod.mlbam.net\njetty02.msu01.ams.gen.clt1.prod.mlbam.net\t\t\thyp72.clt1.prod.mlbam.net\njetty01.archive01.jobmgr.gen.clt1.prod.mlbam.net\thyp30.clt1.prod.mlbam.net\njetty02.archive01.jobmgr.gen.clt1.prod.mlbam.net\thyp80.clt1.prod.mlbam.net\njetty01.worker01.jobmgr.gen.clt1.prod.mlbam.net\t\thyp04.clt1.prod.mlbam.net\njetty02.worker01.jobmgr.gen.clt1.prod.mlbam.net \thyp13.clt1.prod.mlbam.net\njetty03.worker01.jobmgr.gen.clt1.prod.mlbam.net\t\thyp68.clt1.prod.mlbam.net\njetty04.worker01.jobmgr.gen.clt1.prod.mlbam.net\t\thyp69.clt1.prod.mlbam.net\njetty01.controller01.splicer.gen.clt1.prod.mlbam.net  hyp10.clt1.prod.mlbam.net\njetty02.core01.jobmgr.gen.clt1.prod.mlbam.net\t\thyp66.clt1.prod.mlbam.net\njetty02.web01.jobmgr.gen.clt1.prod.mlbam.net\t\thyp67.clt1.prod.mlbam.net\n~~~\n\n\nInstead of providing sos report for the hosts, can you run the followiong from the RHV-M.\n\n~~~\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp65.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp65-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp65.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp65-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp66.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp66-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp66.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp66-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp23.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp23-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp23.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp23-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp15.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp15-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp15.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp15-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp87.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp87-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp87.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp87-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp95.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp95-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp95.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp95-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp85.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp85-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp85.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp85-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp95.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp95-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp95.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp95-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp10.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp10-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp10.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp10-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp72.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp72-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp72.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp72-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp30.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp30-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp30.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp30-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp80.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp80-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp80.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp80-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp04.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp04-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp04.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp04-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp13.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp13-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp13.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp13-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp68.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp68-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp68.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp68-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp69.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp69-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp69.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp69-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp10.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp10-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp10.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp10-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp66.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp66-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp66.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp66-lvm_lvs'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp67.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp67-ls_lR'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp67.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt; /tmp/hyp67-lvm_lvs'\n~~~\n\n\nThen from RHV-M host, sftp to get files and upload them to the case.\n\n~~~\ncd /tmp/\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp65.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp66.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp23.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp15.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp87.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp95.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp85.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp95.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp10.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp72.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp30.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp80.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp04.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp13.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp68.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp69.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp10.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp66.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp67.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n~~~\n\ntar up the /tmp/hyp* and upload to the case.\n\nAlso can you provide logcollector from the RHV-M only.\nWe have one but from Nov 2017.\n\nThanks\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>requested files are attached thaanks</text>, <text>Reviewing logs - all me some time ...</text>, <text>jetty04.worker01.jobmgr.gen.clt1.prod.mlbam.net\t\t\nhyp69.clt1.prod.mlbam.net\n===============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '530f5703-93c7-4593-a521-abec99f790c0' AND snapshot_id != '760d4ce0-bdf2-457e-a544-8054f386ef11';"\n\nFIXING DISK 8d63bcf9-3af6-4441-98c3-a0a395e8c121, CHAIN ['6b6437c9-2ac1-494a-9fcd-e2ef0dc90aea', '8ca16242-fefb-418c-97cc-4b05c37b3a71']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/6b6437c9-2ac1-494a-9fcd-e2ef0dc90aea -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d63bcf9-3af6-4441-98c3-a0a395e8c121/8ca16242-fefb-418c-97cc-4b05c37b3a71'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d63bcf9-3af6-4441-98c3-a0a395e8c121/6b6437c9-2ac1-494a-9fcd-e2ef0dc90aea.meta"\nlvchange -ay /dev/ovirt-local/6b6437c9-2ac1-494a-9fcd-e2ef0dc90aea\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/6b6437c9-2ac1-494a-9fcd-e2ef0dc90aea -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d63bcf9-3af6-4441-98c3-a0a395e8c121/8ca16242-fefb-418c-97cc-4b05c37b3a71'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d63bcf9-3af6-4441-98c3-a0a395e8c121/8ca16242-fefb-418c-97cc-4b05c37b3a71.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d63bcf9-3af6-4441-98c3-a0a395e8c121/8ca16242-fefb-418c-97cc-4b05c37b3a71.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d63bcf9-3af6-4441-98c3-a0a395e8c121/8ca16242-fefb-418c-97cc-4b05c37b3a71.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d63bcf9-3af6-4441-98c3-a0a395e8c121/8ca16242-fefb-418c-97cc-4b05c37b3a71.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d63bcf9-3af6-4441-98c3-a0a395e8c121/8ca16242-fefb-418c-97cc-4b05c37b3a71 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d63bcf9-3af6-4441-98c3-a0a395e8c121/8ca16242-fefb-418c-97cc-4b05c37b3a71.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('8ca16242-fefb-418c-97cc-4b05c37b3a71');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='760d4ce0-bdf2-457e-a544-8054f386ef11' WHERE image_guid ='6b6437c9-2ac1-494a-9fcd-e2ef0dc90aea';"\n\n\n~~~\n\n\n\njetty01.controller01.splicer.gen.clt1.prod.mlbam.net  \nhyp10.clt1.prod.mlbam.net\n===============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '9df1452f-9f61-4fa2-8906-9cb24fc96efb' AND snapshot_id != '97068467-58ef-4461-82f1-025435aa8f2e';"\n\nFIXING DISK c8b43984-39e5-4c07-8032-d6af8683bce6, CHAIN ['4cb47712-50f2-4b26-83fb-f02273224feb', '0424d9c2-1273-4f71-8816-3c04ebcbb234']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/4cb47712-50f2-4b26-83fb-f02273224feb -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b43984-39e5-4c07-8032-d6af8683bce6/0424d9c2-1273-4f71-8816-3c04ebcbb234'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b43984-39e5-4c07-8032-d6af8683bce6/4cb47712-50f2-4b26-83fb-f02273224feb.meta"\nlvchange -ay /dev/ovirt-local/4cb47712-50f2-4b26-83fb-f02273224feb\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/4cb47712-50f2-4b26-83fb-f02273224feb -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b43984-39e5-4c07-8032-d6af8683bce6/0424d9c2-1273-4f71-8816-3c04ebcbb234'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b43984-39e5-4c07-8032-d6af8683bce6/0424d9c2-1273-4f71-8816-3c04ebcbb234.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b43984-39e5-4c07-8032-d6af8683bce6/0424d9c2-1273-4f71-8816-3c04ebcbb234.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b43984-39e5-4c07-8032-d6af8683bce6/0424d9c2-1273-4f71-8816-3c04ebcbb234.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b43984-39e5-4c07-8032-d6af8683bce6/0424d9c2-1273-4f71-8816-3c04ebcbb234.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b43984-39e5-4c07-8032-d6af8683bce6/0424d9c2-1273-4f71-8816-3c04ebcbb234 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c8b43984-39e5-4c07-8032-d6af8683bce6/0424d9c2-1273-4f71-8816-3c04ebcbb234.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0424d9c2-1273-4f71-8816-3c04ebcbb234');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='97068467-58ef-4461-82f1-025435aa8f2e' WHERE image_guid ='4cb47712-50f2-4b26-83fb-f02273224feb';"\n\n\n~~~\n\n\njetty02.core01.jobmgr.gen.clt1.prod.mlbam.net\t\t\nhyp66.clt1.prod.mlbam.net\n===============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '5e6c0789-9226-4e6b-9cd7-bdf265dc0621' AND snapshot_id != '7059af57-0b64-4752-9623-a04b9d5d9604';"\n\nFIXING DISK c0d7d604-af1a-4edf-abf7-6228f360dfd6, CHAIN ['09564095-1427-476c-9f3a-54b9e52d1377', 'd053f595-68e7-4a0c-8d36-8d9e7ccad19c']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/09564095-1427-476c-9f3a-54b9e52d1377 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c0d7d604-af1a-4edf-abf7-6228f360dfd6/d053f595-68e7-4a0c-8d36-8d9e7ccad19c'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c0d7d604-af1a-4edf-abf7-6228f360dfd6/09564095-1427-476c-9f3a-54b9e52d1377.meta"\nlvchange -ay /dev/ovirt-local/09564095-1427-476c-9f3a-54b9e52d1377\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/09564095-1427-476c-9f3a-54b9e52d1377 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c0d7d604-af1a-4edf-abf7-6228f360dfd6/d053f595-68e7-4a0c-8d36-8d9e7ccad19c'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c0d7d604-af1a-4edf-abf7-6228f360dfd6/d053f595-68e7-4a0c-8d36-8d9e7ccad19c.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c0d7d604-af1a-4edf-abf7-6228f360dfd6/d053f595-68e7-4a0c-8d36-8d9e7ccad19c.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c0d7d604-af1a-4edf-abf7-6228f360dfd6/d053f595-68e7-4a0c-8d36-8d9e7ccad19c.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c0d7d604-af1a-4edf-abf7-6228f360dfd6/d053f595-68e7-4a0c-8d36-8d9e7ccad19c.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c0d7d604-af1a-4edf-abf7-6228f360dfd6/d053f595-68e7-4a0c-8d36-8d9e7ccad19c /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c0d7d604-af1a-4edf-abf7-6228f360dfd6/d053f595-68e7-4a0c-8d36-8d9e7ccad19c.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('d053f595-68e7-4a0c-8d36-8d9e7ccad19c');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='7059af57-0b64-4752-9623-a04b9d5d9604' WHERE image_guid ='09564095-1427-476c-9f3a-54b9e52d1377';"\n\n~~~\n\n\njetty02.web01.jobmgr.gen.clt1.prod.mlbam.net\t\t\nhyp67.clt1.prod.mlbam.net\n===============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'e1061272-49d5-42ca-90e5-9f4e047433f5' AND snapshot_id != '93e42a74-aaae-4df5-a88b-5f774e7d6f5b';"\n\nFIXING DISK c664ce90-7352-4326-974c-d06c6baa2173, CHAIN ['ad3a0b1c-0171-4714-996f-628df9843796', 'b1e9bba4-9c02-419f-8129-3ebbfb491e2d']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/ad3a0b1c-0171-4714-996f-628df9843796 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c664ce90-7352-4326-974c-d06c6baa2173/b1e9bba4-9c02-419f-8129-3ebbfb491e2d'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c664ce90-7352-4326-974c-d06c6baa2173/ad3a0b1c-0171-4714-996f-628df9843796.meta"\nlvchange -ay /dev/ovirt-local/ad3a0b1c-0171-4714-996f-628df9843796\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ad3a0b1c-0171-4714-996f-628df9843796 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c664ce90-7352-4326-974c-d06c6baa2173/b1e9bba4-9c02-419f-8129-3ebbfb491e2d'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c664ce90-7352-4326-974c-d06c6baa2173/b1e9bba4-9c02-419f-8129-3ebbfb491e2d.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c664ce90-7352-4326-974c-d06c6baa2173/b1e9bba4-9c02-419f-8129-3ebbfb491e2d.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c664ce90-7352-4326-974c-d06c6baa2173/b1e9bba4-9c02-419f-8129-3ebbfb491e2d.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c664ce90-7352-4326-974c-d06c6baa2173/b1e9bba4-9c02-419f-8129-3ebbfb491e2d.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c664ce90-7352-4326-974c-d06c6baa2173/b1e9bba4-9c02-419f-8129-3ebbfb491e2d /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c664ce90-7352-4326-974c-d06c6baa2173/b1e9bba4-9c02-419f-8129-3ebbfb491e2d.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('b1e9bba4-9c02-419f-8129-3ebbfb491e2d');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='93e42a74-aaae-4df5-a88b-5f774e7d6f5b' WHERE image_guid ='ad3a0b1c-0171-4714-996f-628df9843796';"\n\n\n~~~</text>, <text>jetty01.worker01.jobmgr.gen.clt1.prod.mlbam.net\t\t\n hyp04.clt1.prod.mlbam.net\n================================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '1655dc07-6b39-49ec-b15c-d06a9a701977' AND snapshot_id != '902b14b7-c40c-4d73-93b2-d927c63fb3fb';"\n\nFIXING DISK c3831f60-84cb-4569-882b-88e70d8e13e6, CHAIN ['bb93adf5-8c26-43a4-991b-51dda6fb3436', 'c18eadc1-1364-44d7-95f6-98889c7b05cf']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/bb93adf5-8c26-43a4-991b-51dda6fb3436 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c3831f60-84cb-4569-882b-88e70d8e13e6/c18eadc1-1364-44d7-95f6-98889c7b05cf'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c3831f60-84cb-4569-882b-88e70d8e13e6/bb93adf5-8c26-43a4-991b-51dda6fb3436.meta"\nlvchange -ay /dev/ovirt-local/bb93adf5-8c26-43a4-991b-51dda6fb3436\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/bb93adf5-8c26-43a4-991b-51dda6fb3436 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c3831f60-84cb-4569-882b-88e70d8e13e6/c18eadc1-1364-44d7-95f6-98889c7b05cf'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c3831f60-84cb-4569-882b-88e70d8e13e6/c18eadc1-1364-44d7-95f6-98889c7b05cf.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c3831f60-84cb-4569-882b-88e70d8e13e6/c18eadc1-1364-44d7-95f6-98889c7b05cf.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c3831f60-84cb-4569-882b-88e70d8e13e6/c18eadc1-1364-44d7-95f6-98889c7b05cf.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c3831f60-84cb-4569-882b-88e70d8e13e6/c18eadc1-1364-44d7-95f6-98889c7b05cf.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c3831f60-84cb-4569-882b-88e70d8e13e6/c18eadc1-1364-44d7-95f6-98889c7b05cf /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c3831f60-84cb-4569-882b-88e70d8e13e6/c18eadc1-1364-44d7-95f6-98889c7b05cf.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('c18eadc1-1364-44d7-95f6-98889c7b05cf');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='902b14b7-c40c-4d73-93b2-d927c63fb3fb' WHERE image_guid ='bb93adf5-8c26-43a4-991b-51dda6fb3436';"\n\n~~~\n\n\n\njetty02.worker01.jobmgr.gen.clt1.prod.mlbam.net \t\nhyp13.clt1.prod.mlbam.net\n===============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '411324fc-44b8-4902-a690-03b99bbb3e67' AND snapshot_id != 'a739626b-5f25-4d84-94a7-7ba0de18a1de';"\n\nFIXING DISK 2c29f2b1-78f4-48da-a5bc-2b8ae74d5c31, CHAIN ['e5e37226-959b-46d6-81fe-1c8696721953', 'e235e505-95b4-4e7c-a89a-572c89193f4e']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/e5e37226-959b-46d6-81fe-1c8696721953 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c29f2b1-78f4-48da-a5bc-2b8ae74d5c31/e235e505-95b4-4e7c-a89a-572c89193f4e'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c29f2b1-78f4-48da-a5bc-2b8ae74d5c31/e5e37226-959b-46d6-81fe-1c8696721953.meta"\nlvchange -ay /dev/ovirt-local/e5e37226-959b-46d6-81fe-1c8696721953\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/e5e37226-959b-46d6-81fe-1c8696721953 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c29f2b1-78f4-48da-a5bc-2b8ae74d5c31/e235e505-95b4-4e7c-a89a-572c89193f4e'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c29f2b1-78f4-48da-a5bc-2b8ae74d5c31/e235e505-95b4-4e7c-a89a-572c89193f4e.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c29f2b1-78f4-48da-a5bc-2b8ae74d5c31/e235e505-95b4-4e7c-a89a-572c89193f4e.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c29f2b1-78f4-48da-a5bc-2b8ae74d5c31/e235e505-95b4-4e7c-a89a-572c89193f4e.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c29f2b1-78f4-48da-a5bc-2b8ae74d5c31/e235e505-95b4-4e7c-a89a-572c89193f4e.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c29f2b1-78f4-48da-a5bc-2b8ae74d5c31/e235e505-95b4-4e7c-a89a-572c89193f4e /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c29f2b1-78f4-48da-a5bc-2b8ae74d5c31/e235e505-95b4-4e7c-a89a-572c89193f4e.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('e235e505-95b4-4e7c-a89a-572c89193f4e');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a739626b-5f25-4d84-94a7-7ba0de18a1de' WHERE image_guid ='e5e37226-959b-46d6-81fe-1c8696721953';"\n\n~~~\n\n\n\njetty03.worker01.jobmgr.gen.clt1.prod.mlbam.net\t\t\nhyp68.clt1.prod.mlbam.net\n================================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0fb7d001-2dec-48ea-9e8d-45276a5b0e77' AND snapshot_id != '7f29e3c7-c7bf-4479-acdd-b4fafd36338e';"\n\nFIXING DISK 146ef022-e788-4990-85f1-145047c03a45, CHAIN ['885758e9-8c32-4f31-8690-2a472aaf5f77', '14a59a9c-53cf-4237-bb46-91aea9c4bf35', '53e78fe0-df6e-4711-bc7c-d0ed44d3b6d3']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/14a59a9c-53cf-4237-bb46-91aea9c4bf35 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/53e78fe0-df6e-4711-bc7c-d0ed44d3b6d3'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/53e78fe0-df6e-4711-bc7c-d0ed44d3b6d3.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/53e78fe0-df6e-4711-bc7c-d0ed44d3b6d3.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/53e78fe0-df6e-4711-bc7c-d0ed44d3b6d3.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/53e78fe0-df6e-4711-bc7c-d0ed44d3b6d3.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/53e78fe0-df6e-4711-bc7c-d0ed44d3b6d3 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/53e78fe0-df6e-4711-bc7c-d0ed44d3b6d3.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/885758e9-8c32-4f31-8690-2a472aaf5f77 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/14a59a9c-53cf-4237-bb46-91aea9c4bf35'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/885758e9-8c32-4f31-8690-2a472aaf5f77.meta"\nlvchange -ay /dev/ovirt-local/885758e9-8c32-4f31-8690-2a472aaf5f77\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/885758e9-8c32-4f31-8690-2a472aaf5f77 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/14a59a9c-53cf-4237-bb46-91aea9c4bf35'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/14a59a9c-53cf-4237-bb46-91aea9c4bf35.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/14a59a9c-53cf-4237-bb46-91aea9c4bf35.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/14a59a9c-53cf-4237-bb46-91aea9c4bf35.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/14a59a9c-53cf-4237-bb46-91aea9c4bf35.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/14a59a9c-53cf-4237-bb46-91aea9c4bf35 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/146ef022-e788-4990-85f1-145047c03a45/14a59a9c-53cf-4237-bb46-91aea9c4bf35.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('14a59a9c-53cf-4237-bb46-91aea9c4bf35','53e78fe0-df6e-4711-bc7c-d0ed44d3b6d3');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='7f29e3c7-c7bf-4479-acdd-b4fafd36338e' WHERE image_guid ='885758e9-8c32-4f31-8690-2a472aaf5f77';"\n\n~~~</text>, <text>jetty02.msu01.ams.gen.clt1.prod.mlbam.net\nhyp72.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'c97701e3-f286-451b-8e38-ebe68c60e52f' AND snapshot_id != '6d449f48-9948-4cf8-ab68-a8629000f813';"\n\nFIXING DISK 1df7e670-7240-447c-94ba-67a4374b276f, CHAIN ['2d334956-e41a-442e-b032-4b0c04d0fdf7', '00529f6c-14e2-42ef-b95e-43b5dfd230cd', '17b6f5a4-0150-4e27-a9eb-4c7e54c12aad']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/00529f6c-14e2-42ef-b95e-43b5dfd230cd -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/17b6f5a4-0150-4e27-a9eb-4c7e54c12aad'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/17b6f5a4-0150-4e27-a9eb-4c7e54c12aad.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/17b6f5a4-0150-4e27-a9eb-4c7e54c12aad.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/17b6f5a4-0150-4e27-a9eb-4c7e54c12aad.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/17b6f5a4-0150-4e27-a9eb-4c7e54c12aad.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/17b6f5a4-0150-4e27-a9eb-4c7e54c12aad /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/17b6f5a4-0150-4e27-a9eb-4c7e54c12aad.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/2d334956-e41a-442e-b032-4b0c04d0fdf7 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/00529f6c-14e2-42ef-b95e-43b5dfd230cd'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/2d334956-e41a-442e-b032-4b0c04d0fdf7.meta"\nlvchange -ay /dev/ovirt-local/2d334956-e41a-442e-b032-4b0c04d0fdf7\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/2d334956-e41a-442e-b032-4b0c04d0fdf7 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/00529f6c-14e2-42ef-b95e-43b5dfd230cd'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/00529f6c-14e2-42ef-b95e-43b5dfd230cd.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/00529f6c-14e2-42ef-b95e-43b5dfd230cd.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/00529f6c-14e2-42ef-b95e-43b5dfd230cd.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/00529f6c-14e2-42ef-b95e-43b5dfd230cd.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/00529f6c-14e2-42ef-b95e-43b5dfd230cd /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1df7e670-7240-447c-94ba-67a4374b276f/00529f6c-14e2-42ef-b95e-43b5dfd230cd.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('00529f6c-14e2-42ef-b95e-43b5dfd230cd','17b6f5a4-0150-4e27-a9eb-4c7e54c12aad');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='6d449f48-9948-4cf8-ab68-a8629000f813' WHERE image_guid ='2d334956-e41a-442e-b032-4b0c04d0fdf7';"\n\n~~~\n\njetty01.archive01.jobmgr.gen.clt1.prod.mlbam.net\nhyp30.clt1.prod.mlbam.net\n=================================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '1a602cc3-c2eb-4779-bb44-ca3f82162788' AND snapshot_id != '34cfe960-c15e-41d8-ac48-8f738e44981c';"\n\nFIXING DISK 5542ddf0-89a2-4b66-ba1c-e620ce7a3d2e, CHAIN ['0f0f811d-2111-4023-a6f1-480074352ec0', '0c8df76e-d0de-4ee5-bd3a-200db756ab60']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/0f0f811d-2111-4023-a6f1-480074352ec0 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5542ddf0-89a2-4b66-ba1c-e620ce7a3d2e/0c8df76e-d0de-4ee5-bd3a-200db756ab60'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5542ddf0-89a2-4b66-ba1c-e620ce7a3d2e/0f0f811d-2111-4023-a6f1-480074352ec0.meta"\nlvchange -ay /dev/ovirt-local/0f0f811d-2111-4023-a6f1-480074352ec0\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/0f0f811d-2111-4023-a6f1-480074352ec0 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5542ddf0-89a2-4b66-ba1c-e620ce7a3d2e/0c8df76e-d0de-4ee5-bd3a-200db756ab60'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5542ddf0-89a2-4b66-ba1c-e620ce7a3d2e/0c8df76e-d0de-4ee5-bd3a-200db756ab60.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5542ddf0-89a2-4b66-ba1c-e620ce7a3d2e/0c8df76e-d0de-4ee5-bd3a-200db756ab60.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5542ddf0-89a2-4b66-ba1c-e620ce7a3d2e/0c8df76e-d0de-4ee5-bd3a-200db756ab60.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5542ddf0-89a2-4b66-ba1c-e620ce7a3d2e/0c8df76e-d0de-4ee5-bd3a-200db756ab60.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5542ddf0-89a2-4b66-ba1c-e620ce7a3d2e/0c8df76e-d0de-4ee5-bd3a-200db756ab60 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5542ddf0-89a2-4b66-ba1c-e620ce7a3d2e/0c8df76e-d0de-4ee5-bd3a-200db756ab60.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0c8df76e-d0de-4ee5-bd3a-200db756ab60');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='34cfe960-c15e-41d8-ac48-8f738e44981c' WHERE image_guid ='0f0f811d-2111-4023-a6f1-480074352ec0';"\n\n~~~\n\n\n\njetty02.archive01.jobmgr.gen.clt1.prod.mlbam.net\nhyp80.clt1.prod.mlbam.net\n=================================================\nThis VM has extra lv image 683d8033 on the ovirt-local [I1] and on NFS share.\nThe one on NFS share is in use. So will need to quarantine it first.\nLater can remove it.\n\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ac7da89a-2e41-480e-ae1d-12ba281735f9' AND snapshot_id != '33083dbb-5549-48f1-9382-9bc29176164e';"\n\nFIXING DISK 55e079d2-3092-4718-8183-5e9303e8bbe9, CHAIN ['3d1ecb45-1113-4ea8-bef1-ee41cb4db565', '683d8033-6fe8-4408-95f7-7d1dd70fe07f']\n\nHost Commands:\n==============\nlvchange -an /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvchange --deltag 'IU_642a4077-adb5-4ea3-bcd8-fcf309479d4b' /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvchange --deltag 'VM_7b96fa60-0cd2-43f5-b595-da1f78d6de88' /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvrename /dev/ovirt-local/d1469074-5f8d-4671-93a7-70edfdacc770 /dev/ovirt-local/_temp_683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3d1ecb45-1113-4ea8-bef1-ee41cb4db565 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/3d1ecb45-1113-4ea8-bef1-ee41cb4db565.meta"\n\nlvchange -ay /dev/ovirt-local/3d1ecb45-1113-4ea8-bef1-ee41cb4db565\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3d1ecb45-1113-4ea8-bef1-ee41cb4db565 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('683d8033-6fe8-4408-95f7-7d1dd70fe07f');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='33083dbb-5549-48f1-9382-9bc29176164e' WHERE image_guid ='3d1ecb45-1113-4ea8-bef1-ee41cb4db565';"\n\n\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS\n\n\nINVESTIGATION:\n~~~~~~~~~~~~~~~\n// Working Notes - these notes are not intended as a meaningful communication\n// but rather an indicator of current thought processes and reference.\n// Please feel free to comment and ask questions concerning them.\n\n~~~\nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 3d1ecb45-1113-4ea8-bef1-ee41cb4db565 | 55e079d2-3092-4718-8183-5e9303e8bbe9 | 8c202ef9-ff33-4292-a283-5848ddf9374a | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-02-24 14:39:26+00 |           2 |             4 | f\n 683d8033-6fe8-4408-95f7-7d1dd70fe07f | 55e079d2-3092-4718-8183-5e9303e8bbe9 | 33083dbb-5549-48f1-9382-9bc29176164e | 3d1ecb45-1113-4ea8-bef1-ee41cb4db565 |           1 | 2017-11-11 05:17:25+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = 683d8033-6fe8-4408-95f7-7d1dd70fe07f , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 905913856\n    For image_guid = 3d1ecb45-1113-4ea8-bef1-ee41cb4db565 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------\n 33083dbb-5549-48f1-9382-9bc29176164e | ac7da89a-2e41-480e-ae1d-12ba281735f9 | ACTIVE        | OK     | Active VM                    | 2017-02-24 14:39:21.136+00\n 8c202ef9-ff33-4292-a283-5848ddf9374a | ac7da89a-2e41-480e-ae1d-12ba281735f9 | REGULAR       | OK     | Backup__2017-11-11__05.17.22 | 2017-11-11 05:17:25.229+00\n(2 rows)\n~~~\n\n~~~\n  3d1ecb45-1113-4ea8-bef1-ee41cb4db565       ovirt-local -wi-ao----  50.00g  IU_55e079d2-3092-4718-8183-5e9303e8bbe9,PU_00000000-0000-0000-0000-000000000000,VM_ac7da89a-2e41-480e-ae1d-12ba281735f9\n  683d8033-6fe8-4408-95f7-7d1dd70fe07f       ovirt-local -wi-a-----  50.00g  IU_55e079d2-3092-4718-8183-5e9303e8bbe9,PU_00000000-0000-0000-0000-000000000000,VM_ac7da89a-2e41-480e-ae1d-12ba281735f9\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9:\ntotal 3249542\n\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 Feb 24  2017 3d1ecb45-1113-4ea8-bef1-ee41cb4db565\n-rw-rw----.   1 vdsm kvm    1048576 Feb 24  2017 3d1ecb45-1113-4ea8-bef1-ee41cb4db565.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 11 05:17 3d1ecb45-1113-4ea8-bef1-ee41cb4db565.meta\n\n-rw-rw----.   1 vdsm kvm  722796544 Apr  5 14:30 683d8033-6fe8-4408-95f7-7d1dd70fe07f\n-rw-rw----.   1 vdsm kvm    1048576 Nov 11 05:17 683d8033-6fe8-4408-95f7-7d1dd70fe07f.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 11 05:17 683d8033-6fe8-4408-95f7-7d1dd70fe07f.meta\n~~~</text>, <text>jetty01.ingest01.ams.gen.clt1.prod.mlbam.net\t\t\nhyp85.clt1.prod.mlbam.net\n============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0a82d4ab-cfbe-4e00-9932-ce9d40421db6' AND snapshot_id != '102b6d03-4652-4b18-bd6b-cb694383de25';"\n\nFIXING DISK 86591062-2047-4059-ab23-c8f08fdc7316, CHAIN ['52acffe7-b171-44cb-951e-7523187d4d1d', '3802bf2f-69dd-4c9c-88fc-1ce36da2acce', '9cba6a54-2a61-4a51-8dc1-815c8c0d8748']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/3802bf2f-69dd-4c9c-88fc-1ce36da2acce -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/9cba6a54-2a61-4a51-8dc1-815c8c0d8748'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/9cba6a54-2a61-4a51-8dc1-815c8c0d8748.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/9cba6a54-2a61-4a51-8dc1-815c8c0d8748.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/9cba6a54-2a61-4a51-8dc1-815c8c0d8748.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/9cba6a54-2a61-4a51-8dc1-815c8c0d8748.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/9cba6a54-2a61-4a51-8dc1-815c8c0d8748 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/9cba6a54-2a61-4a51-8dc1-815c8c0d8748.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/52acffe7-b171-44cb-951e-7523187d4d1d -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/3802bf2f-69dd-4c9c-88fc-1ce36da2acce'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/52acffe7-b171-44cb-951e-7523187d4d1d.meta"\nlvchange -ay /dev/ovirt-local/52acffe7-b171-44cb-951e-7523187d4d1d\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/52acffe7-b171-44cb-951e-7523187d4d1d -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/3802bf2f-69dd-4c9c-88fc-1ce36da2acce'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/3802bf2f-69dd-4c9c-88fc-1ce36da2acce.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/3802bf2f-69dd-4c9c-88fc-1ce36da2acce.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/3802bf2f-69dd-4c9c-88fc-1ce36da2acce.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/3802bf2f-69dd-4c9c-88fc-1ce36da2acce.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/3802bf2f-69dd-4c9c-88fc-1ce36da2acce /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86591062-2047-4059-ab23-c8f08fdc7316/3802bf2f-69dd-4c9c-88fc-1ce36da2acce.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('3802bf2f-69dd-4c9c-88fc-1ce36da2acce','9cba6a54-2a61-4a51-8dc1-815c8c0d8748');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='102b6d03-4652-4b18-bd6b-cb694383de25' WHERE image_guid ='52acffe7-b171-44cb-951e-7523187d4d1d';"\n\n\n~~~\n\n\n\njetty02.ingest01.ams.gen.clt1.prod.mlbam.net\nhyp95.clt1.prod.mlbam.net\n=============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '9eedc29e-b011-40cc-9ed3-8630e0599f2e' AND snapshot_id != '69e5f394-ec17-4a60-aaaf-14e15c15e6f0';"\n\nFIXING DISK 49c09374-93f7-402b-8ee5-9fa7f54b1a67, CHAIN ['59b85c48-297e-4783-82da-47d157a824d2', '6a5bbaee-e023-42a4-b4e1-6560561dd4f5', 'bb78c9c7-1a01-4190-b0eb-edabb476657a']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/6a5bbaee-e023-42a4-b4e1-6560561dd4f5 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/bb78c9c7-1a01-4190-b0eb-edabb476657a'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/bb78c9c7-1a01-4190-b0eb-edabb476657a.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/bb78c9c7-1a01-4190-b0eb-edabb476657a.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/bb78c9c7-1a01-4190-b0eb-edabb476657a.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/bb78c9c7-1a01-4190-b0eb-edabb476657a.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/bb78c9c7-1a01-4190-b0eb-edabb476657a /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/bb78c9c7-1a01-4190-b0eb-edabb476657a.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/59b85c48-297e-4783-82da-47d157a824d2 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/6a5bbaee-e023-42a4-b4e1-6560561dd4f5'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/59b85c48-297e-4783-82da-47d157a824d2.meta"\nlvchange -ay /dev/ovirt-local/59b85c48-297e-4783-82da-47d157a824d2\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/59b85c48-297e-4783-82da-47d157a824d2 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/6a5bbaee-e023-42a4-b4e1-6560561dd4f5'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/6a5bbaee-e023-42a4-b4e1-6560561dd4f5.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/6a5bbaee-e023-42a4-b4e1-6560561dd4f5.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/6a5bbaee-e023-42a4-b4e1-6560561dd4f5.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/6a5bbaee-e023-42a4-b4e1-6560561dd4f5.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/6a5bbaee-e023-42a4-b4e1-6560561dd4f5 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49c09374-93f7-402b-8ee5-9fa7f54b1a67/6a5bbaee-e023-42a4-b4e1-6560561dd4f5.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('6a5bbaee-e023-42a4-b4e1-6560561dd4f5','bb78c9c7-1a01-4190-b0eb-edabb476657a');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='69e5f394-ec17-4a60-aaaf-14e15c15e6f0' WHERE image_guid ='59b85c48-297e-4783-82da-47d157a824d2';"\n\n\n~~~\n\n\n\njetty01.msu01.ams.gen.clt1.prod.mlbam.net\nhyp10.clt1.prod.mlbam.net\n===========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'c6229a91-e9d2-4b2f-ba3e-5a2d4f08e5f4' AND snapshot_id != '0ddf8835-0b18-49ef-bde9-b7b6aafee25b';"\n\nFIXING DISK ea39b1bf-6290-44cb-a44f-839da5f52621, CHAIN ['d5c301dc-198d-4338-998c-5196cd9f5752', '32bc0532-99f8-4b23-9d3f-761218e449eb']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/d5c301dc-198d-4338-998c-5196cd9f5752 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea39b1bf-6290-44cb-a44f-839da5f52621/32bc0532-99f8-4b23-9d3f-761218e449eb'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea39b1bf-6290-44cb-a44f-839da5f52621/d5c301dc-198d-4338-998c-5196cd9f5752.meta"\nlvchange -ay /dev/ovirt-local/d5c301dc-198d-4338-998c-5196cd9f5752\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/d5c301dc-198d-4338-998c-5196cd9f5752 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea39b1bf-6290-44cb-a44f-839da5f52621/32bc0532-99f8-4b23-9d3f-761218e449eb'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea39b1bf-6290-44cb-a44f-839da5f52621/32bc0532-99f8-4b23-9d3f-761218e449eb.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea39b1bf-6290-44cb-a44f-839da5f52621/32bc0532-99f8-4b23-9d3f-761218e449eb.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea39b1bf-6290-44cb-a44f-839da5f52621/32bc0532-99f8-4b23-9d3f-761218e449eb.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea39b1bf-6290-44cb-a44f-839da5f52621/32bc0532-99f8-4b23-9d3f-761218e449eb.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea39b1bf-6290-44cb-a44f-839da5f52621/32bc0532-99f8-4b23-9d3f-761218e449eb /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/ea39b1bf-6290-44cb-a44f-839da5f52621/32bc0532-99f8-4b23-9d3f-761218e449eb.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('32bc0532-99f8-4b23-9d3f-761218e449eb');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='0ddf8835-0b18-49ef-bde9-b7b6aafee25b' WHERE image_guid ='d5c301dc-198d-4338-998c-5196cd9f5752';"\n\n~~~</text>, <text>jetty02.ingest01.ams.gen.clt1.qa.mlbam.net\t\t\t\nhyp15.clt1.prod.mlbam.net\n============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'c3d6ca60-89ec-4fa0-9f88-56811246e50e' AND snapshot_id != '16ae53a8-3928-4e4f-9494-4e285d2d6756';"\n\nFIXING DISK 8d4147e4-337e-4c1b-9f06-da403d992845, CHAIN ['eea22561-accb-4f8c-8724-ac080ed418ab', '8025d4a2-01e1-465b-a519-d82f96e5c09a']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/eea22561-accb-4f8c-8724-ac080ed418ab -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d4147e4-337e-4c1b-9f06-da403d992845/8025d4a2-01e1-465b-a519-d82f96e5c09a'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d4147e4-337e-4c1b-9f06-da403d992845/eea22561-accb-4f8c-8724-ac080ed418ab.meta"\nlvchange -ay /dev/ovirt-local/eea22561-accb-4f8c-8724-ac080ed418ab\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/eea22561-accb-4f8c-8724-ac080ed418ab -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d4147e4-337e-4c1b-9f06-da403d992845/8025d4a2-01e1-465b-a519-d82f96e5c09a'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d4147e4-337e-4c1b-9f06-da403d992845/8025d4a2-01e1-465b-a519-d82f96e5c09a.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d4147e4-337e-4c1b-9f06-da403d992845/8025d4a2-01e1-465b-a519-d82f96e5c09a.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d4147e4-337e-4c1b-9f06-da403d992845/8025d4a2-01e1-465b-a519-d82f96e5c09a.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d4147e4-337e-4c1b-9f06-da403d992845/8025d4a2-01e1-465b-a519-d82f96e5c09a.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d4147e4-337e-4c1b-9f06-da403d992845/8025d4a2-01e1-465b-a519-d82f96e5c09a /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/8d4147e4-337e-4c1b-9f06-da403d992845/8025d4a2-01e1-465b-a519-d82f96e5c09a.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('8025d4a2-01e1-465b-a519-d82f96e5c09a');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='16ae53a8-3928-4e4f-9494-4e285d2d6756' WHERE image_guid ='eea22561-accb-4f8c-8724-ac080ed418ab';"\n\n~~~\n\n\njetty01.ingest01.ams.gen.clt1.dev.mlbam.net\t\t\t\nhyp87.clt1.prod.mlbam.net\n============================================\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'dce9e07b-a024-4c77-b08b-5f187dbabb9d' AND snapshot_id != 'c4519545-f307-4364-ab4d-aed8c8b797df';"\n\nFIXING DISK 193ddffc-4d90-40af-8fb6-6fc202d3f9b1, CHAIN ['3623527c-3fa1-4b0d-8763-871b4939db77', 'c779c251-afb0-4e20-96c0-efc8356e06bb']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3623527c-3fa1-4b0d-8763-871b4939db77 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/193ddffc-4d90-40af-8fb6-6fc202d3f9b1/c779c251-afb0-4e20-96c0-efc8356e06bb'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/193ddffc-4d90-40af-8fb6-6fc202d3f9b1/3623527c-3fa1-4b0d-8763-871b4939db77.meta"\nlvchange -ay /dev/ovirt-local/3623527c-3fa1-4b0d-8763-871b4939db77\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3623527c-3fa1-4b0d-8763-871b4939db77 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/193ddffc-4d90-40af-8fb6-6fc202d3f9b1/c779c251-afb0-4e20-96c0-efc8356e06bb'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/193ddffc-4d90-40af-8fb6-6fc202d3f9b1/c779c251-afb0-4e20-96c0-efc8356e06bb.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/193ddffc-4d90-40af-8fb6-6fc202d3f9b1/c779c251-afb0-4e20-96c0-efc8356e06bb.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/193ddffc-4d90-40af-8fb6-6fc202d3f9b1/c779c251-afb0-4e20-96c0-efc8356e06bb.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/193ddffc-4d90-40af-8fb6-6fc202d3f9b1/c779c251-afb0-4e20-96c0-efc8356e06bb.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/193ddffc-4d90-40af-8fb6-6fc202d3f9b1/c779c251-afb0-4e20-96c0-efc8356e06bb /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/193ddffc-4d90-40af-8fb6-6fc202d3f9b1/c779c251-afb0-4e20-96c0-efc8356e06bb.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('c779c251-afb0-4e20-96c0-efc8356e06bb');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='c4519545-f307-4364-ab4d-aed8c8b797df' WHERE image_guid ='3623527c-3fa1-4b0d-8763-871b4939db77';"\n\n~~~\n\n\njetty02.ingest01.ams.gen.clt1.dev.mlbam.net\t\t\t\nhyp95.clt1.prod.mlbam.net\n============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '67bc4232-3028-435e-ae44-e675872a00ef' AND snapshot_id != '318446c1-4ff0-4946-9cfe-d40d6d294550';"\n\nFIXING DISK e128229f-073e-41a8-aaaa-ab2961613120, CHAIN ['c0df9f9d-4784-4b65-ac6b-b0442200aaf4', '7ad46015-6adf-4906-b6f7-30b0f9a082f5']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c0df9f9d-4784-4b65-ac6b-b0442200aaf4 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e128229f-073e-41a8-aaaa-ab2961613120/7ad46015-6adf-4906-b6f7-30b0f9a082f5'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e128229f-073e-41a8-aaaa-ab2961613120/c0df9f9d-4784-4b65-ac6b-b0442200aaf4.meta"\nlvchange -ay /dev/ovirt-local/c0df9f9d-4784-4b65-ac6b-b0442200aaf4\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c0df9f9d-4784-4b65-ac6b-b0442200aaf4 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e128229f-073e-41a8-aaaa-ab2961613120/7ad46015-6adf-4906-b6f7-30b0f9a082f5'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e128229f-073e-41a8-aaaa-ab2961613120/7ad46015-6adf-4906-b6f7-30b0f9a082f5.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e128229f-073e-41a8-aaaa-ab2961613120/7ad46015-6adf-4906-b6f7-30b0f9a082f5.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e128229f-073e-41a8-aaaa-ab2961613120/7ad46015-6adf-4906-b6f7-30b0f9a082f5.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e128229f-073e-41a8-aaaa-ab2961613120/7ad46015-6adf-4906-b6f7-30b0f9a082f5.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e128229f-073e-41a8-aaaa-ab2961613120/7ad46015-6adf-4906-b6f7-30b0f9a082f5 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e128229f-073e-41a8-aaaa-ab2961613120/7ad46015-6adf-4906-b6f7-30b0f9a082f5.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('7ad46015-6adf-4906-b6f7-30b0f9a082f5');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='318446c1-4ff0-4946-9cfe-d40d6d294550' WHERE image_guid ='c0df9f9d-4784-4b65-ac6b-b0442200aaf4';"\n\n~~~</text>, <text>jetty01.control01.provis.gen.clt1.qa.mlbam.net \t\t\nhyp65.clt1.prod.mlbam.net\n===============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '08ad405c-f0cb-4b3b-9a99-00860515a1ab' AND snapshot_id != '862cf7a7-e6f5-456b-a252-11a635995830';"\n\nFIXING DISK f1c9e0f0-dee1-46b2-974a-e9785fbfc624, CHAIN ['7ec57c5e-ebc4-40a8-90b9-621e8756d69c', '73bfae73-1812-458c-befc-489b4843ea3e']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/7ec57c5e-ebc4-40a8-90b9-621e8756d69c -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f1c9e0f0-dee1-46b2-974a-e9785fbfc624/73bfae73-1812-458c-befc-489b4843ea3e'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f1c9e0f0-dee1-46b2-974a-e9785fbfc624/7ec57c5e-ebc4-40a8-90b9-621e8756d69c.meta"\nlvchange -ay /dev/ovirt-local/7ec57c5e-ebc4-40a8-90b9-621e8756d69c\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/7ec57c5e-ebc4-40a8-90b9-621e8756d69c -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f1c9e0f0-dee1-46b2-974a-e9785fbfc624/73bfae73-1812-458c-befc-489b4843ea3e'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f1c9e0f0-dee1-46b2-974a-e9785fbfc624/73bfae73-1812-458c-befc-489b4843ea3e.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f1c9e0f0-dee1-46b2-974a-e9785fbfc624/73bfae73-1812-458c-befc-489b4843ea3e.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f1c9e0f0-dee1-46b2-974a-e9785fbfc624/73bfae73-1812-458c-befc-489b4843ea3e.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f1c9e0f0-dee1-46b2-974a-e9785fbfc624/73bfae73-1812-458c-befc-489b4843ea3e.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f1c9e0f0-dee1-46b2-974a-e9785fbfc624/73bfae73-1812-458c-befc-489b4843ea3e /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f1c9e0f0-dee1-46b2-974a-e9785fbfc624/73bfae73-1812-458c-befc-489b4843ea3e.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('73bfae73-1812-458c-befc-489b4843ea3e');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='862cf7a7-e6f5-456b-a252-11a635995830' WHERE image_guid ='7ec57c5e-ebc4-40a8-90b9-621e8756d69c';"\n~~~\n\n\njetty02.control01.provis.gen.clt1.qa.mlbam.net\t\t\nhyp66.clt1.prod.mlbam.net\n===============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '537a9111-45f5-4fcc-91ea-906beb45f654' AND snapshot_id != '783dac74-b464-4ca2-84b7-a7e622524d94';"\n\nFIXING DISK 76870247-a9dc-4b51-ac70-0bb47e4e3048, CHAIN ['57545ed6-3de0-4f17-b3eb-2dbe0586197c', 'c8d72971-885e-444d-87fc-ffbe43f43a34']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/57545ed6-3de0-4f17-b3eb-2dbe0586197c -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/76870247-a9dc-4b51-ac70-0bb47e4e3048/c8d72971-885e-444d-87fc-ffbe43f43a34'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/76870247-a9dc-4b51-ac70-0bb47e4e3048/57545ed6-3de0-4f17-b3eb-2dbe0586197c.meta"\nlvchange -ay /dev/ovirt-local/57545ed6-3de0-4f17-b3eb-2dbe0586197c\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/57545ed6-3de0-4f17-b3eb-2dbe0586197c -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/76870247-a9dc-4b51-ac70-0bb47e4e3048/c8d72971-885e-444d-87fc-ffbe43f43a34'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/76870247-a9dc-4b51-ac70-0bb47e4e3048/c8d72971-885e-444d-87fc-ffbe43f43a34.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/76870247-a9dc-4b51-ac70-0bb47e4e3048/c8d72971-885e-444d-87fc-ffbe43f43a34.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/76870247-a9dc-4b51-ac70-0bb47e4e3048/c8d72971-885e-444d-87fc-ffbe43f43a34.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/76870247-a9dc-4b51-ac70-0bb47e4e3048/c8d72971-885e-444d-87fc-ffbe43f43a34.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/76870247-a9dc-4b51-ac70-0bb47e4e3048/c8d72971-885e-444d-87fc-ffbe43f43a34 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/76870247-a9dc-4b51-ac70-0bb47e4e3048/c8d72971-885e-444d-87fc-ffbe43f43a34.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('c8d72971-885e-444d-87fc-ffbe43f43a34');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='783dac74-b464-4ca2-84b7-a7e622524d94' WHERE image_guid ='57545ed6-3de0-4f17-b3eb-2dbe0586197c';"\n~~~\n\n\njetty01.ingest01.ams.gen.clt1.qa.mlbam.net \nhyp23.clt1.prod.mlbam.net\n===========================================\n\nVM is ok the base image is on ovirt-local.\n\n\n\nINVESTIGATION:\n~~~~~~~~~~~~~~~\n// Working Notes - these notes are not intended as a meaningful communication\n// but rather an indicator of current thought processes and reference.\n// Please feel free to comment and ask questions concerning them.\n\n~~~\nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n af2436dc-d2ad-44ea-ad38-3406b0f124d4 | 99f02109-e35b-4b09-ac93-4631d19d5b14 | 3eb08c67-22a1-4f34-9c98-6e749987f75c | fa048414-a3e0-44cf-818d-42688a995004 |           1 | 2017-05-18 14:20:35+00 |           2 |             4 | t\n(1 row)\n\n    For image_guid = af2436dc-d2ad-44ea-ad38-3406b0f124d4 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status | description |       creation_date        \n--------------------------------------+--------------------------------------+---------------+--------+-------------+----------------------------\n 3eb08c67-22a1-4f34-9c98-6e749987f75c | 18e6222e-c651-41a2-9834-46ac2fb583d6 | ACTIVE        | OK     | Active VM   | 2017-05-18 14:21:24.839+00\n(1 row)\n~~~\n\n~~~\n  af2436dc-d2ad-44ea-ad38-3406b0f124d4 ovirt-local -wi-ao----  50.00g    IU_99f02109-e35b-4b09-ac93-4631d19d5b14,PU_00000000-0000-0000-0000-000000000000,VM_18e6222e-c651-41a2-9834-46ac2fb583d6\n\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/99f02109-e35b-4b09-ac93-4631d19d5b14:\ntotal 2363529\n\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 May 18  2017 af2436dc-d2ad-44ea-ad38-3406b0f124d4\n-rw-rw----.   1 vdsm kvm    1048576 May 18  2017 af2436dc-d2ad-44ea-ad38-3406b0f124d4.lease\n-rw-r--r--.   1 vdsm kvm        260 May 18  2017 af2436dc-d2ad-44ea-ad38-3406b0f124d4.meta\n~~~</text>, <text>HI Bimal. thanks for that. We'll start working on those and let you know of any issues.\n\nCan we tackle these next please ?\ntomcat01.c01.sxc.gen.clt1.prod.mlbam.net\ntomcat01.core01.lprovis.gen.clt1.qa.mlbam.net\ntomcat01.core01.lprovis.hulu.clt1.prod.mlbam.net\ntomcat01.sweeper01.lprovis.hulu.clt1.prod.mlbam.net\ntomcat01.toc01.lprovis.hulu.clt1.prod.mlbam.net\ntomcat02.core01.lprovis.gen.clt1.qa.mlbam.net\ntomcat02.core01.lprovis.hulu.clt1.prod.mlbam.net\ntomcat02.toc01.lprovis.hulu.clt1.prod.mlbam.net\ntomcat03.core01.lprovis.hulu.clt1.prod.mlbam.net\ntomcat04.core01.lprovis.hulu.clt1.prod.mlbam.net\ntomcat06.core01.lprovis.hulu.clt1.prod.mlbam.net</text>, <text>also:\nsplunkfwd01.c01.noc.gen.clt1.util.mlbam.net\nsplunkfwd02.c01.noc.gen.clt1.util.mlbam.net</text>, <text>Hi Marcus,\n\n\nFrom the RHV-M DB we have, the VM's are shown to be running on various hosts.\n\n~~~\ntomcat01.c01.sxc.gen.clt1.prod.mlbam.net\t\t\thyp80.clt1.prod.mlbam.net\ntomcat01.core01.lprovis.gen.clt1.qa.mlbam.net\t\thyp70.clt1.prod.mlbam.net\ntomcat01.core01.lprovis.hulu.clt1.prod.mlbam.net\thyp06.clt1.prod.mlbam.net\ntomcat01.sweeper01.lprovis.hulu.clt1.prod.mlbam.net\thyp04.clt1.prod.mlbam.net\ntomcat01.toc01.lprovis.hulu.clt1.prod.mlbam.net\t\thyp12.clt1.prod.mlbam.net\ntomcat02.core01.lprovis.gen.clt1.qa.mlbam.net\t\thyp71.clt1.prod.mlbam.net\ntomcat02.core01.lprovis.hulu.clt1.prod.mlbam.net\thyp07.clt1.prod.mlbam.net\ntomcat02.toc01.lprovis.hulu.clt1.prod.mlbam.net\t\thyp67.clt1.prod.mlbam.net\ntomcat03.core01.lprovis.hulu.clt1.prod.mlbam.net\thyp08.clt1.prod.mlbam.net\ntomcat04.core01.lprovis.hulu.clt1.prod.mlbam.net\thyp67.clt1.prod.mlbam.net\ntomcat06.core01.lprovis.hulu.clt1.prod.mlbam.net\thyp69.clt1.prod.mlbam.net\nsplunkfwd01.c01.noc.gen.clt1.util.mlbam.net\t\t\thyp78.clt1.prod.mlbam.net\nsplunkfwd02.c01.noc.gen.clt1.util.mlbam.net\t\t\thyp79.clt1.prod.mlbam.net\n~~~\n\nInstead of providing sos report for the hosts, can you run the followiong from the RHV-M.\n\n~~~\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp80.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp65'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp80.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp65'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp70.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp70'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp70.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp70'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp06.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp06'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp06.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp06'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp04.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp04'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp04.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp04'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp12.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp12'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp12.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp12'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp71.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp71'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp71.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp71'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp07.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp07'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp07.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp07'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp67.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp67'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp67.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp67'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp08.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp08'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp08.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp08'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp67.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp67'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp67.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp67'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp69.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp69'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp69.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp69'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp78.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp78'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp78.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp78'\n\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp79.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp79'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa root@hyp79.clt1.prod.mlbam.net 'lvm lvs -o +tags &gt;&gt; /tmp/hyp79'\n~~~\n\nThen from RHV-M host, sftp to get files and upload them to the case.\n\n~~~\ncd /tmp/\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp80.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp70.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp06.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp04.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp12.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp71.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp07.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp67.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp08.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp67.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp69.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp78.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp79.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp*\nsftp&gt; quit\n~~~\n\ntar up the /tmp/hyp* and upload to the case.\n\nAlso can you provide logcollector from the RHV-M only.\nWe have one but from Nov 2017.\n\nThanks\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Hi Marcus,\n\nFor jetty02.archive01.jobmgr.gen.clt1.prod.mlbam.net, i noticed a typo.\nHave you already run the cleanup action plan on the VM?\n\nThe following lvrename is not correct:\n\n~~~\nlvrename /dev/ovirt-local/d1469074-5f8d-4671-93a7-70edfdacc770 /dev/ovirt-local/_temp_683d8033-6fe8-4408-95f7-7d1dd70fe07f\n~~~\n\nShould be:\n\n~~~\nlvrename /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f /dev/ovirt-local/_temp_683d8033-6fe8-4408-95f7-7d1dd70fe07f\n~~~\n\nBelow is the corrected AP.\n\nSorry for any inconvience.\n\nThanks\nBimal\n\n\n\njetty02.archive01.jobmgr.gen.clt1.prod.mlbam.net\nhyp80.clt1.prod.mlbam.net\n=================================================\nThis VM has extra lv image 683d8033 on the ovirt-local [I1] and on NFS share.\nThe one on NFS share is in use. So will need to quarantine it first.\nLater can remove it.\n\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ac7da89a-2e41-480e-ae1d-12ba281735f9' AND snapshot_id != '33083dbb-5549-48f1-9382-9bc29176164e';"\n\nFIXING DISK 55e079d2-3092-4718-8183-5e9303e8bbe9, CHAIN ['3d1ecb45-1113-4ea8-bef1-ee41cb4db565', '683d8033-6fe8-4408-95f7-7d1dd70fe07f']\n\nHost Commands:\n==============\nlvchange -an /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvchange --deltag 'IU_642a4077-adb5-4ea3-bcd8-fcf309479d4b' /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvchange --deltag 'VM_7b96fa60-0cd2-43f5-b595-da1f78d6de88' /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvrename /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f /dev/ovirt-local/_temp_683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3d1ecb45-1113-4ea8-bef1-ee41cb4db565 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/3d1ecb45-1113-4ea8-bef1-ee41cb4db565.meta"\n\nlvchange -ay /dev/ovirt-local/3d1ecb45-1113-4ea8-bef1-ee41cb4db565\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3d1ecb45-1113-4ea8-bef1-ee41cb4db565 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('683d8033-6fe8-4408-95f7-7d1dd70fe07f');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='33083dbb-5549-48f1-9382-9bc29176164e' WHERE image_guid ='3d1ecb45-1113-4ea8-bef1-ee41cb4db565';"\n\n\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Hi Marcus,\n\nIgnore the last update about jetty02.archive01.jobmgr.gen.clt1.prod.mlbam.net.\nThat wasn't correct either.  My cut/paste wasn't picked.  Just having one of those days ....\nSorry about that.\n\nBelow is the correct on.\nI checked it couple time.\n\n- Bimal.\n\n\n\njetty02.archive01.jobmgr.gen.clt1.prod.mlbam.net\nhyp80.clt1.prod.mlbam.net\n=================================================\nThis VM has extra lv image 683d8033 on the ovirt-local [I1] and on NFS share.\nThe one on NFS share is in use. So will need to quarantine it first.\nLater can remove it.\n\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ac7da89a-2e41-480e-ae1d-12ba281735f9' AND snapshot_id != '33083dbb-5549-48f1-9382-9bc29176164e';"\n\nFIXING DISK 55e079d2-3092-4718-8183-5e9303e8bbe9, CHAIN ['3d1ecb45-1113-4ea8-bef1-ee41cb4db565', '683d8033-6fe8-4408-95f7-7d1dd70fe07f']\n\nHost Commands:\n==============\n\nlvchange -an /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvchange --deltag 'IU_55e079d2-3092-4718-8183-5e9303e8bbe' /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvchange --deltag 'VM_ac7da89a-2e41-480e-ae1d-12ba281735f9' /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nlvrename /dev/ovirt-local/683d8033-6fe8-4408-95f7-7d1dd70fe07f /dev/ovirt-local/_temp_683d8033-6fe8-4408-95f7-7d1dd70fe07f\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3d1ecb45-1113-4ea8-bef1-ee41cb4db565 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/3d1ecb45-1113-4ea8-bef1-ee41cb4db565.meta"\n\nlvchange -ay /dev/ovirt-local/3d1ecb45-1113-4ea8-bef1-ee41cb4db565\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3d1ecb45-1113-4ea8-bef1-ee41cb4db565 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9/683d8033-6fe8-4408-95f7-7d1dd70fe07f.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('683d8033-6fe8-4408-95f7-7d1dd70fe07f');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='33083dbb-5549-48f1-9382-9bc29176164e' WHERE image_guid ='3d1ecb45-1113-4ea8-bef1-ee41cb4db565';"\n\n\n~~~\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS\n\n\nINVESTIGATION:\n~~~~~~~~~~~~~~~\n// Working Notes - these notes are not intended as a meaningful communication\n// but rather an indicator of current thought processes and reference.\n// Please feel free to comment and ask questions concerning them.\n\n~~~\nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 3d1ecb45-1113-4ea8-bef1-ee41cb4db565 | 55e079d2-3092-4718-8183-5e9303e8bbe9 | 8c202ef9-ff33-4292-a283-5848ddf9374a | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-02-24 14:39:26+00 |           2 |             4 | f\n 683d8033-6fe8-4408-95f7-7d1dd70fe07f | 55e079d2-3092-4718-8183-5e9303e8bbe9 | 33083dbb-5549-48f1-9382-9bc29176164e | 3d1ecb45-1113-4ea8-bef1-ee41cb4db565 |           1 | 2017-11-11 05:17:25+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = 683d8033-6fe8-4408-95f7-7d1dd70fe07f , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 905913856\n    For image_guid = 3d1ecb45-1113-4ea8-bef1-ee41cb4db565 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------\n 33083dbb-5549-48f1-9382-9bc29176164e | ac7da89a-2e41-480e-ae1d-12ba281735f9 | ACTIVE        | OK     | Active VM                    | 2017-02-24 14:39:21.136+00\n 8c202ef9-ff33-4292-a283-5848ddf9374a | ac7da89a-2e41-480e-ae1d-12ba281735f9 | REGULAR       | OK     | Backup__2017-11-11__05.17.22 | 2017-11-11 05:17:25.229+00\n(2 rows)\n~~~\n\n\n~~~\n  3d1ecb45-1113-4ea8-bef1-ee41cb4db565       ovirt-local -wi-ao----  50.00g  IU_55e079d2-3092-4718-8183-5e9303e8bbe9,PU_00000000-0000-0000-0000-000000000000,VM_ac7da89a-2e41-480e-ae1d-12ba281735f9\n  683d8033-6fe8-4408-95f7-7d1dd70fe07f       ovirt-local -wi-a-----  50.00g  IU_55e079d2-3092-4718-8183-5e9303e8bbe9,PU_00000000-0000-0000-0000-000000000000,VM_ac7da89a-2e41-480e-ae1d-12ba281735f9\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/55e079d2-3092-4718-8183-5e9303e8bbe9:\ntotal 3249542\n\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 Feb 24  2017 3d1ecb45-1113-4ea8-bef1-ee41cb4db565\n-rw-rw----.   1 vdsm kvm    1048576 Feb 24  2017 3d1ecb45-1113-4ea8-bef1-ee41cb4db565.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 11 05:17 3d1ecb45-1113-4ea8-bef1-ee41cb4db565.meta\n\n-rw-rw----.   1 vdsm kvm  722796544 Apr  5 14:30 683d8033-6fe8-4408-95f7-7d1dd70fe07f\n-rw-rw----.   1 vdsm kvm    1048576 Nov 11 05:17 683d8033-6fe8-4408-95f7-7d1dd70fe07f.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 11 05:17 683d8033-6fe8-4408-95f7-7d1dd70fe07f.meta\n~~~</text>, <text>tomcat01.c01.sxc.gen.clt1.prod.mlbam.net\t\t\t\nhyp80.clt1.prod.mlbam.net\n=========================================\n\nThis VM has extra lv image cdf427ff on the ovirt-local [I1] and on NFS share.\nThe one on NFS share is in use. \nSo will need to quarantine the one onthe ovirt-local first.  Later can remove it.\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '49550769-dc26-49db-bc9b-7865b7ddecae' AND snapshot_id != '74df18de-591f-4547-95aa-71238202f178';"\n\nFIXING DISK d70bc2c3-2ced-4a21-9627-8a5f756bbdcb, CHAIN ['e6cd316a-8da2-40a2-b020-0d497657dd08', '67773c59-ce46-492d-bc46-ad6e09fd5d7c', 'cdf427ff-c2f7-4239-818d-9c9912b7b0d0']\n\nHost Commands:\n==============\n\nlvchange -an /dev/ovirt-local/cdf427ff-c2f7-4239-818d-9c9912b7b0d0\n\nlvchange --deltag 'IU_d70bc2c3-2ced-4a21-9627-8a5f756bbdcb' /dev/ovirt-local/cdf427ff-c2f7-4239-818d-9c9912b7b0d0\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/cdf427ff-c2f7-4239-818d-9c9912b7b0d0\n\nlvchange --deltag 'VM_49550769-dc26-49db-bc9b-7865b7ddecae' /dev/ovirt-local/cdf427ff-c2f7-4239-818d-9c9912b7b0d0\n\nlvrename /dev/ovirt-local/cdf427ff-c2f7-4239-818d-9c9912b7b0d0 /dev/ovirt-local/_temp_cdf427ff-c2f7-4239-818d-9c9912b7b0d0\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/67773c59-ce46-492d-bc46-ad6e09fd5d7c -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/cdf427ff-c2f7-4239-818d-9c9912b7b0d0'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/cdf427ff-c2f7-4239-818d-9c9912b7b0d0.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/cdf427ff-c2f7-4239-818d-9c9912b7b0d0.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/cdf427ff-c2f7-4239-818d-9c9912b7b0d0.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/cdf427ff-c2f7-4239-818d-9c9912b7b0d0.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/cdf427ff-c2f7-4239-818d-9c9912b7b0d0 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/cdf427ff-c2f7-4239-818d-9c9912b7b0d0.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/e6cd316a-8da2-40a2-b020-0d497657dd08 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/67773c59-ce46-492d-bc46-ad6e09fd5d7c'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/e6cd316a-8da2-40a2-b020-0d497657dd08.meta"\n\nlvchange -ay /dev/ovirt-local/e6cd316a-8da2-40a2-b020-0d497657dd08\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/e6cd316a-8da2-40a2-b020-0d497657dd08 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/67773c59-ce46-492d-bc46-ad6e09fd5d7c'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/67773c59-ce46-492d-bc46-ad6e09fd5d7c.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/67773c59-ce46-492d-bc46-ad6e09fd5d7c.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/67773c59-ce46-492d-bc46-ad6e09fd5d7c.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/67773c59-ce46-492d-bc46-ad6e09fd5d7c.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/67773c59-ce46-492d-bc46-ad6e09fd5d7c /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb/67773c59-ce46-492d-bc46-ad6e09fd5d7c.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('67773c59-ce46-492d-bc46-ad6e09fd5d7c','cdf427ff-c2f7-4239-818d-9c9912b7b0d0');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='74df18de-591f-4547-95aa-71238202f178' WHERE image_guid ='e6cd316a-8da2-40a2-b020-0d497657dd08';"\n~~~\n\nINVESTIGATION:\n~~~~~~~~~~~~~~~\n// Working Notes - these notes are not intended as a meaningful communication\n// but rather an indicator of current thought processes and reference.\n// Please feel free to comment and ask questions concerning them.\n\n~~~ \nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n e6cd316a-8da2-40a2-b020-0d497657dd08 | d70bc2c3-2ced-4a21-9627-8a5f756bbdcb | 81a3cd01-6f4c-48ab-8798-b88176f10c43 | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-03-21 12:08:28+00 |           2 |             4 | f\n 67773c59-ce46-492d-bc46-ad6e09fd5d7c | d70bc2c3-2ced-4a21-9627-8a5f756bbdcb | ede417d3-c56b-4f9f-ac81-d599d97fe722 | e6cd316a-8da2-40a2-b020-0d497657dd08 |           1 | 2017-11-11 10:39:22+00 |           2 |             4 | f\n cdf427ff-c2f7-4239-818d-9c9912b7b0d0 | d70bc2c3-2ced-4a21-9627-8a5f756bbdcb | 74df18de-591f-4547-95aa-71238202f178 | 67773c59-ce46-492d-bc46-ad6e09fd5d7c |           1 | 2017-11-12 07:10:24+00 |           2 |             4 | t\n(3 rows)\n\n    For image_guid = 67773c59-ce46-492d-bc46-ad6e09fd5d7c , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 74999296\n    For image_guid = e6cd316a-8da2-40a2-b020-0d497657dd08 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n    For image_guid = cdf427ff-c2f7-4239-818d-9c9912b7b0d0 , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 641811968\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------\n 74df18de-591f-4547-95aa-71238202f178 | 49550769-dc26-49db-bc9b-7865b7ddecae | ACTIVE        | OK     | Active VM                    | 2017-03-21 12:08:13.085+00\n 81a3cd01-6f4c-48ab-8798-b88176f10c43 | 49550769-dc26-49db-bc9b-7865b7ddecae | REGULAR       | OK     | Backup__2017-11-11__10.39.16 | 2017-11-11 10:39:23.123+00\n ede417d3-c56b-4f9f-ac81-d599d97fe722 | 49550769-dc26-49db-bc9b-7865b7ddecae | REGULAR       | OK     | Backup__2017-11-12__07.10.20 | 2017-11-12 07:10:25+00\n(3 rows)\n~~~\n\n~~~\n  e6cd316a-8da2-40a2-b020-0d497657dd08       ovirt-local -wi-ao----  50.00g   IU_d70bc2c3-2ced-4a21-9627-8a5f756bbdcb,PU_00000000-0000-0000-0000-000000000000,VM_49550769-dc26-49db-bc9b-7865b7ddecae\n  cdf427ff-c2f7-4239-818d-9c9912b7b0d0       ovirt-local -wi-a-----  50.00g   IU_d70bc2c3-2ced-4a21-9627-8a5f756bbdcb,PU_00000000-0000-0000-0000-000000000000,VM_49550769-dc26-49db-bc9b-7865b7ddecae\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d70bc2c3-2ced-4a21-9627-8a5f756bbdcb:\ntotal 3071178\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 Mar 21  2017 e6cd316a-8da2-40a2-b020-0d497657dd08\n-rw-rw----.   1 vdsm kvm    1048576 Mar 21  2017 e6cd316a-8da2-40a2-b020-0d497657dd08.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 11 10:39 e6cd316a-8da2-40a2-b020-0d497657dd08.meta\n\n-rw-rw----.   1 vdsm kvm   63569920 Nov 12 13:06 67773c59-ce46-492d-bc46-ad6e09fd5d7c\n-rw-rw----.   1 vdsm kvm    1048576 Nov 11 10:39 67773c59-ce46-492d-bc46-ad6e09fd5d7c.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 12 07:10 67773c59-ce46-492d-bc46-ad6e09fd5d7c.meta\n\n-rw-rw----.   1 vdsm kvm  512491520 Apr  9 21:38 cdf427ff-c2f7-4239-818d-9c9912b7b0d0\n-rw-rw----.   1 vdsm kvm    1048576 Nov 12 07:10 cdf427ff-c2f7-4239-818d-9c9912b7b0d0.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 07:10 cdf427ff-c2f7-4239-818d-9c9912b7b0d0.meta\n~~~</text>, <text>tomcat01.core01.lprovis.gen.clt1.qa.mlbam.net\t\t\nhyp70.clt1.prod.mlbam.net\n==============================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '56076e6a-8636-4c54-8c1c-944923ad7727' AND snapshot_id != '1669c488-258a-49a6-8454-7d6190e972e8';"\n\nFIXING DISK 24970edc-ca0d-4578-b5cf-29a1aa6d27ad, CHAIN ['f1aaa038-51f7-4dd5-8923-7375c1535030', 'b1d08958-b730-464b-83ab-9410c45bfaab']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f1aaa038-51f7-4dd5-8923-7375c1535030 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/24970edc-ca0d-4578-b5cf-29a1aa6d27ad/b1d08958-b730-464b-83ab-9410c45bfaab'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/24970edc-ca0d-4578-b5cf-29a1aa6d27ad/f1aaa038-51f7-4dd5-8923-7375c1535030.meta"\nlvchange -ay /dev/ovirt-local/f1aaa038-51f7-4dd5-8923-7375c1535030\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f1aaa038-51f7-4dd5-8923-7375c1535030 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/24970edc-ca0d-4578-b5cf-29a1aa6d27ad/b1d08958-b730-464b-83ab-9410c45bfaab'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/24970edc-ca0d-4578-b5cf-29a1aa6d27ad/b1d08958-b730-464b-83ab-9410c45bfaab.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/24970edc-ca0d-4578-b5cf-29a1aa6d27ad/b1d08958-b730-464b-83ab-9410c45bfaab.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/24970edc-ca0d-4578-b5cf-29a1aa6d27ad/b1d08958-b730-464b-83ab-9410c45bfaab.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/24970edc-ca0d-4578-b5cf-29a1aa6d27ad/b1d08958-b730-464b-83ab-9410c45bfaab.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/24970edc-ca0d-4578-b5cf-29a1aa6d27ad/b1d08958-b730-464b-83ab-9410c45bfaab /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/24970edc-ca0d-4578-b5cf-29a1aa6d27ad/b1d08958-b730-464b-83ab-9410c45bfaab.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('b1d08958-b730-464b-83ab-9410c45bfaab');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1669c488-258a-49a6-8454-7d6190e972e8' WHERE image_guid ='f1aaa038-51f7-4dd5-8923-7375c1535030';"\n~~~\n\ntomcat01.core01.lprovis.hulu.clt1.prod.mlbam.net\t\nhyp06.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '28672289-bb34-458b-ae44-f049bdca02e1' AND snapshot_id != '54de2413-9308-434a-aa22-26057908f2c9';"\n\nFIXING DISK 7c658015-8d06-46f8-b1e8-b119e0345ac8, CHAIN ['514a6c67-5687-4d60-b88f-cd660d450a67', '299d7d07-31f0-46d2-9245-f19b31098c08', 'cd11cfb4-da5c-42a1-b709-603999dfd8fc']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/299d7d07-31f0-46d2-9245-f19b31098c08 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/cd11cfb4-da5c-42a1-b709-603999dfd8fc'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/cd11cfb4-da5c-42a1-b709-603999dfd8fc.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/cd11cfb4-da5c-42a1-b709-603999dfd8fc.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/cd11cfb4-da5c-42a1-b709-603999dfd8fc.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/cd11cfb4-da5c-42a1-b709-603999dfd8fc.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/cd11cfb4-da5c-42a1-b709-603999dfd8fc /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/cd11cfb4-da5c-42a1-b709-603999dfd8fc.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/514a6c67-5687-4d60-b88f-cd660d450a67 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/299d7d07-31f0-46d2-9245-f19b31098c08'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/514a6c67-5687-4d60-b88f-cd660d450a67.meta"\nlvchange -ay /dev/ovirt-local/514a6c67-5687-4d60-b88f-cd660d450a67\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/514a6c67-5687-4d60-b88f-cd660d450a67 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/299d7d07-31f0-46d2-9245-f19b31098c08'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/299d7d07-31f0-46d2-9245-f19b31098c08.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/299d7d07-31f0-46d2-9245-f19b31098c08.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/299d7d07-31f0-46d2-9245-f19b31098c08.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/299d7d07-31f0-46d2-9245-f19b31098c08.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/299d7d07-31f0-46d2-9245-f19b31098c08 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7c658015-8d06-46f8-b1e8-b119e0345ac8/299d7d07-31f0-46d2-9245-f19b31098c08.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('299d7d07-31f0-46d2-9245-f19b31098c08','cd11cfb4-da5c-42a1-b709-603999dfd8fc');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='54de2413-9308-434a-aa22-26057908f2c9' WHERE image_guid ='514a6c67-5687-4d60-b88f-cd660d450a67';"\n~~~\n\ntomcat01.sweeper01.lprovis.hulu.clt1.prod.mlbam.net\t\nhyp04.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'a15f6243-18d2-4a8e-83c5-6686ce05289e' AND snapshot_id != '897bafed-ec69-4592-ba86-9ef0eacbe6a3';"\n\nFIXING DISK 89b9d191-560b-40ee-8a3d-b7dbf82ab494, CHAIN ['f7c2a168-7e06-43e8-8666-a7c4f0079ec4', '4990febc-6ad0-4922-b6ea-fb809e2abc2a', 'e28370a9-ccc3-4d43-b7e5-1046220f448b']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/4990febc-6ad0-4922-b6ea-fb809e2abc2a -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/e28370a9-ccc3-4d43-b7e5-1046220f448b'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/e28370a9-ccc3-4d43-b7e5-1046220f448b.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/e28370a9-ccc3-4d43-b7e5-1046220f448b.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/e28370a9-ccc3-4d43-b7e5-1046220f448b.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/e28370a9-ccc3-4d43-b7e5-1046220f448b.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/e28370a9-ccc3-4d43-b7e5-1046220f448b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/e28370a9-ccc3-4d43-b7e5-1046220f448b.orig'\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f7c2a168-7e06-43e8-8666-a7c4f0079ec4 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/4990febc-6ad0-4922-b6ea-fb809e2abc2a'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/f7c2a168-7e06-43e8-8666-a7c4f0079ec4.meta"\nlvchange -ay /dev/ovirt-local/f7c2a168-7e06-43e8-8666-a7c4f0079ec4\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f7c2a168-7e06-43e8-8666-a7c4f0079ec4 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/4990febc-6ad0-4922-b6ea-fb809e2abc2a'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/4990febc-6ad0-4922-b6ea-fb809e2abc2a.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/4990febc-6ad0-4922-b6ea-fb809e2abc2a.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/4990febc-6ad0-4922-b6ea-fb809e2abc2a.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/4990febc-6ad0-4922-b6ea-fb809e2abc2a.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/4990febc-6ad0-4922-b6ea-fb809e2abc2a /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/89b9d191-560b-40ee-8a3d-b7dbf82ab494/4990febc-6ad0-4922-b6ea-fb809e2abc2a.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('4990febc-6ad0-4922-b6ea-fb809e2abc2a','e28370a9-ccc3-4d43-b7e5-1046220f448b');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='897bafed-ec69-4592-ba86-9ef0eacbe6a3' WHERE image_guid ='f7c2a168-7e06-43e8-8666-a7c4f0079ec4';"\n~~~</text>, <text>tomcat01.toc01.lprovis.hulu.clt1.prod.mlbam.net\t\t\nhyp12.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'cccdc940-4e70-4d25-9f06-8932631f80f8' AND snapshot_id != '2516bc45-073d-49a7-a6e7-2e72ee8552e6';"\n\nFIXING DISK 096ed437-5c0b-4ab6-869d-e9485d6cab25, CHAIN ['122e6f8b-e39f-49c3-979d-e794cbdf223d', '0699adc4-6b07-4565-b948-6b6a6a15d9bd']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/122e6f8b-e39f-49c3-979d-e794cbdf223d -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/096ed437-5c0b-4ab6-869d-e9485d6cab25/0699adc4-6b07-4565-b948-6b6a6a15d9bd'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/096ed437-5c0b-4ab6-869d-e9485d6cab25/122e6f8b-e39f-49c3-979d-e794cbdf223d.meta"\nlvchange -ay /dev/ovirt-local/122e6f8b-e39f-49c3-979d-e794cbdf223d\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/122e6f8b-e39f-49c3-979d-e794cbdf223d -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/096ed437-5c0b-4ab6-869d-e9485d6cab25/0699adc4-6b07-4565-b948-6b6a6a15d9bd'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/096ed437-5c0b-4ab6-869d-e9485d6cab25/0699adc4-6b07-4565-b948-6b6a6a15d9bd.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/096ed437-5c0b-4ab6-869d-e9485d6cab25/0699adc4-6b07-4565-b948-6b6a6a15d9bd.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/096ed437-5c0b-4ab6-869d-e9485d6cab25/0699adc4-6b07-4565-b948-6b6a6a15d9bd.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/096ed437-5c0b-4ab6-869d-e9485d6cab25/0699adc4-6b07-4565-b948-6b6a6a15d9bd.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/096ed437-5c0b-4ab6-869d-e9485d6cab25/0699adc4-6b07-4565-b948-6b6a6a15d9bd /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/096ed437-5c0b-4ab6-869d-e9485d6cab25/0699adc4-6b07-4565-b948-6b6a6a15d9bd.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0699adc4-6b07-4565-b948-6b6a6a15d9bd');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='2516bc45-073d-49a7-a6e7-2e72ee8552e6' WHERE image_guid ='122e6f8b-e39f-49c3-979d-e794cbdf223d';"\n~~~\n\ntomcat02.core01.lprovis.gen.clt1.qa.mlbam.net\t\t\nhyp71.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '5ef2d6fd-234b-420f-ad5d-ac6254831b17' AND snapshot_id != '598fa2d7-137d-41ca-b30d-bf29dba74760';"\n\nFIXING DISK c7de12c7-6ab0-4059-b016-c3c172d55b7f, CHAIN ['3ea1e262-df6b-4952-b456-a5121d316727', '29c14a5d-e2e1-40a8-913a-561f95edeaa5']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3ea1e262-df6b-4952-b456-a5121d316727 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c7de12c7-6ab0-4059-b016-c3c172d55b7f/29c14a5d-e2e1-40a8-913a-561f95edeaa5'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c7de12c7-6ab0-4059-b016-c3c172d55b7f/3ea1e262-df6b-4952-b456-a5121d316727.meta"\nlvchange -ay /dev/ovirt-local/3ea1e262-df6b-4952-b456-a5121d316727\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3ea1e262-df6b-4952-b456-a5121d316727 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c7de12c7-6ab0-4059-b016-c3c172d55b7f/29c14a5d-e2e1-40a8-913a-561f95edeaa5'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c7de12c7-6ab0-4059-b016-c3c172d55b7f/29c14a5d-e2e1-40a8-913a-561f95edeaa5.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c7de12c7-6ab0-4059-b016-c3c172d55b7f/29c14a5d-e2e1-40a8-913a-561f95edeaa5.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c7de12c7-6ab0-4059-b016-c3c172d55b7f/29c14a5d-e2e1-40a8-913a-561f95edeaa5.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c7de12c7-6ab0-4059-b016-c3c172d55b7f/29c14a5d-e2e1-40a8-913a-561f95edeaa5.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c7de12c7-6ab0-4059-b016-c3c172d55b7f/29c14a5d-e2e1-40a8-913a-561f95edeaa5 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c7de12c7-6ab0-4059-b016-c3c172d55b7f/29c14a5d-e2e1-40a8-913a-561f95edeaa5.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('29c14a5d-e2e1-40a8-913a-561f95edeaa5');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='598fa2d7-137d-41ca-b30d-bf29dba74760' WHERE image_guid ='3ea1e262-df6b-4952-b456-a5121d316727';"\n~~~\n\ntomcat02.core01.lprovis.hulu.clt1.prod.mlbam.net\t\nhyp07.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '9f9778c9-8e43-4633-9189-812cbf873925' AND snapshot_id != 'aecfc774-f274-4be0-a608-ed62c077ab11';"\n\nFIXING DISK f044f14d-193f-4967-b2e7-2425e1f4c38e, CHAIN ['c0b9f83e-970f-44d5-985a-1ac57b19c284', '3a7f9ecc-c430-4cc2-8a21-10916393893d']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c0b9f83e-970f-44d5-985a-1ac57b19c284 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f044f14d-193f-4967-b2e7-2425e1f4c38e/3a7f9ecc-c430-4cc2-8a21-10916393893d'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f044f14d-193f-4967-b2e7-2425e1f4c38e/c0b9f83e-970f-44d5-985a-1ac57b19c284.meta"\nlvchange -ay /dev/ovirt-local/c0b9f83e-970f-44d5-985a-1ac57b19c284\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c0b9f83e-970f-44d5-985a-1ac57b19c284 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f044f14d-193f-4967-b2e7-2425e1f4c38e/3a7f9ecc-c430-4cc2-8a21-10916393893d'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f044f14d-193f-4967-b2e7-2425e1f4c38e/3a7f9ecc-c430-4cc2-8a21-10916393893d.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f044f14d-193f-4967-b2e7-2425e1f4c38e/3a7f9ecc-c430-4cc2-8a21-10916393893d.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f044f14d-193f-4967-b2e7-2425e1f4c38e/3a7f9ecc-c430-4cc2-8a21-10916393893d.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f044f14d-193f-4967-b2e7-2425e1f4c38e/3a7f9ecc-c430-4cc2-8a21-10916393893d.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f044f14d-193f-4967-b2e7-2425e1f4c38e/3a7f9ecc-c430-4cc2-8a21-10916393893d /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f044f14d-193f-4967-b2e7-2425e1f4c38e/3a7f9ecc-c430-4cc2-8a21-10916393893d.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('3a7f9ecc-c430-4cc2-8a21-10916393893d');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='aecfc774-f274-4be0-a608-ed62c077ab11' WHERE image_guid ='c0b9f83e-970f-44d5-985a-1ac57b19c284';"\n~~~</text>, <text>tomcat02.toc01.lprovis.hulu.clt1.prod.mlbam.net\t\t\nhyp67.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '34651f11-5957-4e4b-9644-2df4dec38379' AND snapshot_id != '2dbf818f-89ea-49d5-91e5-d8a43338dbd7';"\n\nFIXING DISK 1ef648b4-f38d-4c46-8be6-805605206560, CHAIN ['92f15f61-0628-4079-ad96-8b022c44da95', '4ddcc357-8e19-44c1-b607-b66365b76da3']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/92f15f61-0628-4079-ad96-8b022c44da95 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1ef648b4-f38d-4c46-8be6-805605206560/4ddcc357-8e19-44c1-b607-b66365b76da3'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1ef648b4-f38d-4c46-8be6-805605206560/92f15f61-0628-4079-ad96-8b022c44da95.meta"\nlvchange -ay /dev/ovirt-local/92f15f61-0628-4079-ad96-8b022c44da95\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/92f15f61-0628-4079-ad96-8b022c44da95 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1ef648b4-f38d-4c46-8be6-805605206560/4ddcc357-8e19-44c1-b607-b66365b76da3'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1ef648b4-f38d-4c46-8be6-805605206560/4ddcc357-8e19-44c1-b607-b66365b76da3.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1ef648b4-f38d-4c46-8be6-805605206560/4ddcc357-8e19-44c1-b607-b66365b76da3.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1ef648b4-f38d-4c46-8be6-805605206560/4ddcc357-8e19-44c1-b607-b66365b76da3.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1ef648b4-f38d-4c46-8be6-805605206560/4ddcc357-8e19-44c1-b607-b66365b76da3.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1ef648b4-f38d-4c46-8be6-805605206560/4ddcc357-8e19-44c1-b607-b66365b76da3 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1ef648b4-f38d-4c46-8be6-805605206560/4ddcc357-8e19-44c1-b607-b66365b76da3.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('4ddcc357-8e19-44c1-b607-b66365b76da3');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='2dbf818f-89ea-49d5-91e5-d8a43338dbd7' WHERE image_guid ='92f15f61-0628-4079-ad96-8b022c44da95';"\n~~~\n\ntomcat03.core01.lprovis.hulu.clt1.prod.mlbam.net\t\nhyp08.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '84e327bf-5705-4541-bf59-1cc7092c4857' AND snapshot_id != '2707acdc-1ea7-4565-957b-8736e006d1cf';"\n\nFIXING DISK 97b13044-695e-4175-b3b8-eb101583ac1b, CHAIN ['3bcf3797-778a-41e1-aa12-ddd4b6689549', '0454002c-00fb-420e-98e0-eb2ed14b49f1']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3bcf3797-778a-41e1-aa12-ddd4b6689549 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/97b13044-695e-4175-b3b8-eb101583ac1b/0454002c-00fb-420e-98e0-eb2ed14b49f1'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/97b13044-695e-4175-b3b8-eb101583ac1b/3bcf3797-778a-41e1-aa12-ddd4b6689549.meta"\nlvchange -ay /dev/ovirt-local/3bcf3797-778a-41e1-aa12-ddd4b6689549\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3bcf3797-778a-41e1-aa12-ddd4b6689549 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/97b13044-695e-4175-b3b8-eb101583ac1b/0454002c-00fb-420e-98e0-eb2ed14b49f1'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/97b13044-695e-4175-b3b8-eb101583ac1b/0454002c-00fb-420e-98e0-eb2ed14b49f1.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/97b13044-695e-4175-b3b8-eb101583ac1b/0454002c-00fb-420e-98e0-eb2ed14b49f1.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/97b13044-695e-4175-b3b8-eb101583ac1b/0454002c-00fb-420e-98e0-eb2ed14b49f1.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/97b13044-695e-4175-b3b8-eb101583ac1b/0454002c-00fb-420e-98e0-eb2ed14b49f1.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/97b13044-695e-4175-b3b8-eb101583ac1b/0454002c-00fb-420e-98e0-eb2ed14b49f1 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/97b13044-695e-4175-b3b8-eb101583ac1b/0454002c-00fb-420e-98e0-eb2ed14b49f1.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0454002c-00fb-420e-98e0-eb2ed14b49f1');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='2707acdc-1ea7-4565-957b-8736e006d1cf' WHERE image_guid ='3bcf3797-778a-41e1-aa12-ddd4b6689549';"\n~~~\n\ntomcat04.core01.lprovis.hulu.clt1.prod.mlbam.net\t\nhyp67.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0baaabf5-f71e-4ed4-815d-7cc694486dc3' AND snapshot_id != '84705c12-5957-4d5f-9651-215156af6b25';"\n\nFIXING DISK c15eae22-7a99-4dd2-8340-006d4642bbc1, CHAIN ['3c93e734-b954-48cc-9b12-87f94584d47b', '70bd4772-82be-4ec9-ac2b-7670f11eff46']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3c93e734-b954-48cc-9b12-87f94584d47b -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c15eae22-7a99-4dd2-8340-006d4642bbc1/70bd4772-82be-4ec9-ac2b-7670f11eff46'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c15eae22-7a99-4dd2-8340-006d4642bbc1/3c93e734-b954-48cc-9b12-87f94584d47b.meta"\nlvchange -ay /dev/ovirt-local/3c93e734-b954-48cc-9b12-87f94584d47b\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3c93e734-b954-48cc-9b12-87f94584d47b -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c15eae22-7a99-4dd2-8340-006d4642bbc1/70bd4772-82be-4ec9-ac2b-7670f11eff46'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c15eae22-7a99-4dd2-8340-006d4642bbc1/70bd4772-82be-4ec9-ac2b-7670f11eff46.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c15eae22-7a99-4dd2-8340-006d4642bbc1/70bd4772-82be-4ec9-ac2b-7670f11eff46.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c15eae22-7a99-4dd2-8340-006d4642bbc1/70bd4772-82be-4ec9-ac2b-7670f11eff46.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c15eae22-7a99-4dd2-8340-006d4642bbc1/70bd4772-82be-4ec9-ac2b-7670f11eff46.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c15eae22-7a99-4dd2-8340-006d4642bbc1/70bd4772-82be-4ec9-ac2b-7670f11eff46 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/c15eae22-7a99-4dd2-8340-006d4642bbc1/70bd4772-82be-4ec9-ac2b-7670f11eff46.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('70bd4772-82be-4ec9-ac2b-7670f11eff46');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='84705c12-5957-4d5f-9651-215156af6b25' WHERE image_guid ='3c93e734-b954-48cc-9b12-87f94584d47b';"\n~~~</text>, <text>tomcat06.core01.lprovis.hulu.clt1.prod.mlbam.net\t\nhyp69.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '53948b0b-6315-4c50-b723-8caaa5800f80' AND snapshot_id != 'd3de3c26-04a4-483c-80de-d749d2ec1e79';"\n\nFIXING DISK 2c64c2ef-447c-43ac-bd32-6c0779a9cbb8, CHAIN ['7ca44aea-cb8a-419d-ba12-e5093976810d', 'ccbe1708-d8f9-40a5-bec6-0fa2626bc4ee']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/7ca44aea-cb8a-419d-ba12-e5093976810d -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c64c2ef-447c-43ac-bd32-6c0779a9cbb8/ccbe1708-d8f9-40a5-bec6-0fa2626bc4ee'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c64c2ef-447c-43ac-bd32-6c0779a9cbb8/7ca44aea-cb8a-419d-ba12-e5093976810d.meta"\nlvchange -ay /dev/ovirt-local/7ca44aea-cb8a-419d-ba12-e5093976810d\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/7ca44aea-cb8a-419d-ba12-e5093976810d -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c64c2ef-447c-43ac-bd32-6c0779a9cbb8/ccbe1708-d8f9-40a5-bec6-0fa2626bc4ee'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c64c2ef-447c-43ac-bd32-6c0779a9cbb8/ccbe1708-d8f9-40a5-bec6-0fa2626bc4ee.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c64c2ef-447c-43ac-bd32-6c0779a9cbb8/ccbe1708-d8f9-40a5-bec6-0fa2626bc4ee.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c64c2ef-447c-43ac-bd32-6c0779a9cbb8/ccbe1708-d8f9-40a5-bec6-0fa2626bc4ee.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c64c2ef-447c-43ac-bd32-6c0779a9cbb8/ccbe1708-d8f9-40a5-bec6-0fa2626bc4ee.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c64c2ef-447c-43ac-bd32-6c0779a9cbb8/ccbe1708-d8f9-40a5-bec6-0fa2626bc4ee /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c64c2ef-447c-43ac-bd32-6c0779a9cbb8/ccbe1708-d8f9-40a5-bec6-0fa2626bc4ee.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('ccbe1708-d8f9-40a5-bec6-0fa2626bc4ee');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d3de3c26-04a4-483c-80de-d749d2ec1e79' WHERE image_guid ='7ca44aea-cb8a-419d-ba12-e5093976810d';"\n~~~\n\nsplunkfwd02.c01.noc.gen.clt1.util.mlbam.net\t\t\t\nhyp79.clt1.prod.mlbam.net\n=========================================\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'bf63445b-3b93-44d8-8c65-e6da4fa67f73' AND snapshot_id != 'b8adf8b5-b20e-4999-aa44-122541e4a6a6';"\n\nFIXING DISK 9131e186-6c32-40a2-9925-d7522b559beb, CHAIN ['67ed3056-102c-4a1e-9f6a-abdd195722e7', '86c9530a-3e92-417f-b337-943f26e07c90']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/67ed3056-102c-4a1e-9f6a-abdd195722e7 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/67ed3056-102c-4a1e-9f6a-abdd195722e7.meta"\nlvchange -ay /dev/ovirt-local/67ed3056-102c-4a1e-9f6a-abdd195722e7\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/67ed3056-102c-4a1e-9f6a-abdd195722e7 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('86c9530a-3e92-417f-b337-943f26e07c90');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='b8adf8b5-b20e-4999-aa44-122541e4a6a6' WHERE image_guid ='67ed3056-102c-4a1e-9f6a-abdd195722e7';"\n~~~</text>, <text>splunkfwd01.c01.noc.gen.clt1.util.mlbam.net\t\t\t\nhyp78.clt1.prod.mlbam.net\n=========================================\n\nThis VM has extra lv image a38a9f84  on the ovirt-local [I1] and on NFS share.\nThe one on NFS share is in use. \nSo will need to quarantine the one on the ovirt-local first.  Later can remove it.\n\n\n~~~\n1.  Shutdown the VM.\n\n2.  Run the following commands:\n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'f2f4cd76-48b3-4751-81af-c70ce5d4ec47' AND snapshot_id != '1ae35a65-b08e-44a9-8729-9ba2f7be4f87';"\n\nFIXING DISK 4836e235-8850-46f1-a1e4-eb277e26c099, CHAIN ['6f567408-edc0-4eac-8e38-79cb47787d9a', 'a38a9f84-68f8-46a7-8a42-ad57aca95cea']\n\nHost Commands:\n==============\n\nlvchange -an /dev/ovirt-local/a38a9f84-68f8-46a7-8a42-ad57aca95cea\n\nlvchange --deltag 'IU_4836e235-8850-46f1-a1e4-eb277e26c099' /dev/ovirt-local/a38a9f84-68f8-46a7-8a42-ad57aca95cea\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/a38a9f84-68f8-46a7-8a42-ad57aca95cea\n\nlvchange --deltag 'VM_f2f4cd76-48b3-4751-81af-c70ce5d4ec47' /dev/ovirt-local/a38a9f84-68f8-46a7-8a42-ad57aca95cea\n\nlvrename /dev/ovirt-local/a38a9f84-68f8-46a7-8a42-ad57aca95cea /dev/ovirt-local/_temp_a38a9f84-68f8-46a7-8a42-ad57aca95cea\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/6f567408-edc0-4eac-8e38-79cb47787d9a -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea'\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/6f567408-edc0-4eac-8e38-79cb47787d9a.meta"\nlvchange -ay /dev/ovirt-local/6f567408-edc0-4eac-8e38-79cb47787d9a\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/6f567408-edc0-4eac-8e38-79cb47787d9a -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea.meta.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea.lease.orig'\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('a38a9f84-68f8-46a7-8a42-ad57aca95cea');"\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1ae35a65-b08e-44a9-8729-9ba2f7be4f87' WHERE image_guid ='6f567408-edc0-4eac-8e38-79cb47787d9a';"\n~~~\n\nINVESTIGATION:\n~~~~~~~~~~~~~~~\n// Working Notes - these notes are not intended as a meaningful communication\n// but rather an indicator of current thought processes and reference.\n// Please feel free to comment and ask questions concerning them.\n\n\n~~~\nimages:                                                                                                                                                    (1 = ok, 2 = locked, 4 = illegal)      (1/2 = preall/thin ; 4/5= qcow2/raw)\n \n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 6f567408-edc0-4eac-8e38-79cb47787d9a | 4836e235-8850-46f1-a1e4-eb277e26c099 | 759d0b78-6f65-45c4-bd63-5dec72dfe0f3 | fa048414-a3e0-44cf-818d-42688a995004 |           4 | 2017-02-21 08:06:12+00 |           2 |             4 | f\n a38a9f84-68f8-46a7-8a42-ad57aca95cea | 4836e235-8850-46f1-a1e4-eb277e26c099 | 1ae35a65-b08e-44a9-8729-9ba2f7be4f87 | 6f567408-edc0-4eac-8e38-79cb47787d9a |           1 | 2017-11-12 05:18:08+00 |           2 |             4 | t\n(2 rows)\n\n    For image_guid = a38a9f84-68f8-46a7-8a42-ad57aca95cea , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 1366197760\n    For image_guid = 6f567408-edc0-4eac-8e38-79cb47787d9a , SD = c2ba9087-43d9-44b5-a862-32f3a4dafb95 ( clt-nfs-d03 ) ;    virtual size = 53687091200 ,  actual size = 493056\n \nsnapshots:\n             snapshot_id              |                vm_id                 | snapshot_type | status |         description          |       creation_date        \n--------------------------------------+--------------------------------------+---------------+--------+------------------------------+----------------------------\n 1ae35a65-b08e-44a9-8729-9ba2f7be4f87 | f2f4cd76-48b3-4751-81af-c70ce5d4ec47 | ACTIVE        | OK     | Active VM                    | 2017-02-21 08:06:09+00\n 759d0b78-6f65-45c4-bd63-5dec72dfe0f3 | f2f4cd76-48b3-4751-81af-c70ce5d4ec47 | REGULAR       | OK     | Backup__2017-11-12__05.18.04 | 2017-11-12 05:18:07.781+00\n(2 rows)\n~~~\n\n~~~\n  6f567408-edc0-4eac-8e38-79cb47787d9a ovirt-local -wi-ao----  50.00g  IU_4836e235-8850-46f1-a1e4-eb277e26c099,PU_00000000-0000-0000-0000-000000000000,VM_f2f4cd76-48b3-4751-81af-c70ce5d4ec47\n  a38a9f84-68f8-46a7-8a42-ad57aca95cea ovirt-local -wi-a-----  50.00g  IU_4836e235-8850-46f1-a1e4-eb277e26c099,PU_00000000-0000-0000-0000-000000000000,VM_f2f4cd76-48b3-4751-81af-c70ce5d4ec47\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099:\ntotal 3726862\n\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 Feb 21  2017 6f567408-edc0-4eac-8e38-79cb47787d9a\n-rw-rw----.   1 vdsm kvm    1048576 Feb 21  2017 6f567408-edc0-4eac-8e38-79cb47787d9a.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 12 05:18 6f567408-edc0-4eac-8e38-79cb47787d9a.meta\n\n-rw-rw----.   1 vdsm kvm 1113194496 Apr  9 21:38 a38a9f84-68f8-46a7-8a42-ad57aca95cea\n-rw-rw----.   1 vdsm kvm    1048576 Nov 12 05:18 a38a9f84-68f8-46a7-8a42-ad57aca95cea.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 05:18 a38a9f84-68f8-46a7-8a42-ad57aca95cea.meta\n~~~</text>, <text>Hello,\n\nWe are following up on your case as it has been awhile since we last heard from you. \nAre you ok with us archiving this case at this time?  You can always reopen it by simply \nupdating it or dialing into support.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>This case is active still. It should remain open for some time until we finish these VM snapshot cleanups please</text>, <text>Hi Marcus,\n\nThank you for the confirmation. If for some reason the case goes in-active for a period of time and the system auto-closes it please just re-reply via email/case update and it will re-open it.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series was retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Hello Bimal &amp; Jason,\nSo far out of the 30 vm fix command sets you gave me, we're complete with about half of them and running through the other half today. No issues so far except one host experienced xfs corruption issues which i was able to fix booting rescue cd and running an xfs_repair.\n\nI'll give another status on the remaining hosts later today. thanks for all your help</text>, <text>Hi Marvus,\n\nThat is great news, we will await your further status changes.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series was retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Hi guys\nAll 30 fixes went OK. We're all set with these thank you. From the 120 hosts we started with, i think we're down to about 30-35 of these left. We're creating a list of the next batch and will update this ticket probably Monday. Thanks</text>, <text>Hi Marcus,\n\nSounds great, we will await the list.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series was retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Hi guys,\nHere is the list of remaining hosts we have to do (49 total). If you can supply the fix commands for these remaining hosts that should be all that's left and we'll split this work up over the span of 2 weeks. Please let us know what other information you require for these hosts. thanks\n\n\naspera01.c01.inf.wwe.clt1.prod.mlbam.net\naspera01.c01.jobmgr.gen.clt1.prod.mlbam.net\naspera02.c01.jobmgr.gen.clt1.prod.mlbam.net\nasperaproxy01.c01.inf.datg.clt1.prod.mlbam.net\nharmonic02.c01.bc.gen.clt1.prod.mlbam.net\ninflux01.c01.inf.hulu.clt1.prod.mlbam.net\nmdtools01.c01.bpa.hulu.clt1.prod.mlbam.net\nmdtools02.c01.bpa.hulu.clt1.prod.mlbam.net\nmemcached02.c01.lprovis.gen.clt1.qa.mlbam.net\nmemcached02.c01.lprovis.hulu.clt1.prod.mlbam.net\nmf01.c01.web.wwe.clt1.prod.bamtech.co\nmf02.c01.web.gen.clt1.prod.bamtech.co\nmf03.c01.web.gen.clt1.prod.bamtech.co\nmf03.c01.web.wwe.clt1.prod.bamtech.co\nmf04.c01.web.gen.clt1.prod.bamtech.co\nmf04.c01.web.wwe.clt1.prod.bamtech.co\nmf05.c01.web.gen.clt1.prod.bamtech.co\nmf05.c01.web.wwe.clt1.prod.bamtech.co\nmf07.c01.web.gen.clt1.prod.bamtech.co\nmf08.c01.web.gen.clt1.prod.bamtech.co\nmf08.c01.web.wwe.clt1.prod.bamtech.co\nmf09.c01.web.wwe.clt1.prod.bamtech.co\nmf10.c01.web.wwe.clt1.prod.bamtech.co\nmf11.c01.web.wwe.clt1.prod.bamtech.co\nmf12.c01.web.wwe.clt1.prod.bamtech.co\nnginx01.c01.medtrans.gen.clt1.prod.mlbam.net\nnginx01.c01.medtrans.gen.clt1.qa.mlbam.net\nnginx01.event01.hls.wwe.clt1.prod.mlbam.net\nnginx01.feed01.hls.mls.clt1.prod.mlbam.net\nnginx01.ingest01.hls.hulu.clt1.prod.mlbam.net\nnginx01.linear01.hls.wwe.clt1.prod.mlbam.net\nnginx01.live01.hls.gen.clt1.dev.mlbam.net\nnginx01.live01.hls.gen.clt1.prod.mlbam.net\nnginx01.live01.hls.riot.clt1.prod.mlbam.net\nnginx01.manifest01.streaming.hulu.clt1.prod.mlbam.net\nnginx01.proxy01.hls.gen.clt1.prod.mlbam.net\nnginx01.proxy01.hls.gen.clt1.qa.mlbam.net\nproxy01.c01.streaming.gen.clt1.dev.mlbam.net\nproxy01.linear01.infra.hulu.clt1.prod.mlbam.net\nproxy01.linear02.infra.hulu.clt1.prod.mlbam.net\nredis02.c01.inf.gen.clt1.util.mlbam.net\nsmtp01.clt1.util.mlbam.net\nsmtp02.clt1.util.mlbam.net\nsplunkfwd01.c01.noc.gen.clt1.util.mlbam.net\nsplunkfwd02.c01.noc.gen.clt1.util.mlbam.net\nxcoder01.c02.streaming.gen.clt1.dev.mlbam.net\nxcoder01.c03.streaming.gen.clt1.dev.mlbam.net\nxcoder02.c02.streaming.gen.clt1.dev.mlbam.net</text>, <text>sorry, last list was missing couple of hosts. please use this list instead...\n\n\naspera01.c01.inf.wwe.clt1.prod.mlbam.net\naspera01.c01.jobmgr.gen.clt1.prod.mlbam.net\naspera02.c01.jobmgr.gen.clt1.prod.mlbam.net\nasperaproxy01.c01.inf.datg.clt1.prod.mlbam.net\nharmonic02.c01.bc.gen.clt1.prod.mlbam.net\ninflux01.c01.inf.hulu.clt1.prod.mlbam.net\nmdtools01.c01.bpa.hulu.clt1.prod.mlbam.net\nmdtools02.c01.bpa.hulu.clt1.prod.mlbam.net\nmemcached02.c01.lprovis.gen.clt1.qa.mlbam.net\nmemcached02.c01.lprovis.hulu.clt1.prod.mlbam.net\nmf01.c01.web.wwe.clt1.prod.bamtech.co\nmf02.c01.web.gen.clt1.prod.bamtech.co\nmf03.c01.web.gen.clt1.prod.bamtech.co\nmf03.c01.web.wwe.clt1.prod.bamtech.co\nmf04.c01.web.gen.clt1.prod.bamtech.co\nmf04.c01.web.wwe.clt1.prod.bamtech.co\nmf05.c01.web.gen.clt1.prod.bamtech.co\nmf05.c01.web.wwe.clt1.prod.bamtech.co\nmf07.c01.web.gen.clt1.prod.bamtech.co\nmf08.c01.web.gen.clt1.prod.bamtech.co\nmf08.c01.web.wwe.clt1.prod.bamtech.co\nmf09.c01.web.wwe.clt1.prod.bamtech.co\nmf10.c01.web.wwe.clt1.prod.bamtech.co\nmf11.c01.web.wwe.clt1.prod.bamtech.co\nmf12.c01.web.wwe.clt1.prod.bamtech.co\nnginx01.c01.medtrans.gen.clt1.prod.mlbam.net\nnginx01.c01.medtrans.gen.clt1.qa.mlbam.net\nnginx01.event01.hls.wwe.clt1.prod.mlbam.net\nnginx01.feed01.hls.mls.clt1.prod.mlbam.net\nnginx01.ingest01.hls.hulu.clt1.prod.mlbam.net\nnginx01.linear01.hls.wwe.clt1.prod.mlbam.net\nnginx01.live01.hls.gen.clt1.dev.mlbam.net\nnginx01.live01.hls.gen.clt1.prod.mlbam.net\nnginx01.live01.hls.riot.clt1.prod.mlbam.net\nnginx01.manifest01.streaming.hulu.clt1.prod.mlbam.net\nnginx01.proxy01.hls.gen.clt1.prod.mlbam.net\nnginx01.proxy01.hls.gen.clt1.qa.mlbam.net\nproxy01.c01.streaming.gen.clt1.dev.mlbam.net\nproxy01.linear01.infra.hulu.clt1.prod.mlbam.net\nproxy01.linear02.infra.hulu.clt1.prod.mlbam.net\nredis02.c01.inf.gen.clt1.util.mlbam.net\nsmtp01.clt1.util.mlbam.net\nsmtp02.clt1.util.mlbam.net\nsplunkfwd01.c01.noc.gen.clt1.util.mlbam.net\nsplunkfwd02.c01.noc.gen.clt1.util.mlbam.net\nxcoder01.c02.streaming.gen.clt1.dev.mlbam.net\nxcoder01.c03.streaming.gen.clt1.dev.mlbam.net\nxcoder02.c02.streaming.gen.clt1.dev.mlbam.net</text>, <text>Hi Marcus,\n\nThank you for the update. We will update you once we have the information available.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series was retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Please allow me some time for these.</text>, <text>Added 14 day nep per BimalC.</text>, <text>From the RHV-M DB we have, the VM's are shown to be running on various hosts.\n\nInstead of providing sos report for the hosts, can you run the followiong from the RHV-M.\n\n~~~\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp103.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp103.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp103.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp07.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp07.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp07.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp70.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp70.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp70.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp101.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp101.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp101.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp92.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp92.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp92.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp89.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp89.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp89.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp95.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp95.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp95.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp25.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp25.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp25.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp66.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp66.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp66.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp75.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp75.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp75.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp102.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp102.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp102.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp29.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp29.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp29.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp31.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp31.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp31.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp81.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp81.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp81.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp25.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp25.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp25.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp89.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp89.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp89.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp85.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp85.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp85.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp32.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp32.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp32.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp102.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp102.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp102.clt1.prod.mlbam.net'\nVM mf08.c01.web.gen.clt1.prod.bamtech.co does not exist\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp90.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp90.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp90.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp102.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp102.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp102.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp103.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp103.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp103.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp104.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp104.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp104.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp105.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp105.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp105.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp29.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp29.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp29.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp104.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp104.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp104.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp28.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp28.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp28.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp04.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp04.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp04.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp13.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp13.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp13.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp72.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp72.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp72.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp94.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp94.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp94.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp24.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp24.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp24.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp27.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp27.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp27.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp07.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp07.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp07.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp98.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp98.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp98.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp99.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp99.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp99.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp03.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp03.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp03.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp10.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp10.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp10.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp23.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp23.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp23.clt1.prod.mlbam.net'\nVM redis02.c01.inf.gen.clt1.util.mlbam.net does not exist\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp20.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp20.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp20.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp23.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp23.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp23.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp78.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp78.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp78.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp79.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp79.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp79.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp101.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp101.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp101.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp25.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp25.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp25.clt1.prod.mlbam.net'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp14.clt1.prod.mlbam.net 'ls -lR /rhev/data-center &gt; /tmp/hyp14.clt1.prod.mlbam.net; lvm lvs -o +tags &gt;&gt; /tmp/hyp14.clt1.prod.mlbam.net'\n~~~\n\n\nThen from RHV-M host, sftp to get files and upload them to the case.\n\n~~~\ncd /tmp/\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp103.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp103.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp07.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp07.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp70.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp70.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp101.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp101.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp92.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp92.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp89.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp89.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp95.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp95.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp25.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp25.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp66.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp66.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp75.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp75.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp102.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp102.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp29.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp29.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp31.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp31.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp81.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp81.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp25.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp25.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp89.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp89.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp85.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp85.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp32.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp32.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp102.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp102.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nVM mf08.c01.web.gen.clt1.prod.bamtech.co does not exist\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp90.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp90.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp102.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp102.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp103.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp103.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp104.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp104.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp105.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp105.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp29.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp29.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp104.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp104.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp28.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp28.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp04.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp04.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp13.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp13.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp72.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp72.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp94.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp94.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp24.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp24.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp27.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp27.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp07.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp07.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp98.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp98.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp99.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp99.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp03.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp03.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp10.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp10.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp23.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp23.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nVM redis02.c01.inf.gen.clt1.util.mlbam.net does not exist\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp20.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp20.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp23.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp23.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp78.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp78.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp79.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp79.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp101.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp101.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp25.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp25.clt1.prod.mlbam.net*\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp14.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/hyp14.clt1.prod.mlbam.net*\nsftp&gt; quit\n~~~\n\n\ntar up the /tmp/hyp* and upload to the case.\n\n\nThanks\nBimal Chollera\nRed Hat, Inc.\nGSS</text>, <text>Here you go ...</text>, <text>Thanks Marcus,\nWill start working on this soon.\n\n~ Bimal.</text>, <text>Marcus,\n\nCan you check where harmonic02.c01.bc.gen.clt1.prod.mlbam.net VM is running?\nThe db that we have shows running on hyp92.clt1.prod.mlbam.net but I don't its correct.\n\n- Bimal.</text>, <text>That Vm was deleted recently (yesterday actually)\n\n\n\n\nOn 5/2/18, 2:43 PM, "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>Marcus, \nThe DB i have is from  April 5th.\nCan you get LC only from RHV-M that contains the DB as well.\n\nThanks\nBimal.</text>, <text>Hi Marcus,\n\nSorry but I need to review sos from the following host.\nNeed to which volumes are open for some of the images.\n\n~~~\nhyp89.clt1.prod.mlbam.net\nhyp95.clt1.prod.mlbam.net\nhyp25.clt1.prod.mlbam.net\nhyp81.clt1.prod.mlbam.net\nhyp89.clt1.prod.mlbam.net\nhyp32.clt1.prod.mlbam.net\nhyp90.clt1.prod.mlbam.net\n~~~\n\nThanks \nBimal Chollera\nRed Hat, Inc\nGSS</text>, <text>Attaching an engine-backup (--mode=backup --scope-files --scope=db)</text>, <text>attaching an ovirt-log-collection from RHVM</text>, <text>Hi Marcus,\nCan you get the sos report from just host hyp95.clt1.prod.mlbam.net.\nNeed to check the volumes.\n\n- Bimal.</text>, <text>Hi Bimal,\nthe sos report from host hyp95 is included in sos-reports.tar (attached)</text>, <text>Thanks Marcus,\n\n\nWill review them but so far the following VM will be an issue.\n\n\n~~~\nnginx01.live01.hls.gen.clt1.prod.mlbam.net  \nVM appears to have been restarted and the ACTIVE is on ovirt local.\nThere are 2 volumes/images on ovirt-local\n\nIf we fix this - will go back to Dec 26 data.\n\n\n              image_guid              |            image_group_id            |            vm_snapshot_id            |               parentid               | imagestatus |     creation_date      | volume_type | volume_format | active \n--------------------------------------+--------------------------------------+--------------------------------------+--------------------------------------+-------------+------------------------+-------------+---------------+--------\n 755fb725-245e-478f-aca5-a14efe65de78 | 49ffa3fd-2038-4256-88eb-43ebeaaf767d | 7e925eca-008e-4e31-931d-b80740fe6177 | fa048414-a3e0-44cf-818d-42688a995004 |           1 | 2017-03-27 12:25:07+00 |           2 |             4 | f\n ff08a316-29f8-4f72-8ac6-91fe37316f39 | 49ffa3fd-2038-4256-88eb-43ebeaaf767d | 5c68e92a-d158-4549-abae-9fd3fe4765b9 | 755fb725-245e-478f-aca5-a14efe65de78 |           1 | 2017-11-12 11:18:54+00 |           2 |             4 | t\n(2 rows)\n\n\n  755fb725-245e-478f-aca5-a14efe65de78 ovirt-local -wi-------  50.00g  IU_49ffa3fd-2038-4256-88eb-43ebeaaf767d,PU_00000000-0000-0000-0000-000000000000,VM_1c28c86d-a473-4bd4-a6d2-a47879ab3cee\n  ff08a316-29f8-4f72-8ac6-91fe37316f39 ovirt-local -wi-ao----  50.00g  IU_49ffa3fd-2038-4256-88eb-43ebeaaf767d,PU_00000000-0000-0000-0000-000000000000,VM_1c28c86d-a473-4bd4-a6d2-a47879ab3cee\n\n\n/rhev/data-center/mnt/msnasclt101-client.clt1.prod.mlbam.net:_ifs_data_coreinf_kvm_nfs__datastore03/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/49ffa3fd-2038-4256-88eb-43ebeaaf767d:\ntotal 3167438\n\n-rw-rw----. 409 vdsm kvm 1929641984 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004\n-rw-rw----. 409 vdsm kvm    1048576 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.lease\n-rw-r--r--. 409 vdsm kvm        262 Feb 10  2017 fa048414-a3e0-44cf-818d-42688a995004.meta\n\n-rw-rw----.   1 vdsm kvm     197632 Mar 27  2017 755fb725-245e-478f-aca5-a14efe65de78\n-rw-rw----.   1 vdsm kvm    1048576 Mar 27  2017 755fb725-245e-478f-aca5-a14efe65de78.lease\n-rw-r--r--.   1 vdsm kvm        264 Nov 12 11:18 755fb725-245e-478f-aca5-a14efe65de78.meta\n\n-rw-rw----.   1 vdsm kvm  656015360 Dec 26 11:54 ff08a316-29f8-4f72-8ac6-91fe37316f39\n-rw-rw----.   1 vdsm kvm    1048576 Nov 12 11:18 ff08a316-29f8-4f72-8ac6-91fe37316f39.lease\n-rw-r--r--.   1 vdsm kvm        260 Nov 12 11:18 ff08a316-29f8-4f72-8ac6-91fe37316f39.meta\n~~~</text>, <text>Thanks Bimal,\nI\u2019ll make a note of this ! We\u2019ll probably look to rebuild this VM entirely then. \n\n\n\nFor my own notes (please disregard)\u2026.\n\nRony, \nnginx01.live01.hls.gen.clt1.prod.mlbam.net will need to be rebuilt.\n\n\n\n\nOn 5/3/18, 12:50 PM, "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>Thanks Marcus,\n\nBut didn't get entire sos report from the some of the hosts.\n\nCan you just provide the following.\n\nFrom RHV-M\n\n~~~\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp89.clt1.prod.mlbam.net 'lsof &gt; /tmp/lsof-hyp89'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp25.clt1.prod.mlbam.net 'lsof &gt; /tmp/lsof-hyp25'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp81.clt1.prod.mlbam.net 'lsof &gt; /tmp/lsof-hyp81'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp32.clt1.prod.mlbam.net 'lsof &gt; /tmp/lsof-hyp32'\nssh -i /etc/pki/ovirt-engine/keys/engine_id_rsa hyp90.clt1.prod.mlbam.net 'lsof &gt; /tmp/lsof-hyp90'\n~~~\n\n\nThen from RHV-M host, sftp to get files and upload them to the case.\n\n~~~\ncd /tmp/\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp89.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/lsof-hyp89\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp25.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/lsof-hyp25\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp81.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/lsof-hyp81\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp32.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/lsof-hyp32\nsftp&gt; quit\n\nsftp -i /etc/pki/ovirt-engine/keys/engine_id_rsa  hyp90.clt1.prod.mlbam.net\nsftp&gt; mget /tmp/lsof-hyp90\nsftp&gt; quit\n~~~</text>, <text>FIXING VM:  aspera01.c01.inf.wwe.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp103.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'fd4d736a-b95e-4787-b892-9b7559e9ad48' AND snapshot_id != '1fc66795-1fcc-4be4-8f40-3ed8292a6060';"\n\nFIXING DISK d20b7543-9d24-40a6-b159-92ded7f31e8f, CHAIN ['7f68f112-c3c6-4338-a53a-46382c591d36', 'd3d769ad-0326-4aa8-850f-8684abd2cebd']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/7f68f112-c3c6-4338-a53a-46382c591d36 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d20b7543-9d24-40a6-b159-92ded7f31e8f/d3d769ad-0326-4aa8-850f-8684abd2cebd'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d20b7543-9d24-40a6-b159-92ded7f31e8f/7f68f112-c3c6-4338-a53a-46382c591d36.meta"\n\nlvchange -ay /dev/ovirt-local/7f68f112-c3c6-4338-a53a-46382c591d36\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/7f68f112-c3c6-4338-a53a-46382c591d36 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d20b7543-9d24-40a6-b159-92ded7f31e8f/d3d769ad-0326-4aa8-850f-8684abd2cebd'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d20b7543-9d24-40a6-b159-92ded7f31e8f/d3d769ad-0326-4aa8-850f-8684abd2cebd.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d20b7543-9d24-40a6-b159-92ded7f31e8f/d3d769ad-0326-4aa8-850f-8684abd2cebd.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d20b7543-9d24-40a6-b159-92ded7f31e8f/d3d769ad-0326-4aa8-850f-8684abd2cebd.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d20b7543-9d24-40a6-b159-92ded7f31e8f/d3d769ad-0326-4aa8-850f-8684abd2cebd.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d20b7543-9d24-40a6-b159-92ded7f31e8f/d3d769ad-0326-4aa8-850f-8684abd2cebd /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d20b7543-9d24-40a6-b159-92ded7f31e8f/d3d769ad-0326-4aa8-850f-8684abd2cebd.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('d3d769ad-0326-4aa8-850f-8684abd2cebd');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1fc66795-1fcc-4be4-8f40-3ed8292a6060' WHERE image_guid ='7f68f112-c3c6-4338-a53a-46382c591d36';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  aspera01.c01.jobmgr.gen.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp07.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '051c1f6c-a642-4552-9c7b-9245a84995b8' AND snapshot_id != '9e8fbb8f-7f2a-453c-bd6c-2250498eab4d';"\n\nFIXING DISK 2aa3e21a-1f73-42e3-89e7-4eec35de0ca6, CHAIN ['30e410fe-9d3a-4584-a027-98efa3693f54', 'ec954a1b-d124-436a-a72d-f5b58237e7dd']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/30e410fe-9d3a-4584-a027-98efa3693f54 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2aa3e21a-1f73-42e3-89e7-4eec35de0ca6/ec954a1b-d124-436a-a72d-f5b58237e7dd'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2aa3e21a-1f73-42e3-89e7-4eec35de0ca6/30e410fe-9d3a-4584-a027-98efa3693f54.meta"\n\nlvchange -ay /dev/ovirt-local/30e410fe-9d3a-4584-a027-98efa3693f54\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/30e410fe-9d3a-4584-a027-98efa3693f54 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2aa3e21a-1f73-42e3-89e7-4eec35de0ca6/ec954a1b-d124-436a-a72d-f5b58237e7dd'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2aa3e21a-1f73-42e3-89e7-4eec35de0ca6/ec954a1b-d124-436a-a72d-f5b58237e7dd.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2aa3e21a-1f73-42e3-89e7-4eec35de0ca6/ec954a1b-d124-436a-a72d-f5b58237e7dd.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2aa3e21a-1f73-42e3-89e7-4eec35de0ca6/ec954a1b-d124-436a-a72d-f5b58237e7dd.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2aa3e21a-1f73-42e3-89e7-4eec35de0ca6/ec954a1b-d124-436a-a72d-f5b58237e7dd.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2aa3e21a-1f73-42e3-89e7-4eec35de0ca6/ec954a1b-d124-436a-a72d-f5b58237e7dd /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2aa3e21a-1f73-42e3-89e7-4eec35de0ca6/ec954a1b-d124-436a-a72d-f5b58237e7dd.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('ec954a1b-d124-436a-a72d-f5b58237e7dd');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='9e8fbb8f-7f2a-453c-bd6c-2250498eab4d' WHERE image_guid ='30e410fe-9d3a-4584-a027-98efa3693f54';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  aspera02.c01.jobmgr.gen.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp70.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0bc48709-cc3c-4338-a10b-cf49989cd5d2' AND snapshot_id != '38ca4ded-9a8f-4753-bfc5-edb9723cb79d';"\n\nFIXING DISK 5bba3b9e-bf7e-45f6-ad37-bdc89d2ec45d, CHAIN ['b5468501-1d7d-4dcc-bde4-a1f1933759e9', '950cb0dc-6195-46ee-ae92-1dd3bb4f8f77']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/b5468501-1d7d-4dcc-bde4-a1f1933759e9 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5bba3b9e-bf7e-45f6-ad37-bdc89d2ec45d/950cb0dc-6195-46ee-ae92-1dd3bb4f8f77'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5bba3b9e-bf7e-45f6-ad37-bdc89d2ec45d/b5468501-1d7d-4dcc-bde4-a1f1933759e9.meta"\n\nlvchange -ay /dev/ovirt-local/b5468501-1d7d-4dcc-bde4-a1f1933759e9\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/b5468501-1d7d-4dcc-bde4-a1f1933759e9 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5bba3b9e-bf7e-45f6-ad37-bdc89d2ec45d/950cb0dc-6195-46ee-ae92-1dd3bb4f8f77'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5bba3b9e-bf7e-45f6-ad37-bdc89d2ec45d/950cb0dc-6195-46ee-ae92-1dd3bb4f8f77.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5bba3b9e-bf7e-45f6-ad37-bdc89d2ec45d/950cb0dc-6195-46ee-ae92-1dd3bb4f8f77.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5bba3b9e-bf7e-45f6-ad37-bdc89d2ec45d/950cb0dc-6195-46ee-ae92-1dd3bb4f8f77.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5bba3b9e-bf7e-45f6-ad37-bdc89d2ec45d/950cb0dc-6195-46ee-ae92-1dd3bb4f8f77.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5bba3b9e-bf7e-45f6-ad37-bdc89d2ec45d/950cb0dc-6195-46ee-ae92-1dd3bb4f8f77 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5bba3b9e-bf7e-45f6-ad37-bdc89d2ec45d/950cb0dc-6195-46ee-ae92-1dd3bb4f8f77.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('950cb0dc-6195-46ee-ae92-1dd3bb4f8f77');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='38ca4ded-9a8f-4753-bfc5-edb9723cb79d' WHERE image_guid ='b5468501-1d7d-4dcc-bde4-a1f1933759e9';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  asperaproxy01.c01.inf.datg.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host: hyp101.clt1.prod.mlbam.net \n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '9ec3d564-c687-46f0-8cd3-e3b86330a940' AND snapshot_id != '06f139f9-51f8-41d3-a361-98cedd7b8b33';"\n\nFIXING DISK 12931a7c-f359-45aa-b23e-8e6a722767da, CHAIN ['3b409e57-9a25-4886-a10b-04ef664e1c3a', '9ee780cc-f3db-4c6c-bcb0-855e8507f6bb']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3b409e57-9a25-4886-a10b-04ef664e1c3a -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/12931a7c-f359-45aa-b23e-8e6a722767da/9ee780cc-f3db-4c6c-bcb0-855e8507f6bb'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/12931a7c-f359-45aa-b23e-8e6a722767da/3b409e57-9a25-4886-a10b-04ef664e1c3a.meta"\n\nlvchange -ay /dev/ovirt-local/3b409e57-9a25-4886-a10b-04ef664e1c3a\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3b409e57-9a25-4886-a10b-04ef664e1c3a -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/12931a7c-f359-45aa-b23e-8e6a722767da/9ee780cc-f3db-4c6c-bcb0-855e8507f6bb'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/12931a7c-f359-45aa-b23e-8e6a722767da/9ee780cc-f3db-4c6c-bcb0-855e8507f6bb.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/12931a7c-f359-45aa-b23e-8e6a722767da/9ee780cc-f3db-4c6c-bcb0-855e8507f6bb.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/12931a7c-f359-45aa-b23e-8e6a722767da/9ee780cc-f3db-4c6c-bcb0-855e8507f6bb.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/12931a7c-f359-45aa-b23e-8e6a722767da/9ee780cc-f3db-4c6c-bcb0-855e8507f6bb.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/12931a7c-f359-45aa-b23e-8e6a722767da/9ee780cc-f3db-4c6c-bcb0-855e8507f6bb /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/12931a7c-f359-45aa-b23e-8e6a722767da/9ee780cc-f3db-4c6c-bcb0-855e8507f6bb.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('9ee780cc-f3db-4c6c-bcb0-855e8507f6bb');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='06f139f9-51f8-41d3-a361-98cedd7b8b33' WHERE image_guid ='3b409e57-9a25-4886-a10b-04ef664e1c3a';"\n\n=======================================================================</text>, <text>FIXING VM:  memcached02.c01.lprovis.gen.clt1.qa.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp66.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'fb590708-03be-41bd-9fc6-244e81f12ca1' AND snapshot_id != '0d4d873f-cb2e-4fb1-ae11-269fff22ea8f';"\n\nFIXING DISK 989a680f-8652-44cb-8c4f-2262ee9e6de7, CHAIN ['5b5feaef-8bac-4323-851f-cf0a65f1b442', '35af780b-ca14-4361-82f3-cb7edd276e63', '98e5d5cd-035e-42de-890d-6363b6b7b2ca']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/35af780b-ca14-4361-82f3-cb7edd276e63 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/98e5d5cd-035e-42de-890d-6363b6b7b2ca'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/98e5d5cd-035e-42de-890d-6363b6b7b2ca.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/98e5d5cd-035e-42de-890d-6363b6b7b2ca.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/98e5d5cd-035e-42de-890d-6363b6b7b2ca.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/98e5d5cd-035e-42de-890d-6363b6b7b2ca.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/98e5d5cd-035e-42de-890d-6363b6b7b2ca /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/98e5d5cd-035e-42de-890d-6363b6b7b2ca.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/5b5feaef-8bac-4323-851f-cf0a65f1b442 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/35af780b-ca14-4361-82f3-cb7edd276e63'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/5b5feaef-8bac-4323-851f-cf0a65f1b442.meta"\n\nlvchange -ay /dev/ovirt-local/5b5feaef-8bac-4323-851f-cf0a65f1b442\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/5b5feaef-8bac-4323-851f-cf0a65f1b442 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/35af780b-ca14-4361-82f3-cb7edd276e63'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/35af780b-ca14-4361-82f3-cb7edd276e63.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/35af780b-ca14-4361-82f3-cb7edd276e63.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/35af780b-ca14-4361-82f3-cb7edd276e63.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/35af780b-ca14-4361-82f3-cb7edd276e63.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/35af780b-ca14-4361-82f3-cb7edd276e63 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/989a680f-8652-44cb-8c4f-2262ee9e6de7/35af780b-ca14-4361-82f3-cb7edd276e63.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('35af780b-ca14-4361-82f3-cb7edd276e63','98e5d5cd-035e-42de-890d-6363b6b7b2ca');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='0d4d873f-cb2e-4fb1-ae11-269fff22ea8f' WHERE image_guid ='5b5feaef-8bac-4323-851f-cf0a65f1b442';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  memcached02.c01.lprovis.hulu.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp75.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '13d1756f-2c41-454d-892e-e0c31d7a219a' AND snapshot_id != '4f4d49bd-8a17-485e-b38d-4cbd0eefd8e8';"\n\nFIXING DISK 6013a2eb-8d27-4ea3-bf6c-58670e1e2bc1, CHAIN ['0b12bfe2-8789-4231-8278-c75bfcdc438e', 'a1778cb2-ee9a-48b5-8234-0ca3d33cd467']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/0b12bfe2-8789-4231-8278-c75bfcdc438e -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/6013a2eb-8d27-4ea3-bf6c-58670e1e2bc1/a1778cb2-ee9a-48b5-8234-0ca3d33cd467'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/6013a2eb-8d27-4ea3-bf6c-58670e1e2bc1/0b12bfe2-8789-4231-8278-c75bfcdc438e.meta"\n\nlvchange -ay /dev/ovirt-local/0b12bfe2-8789-4231-8278-c75bfcdc438e\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/0b12bfe2-8789-4231-8278-c75bfcdc438e -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/6013a2eb-8d27-4ea3-bf6c-58670e1e2bc1/a1778cb2-ee9a-48b5-8234-0ca3d33cd467'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/6013a2eb-8d27-4ea3-bf6c-58670e1e2bc1/a1778cb2-ee9a-48b5-8234-0ca3d33cd467.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/6013a2eb-8d27-4ea3-bf6c-58670e1e2bc1/a1778cb2-ee9a-48b5-8234-0ca3d33cd467.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/6013a2eb-8d27-4ea3-bf6c-58670e1e2bc1/a1778cb2-ee9a-48b5-8234-0ca3d33cd467.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/6013a2eb-8d27-4ea3-bf6c-58670e1e2bc1/a1778cb2-ee9a-48b5-8234-0ca3d33cd467.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/6013a2eb-8d27-4ea3-bf6c-58670e1e2bc1/a1778cb2-ee9a-48b5-8234-0ca3d33cd467 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/6013a2eb-8d27-4ea3-bf6c-58670e1e2bc1/a1778cb2-ee9a-48b5-8234-0ca3d33cd467.orig'\n\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('a1778cb2-ee9a-48b5-8234-0ca3d33cd467');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='4f4d49bd-8a17-485e-b38d-4cbd0eefd8e8' WHERE image_guid ='0b12bfe2-8789-4231-8278-c75bfcdc438e';"\n\n=======================================================================</text>, <text>FIXING VM:  mf01.c01.web.wwe.clt1.prod.bamtech.co:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp102.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '17a381f7-fd99-448d-81cc-d9a8c94831b5' AND snapshot_id != 'a2cd9a6f-2774-4f64-b7d2-dd6369801463';"\n\nFIXING DISK e15dbf56-dc3c-4b5c-b129-0cd7dc0c2a5c, CHAIN ['019f1aa5-9f22-457a-9c71-693f5206987a', '423e16b7-d2e5-4adb-bc0f-f89a99306027']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/019f1aa5-9f22-457a-9c71-693f5206987a -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e15dbf56-dc3c-4b5c-b129-0cd7dc0c2a5c/423e16b7-d2e5-4adb-bc0f-f89a99306027'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e15dbf56-dc3c-4b5c-b129-0cd7dc0c2a5c/019f1aa5-9f22-457a-9c71-693f5206987a.meta"\n\nlvchange -ay /dev/ovirt-local/019f1aa5-9f22-457a-9c71-693f5206987a\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/019f1aa5-9f22-457a-9c71-693f5206987a -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e15dbf56-dc3c-4b5c-b129-0cd7dc0c2a5c/423e16b7-d2e5-4adb-bc0f-f89a99306027'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e15dbf56-dc3c-4b5c-b129-0cd7dc0c2a5c/423e16b7-d2e5-4adb-bc0f-f89a99306027.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e15dbf56-dc3c-4b5c-b129-0cd7dc0c2a5c/423e16b7-d2e5-4adb-bc0f-f89a99306027.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e15dbf56-dc3c-4b5c-b129-0cd7dc0c2a5c/423e16b7-d2e5-4adb-bc0f-f89a99306027.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e15dbf56-dc3c-4b5c-b129-0cd7dc0c2a5c/423e16b7-d2e5-4adb-bc0f-f89a99306027.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e15dbf56-dc3c-4b5c-b129-0cd7dc0c2a5c/423e16b7-d2e5-4adb-bc0f-f89a99306027 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e15dbf56-dc3c-4b5c-b129-0cd7dc0c2a5c/423e16b7-d2e5-4adb-bc0f-f89a99306027.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('423e16b7-d2e5-4adb-bc0f-f89a99306027');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a2cd9a6f-2774-4f64-b7d2-dd6369801463' WHERE image_guid ='019f1aa5-9f22-457a-9c71-693f5206987a';"\n\n=======================================================================\n\n\nFIXING VM:  mf09.c01.web.wwe.clt1.prod.bamtech.co:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp102.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '6b3d7ae7-c96d-4fe9-8081-14145160d3eb' AND snapshot_id != '49ddaafb-3a3b-4d1b-89ff-9379761eb022';"\n\nFIXING DISK 574fd8f3-3cca-4477-9ea5-e3b57dfe0581, CHAIN ['b3e909fa-94af-4c43-b616-1d27637c8715', '0b1b7c15-d1d1-42a5-aeda-f31e0e537792']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/b3e909fa-94af-4c43-b616-1d27637c8715 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/574fd8f3-3cca-4477-9ea5-e3b57dfe0581/0b1b7c15-d1d1-42a5-aeda-f31e0e537792'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/574fd8f3-3cca-4477-9ea5-e3b57dfe0581/b3e909fa-94af-4c43-b616-1d27637c8715.meta"\n\nlvchange -ay /dev/ovirt-local/b3e909fa-94af-4c43-b616-1d27637c8715\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/b3e909fa-94af-4c43-b616-1d27637c8715 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/574fd8f3-3cca-4477-9ea5-e3b57dfe0581/0b1b7c15-d1d1-42a5-aeda-f31e0e537792'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/574fd8f3-3cca-4477-9ea5-e3b57dfe0581/0b1b7c15-d1d1-42a5-aeda-f31e0e537792.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/574fd8f3-3cca-4477-9ea5-e3b57dfe0581/0b1b7c15-d1d1-42a5-aeda-f31e0e537792.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/574fd8f3-3cca-4477-9ea5-e3b57dfe0581/0b1b7c15-d1d1-42a5-aeda-f31e0e537792.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/574fd8f3-3cca-4477-9ea5-e3b57dfe0581/0b1b7c15-d1d1-42a5-aeda-f31e0e537792.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/574fd8f3-3cca-4477-9ea5-e3b57dfe0581/0b1b7c15-d1d1-42a5-aeda-f31e0e537792 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/574fd8f3-3cca-4477-9ea5-e3b57dfe0581/0b1b7c15-d1d1-42a5-aeda-f31e0e537792.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0b1b7c15-d1d1-42a5-aeda-f31e0e537792');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='49ddaafb-3a3b-4d1b-89ff-9379761eb022' WHERE image_guid ='b3e909fa-94af-4c43-b616-1d27637c8715';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  mf10.c01.web.wwe.clt1.prod.bamtech.co:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp103.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0449bf09-f0dc-4cee-b4ab-3d584a2d6959' AND snapshot_id != 'e4257251-a751-4fdf-8852-1879ae31fab7';"\n\nFIXING DISK 5d5c74bc-e9e8-43c1-83de-64b8e1772609, CHAIN ['6dd690ce-6116-419b-9a47-5b0907828b95', 'e1d855de-70cb-42f4-9231-2b73f8c7d626']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/6dd690ce-6116-419b-9a47-5b0907828b95 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5d5c74bc-e9e8-43c1-83de-64b8e1772609/e1d855de-70cb-42f4-9231-2b73f8c7d626'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5d5c74bc-e9e8-43c1-83de-64b8e1772609/6dd690ce-6116-419b-9a47-5b0907828b95.meta"\n\nlvchange -ay /dev/ovirt-local/6dd690ce-6116-419b-9a47-5b0907828b95\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/6dd690ce-6116-419b-9a47-5b0907828b95 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5d5c74bc-e9e8-43c1-83de-64b8e1772609/e1d855de-70cb-42f4-9231-2b73f8c7d626'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5d5c74bc-e9e8-43c1-83de-64b8e1772609/e1d855de-70cb-42f4-9231-2b73f8c7d626.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5d5c74bc-e9e8-43c1-83de-64b8e1772609/e1d855de-70cb-42f4-9231-2b73f8c7d626.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5d5c74bc-e9e8-43c1-83de-64b8e1772609/e1d855de-70cb-42f4-9231-2b73f8c7d626.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5d5c74bc-e9e8-43c1-83de-64b8e1772609/e1d855de-70cb-42f4-9231-2b73f8c7d626.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5d5c74bc-e9e8-43c1-83de-64b8e1772609/e1d855de-70cb-42f4-9231-2b73f8c7d626 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5d5c74bc-e9e8-43c1-83de-64b8e1772609/e1d855de-70cb-42f4-9231-2b73f8c7d626.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('e1d855de-70cb-42f4-9231-2b73f8c7d626');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='e4257251-a751-4fdf-8852-1879ae31fab7' WHERE image_guid ='6dd690ce-6116-419b-9a47-5b0907828b95';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  mf11.c01.web.wwe.clt1.prod.bamtech.co:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host: hyp104.clt1.prod.mlbam.net \n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'c4be8fe9-258c-4a3b-9c58-c15ae125837b' AND snapshot_id != 'd736758a-4067-46cf-8f8f-a2a32f7e5128';"\n\nFIXING DISK 5f44a086-1737-44fc-936e-c585f17a0b83, CHAIN ['3afc32bb-68aa-4d93-a03a-583f2d06d151', '20b5c17d-bbf8-4877-b728-86e1c956a8cb']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3afc32bb-68aa-4d93-a03a-583f2d06d151 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5f44a086-1737-44fc-936e-c585f17a0b83/20b5c17d-bbf8-4877-b728-86e1c956a8cb'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5f44a086-1737-44fc-936e-c585f17a0b83/3afc32bb-68aa-4d93-a03a-583f2d06d151.meta"\n\nlvchange -ay /dev/ovirt-local/3afc32bb-68aa-4d93-a03a-583f2d06d151\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3afc32bb-68aa-4d93-a03a-583f2d06d151 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5f44a086-1737-44fc-936e-c585f17a0b83/20b5c17d-bbf8-4877-b728-86e1c956a8cb'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5f44a086-1737-44fc-936e-c585f17a0b83/20b5c17d-bbf8-4877-b728-86e1c956a8cb.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5f44a086-1737-44fc-936e-c585f17a0b83/20b5c17d-bbf8-4877-b728-86e1c956a8cb.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5f44a086-1737-44fc-936e-c585f17a0b83/20b5c17d-bbf8-4877-b728-86e1c956a8cb.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5f44a086-1737-44fc-936e-c585f17a0b83/20b5c17d-bbf8-4877-b728-86e1c956a8cb.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5f44a086-1737-44fc-936e-c585f17a0b83/20b5c17d-bbf8-4877-b728-86e1c956a8cb /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5f44a086-1737-44fc-936e-c585f17a0b83/20b5c17d-bbf8-4877-b728-86e1c956a8cb.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('20b5c17d-bbf8-4877-b728-86e1c956a8cb');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d736758a-4067-46cf-8f8f-a2a32f7e5128' WHERE image_guid ='3afc32bb-68aa-4d93-a03a-583f2d06d151';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  mf12.c01.web.wwe.clt1.prod.bamtech.co:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp105.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '64823cb9-d32d-4c94-b8dc-4f855134eeda' AND snapshot_id != '703c1540-bd1e-4fc7-a0aa-5c5d97dca621';"\n\nFIXING DISK 03ac71ff-0313-4b2e-abf4-66ed2c023de2, CHAIN ['3d6f7be5-ba77-4f8d-964c-3d7f634b7f47', '7b386c9a-092a-45b6-8b99-191bdcfee087']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3d6f7be5-ba77-4f8d-964c-3d7f634b7f47 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03ac71ff-0313-4b2e-abf4-66ed2c023de2/7b386c9a-092a-45b6-8b99-191bdcfee087'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03ac71ff-0313-4b2e-abf4-66ed2c023de2/3d6f7be5-ba77-4f8d-964c-3d7f634b7f47.meta"\n\nlvchange -ay /dev/ovirt-local/3d6f7be5-ba77-4f8d-964c-3d7f634b7f47\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3d6f7be5-ba77-4f8d-964c-3d7f634b7f47 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03ac71ff-0313-4b2e-abf4-66ed2c023de2/7b386c9a-092a-45b6-8b99-191bdcfee087'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03ac71ff-0313-4b2e-abf4-66ed2c023de2/7b386c9a-092a-45b6-8b99-191bdcfee087.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03ac71ff-0313-4b2e-abf4-66ed2c023de2/7b386c9a-092a-45b6-8b99-191bdcfee087.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03ac71ff-0313-4b2e-abf4-66ed2c023de2/7b386c9a-092a-45b6-8b99-191bdcfee087.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03ac71ff-0313-4b2e-abf4-66ed2c023de2/7b386c9a-092a-45b6-8b99-191bdcfee087.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03ac71ff-0313-4b2e-abf4-66ed2c023de2/7b386c9a-092a-45b6-8b99-191bdcfee087 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/03ac71ff-0313-4b2e-abf4-66ed2c023de2/7b386c9a-092a-45b6-8b99-191bdcfee087.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('7b386c9a-092a-45b6-8b99-191bdcfee087');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='703c1540-bd1e-4fc7-a0aa-5c5d97dca621' WHERE image_guid ='3d6f7be5-ba77-4f8d-964c-3d7f634b7f47';"\n\n=======================================================================</text>, <text>FIXING VM:  nginx01.c01.medtrans.gen.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp29.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '272d4e98-7be9-4957-814d-1d611b31032a' AND snapshot_id != '3272a5e6-789b-4e5f-9623-521ace3e607d';"\n\nFIXING DISK 71cdf75d-c3cc-4ff5-b110-8afc4177ad83, CHAIN ['9ecf821e-a333-46a6-8ac8-bfe47d66d0d0', 'dac8d927-9d16-4b8e-a249-48651bba8643']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/9ecf821e-a333-46a6-8ac8-bfe47d66d0d0 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/71cdf75d-c3cc-4ff5-b110-8afc4177ad83/dac8d927-9d16-4b8e-a249-48651bba8643'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/71cdf75d-c3cc-4ff5-b110-8afc4177ad83/9ecf821e-a333-46a6-8ac8-bfe47d66d0d0.meta"\n\nlvchange -ay /dev/ovirt-local/9ecf821e-a333-46a6-8ac8-bfe47d66d0d0\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/9ecf821e-a333-46a6-8ac8-bfe47d66d0d0 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/71cdf75d-c3cc-4ff5-b110-8afc4177ad83/dac8d927-9d16-4b8e-a249-48651bba8643'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/71cdf75d-c3cc-4ff5-b110-8afc4177ad83/dac8d927-9d16-4b8e-a249-48651bba8643.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/71cdf75d-c3cc-4ff5-b110-8afc4177ad83/dac8d927-9d16-4b8e-a249-48651bba8643.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/71cdf75d-c3cc-4ff5-b110-8afc4177ad83/dac8d927-9d16-4b8e-a249-48651bba8643.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/71cdf75d-c3cc-4ff5-b110-8afc4177ad83/dac8d927-9d16-4b8e-a249-48651bba8643.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/71cdf75d-c3cc-4ff5-b110-8afc4177ad83/dac8d927-9d16-4b8e-a249-48651bba8643 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/71cdf75d-c3cc-4ff5-b110-8afc4177ad83/dac8d927-9d16-4b8e-a249-48651bba8643.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('dac8d927-9d16-4b8e-a249-48651bba8643');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='3272a5e6-789b-4e5f-9623-521ace3e607d' WHERE image_guid ='9ecf821e-a333-46a6-8ac8-bfe47d66d0d0';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  nginx01.c01.medtrans.gen.clt1.qa.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp104.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '64952c98-a878-44d7-a9b0-00d27a5beba0' AND snapshot_id != 'a028d444-8e18-4c5b-beb8-e79737cc41e1';"\n\nFIXING DISK a5f8c643-4642-4164-95c7-454bd5487f75, CHAIN ['83be32ad-114a-49bb-8e42-a551e0eea1f2', '9c92fb02-c06e-473e-8a3d-096a8f428876', 'e5a6e578-355a-4a6c-8d71-aca650794e94']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/9c92fb02-c06e-473e-8a3d-096a8f428876 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/e5a6e578-355a-4a6c-8d71-aca650794e94'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/e5a6e578-355a-4a6c-8d71-aca650794e94.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/e5a6e578-355a-4a6c-8d71-aca650794e94.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/e5a6e578-355a-4a6c-8d71-aca650794e94.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/e5a6e578-355a-4a6c-8d71-aca650794e94.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/e5a6e578-355a-4a6c-8d71-aca650794e94 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/e5a6e578-355a-4a6c-8d71-aca650794e94.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/83be32ad-114a-49bb-8e42-a551e0eea1f2 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/9c92fb02-c06e-473e-8a3d-096a8f428876'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/83be32ad-114a-49bb-8e42-a551e0eea1f2.meta"\n\nlvchange -ay /dev/ovirt-local/83be32ad-114a-49bb-8e42-a551e0eea1f2\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/83be32ad-114a-49bb-8e42-a551e0eea1f2 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/9c92fb02-c06e-473e-8a3d-096a8f428876'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/9c92fb02-c06e-473e-8a3d-096a8f428876.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/9c92fb02-c06e-473e-8a3d-096a8f428876.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/9c92fb02-c06e-473e-8a3d-096a8f428876.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/9c92fb02-c06e-473e-8a3d-096a8f428876.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/9c92fb02-c06e-473e-8a3d-096a8f428876 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a5f8c643-4642-4164-95c7-454bd5487f75/9c92fb02-c06e-473e-8a3d-096a8f428876.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('9c92fb02-c06e-473e-8a3d-096a8f428876','e5a6e578-355a-4a6c-8d71-aca650794e94');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='a028d444-8e18-4c5b-beb8-e79737cc41e1' WHERE image_guid ='83be32ad-114a-49bb-8e42-a551e0eea1f2';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  nginx01.event01.hls.wwe.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host: hyp28.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '2aea5ddd-0b14-4141-b4c9-b3764f7a35f7' AND snapshot_id != 'd5edffa5-4b02-4413-ad03-25df09e203f9';"\n\nFIXING DISK bf6ebc75-2cb9-4eba-891e-2737f464b072, CHAIN ['87d2c48e-0975-4537-8f79-4c6112fb2d1b', 'b399add7-2bb0-4404-8633-c2da172686d2']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/87d2c48e-0975-4537-8f79-4c6112fb2d1b -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bf6ebc75-2cb9-4eba-891e-2737f464b072/b399add7-2bb0-4404-8633-c2da172686d2'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bf6ebc75-2cb9-4eba-891e-2737f464b072/87d2c48e-0975-4537-8f79-4c6112fb2d1b.meta"\n\nlvchange -ay /dev/ovirt-local/87d2c48e-0975-4537-8f79-4c6112fb2d1b\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/87d2c48e-0975-4537-8f79-4c6112fb2d1b -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bf6ebc75-2cb9-4eba-891e-2737f464b072/b399add7-2bb0-4404-8633-c2da172686d2'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bf6ebc75-2cb9-4eba-891e-2737f464b072/b399add7-2bb0-4404-8633-c2da172686d2.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bf6ebc75-2cb9-4eba-891e-2737f464b072/b399add7-2bb0-4404-8633-c2da172686d2.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bf6ebc75-2cb9-4eba-891e-2737f464b072/b399add7-2bb0-4404-8633-c2da172686d2.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bf6ebc75-2cb9-4eba-891e-2737f464b072/b399add7-2bb0-4404-8633-c2da172686d2.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bf6ebc75-2cb9-4eba-891e-2737f464b072/b399add7-2bb0-4404-8633-c2da172686d2 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/bf6ebc75-2cb9-4eba-891e-2737f464b072/b399add7-2bb0-4404-8633-c2da172686d2.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('b399add7-2bb0-4404-8633-c2da172686d2');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d5edffa5-4b02-4413-ad03-25df09e203f9' WHERE image_guid ='87d2c48e-0975-4537-8f79-4c6112fb2d1b';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  nginx01.feed01.hls.mls.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp04.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '30413b14-084a-4024-90e0-e4caec35ff8a' AND snapshot_id != '48b1b305-7951-45b5-8100-56a56cc39c42';"\n\nFIXING DISK a4d9675a-a763-48ed-ad20-9e89e8aa77f0, CHAIN ['41401383-9011-499e-b841-f04dccfb868c', '8a7c25ff-d402-4f33-81a8-5b91fd11af43']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/41401383-9011-499e-b841-f04dccfb868c -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a4d9675a-a763-48ed-ad20-9e89e8aa77f0/8a7c25ff-d402-4f33-81a8-5b91fd11af43'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a4d9675a-a763-48ed-ad20-9e89e8aa77f0/41401383-9011-499e-b841-f04dccfb868c.meta"\n\nlvchange -ay /dev/ovirt-local/41401383-9011-499e-b841-f04dccfb868c\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/41401383-9011-499e-b841-f04dccfb868c -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a4d9675a-a763-48ed-ad20-9e89e8aa77f0/8a7c25ff-d402-4f33-81a8-5b91fd11af43'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a4d9675a-a763-48ed-ad20-9e89e8aa77f0/8a7c25ff-d402-4f33-81a8-5b91fd11af43.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a4d9675a-a763-48ed-ad20-9e89e8aa77f0/8a7c25ff-d402-4f33-81a8-5b91fd11af43.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a4d9675a-a763-48ed-ad20-9e89e8aa77f0/8a7c25ff-d402-4f33-81a8-5b91fd11af43.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a4d9675a-a763-48ed-ad20-9e89e8aa77f0/8a7c25ff-d402-4f33-81a8-5b91fd11af43.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a4d9675a-a763-48ed-ad20-9e89e8aa77f0/8a7c25ff-d402-4f33-81a8-5b91fd11af43 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/a4d9675a-a763-48ed-ad20-9e89e8aa77f0/8a7c25ff-d402-4f33-81a8-5b91fd11af43.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('8a7c25ff-d402-4f33-81a8-5b91fd11af43');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='48b1b305-7951-45b5-8100-56a56cc39c42' WHERE image_guid ='41401383-9011-499e-b841-f04dccfb868c';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  nginx01.ingest01.hls.hulu.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp13.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '2702183a-f2d3-43e4-9624-77804542d91d' AND snapshot_id != 'bde55bdf-15e0-4609-97e2-d1895b413474';"\n\nFIXING DISK 308efcd7-b67f-4c5c-832f-769f8a8c0766, CHAIN ['50bd64db-9b63-4ad2-b530-3436a359af3f', '1775f30b-9c92-4a86-be44-325869029452', 'fb77a8e1-5655-48b2-a8f7-aec63be5ddff']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/1775f30b-9c92-4a86-be44-325869029452 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/fb77a8e1-5655-48b2-a8f7-aec63be5ddff'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/fb77a8e1-5655-48b2-a8f7-aec63be5ddff.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/fb77a8e1-5655-48b2-a8f7-aec63be5ddff.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/fb77a8e1-5655-48b2-a8f7-aec63be5ddff.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/fb77a8e1-5655-48b2-a8f7-aec63be5ddff.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/fb77a8e1-5655-48b2-a8f7-aec63be5ddff /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/fb77a8e1-5655-48b2-a8f7-aec63be5ddff.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/50bd64db-9b63-4ad2-b530-3436a359af3f -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/1775f30b-9c92-4a86-be44-325869029452'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/50bd64db-9b63-4ad2-b530-3436a359af3f.meta"\n\nlvchange -ay /dev/ovirt-local/50bd64db-9b63-4ad2-b530-3436a359af3f\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/50bd64db-9b63-4ad2-b530-3436a359af3f -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/1775f30b-9c92-4a86-be44-325869029452'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/1775f30b-9c92-4a86-be44-325869029452.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/1775f30b-9c92-4a86-be44-325869029452.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/1775f30b-9c92-4a86-be44-325869029452.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/1775f30b-9c92-4a86-be44-325869029452.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/1775f30b-9c92-4a86-be44-325869029452 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/308efcd7-b67f-4c5c-832f-769f8a8c0766/1775f30b-9c92-4a86-be44-325869029452.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('1775f30b-9c92-4a86-be44-325869029452','fb77a8e1-5655-48b2-a8f7-aec63be5ddff');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='bde55bdf-15e0-4609-97e2-d1895b413474' WHERE image_guid ='50bd64db-9b63-4ad2-b530-3436a359af3f';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  nginx01.linear01.hls.wwe.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp72.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ac7f1743-1896-4fd6-8087-b8e495e19eed' AND snapshot_id != '50f452ce-5eb4-4229-ae14-f5051709e2b0';"\n\nFIXING DISK f5a99ec6-555c-413c-ae2c-826d42dda893, CHAIN ['c0865514-be38-4994-9af9-1892d0236796', '8cf73e9e-79a4-47a5-a83c-2ce5a102657c']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c0865514-be38-4994-9af9-1892d0236796 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f5a99ec6-555c-413c-ae2c-826d42dda893/8cf73e9e-79a4-47a5-a83c-2ce5a102657c'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f5a99ec6-555c-413c-ae2c-826d42dda893/c0865514-be38-4994-9af9-1892d0236796.meta"\n\nlvchange -ay /dev/ovirt-local/c0865514-be38-4994-9af9-1892d0236796\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c0865514-be38-4994-9af9-1892d0236796 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f5a99ec6-555c-413c-ae2c-826d42dda893/8cf73e9e-79a4-47a5-a83c-2ce5a102657c'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f5a99ec6-555c-413c-ae2c-826d42dda893/8cf73e9e-79a4-47a5-a83c-2ce5a102657c.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f5a99ec6-555c-413c-ae2c-826d42dda893/8cf73e9e-79a4-47a5-a83c-2ce5a102657c.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f5a99ec6-555c-413c-ae2c-826d42dda893/8cf73e9e-79a4-47a5-a83c-2ce5a102657c.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f5a99ec6-555c-413c-ae2c-826d42dda893/8cf73e9e-79a4-47a5-a83c-2ce5a102657c.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f5a99ec6-555c-413c-ae2c-826d42dda893/8cf73e9e-79a4-47a5-a83c-2ce5a102657c /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/f5a99ec6-555c-413c-ae2c-826d42dda893/8cf73e9e-79a4-47a5-a83c-2ce5a102657c.orig'\n\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('8cf73e9e-79a4-47a5-a83c-2ce5a102657c');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='50f452ce-5eb4-4229-ae14-f5051709e2b0' WHERE image_guid ='c0865514-be38-4994-9af9-1892d0236796';"\n\n=======================================================================</text>, <text>FIXING VM:  nginx01.live01.hls.gen.clt1.dev.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp94.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'd3b75d1a-00ac-4bd0-8003-da1a930392e8' AND snapshot_id != 'f11cf6a2-5bc6-4332-8f56-c3f2609f6635';"\n\nFIXING DISK 1c3ad4f7-9258-44f3-9a31-7bcda2b1c3ed, CHAIN ['a079e9f5-03f1-49a9-802b-362469aa24e6', 'af7b845d-0cfc-4291-bc2a-702de6ec2a12']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/a079e9f5-03f1-49a9-802b-362469aa24e6 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c3ad4f7-9258-44f3-9a31-7bcda2b1c3ed/af7b845d-0cfc-4291-bc2a-702de6ec2a12'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c3ad4f7-9258-44f3-9a31-7bcda2b1c3ed/a079e9f5-03f1-49a9-802b-362469aa24e6.meta"\n\nlvchange -ay /dev/ovirt-local/a079e9f5-03f1-49a9-802b-362469aa24e6\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/a079e9f5-03f1-49a9-802b-362469aa24e6 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c3ad4f7-9258-44f3-9a31-7bcda2b1c3ed/af7b845d-0cfc-4291-bc2a-702de6ec2a12'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c3ad4f7-9258-44f3-9a31-7bcda2b1c3ed/af7b845d-0cfc-4291-bc2a-702de6ec2a12.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c3ad4f7-9258-44f3-9a31-7bcda2b1c3ed/af7b845d-0cfc-4291-bc2a-702de6ec2a12.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c3ad4f7-9258-44f3-9a31-7bcda2b1c3ed/af7b845d-0cfc-4291-bc2a-702de6ec2a12.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c3ad4f7-9258-44f3-9a31-7bcda2b1c3ed/af7b845d-0cfc-4291-bc2a-702de6ec2a12.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c3ad4f7-9258-44f3-9a31-7bcda2b1c3ed/af7b845d-0cfc-4291-bc2a-702de6ec2a12 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1c3ad4f7-9258-44f3-9a31-7bcda2b1c3ed/af7b845d-0cfc-4291-bc2a-702de6ec2a12.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('af7b845d-0cfc-4291-bc2a-702de6ec2a12');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='f11cf6a2-5bc6-4332-8f56-c3f2609f6635' WHERE image_guid ='a079e9f5-03f1-49a9-802b-362469aa24e6';"\n\n=======================================================================\n\n~~~\n\n\n=======================================================================\n\n~~~\n\nFIXING VM:  nginx01.live01.hls.riot.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp27.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '96a6820d-5e05-406e-b87e-2ab86a1d0024' AND snapshot_id != '49aab18b-ff1b-4462-a363-b7580e717c42';"\n\nFIXING DISK fc79a63b-c023-4726-964c-2a206d9e9d9f, CHAIN ['f8f43ca1-a6f0-47a8-be7d-e85c3997cf56', '564748a5-d2c8-4cc0-8ecd-a2379ed2aa19']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f8f43ca1-a6f0-47a8-be7d-e85c3997cf56 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc79a63b-c023-4726-964c-2a206d9e9d9f/564748a5-d2c8-4cc0-8ecd-a2379ed2aa19'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc79a63b-c023-4726-964c-2a206d9e9d9f/f8f43ca1-a6f0-47a8-be7d-e85c3997cf56.meta"\n\nlvchange -ay /dev/ovirt-local/f8f43ca1-a6f0-47a8-be7d-e85c3997cf56\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f8f43ca1-a6f0-47a8-be7d-e85c3997cf56 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc79a63b-c023-4726-964c-2a206d9e9d9f/564748a5-d2c8-4cc0-8ecd-a2379ed2aa19'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc79a63b-c023-4726-964c-2a206d9e9d9f/564748a5-d2c8-4cc0-8ecd-a2379ed2aa19.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc79a63b-c023-4726-964c-2a206d9e9d9f/564748a5-d2c8-4cc0-8ecd-a2379ed2aa19.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc79a63b-c023-4726-964c-2a206d9e9d9f/564748a5-d2c8-4cc0-8ecd-a2379ed2aa19.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc79a63b-c023-4726-964c-2a206d9e9d9f/564748a5-d2c8-4cc0-8ecd-a2379ed2aa19.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc79a63b-c023-4726-964c-2a206d9e9d9f/564748a5-d2c8-4cc0-8ecd-a2379ed2aa19 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/fc79a63b-c023-4726-964c-2a206d9e9d9f/564748a5-d2c8-4cc0-8ecd-a2379ed2aa19.orig'\n\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('564748a5-d2c8-4cc0-8ecd-a2379ed2aa19');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='49aab18b-ff1b-4462-a363-b7580e717c42' WHERE image_guid ='f8f43ca1-a6f0-47a8-be7d-e85c3997cf56';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  nginx01.manifest01.streaming.hulu.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp07.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '41f53ffe-0bda-4cf0-93ae-7cf4e79e7091' AND snapshot_id != 'ed1af634-707e-46e7-9c6f-16e7f216845b';"\n\nFIXING DISK 0503cc20-d0cb-4c52-9134-63b2a8ad4b92, CHAIN ['14d1d5be-d934-4e59-be88-ac62bc1ccaf3', '37034fff-f220-4189-ba95-3d936d081bf9', '4f92329f-75c4-4b2d-a3bd-180053984975']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/37034fff-f220-4189-ba95-3d936d081bf9 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/4f92329f-75c4-4b2d-a3bd-180053984975'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/4f92329f-75c4-4b2d-a3bd-180053984975.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/4f92329f-75c4-4b2d-a3bd-180053984975.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/4f92329f-75c4-4b2d-a3bd-180053984975.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/4f92329f-75c4-4b2d-a3bd-180053984975.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/4f92329f-75c4-4b2d-a3bd-180053984975 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/4f92329f-75c4-4b2d-a3bd-180053984975.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/14d1d5be-d934-4e59-be88-ac62bc1ccaf3 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/37034fff-f220-4189-ba95-3d936d081bf9'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/14d1d5be-d934-4e59-be88-ac62bc1ccaf3.meta"\n\nlvchange -ay /dev/ovirt-local/14d1d5be-d934-4e59-be88-ac62bc1ccaf3\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/14d1d5be-d934-4e59-be88-ac62bc1ccaf3 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/37034fff-f220-4189-ba95-3d936d081bf9'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/37034fff-f220-4189-ba95-3d936d081bf9.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/37034fff-f220-4189-ba95-3d936d081bf9.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/37034fff-f220-4189-ba95-3d936d081bf9.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/37034fff-f220-4189-ba95-3d936d081bf9.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/37034fff-f220-4189-ba95-3d936d081bf9 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0503cc20-d0cb-4c52-9134-63b2a8ad4b92/37034fff-f220-4189-ba95-3d936d081bf9.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('37034fff-f220-4189-ba95-3d936d081bf9','4f92329f-75c4-4b2d-a3bd-180053984975');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='ed1af634-707e-46e7-9c6f-16e7f216845b' WHERE image_guid ='14d1d5be-d934-4e59-be88-ac62bc1ccaf3';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  nginx01.proxy01.hls.gen.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp98.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ed07282a-44df-4872-be82-2ce588d9b1b2' AND snapshot_id != '6eec1118-9e5c-45bd-90a8-78f242191651';"\n\nFIXING DISK 04e2c88a-f4e1-4fb9-8251-28769b5b74a1, CHAIN ['04d5612b-db02-452a-a1e4-c87c6caa3c5d', '1e597de7-b9cf-4d33-8580-8a4e4b43dd12', '5216f318-dce8-455a-af7c-32711f038906']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/1e597de7-b9cf-4d33-8580-8a4e4b43dd12 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/5216f318-dce8-455a-af7c-32711f038906'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/5216f318-dce8-455a-af7c-32711f038906.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/5216f318-dce8-455a-af7c-32711f038906.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/5216f318-dce8-455a-af7c-32711f038906.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/5216f318-dce8-455a-af7c-32711f038906.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/5216f318-dce8-455a-af7c-32711f038906 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/5216f318-dce8-455a-af7c-32711f038906.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/04d5612b-db02-452a-a1e4-c87c6caa3c5d -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/1e597de7-b9cf-4d33-8580-8a4e4b43dd12'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/04d5612b-db02-452a-a1e4-c87c6caa3c5d.meta"\n\nlvchange -ay /dev/ovirt-local/04d5612b-db02-452a-a1e4-c87c6caa3c5d\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/04d5612b-db02-452a-a1e4-c87c6caa3c5d -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/1e597de7-b9cf-4d33-8580-8a4e4b43dd12'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/1e597de7-b9cf-4d33-8580-8a4e4b43dd12.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/1e597de7-b9cf-4d33-8580-8a4e4b43dd12.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/1e597de7-b9cf-4d33-8580-8a4e4b43dd12.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/1e597de7-b9cf-4d33-8580-8a4e4b43dd12.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/1e597de7-b9cf-4d33-8580-8a4e4b43dd12 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/04e2c88a-f4e1-4fb9-8251-28769b5b74a1/1e597de7-b9cf-4d33-8580-8a4e4b43dd12.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('1e597de7-b9cf-4d33-8580-8a4e4b43dd12','5216f318-dce8-455a-af7c-32711f038906');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='6eec1118-9e5c-45bd-90a8-78f242191651' WHERE image_guid ='04d5612b-db02-452a-a1e4-c87c6caa3c5d';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  nginx01.proxy01.hls.gen.clt1.qa.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp99.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '10a3d1bc-33b5-4662-a56b-4c3db8a1ebe9' AND snapshot_id != 'c13a6d32-c7f5-43dd-ba2d-1426f41598db';"\n\nFIXING DISK 0fa19cca-4b75-4886-80fc-b7042562a36d, CHAIN ['c25e9d85-d630-42ba-86f2-775ce2687c90', '90e58c59-ebf1-4a85-930e-f9e449ee4311']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c25e9d85-d630-42ba-86f2-775ce2687c90 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0fa19cca-4b75-4886-80fc-b7042562a36d/90e58c59-ebf1-4a85-930e-f9e449ee4311'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0fa19cca-4b75-4886-80fc-b7042562a36d/c25e9d85-d630-42ba-86f2-775ce2687c90.meta"\n\nlvchange -ay /dev/ovirt-local/c25e9d85-d630-42ba-86f2-775ce2687c90\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c25e9d85-d630-42ba-86f2-775ce2687c90 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0fa19cca-4b75-4886-80fc-b7042562a36d/90e58c59-ebf1-4a85-930e-f9e449ee4311'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0fa19cca-4b75-4886-80fc-b7042562a36d/90e58c59-ebf1-4a85-930e-f9e449ee4311.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0fa19cca-4b75-4886-80fc-b7042562a36d/90e58c59-ebf1-4a85-930e-f9e449ee4311.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0fa19cca-4b75-4886-80fc-b7042562a36d/90e58c59-ebf1-4a85-930e-f9e449ee4311.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0fa19cca-4b75-4886-80fc-b7042562a36d/90e58c59-ebf1-4a85-930e-f9e449ee4311.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0fa19cca-4b75-4886-80fc-b7042562a36d/90e58c59-ebf1-4a85-930e-f9e449ee4311 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0fa19cca-4b75-4886-80fc-b7042562a36d/90e58c59-ebf1-4a85-930e-f9e449ee4311.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('90e58c59-ebf1-4a85-930e-f9e449ee4311');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='c13a6d32-c7f5-43dd-ba2d-1426f41598db' WHERE image_guid ='c25e9d85-d630-42ba-86f2-775ce2687c90';"\n\n=======================================================================</text>, <text>~~~\n\nFIXING VM:  proxy01.c01.streaming.gen.clt1.dev.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp03.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '05a1119a-c660-42e8-98c4-de44ce9e34ce' AND snapshot_id != 'b8407cd1-4d17-44aa-94ac-daa346a6a121';"\n\nFIXING DISK dca588d6-5ffa-4a6d-ae1b-3965abcbbde1, CHAIN ['3c918d93-d4c9-41d3-8381-6581a5061eb1', '5582ca4c-6f59-4106-859f-75696ce10467']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3c918d93-d4c9-41d3-8381-6581a5061eb1 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/dca588d6-5ffa-4a6d-ae1b-3965abcbbde1/5582ca4c-6f59-4106-859f-75696ce10467'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/dca588d6-5ffa-4a6d-ae1b-3965abcbbde1/3c918d93-d4c9-41d3-8381-6581a5061eb1.meta"\n\nlvchange -ay /dev/ovirt-local/3c918d93-d4c9-41d3-8381-6581a5061eb1\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3c918d93-d4c9-41d3-8381-6581a5061eb1 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/dca588d6-5ffa-4a6d-ae1b-3965abcbbde1/5582ca4c-6f59-4106-859f-75696ce10467'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/dca588d6-5ffa-4a6d-ae1b-3965abcbbde1/5582ca4c-6f59-4106-859f-75696ce10467.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/dca588d6-5ffa-4a6d-ae1b-3965abcbbde1/5582ca4c-6f59-4106-859f-75696ce10467.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/dca588d6-5ffa-4a6d-ae1b-3965abcbbde1/5582ca4c-6f59-4106-859f-75696ce10467.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/dca588d6-5ffa-4a6d-ae1b-3965abcbbde1/5582ca4c-6f59-4106-859f-75696ce10467.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/dca588d6-5ffa-4a6d-ae1b-3965abcbbde1/5582ca4c-6f59-4106-859f-75696ce10467 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/dca588d6-5ffa-4a6d-ae1b-3965abcbbde1/5582ca4c-6f59-4106-859f-75696ce10467.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('5582ca4c-6f59-4106-859f-75696ce10467');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='b8407cd1-4d17-44aa-94ac-daa346a6a121' WHERE image_guid ='3c918d93-d4c9-41d3-8381-6581a5061eb1';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  proxy01.linear01.infra.hulu.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp10.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '5521f495-0770-4a7f-a6a5-fb6a39b25276' AND snapshot_id != '3e0c6c92-ed4b-444d-956b-3849dc37b366';"\n\nFIXING DISK 1061d6f8-ad8b-4932-9d39-fb8e825324fc, CHAIN ['1ed50b96-e752-4159-91cc-7c3ac6e54a60', '2a0082df-15d7-4802-a112-7c69c3b17a99']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/1ed50b96-e752-4159-91cc-7c3ac6e54a60 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1061d6f8-ad8b-4932-9d39-fb8e825324fc/2a0082df-15d7-4802-a112-7c69c3b17a99'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1061d6f8-ad8b-4932-9d39-fb8e825324fc/1ed50b96-e752-4159-91cc-7c3ac6e54a60.meta"\n\nlvchange -ay /dev/ovirt-local/1ed50b96-e752-4159-91cc-7c3ac6e54a60\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/1ed50b96-e752-4159-91cc-7c3ac6e54a60 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1061d6f8-ad8b-4932-9d39-fb8e825324fc/2a0082df-15d7-4802-a112-7c69c3b17a99'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1061d6f8-ad8b-4932-9d39-fb8e825324fc/2a0082df-15d7-4802-a112-7c69c3b17a99.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1061d6f8-ad8b-4932-9d39-fb8e825324fc/2a0082df-15d7-4802-a112-7c69c3b17a99.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1061d6f8-ad8b-4932-9d39-fb8e825324fc/2a0082df-15d7-4802-a112-7c69c3b17a99.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1061d6f8-ad8b-4932-9d39-fb8e825324fc/2a0082df-15d7-4802-a112-7c69c3b17a99.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1061d6f8-ad8b-4932-9d39-fb8e825324fc/2a0082df-15d7-4802-a112-7c69c3b17a99 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/1061d6f8-ad8b-4932-9d39-fb8e825324fc/2a0082df-15d7-4802-a112-7c69c3b17a99.orig'\n\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('2a0082df-15d7-4802-a112-7c69c3b17a99');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='3e0c6c92-ed4b-444d-956b-3849dc37b366' WHERE image_guid ='1ed50b96-e752-4159-91cc-7c3ac6e54a60';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  proxy01.linear02.infra.hulu.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp23.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0acefd2d-3938-4366-be59-230231bd7e4a' AND snapshot_id != '5b3c2fcc-20b6-4c81-8e7a-2974666ad8e9';"\n\nFIXING DISK 835fb971-4841-498a-b7a9-79e50c467c0c, CHAIN ['d514469d-ea00-4426-bd56-59c2055cce10', '85b0844a-a416-4788-a5ce-53c82558eb1f']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/d514469d-ea00-4426-bd56-59c2055cce10 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/835fb971-4841-498a-b7a9-79e50c467c0c/85b0844a-a416-4788-a5ce-53c82558eb1f'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/835fb971-4841-498a-b7a9-79e50c467c0c/d514469d-ea00-4426-bd56-59c2055cce10.meta"\n\nlvchange -ay /dev/ovirt-local/d514469d-ea00-4426-bd56-59c2055cce10\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/d514469d-ea00-4426-bd56-59c2055cce10 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/835fb971-4841-498a-b7a9-79e50c467c0c/85b0844a-a416-4788-a5ce-53c82558eb1f'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/835fb971-4841-498a-b7a9-79e50c467c0c/85b0844a-a416-4788-a5ce-53c82558eb1f.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/835fb971-4841-498a-b7a9-79e50c467c0c/85b0844a-a416-4788-a5ce-53c82558eb1f.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/835fb971-4841-498a-b7a9-79e50c467c0c/85b0844a-a416-4788-a5ce-53c82558eb1f.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/835fb971-4841-498a-b7a9-79e50c467c0c/85b0844a-a416-4788-a5ce-53c82558eb1f.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/835fb971-4841-498a-b7a9-79e50c467c0c/85b0844a-a416-4788-a5ce-53c82558eb1f /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/835fb971-4841-498a-b7a9-79e50c467c0c/85b0844a-a416-4788-a5ce-53c82558eb1f.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('85b0844a-a416-4788-a5ce-53c82558eb1f');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='5b3c2fcc-20b6-4c81-8e7a-2974666ad8e9' WHERE image_guid ='d514469d-ea00-4426-bd56-59c2055cce10';"\n\n=======================================================================\n\n~~~</text>, <text>FIXING VM:  smtp01.clt1.util.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp20.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'fa705cb2-3849-4722-9ff9-e0c074822f04' AND snapshot_id != 'c30fca3c-430d-4b5f-9149-f98345d779d5';"\n\nFIXING DISK afdf5fd9-9e98-49b6-998f-0031d2dd6df1, CHAIN ['1fb4af7f-7348-4362-8ad2-c515668c535d', '97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e', 'ae93d6cc-2f99-4543-ac78-f48098dcc79c']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/ae93d6cc-2f99-4543-ac78-f48098dcc79c'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/ae93d6cc-2f99-4543-ac78-f48098dcc79c.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/ae93d6cc-2f99-4543-ac78-f48098dcc79c.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/ae93d6cc-2f99-4543-ac78-f48098dcc79c.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/ae93d6cc-2f99-4543-ac78-f48098dcc79c.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/ae93d6cc-2f99-4543-ac78-f48098dcc79c /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/ae93d6cc-2f99-4543-ac78-f48098dcc79c.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/1fb4af7f-7348-4362-8ad2-c515668c535d -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/1fb4af7f-7348-4362-8ad2-c515668c535d.meta"\n\nlvchange -ay /dev/ovirt-local/1fb4af7f-7348-4362-8ad2-c515668c535d\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/1fb4af7f-7348-4362-8ad2-c515668c535d -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/afdf5fd9-9e98-49b6-998f-0031d2dd6df1/97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('97cfb3fe-c77a-4094-83ec-9ac15ffc1e8e','ae93d6cc-2f99-4543-ac78-f48098dcc79c');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='c30fca3c-430d-4b5f-9149-f98345d779d5' WHERE image_guid ='1fb4af7f-7348-4362-8ad2-c515668c535d';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  smtp02.clt1.util.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp23.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'c56bc5af-d121-4a61-8994-6e2cccead8e2' AND snapshot_id != 'd3cea8eb-c9c9-48c2-8198-c0c5f3cf987f';"\n\nFIXING DISK 50f95fe7-f83a-467c-b43b-6ae7f4e231a3, CHAIN ['1ab958bc-dd29-4ca2-a856-9d4a652fb4df', 'a4b669b0-f7c4-4885-983e-29e372c01778', '4bf544bc-dc56-46c1-8cbb-bbbd55942ceb']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/a4b669b0-f7c4-4885-983e-29e372c01778 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/4bf544bc-dc56-46c1-8cbb-bbbd55942ceb'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/4bf544bc-dc56-46c1-8cbb-bbbd55942ceb.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/4bf544bc-dc56-46c1-8cbb-bbbd55942ceb.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/4bf544bc-dc56-46c1-8cbb-bbbd55942ceb.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/4bf544bc-dc56-46c1-8cbb-bbbd55942ceb.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/4bf544bc-dc56-46c1-8cbb-bbbd55942ceb /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/4bf544bc-dc56-46c1-8cbb-bbbd55942ceb.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/1ab958bc-dd29-4ca2-a856-9d4a652fb4df -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/a4b669b0-f7c4-4885-983e-29e372c01778'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/1ab958bc-dd29-4ca2-a856-9d4a652fb4df.meta"\n\nlvchange -ay /dev/ovirt-local/1ab958bc-dd29-4ca2-a856-9d4a652fb4df\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/1ab958bc-dd29-4ca2-a856-9d4a652fb4df -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/a4b669b0-f7c4-4885-983e-29e372c01778'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/a4b669b0-f7c4-4885-983e-29e372c01778.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/a4b669b0-f7c4-4885-983e-29e372c01778.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/a4b669b0-f7c4-4885-983e-29e372c01778.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/a4b669b0-f7c4-4885-983e-29e372c01778.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/a4b669b0-f7c4-4885-983e-29e372c01778 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/50f95fe7-f83a-467c-b43b-6ae7f4e231a3/a4b669b0-f7c4-4885-983e-29e372c01778.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('a4b669b0-f7c4-4885-983e-29e372c01778','4bf544bc-dc56-46c1-8cbb-bbbd55942ceb');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d3cea8eb-c9c9-48c2-8198-c0c5f3cf987f' WHERE image_guid ='1ab958bc-dd29-4ca2-a856-9d4a652fb4df';"\n\n=======================================================================\n\n~~~\n\n\nFIXING VM:  splunkfwd01.c01.noc.gen.clt1.util.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp78.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'f2f4cd76-48b3-4751-81af-c70ce5d4ec47' AND snapshot_id != '1ae35a65-b08e-44a9-8729-9ba2f7be4f87';"\n\nFIXING DISK 4836e235-8850-46f1-a1e4-eb277e26c099, CHAIN ['6f567408-edc0-4eac-8e38-79cb47787d9a', 'a38a9f84-68f8-46a7-8a42-ad57aca95cea']\n\nHost Commands:\n==============\n\nlvchange -an /dev/ovirt-local/a38a9f84-68f8-46a7-8a42-ad57aca95cea\n\nlvchange --deltag 'IU_4836e235-8850-46f1-a1e4-eb277e26c099' /dev/ovirt-local/a38a9f84-68f8-46a7-8a42-ad57aca95cea\n\nlvchange --deltag 'PU_00000000-0000-0000-0000-000000000000' /dev/ovirt-local/a38a9f84-68f8-46a7-8a42-ad57aca95cea\n\nlvchange --deltag 'VM_f2f4cd76-48b3-4751-81af-c70ce5d4ec47' /dev/ovirt-local/a38a9f84-68f8-46a7-8a42-ad57aca95cea\n\nlvrename /dev/ovirt-local/a38a9f84-68f8-46a7-8a42-ad57aca95cea /dev/ovirt-local/_temp_a38a9f84-68f8-46a7-8a42-ad57aca95cea\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/6f567408-edc0-4eac-8e38-79cb47787d9a -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/6f567408-edc0-4eac-8e38-79cb47787d9a.meta"\n\nlvchange -ay /dev/ovirt-local/6f567408-edc0-4eac-8e38-79cb47787d9a\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/6f567408-edc0-4eac-8e38-79cb47787d9a -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/4836e235-8850-46f1-a1e4-eb277e26c099/a38a9f84-68f8-46a7-8a42-ad57aca95cea.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('a38a9f84-68f8-46a7-8a42-ad57aca95cea');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='1ae35a65-b08e-44a9-8729-9ba2f7be4f87' WHERE image_guid ='6f567408-edc0-4eac-8e38-79cb47787d9a';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  splunkfwd02.c01.noc.gen.clt1.util.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp79.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'bf63445b-3b93-44d8-8c65-e6da4fa67f73' AND snapshot_id != 'b8adf8b5-b20e-4999-aa44-122541e4a6a6';"\n\nFIXING DISK 9131e186-6c32-40a2-9925-d7522b559beb, CHAIN ['67ed3056-102c-4a1e-9f6a-abdd195722e7', '86c9530a-3e92-417f-b337-943f26e07c90']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/67ed3056-102c-4a1e-9f6a-abdd195722e7 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/67ed3056-102c-4a1e-9f6a-abdd195722e7.meta"\n\nlvchange -ay /dev/ovirt-local/67ed3056-102c-4a1e-9f6a-abdd195722e7\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/67ed3056-102c-4a1e-9f6a-abdd195722e7 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9131e186-6c32-40a2-9925-d7522b559beb/86c9530a-3e92-417f-b337-943f26e07c90.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('86c9530a-3e92-417f-b337-943f26e07c90');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='b8adf8b5-b20e-4999-aa44-122541e4a6a6' WHERE image_guid ='67ed3056-102c-4a1e-9f6a-abdd195722e7';"\n\n=======================================================================\n\n~~~</text>, <text>FIXING VM:  xcoder01.c02.streaming.gen.clt1.dev.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp101.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'adb5d35b-de96-4f00-9a81-5a1bfff4d4c6' AND snapshot_id != 'cc377d21-b847-4249-bb93-f6faffa66fa6';"\n\nFIXING DISK 0685dcd6-d295-4495-ac52-52301e9bcc55, CHAIN ['3ecabf66-27ef-4783-9911-6b04833db6ab', '0f41f634-34b6-4ba5-8173-1c3a888deeaf']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/3ecabf66-27ef-4783-9911-6b04833db6ab -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0685dcd6-d295-4495-ac52-52301e9bcc55/0f41f634-34b6-4ba5-8173-1c3a888deeaf'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0685dcd6-d295-4495-ac52-52301e9bcc55/3ecabf66-27ef-4783-9911-6b04833db6ab.meta"\n\nlvchange -ay /dev/ovirt-local/3ecabf66-27ef-4783-9911-6b04833db6ab\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/3ecabf66-27ef-4783-9911-6b04833db6ab -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0685dcd6-d295-4495-ac52-52301e9bcc55/0f41f634-34b6-4ba5-8173-1c3a888deeaf'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0685dcd6-d295-4495-ac52-52301e9bcc55/0f41f634-34b6-4ba5-8173-1c3a888deeaf.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0685dcd6-d295-4495-ac52-52301e9bcc55/0f41f634-34b6-4ba5-8173-1c3a888deeaf.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0685dcd6-d295-4495-ac52-52301e9bcc55/0f41f634-34b6-4ba5-8173-1c3a888deeaf.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0685dcd6-d295-4495-ac52-52301e9bcc55/0f41f634-34b6-4ba5-8173-1c3a888deeaf.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0685dcd6-d295-4495-ac52-52301e9bcc55/0f41f634-34b6-4ba5-8173-1c3a888deeaf /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/0685dcd6-d295-4495-ac52-52301e9bcc55/0f41f634-34b6-4ba5-8173-1c3a888deeaf.orig'\n\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0f41f634-34b6-4ba5-8173-1c3a888deeaf');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='cc377d21-b847-4249-bb93-f6faffa66fa6' WHERE image_guid ='3ecabf66-27ef-4783-9911-6b04833db6ab';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  xcoder01.c03.streaming.gen.clt1.dev.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp25.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ee84c991-b575-4110-a80b-ed818c4b0e58' AND snapshot_id != 'd015ce97-7946-4b4b-acd6-236ded127518';"\n\nFIXING DISK 7eedc904-178e-48e9-bb2c-43e693ad4d37, CHAIN ['7b08005b-2f02-4171-b904-704465a602a9', 'a15d536f-df36-4cde-9a0e-799ee3a452bd']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/7b08005b-2f02-4171-b904-704465a602a9 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7eedc904-178e-48e9-bb2c-43e693ad4d37/a15d536f-df36-4cde-9a0e-799ee3a452bd'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7eedc904-178e-48e9-bb2c-43e693ad4d37/7b08005b-2f02-4171-b904-704465a602a9.meta"\n\nlvchange -ay /dev/ovirt-local/7b08005b-2f02-4171-b904-704465a602a9\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/7b08005b-2f02-4171-b904-704465a602a9 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7eedc904-178e-48e9-bb2c-43e693ad4d37/a15d536f-df36-4cde-9a0e-799ee3a452bd'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7eedc904-178e-48e9-bb2c-43e693ad4d37/a15d536f-df36-4cde-9a0e-799ee3a452bd.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7eedc904-178e-48e9-bb2c-43e693ad4d37/a15d536f-df36-4cde-9a0e-799ee3a452bd.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7eedc904-178e-48e9-bb2c-43e693ad4d37/a15d536f-df36-4cde-9a0e-799ee3a452bd.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7eedc904-178e-48e9-bb2c-43e693ad4d37/a15d536f-df36-4cde-9a0e-799ee3a452bd.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7eedc904-178e-48e9-bb2c-43e693ad4d37/a15d536f-df36-4cde-9a0e-799ee3a452bd /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/7eedc904-178e-48e9-bb2c-43e693ad4d37/a15d536f-df36-4cde-9a0e-799ee3a452bd.orig'\n\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('a15d536f-df36-4cde-9a0e-799ee3a452bd');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='d015ce97-7946-4b4b-acd6-236ded127518' WHERE image_guid ='7b08005b-2f02-4171-b904-704465a602a9';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  xcoder02.c02.streaming.gen.clt1.dev.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp14.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '9689d5f7-26f8-41a9-8dd1-f2bb61d69243' AND snapshot_id != 'fafa8351-555d-49f5-9944-d5d3da754f5d';"\n\nFIXING DISK 2327df22-fcad-4bf5-8d6d-ae7b7be9c92c, CHAIN ['7ebfad25-133b-443f-985c-c5471bcba5e9', '06bfb8f3-c060-4a3c-a896-fccce1635917']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/7ebfad25-133b-443f-985c-c5471bcba5e9 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2327df22-fcad-4bf5-8d6d-ae7b7be9c92c/06bfb8f3-c060-4a3c-a896-fccce1635917'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2327df22-fcad-4bf5-8d6d-ae7b7be9c92c/7ebfad25-133b-443f-985c-c5471bcba5e9.meta"\n\nlvchange -ay /dev/ovirt-local/7ebfad25-133b-443f-985c-c5471bcba5e9\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/7ebfad25-133b-443f-985c-c5471bcba5e9 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2327df22-fcad-4bf5-8d6d-ae7b7be9c92c/06bfb8f3-c060-4a3c-a896-fccce1635917'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2327df22-fcad-4bf5-8d6d-ae7b7be9c92c/06bfb8f3-c060-4a3c-a896-fccce1635917.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2327df22-fcad-4bf5-8d6d-ae7b7be9c92c/06bfb8f3-c060-4a3c-a896-fccce1635917.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2327df22-fcad-4bf5-8d6d-ae7b7be9c92c/06bfb8f3-c060-4a3c-a896-fccce1635917.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2327df22-fcad-4bf5-8d6d-ae7b7be9c92c/06bfb8f3-c060-4a3c-a896-fccce1635917.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2327df22-fcad-4bf5-8d6d-ae7b7be9c92c/06bfb8f3-c060-4a3c-a896-fccce1635917 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2327df22-fcad-4bf5-8d6d-ae7b7be9c92c/06bfb8f3-c060-4a3c-a896-fccce1635917.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('06bfb8f3-c060-4a3c-a896-fccce1635917');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='fafa8351-555d-49f5-9944-d5d3da754f5d' WHERE image_guid ='7ebfad25-133b-443f-985c-c5471bcba5e9';"\n\n=======================================================================\n\n~~~</text>, <text>~~~\n\nFIXING VM:  influx01.c01.inf.hulu.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp89.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ae6703e8-e18e-4d1d-9dee-6933e8258448' AND snapshot_id != '3d62c154-b178-4751-87c5-c8994dc58b56';"\n\nFIXING DISK 2c166f67-9b33-43a6-9635-6be20e1fce39, CHAIN ['d61bd72c-488f-42e0-b6a4-62abb6a77bd1', 'd40f41de-4e4b-4b1f-b60a-75be2397db7b']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/d61bd72c-488f-42e0-b6a4-62abb6a77bd1 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c166f67-9b33-43a6-9635-6be20e1fce39/d40f41de-4e4b-4b1f-b60a-75be2397db7b'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c166f67-9b33-43a6-9635-6be20e1fce39/d61bd72c-488f-42e0-b6a4-62abb6a77bd1.meta"\n\nlvchange -ay /dev/ovirt-local/d61bd72c-488f-42e0-b6a4-62abb6a77bd1\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/d61bd72c-488f-42e0-b6a4-62abb6a77bd1 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c166f67-9b33-43a6-9635-6be20e1fce39/d40f41de-4e4b-4b1f-b60a-75be2397db7b'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c166f67-9b33-43a6-9635-6be20e1fce39/d40f41de-4e4b-4b1f-b60a-75be2397db7b.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c166f67-9b33-43a6-9635-6be20e1fce39/d40f41de-4e4b-4b1f-b60a-75be2397db7b.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c166f67-9b33-43a6-9635-6be20e1fce39/d40f41de-4e4b-4b1f-b60a-75be2397db7b.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c166f67-9b33-43a6-9635-6be20e1fce39/d40f41de-4e4b-4b1f-b60a-75be2397db7b.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c166f67-9b33-43a6-9635-6be20e1fce39/d40f41de-4e4b-4b1f-b60a-75be2397db7b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/2c166f67-9b33-43a6-9635-6be20e1fce39/d40f41de-4e4b-4b1f-b60a-75be2397db7b.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('d40f41de-4e4b-4b1f-b60a-75be2397db7b');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='3d62c154-b178-4751-87c5-c8994dc58b56' WHERE image_guid ='d61bd72c-488f-42e0-b6a4-62abb6a77bd1';"\n\n\nFIXING DISK 92fce40a-4c26-4e90-a228-e685e32c8739, CHAIN ['8772e9ff-5f01-4dd4-a730-ef3d79bb07b3', '78b16f6f-90e6-478e-8da7-2ab4896d4485']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/8772e9ff-5f01-4dd4-a730-ef3d79bb07b3 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/92fce40a-4c26-4e90-a228-e685e32c8739/78b16f6f-90e6-478e-8da7-2ab4896d4485'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/92fce40a-4c26-4e90-a228-e685e32c8739/8772e9ff-5f01-4dd4-a730-ef3d79bb07b3.meta"\n\nlvchange -ay /dev/ovirt-local/8772e9ff-5f01-4dd4-a730-ef3d79bb07b3\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/8772e9ff-5f01-4dd4-a730-ef3d79bb07b3 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/92fce40a-4c26-4e90-a228-e685e32c8739/78b16f6f-90e6-478e-8da7-2ab4896d4485'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/92fce40a-4c26-4e90-a228-e685e32c8739/78b16f6f-90e6-478e-8da7-2ab4896d4485.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/92fce40a-4c26-4e90-a228-e685e32c8739/78b16f6f-90e6-478e-8da7-2ab4896d4485.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/92fce40a-4c26-4e90-a228-e685e32c8739/78b16f6f-90e6-478e-8da7-2ab4896d4485.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/92fce40a-4c26-4e90-a228-e685e32c8739/78b16f6f-90e6-478e-8da7-2ab4896d4485.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/92fce40a-4c26-4e90-a228-e685e32c8739/78b16f6f-90e6-478e-8da7-2ab4896d4485 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/92fce40a-4c26-4e90-a228-e685e32c8739/78b16f6f-90e6-478e-8da7-2ab4896d4485.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('78b16f6f-90e6-478e-8da7-2ab4896d4485');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='3d62c154-b178-4751-87c5-c8994dc58b56' WHERE image_guid ='8772e9ff-5f01-4dd4-a730-ef3d79bb07b3';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  mdtools01.c01.bpa.hulu.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host: hyp95.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '9121042e-2ce4-405d-bc21-8cb0c3030aa0' AND snapshot_id != '08d240f8-94f5-4b66-8e2a-2cdcdd84b5de';"\n\nFIXING DISK 80fbfdf1-9e38-402b-bb3e-87aabcf73477, CHAIN ['da19bc04-93e3-460a-9a8e-d98de45fe18f', '9804d78f-0aa4-43f9-b3ec-0a167a82a3ff']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/da19bc04-93e3-460a-9a8e-d98de45fe18f -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/80fbfdf1-9e38-402b-bb3e-87aabcf73477/9804d78f-0aa4-43f9-b3ec-0a167a82a3ff'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/80fbfdf1-9e38-402b-bb3e-87aabcf73477/da19bc04-93e3-460a-9a8e-d98de45fe18f.meta"\n\nlvchange -ay /dev/ovirt-local/da19bc04-93e3-460a-9a8e-d98de45fe18f\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/da19bc04-93e3-460a-9a8e-d98de45fe18f -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/80fbfdf1-9e38-402b-bb3e-87aabcf73477/9804d78f-0aa4-43f9-b3ec-0a167a82a3ff'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/80fbfdf1-9e38-402b-bb3e-87aabcf73477/9804d78f-0aa4-43f9-b3ec-0a167a82a3ff.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/80fbfdf1-9e38-402b-bb3e-87aabcf73477/9804d78f-0aa4-43f9-b3ec-0a167a82a3ff.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/80fbfdf1-9e38-402b-bb3e-87aabcf73477/9804d78f-0aa4-43f9-b3ec-0a167a82a3ff.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/80fbfdf1-9e38-402b-bb3e-87aabcf73477/9804d78f-0aa4-43f9-b3ec-0a167a82a3ff.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/80fbfdf1-9e38-402b-bb3e-87aabcf73477/9804d78f-0aa4-43f9-b3ec-0a167a82a3ff /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/80fbfdf1-9e38-402b-bb3e-87aabcf73477/9804d78f-0aa4-43f9-b3ec-0a167a82a3ff.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('9804d78f-0aa4-43f9-b3ec-0a167a82a3ff');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='08d240f8-94f5-4b66-8e2a-2cdcdd84b5de' WHERE image_guid ='da19bc04-93e3-460a-9a8e-d98de45fe18f';"\n\n\nFIXING DISK 9636cd9e-8a4f-435d-9e9c-c32beb2a3a13, CHAIN ['25da7450-ab5d-4cab-a49b-89e4d8edae65', '65201f1e-1df6-4bc1-accc-35a7b1df0bbd']\n\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/25da7450-ab5d-4cab-a49b-89e4d8edae65 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9636cd9e-8a4f-435d-9e9c-c32beb2a3a13/65201f1e-1df6-4bc1-accc-35a7b1df0bbd'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9636cd9e-8a4f-435d-9e9c-c32beb2a3a13/25da7450-ab5d-4cab-a49b-89e4d8edae65.meta"\n\nlvchange -ay /dev/ovirt-local/25da7450-ab5d-4cab-a49b-89e4d8edae65\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/25da7450-ab5d-4cab-a49b-89e4d8edae65 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9636cd9e-8a4f-435d-9e9c-c32beb2a3a13/65201f1e-1df6-4bc1-accc-35a7b1df0bbd'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9636cd9e-8a4f-435d-9e9c-c32beb2a3a13/65201f1e-1df6-4bc1-accc-35a7b1df0bbd.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9636cd9e-8a4f-435d-9e9c-c32beb2a3a13/65201f1e-1df6-4bc1-accc-35a7b1df0bbd.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9636cd9e-8a4f-435d-9e9c-c32beb2a3a13/65201f1e-1df6-4bc1-accc-35a7b1df0bbd.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9636cd9e-8a4f-435d-9e9c-c32beb2a3a13/65201f1e-1df6-4bc1-accc-35a7b1df0bbd.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9636cd9e-8a4f-435d-9e9c-c32beb2a3a13/65201f1e-1df6-4bc1-accc-35a7b1df0bbd /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/9636cd9e-8a4f-435d-9e9c-c32beb2a3a13/65201f1e-1df6-4bc1-accc-35a7b1df0bbd.orig'\n\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('65201f1e-1df6-4bc1-accc-35a7b1df0bbd');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='08d240f8-94f5-4b66-8e2a-2cdcdd84b5de' WHERE image_guid ='25da7450-ab5d-4cab-a49b-89e4d8edae65';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  mdtools02.c01.bpa.hulu.clt1.prod.mlbam.net:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp25.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '1b939932-fb0e-4c65-b6ec-a91caea11cb0' AND snapshot_id != '02f6a8b4-3a1c-491a-a0f7-4ff840d6941c';"\n\nFIXING DISK 86ea7362-863a-42a3-9830-eb9c9f4057ab, CHAIN ['cde481a9-923e-4007-b3b7-0fe6edfb85b5', '0bfc7619-dcfd-476e-9d32-acb2aacb48ec', '1e29e060-8ad1-4a84-8825-7b0f654a0578']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/cde481a9-923e-4007-b3b7-0fe6edfb85b5 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/cde481a9-923e-4007-b3b7-0fe6edfb85b5.meta"\n\nlvchange -ay /dev/ovirt-local/cde481a9-923e-4007-b3b7-0fe6edfb85b5\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/cde481a9-923e-4007-b3b7-0fe6edfb85b5 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('0bfc7619-dcfd-476e-9d32-acb2aacb48ec','1e29e060-8ad1-4a84-8825-7b0f654a0578');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='02f6a8b4-3a1c-491a-a0f7-4ff840d6941c' WHERE image_guid ='cde481a9-923e-4007-b3b7-0fe6edfb85b5';"\n\n\nFIXING DISK 922899f6-49f9-4fad-b37e-cd11761d51e8, CHAIN ['2f6b39d6-43a1-40ff-99cf-afe9cc1ae34a', 'd3b3dd57-2306-480c-ad06-7e3f789433b7', '43a3f816-c6d9-49d1-aaf0-96d0e196b47e']\n\nHost Commands:\n==============\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/d3b3dd57-2306-480c-ad06-7e3f789433b7 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/43a3f816-c6d9-49d1-aaf0-96d0e196b47e'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/43a3f816-c6d9-49d1-aaf0-96d0e196b47e.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/43a3f816-c6d9-49d1-aaf0-96d0e196b47e.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/43a3f816-c6d9-49d1-aaf0-96d0e196b47e.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/43a3f816-c6d9-49d1-aaf0-96d0e196b47e.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/43a3f816-c6d9-49d1-aaf0-96d0e196b47e /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/43a3f816-c6d9-49d1-aaf0-96d0e196b47e.orig'\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/2f6b39d6-43a1-40ff-99cf-afe9cc1ae34a -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/d3b3dd57-2306-480c-ad06-7e3f789433b7'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/2f6b39d6-43a1-40ff-99cf-afe9cc1ae34a.meta"\n\nlvchange -ay /dev/ovirt-local/2f6b39d6-43a1-40ff-99cf-afe9cc1ae34a\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/2f6b39d6-43a1-40ff-99cf-afe9cc1ae34a -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/d3b3dd57-2306-480c-ad06-7e3f789433b7'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/d3b3dd57-2306-480c-ad06-7e3f789433b7.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/d3b3dd57-2306-480c-ad06-7e3f789433b7.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/d3b3dd57-2306-480c-ad06-7e3f789433b7.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/d3b3dd57-2306-480c-ad06-7e3f789433b7.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/d3b3dd57-2306-480c-ad06-7e3f789433b7 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/d3b3dd57-2306-480c-ad06-7e3f789433b7.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('d3b3dd57-2306-480c-ad06-7e3f789433b7','43a3f816-c6d9-49d1-aaf0-96d0e196b47e');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='02f6a8b4-3a1c-491a-a0f7-4ff840d6941c' WHERE image_guid ='2f6b39d6-43a1-40ff-99cf-afe9cc1ae34a';"\n\n=======================================================================\n\n~~~\n\n\n~~~\n\nFIXING VM:  mf03.c01.web.wwe.clt1.prod.bamtech.co:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp81.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = 'ba174185-b4f9-4298-85ec-3b0dd056d9b1' AND snapshot_id != '91dd9df4-e133-43f4-9330-63bd701da146';"\n\nFIXING DISK d4380311-c5bf-4878-adc7-7aebf39f321a, CHAIN ['ae36dae8-7317-4c55-9b81-0f7eb8e8aa50', 'eced7f84-9b5d-4e4b-8bb7-67be428f72f2']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/ae36dae8-7317-4c55-9b81-0f7eb8e8aa50 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d4380311-c5bf-4878-adc7-7aebf39f321a/eced7f84-9b5d-4e4b-8bb7-67be428f72f2'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d4380311-c5bf-4878-adc7-7aebf39f321a/ae36dae8-7317-4c55-9b81-0f7eb8e8aa50.meta"\n\nlvchange -ay /dev/ovirt-local/ae36dae8-7317-4c55-9b81-0f7eb8e8aa50\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/ae36dae8-7317-4c55-9b81-0f7eb8e8aa50 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d4380311-c5bf-4878-adc7-7aebf39f321a/eced7f84-9b5d-4e4b-8bb7-67be428f72f2'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d4380311-c5bf-4878-adc7-7aebf39f321a/eced7f84-9b5d-4e4b-8bb7-67be428f72f2.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d4380311-c5bf-4878-adc7-7aebf39f321a/eced7f84-9b5d-4e4b-8bb7-67be428f72f2.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d4380311-c5bf-4878-adc7-7aebf39f321a/eced7f84-9b5d-4e4b-8bb7-67be428f72f2.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d4380311-c5bf-4878-adc7-7aebf39f321a/eced7f84-9b5d-4e4b-8bb7-67be428f72f2.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d4380311-c5bf-4878-adc7-7aebf39f321a/eced7f84-9b5d-4e4b-8bb7-67be428f72f2 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/d4380311-c5bf-4878-adc7-7aebf39f321a/eced7f84-9b5d-4e4b-8bb7-67be428f72f2.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('eced7f84-9b5d-4e4b-8bb7-67be428f72f2');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='91dd9df4-e133-43f4-9330-63bd701da146' WHERE image_guid ='ae36dae8-7317-4c55-9b81-0f7eb8e8aa50';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  mf04.c01.web.wwe.clt1.prod.bamtech.co:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp89.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '3f3d5768-c8b5-4104-b91f-e2f24c7a1d15' AND snapshot_id != '9d219ecc-6b4d-4573-993b-b2c16f40a4f7';"\n\nFIXING DISK e958e95d-9fdf-458c-8aca-ab5d2def3cc7, CHAIN ['23d12748-7477-40ac-b2a9-af4802c930ee', 'a6e13a0f-0486-419a-987c-cced150b7f49']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/23d12748-7477-40ac-b2a9-af4802c930ee -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e958e95d-9fdf-458c-8aca-ab5d2def3cc7/a6e13a0f-0486-419a-987c-cced150b7f49'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e958e95d-9fdf-458c-8aca-ab5d2def3cc7/23d12748-7477-40ac-b2a9-af4802c930ee.meta"\n\nlvchange -ay /dev/ovirt-local/23d12748-7477-40ac-b2a9-af4802c930ee\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/23d12748-7477-40ac-b2a9-af4802c930ee -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e958e95d-9fdf-458c-8aca-ab5d2def3cc7/a6e13a0f-0486-419a-987c-cced150b7f49'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e958e95d-9fdf-458c-8aca-ab5d2def3cc7/a6e13a0f-0486-419a-987c-cced150b7f49.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e958e95d-9fdf-458c-8aca-ab5d2def3cc7/a6e13a0f-0486-419a-987c-cced150b7f49.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e958e95d-9fdf-458c-8aca-ab5d2def3cc7/a6e13a0f-0486-419a-987c-cced150b7f49.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e958e95d-9fdf-458c-8aca-ab5d2def3cc7/a6e13a0f-0486-419a-987c-cced150b7f49.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e958e95d-9fdf-458c-8aca-ab5d2def3cc7/a6e13a0f-0486-419a-987c-cced150b7f49 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/e958e95d-9fdf-458c-8aca-ab5d2def3cc7/a6e13a0f-0486-419a-987c-cced150b7f49.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('a6e13a0f-0486-419a-987c-cced150b7f49');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='9d219ecc-6b4d-4573-993b-b2c16f40a4f7' WHERE image_guid ='23d12748-7477-40ac-b2a9-af4802c930ee';"\n\n=======================================================================\n\n~~~\n\n\nFIXING VM:  mf05.c01.web.wwe.clt1.prod.bamtech.co:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp32.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '26603bdf-ccfa-451c-8843-55381f7affb4' AND snapshot_id != '7ed1ee1e-af5c-4867-aa7f-c81934082f84';"\n\nFIXING DISK 5035bf00-f32e-45f0-b6ea-08dbf82babf2, CHAIN ['f269a7af-c7db-4c18-9f08-b1a5345e67da', '70705ced-2990-46f8-93eb-2e2cafc49e37']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/f269a7af-c7db-4c18-9f08-b1a5345e67da -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5035bf00-f32e-45f0-b6ea-08dbf82babf2/70705ced-2990-46f8-93eb-2e2cafc49e37'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5035bf00-f32e-45f0-b6ea-08dbf82babf2/f269a7af-c7db-4c18-9f08-b1a5345e67da.meta"\n\nlvchange -ay /dev/ovirt-local/f269a7af-c7db-4c18-9f08-b1a5345e67da\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/f269a7af-c7db-4c18-9f08-b1a5345e67da -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5035bf00-f32e-45f0-b6ea-08dbf82babf2/70705ced-2990-46f8-93eb-2e2cafc49e37'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5035bf00-f32e-45f0-b6ea-08dbf82babf2/70705ced-2990-46f8-93eb-2e2cafc49e37.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5035bf00-f32e-45f0-b6ea-08dbf82babf2/70705ced-2990-46f8-93eb-2e2cafc49e37.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5035bf00-f32e-45f0-b6ea-08dbf82babf2/70705ced-2990-46f8-93eb-2e2cafc49e37.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5035bf00-f32e-45f0-b6ea-08dbf82babf2/70705ced-2990-46f8-93eb-2e2cafc49e37.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5035bf00-f32e-45f0-b6ea-08dbf82babf2/70705ced-2990-46f8-93eb-2e2cafc49e37 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/5035bf00-f32e-45f0-b6ea-08dbf82babf2/70705ced-2990-46f8-93eb-2e2cafc49e37.orig'\n\nRHVM Commands:\n==============\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('70705ced-2990-46f8-93eb-2e2cafc49e37');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='7ed1ee1e-af5c-4867-aa7f-c81934082f84' WHERE image_guid ='f269a7af-c7db-4c18-9f08-b1a5345e67da';"\n\n=======================================================================\n\n~~~\n\nFIXING VM:  mf08.c01.web.wwe.clt1.prod.bamtech.co:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp90.clt1.prod.mlbam.net\n \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0d9a33e2-12e5-4313-83af-435420e24910' AND snapshot_id != '69311d07-45a4-4296-96d4-51024cc8edaa';"\n\nFIXING DISK 261a89ed-6cd0-4319-99b1-b3138bf8a1f3, CHAIN ['c6d07f99-c7ec-444e-acef-7a177e083b9b', 'c9477d51-6a03-41f3-b6ae-3f5826327f5d']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c6d07f99-c7ec-444e-acef-7a177e083b9b -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c6d07f99-c7ec-444e-acef-7a177e083b9b.meta"\n\nlvchange -ay /dev/ovirt-local/c6d07f99-c7ec-444e-acef-7a177e083b9b\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c6d07f99-c7ec-444e-acef-7a177e083b9b -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477</text>, <text>Marcus,\n\nThe previous few case updates contain the action plan to fix the VMs.\n\nThere were VM's that were ok, couple didn't exist and one you will will rebuilt [I1].\n\nPlease complete the action plan on each VM the move to the next one.\n\nDon't try to run the commands in a script or batch.\n\nLet me know if you have any questions.\n\nThanks and best regards,\nBimal Chollera\nRed Hat, Inc.\nGSS\n\n\n[I1]  VM list\n\n~~~\n\naspera01.c01.inf.wwe.clt1.prod.mlbam.net - AP given\naspera01.c01.jobmgr.gen.clt1.prod.mlbam.net - AP given\naspera02.c01.jobmgr.gen.clt1.prod.mlbam.net - AP given\nasperaproxy01.c01.inf.datg.clt1.prod.mlbam.net - AP given\n\nharmonic02.c01.bc.gen.clt1.prod.mlbam.net  - VM deleted\n\nmf01.c01.web.wwe.clt1.prod.bamtech.co - AP given\n\nmemcached02.c01.lprovis.gen.clt1.qa.mlbam.net - AP given\nmemcached02.c01.lprovis.hulu.clt1.prod.mlbam.net - AP given\n\nmf01.c01.web.wwe.clt1.prod.bamtech.co - AP given\n\nmf08.c01.web.gen.clt1.prod.bamtech.co - vm doesn't exist\n\nmf09.c01.web.wwe.clt1.prod.bamtech.co - AP given\nmf10.c01.web.wwe.clt1.prod.bamtech.co - AP given\nmf11.c01.web.wwe.clt1.prod.bamtech.co - AP given\nmf12.c01.web.wwe.clt1.prod.bamtech.co - AP given\n\nnginx01.c01.medtrans.gen.clt1.prod.mlbam.net - AP given\nnginx01.c01.medtrans.gen.clt1.qa.mlbam.net - AP given\nnginx01.event01.hls.wwe.clt1.prod.mlbam.net - AP given\nnginx01.feed01.hls.mls.clt1.prod.mlbam.net - AP given\nnginx01.ingest01.hls.hulu.clt1.prod.mlbam.net - AP given\nnginx01.linear01.hls.wwe.clt1.prod.mlbam.net - AP given\nnginx01.live01.hls.gen.clt1.dev.mlbam.net - AP given\n\nnginx01.live01.hls.gen.clt1.prod.mlbam.net  - VM will be rebuilt\n\nnginx01.live01.hls.riot.clt1.prod.mlbam.net - AP given\nnginx01.manifest01.streaming.hulu.clt1.prod.mlbam.net - AP given\nnginx01.proxy01.hls.gen.clt1.prod.mlbam.net - AP given\nnginx01.proxy01.hls.gen.clt1.qa.mlbam.net - AP given\n\nproxy01.c01.streaming.gen.clt1.dev.mlbam.net - AP given\nproxy01.linear01.infra.hulu.clt1.prod.mlbam.net - AP given\nproxy01.linear02.infra.hulu.clt1.prod.mlbam.net - AP given\n\nredis02.c01.inf.gen.clt1.util.mlbam.net - VM does not exist\n\nsmtp01.clt1.util.mlbam.net - AP given\nsmtp02.clt1.util.mlbam.net - AP given\nsplunkfwd01.c01.noc.gen.clt1.util.mlbam.net - AP given\nsplunkfwd02.c01.noc.gen.clt1.util.mlbam.net - AP given\n\nxcoder01.c02.streaming.gen.clt1.dev.mlbam.net - AP given\nxcoder01.c03.streaming.gen.clt1.dev.mlbam.net - AP given\nxcoder02.c02.streaming.gen.clt1.dev.mlbam.net - AP given\n\n\ninflux01.c01.inf.hulu.clt1.prod.mlbam.net  - AP given\nmdtools01.c01.bpa.hulu.clt1.prod.mlbam.net  - AP given\nmdtools02.c01.bpa.hulu.clt1.prod.mlbam.net  - AP given\n\nmf02.c01.web.gen.clt1.prod.bamtech.co - vm is ok\nmf03.c01.web.gen.clt1.prod.bamtech.co - vm is ok\n\nmf03.c01.web.wwe.clt1.prod.bamtech.co  - AP given\n\nmf04.c01.web.gen.clt1.prod.bamtech.co  - vm is ok.\n\nmf04.c01.web.wwe.clt1.prod.bamtech.co  - AP given\n\nmf05.c01.web.gen.clt1.prod.bamtech.co - vm is ok.\n\nmf05.c01.web.wwe.clt1.prod.bamtech.co  - AP given\n\nmf07.c01.web.gen.clt1.prod.bamtech.co - vm is ok.\n\nmf08.c01.web.wwe.clt1.prod.bamtech.co   - AP given\n~~~</text>, <text>Advisory RHEA-2017:29661-03 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory\nhttps://errata.devel.redhat.com/advisory/29661\n\nCurrent planned release date:\n2018-05-15\n\nNote: release dates are Red Hat Confidential. Please be mindful about who the information is shared with. This date is subject to change. Please check Errata Tool for current release information.</text>, <text>Bug report changed to RELEASE_PENDING status by bugzilla-updater. Advisory RHEA-2017:29661-03 has been changed to PUSH_READY status.\nhttps://errata.devel.redhat.com/advisory/29661\n\nCurrent planned release date:\n2018-05-15\n\nNote: release dates are Red Hat Confidential. Please be mindful about who the information is shared with. This date is subject to change. Please check Errata Tool for current release information.</text>, <text>Bug report changed to RELEASE_PENDING status by bugzilla-updater. Advisory RHEA-2017:29661-03 has been changed to PUSH_READY status.\nhttps://errata.devel.redhat.com/advisory/29661\n\nCurrent planned release date:\n2018-05-15\n\nNote: release dates are Red Hat Confidential. Please be mindful about who the information is shared with. This date is subject to change. Please check Errata Tool for current release information.</text>, <text>Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHEA-2018:1488</text>, <text>Hi Marcus,\n\nI just wanted to let you know that bug 1513800 has now been closed. This was the Localdisk hook must prevent VM from being snapshot bug.\n\nLet me know if you have any questions.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series was retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Great. Thanks for the update Jason!\n\n\n\n\nOn 5/17/18, 12:29 PM, "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>Hi Marcus,\nHow are you coming along on AP that were provided from the last list run?\n\nThanks\nBimal.</text>, <text>HI Bimal,\nWe've been waiting on client approval first, we hope to start the rest of the work next week. I'll keep you updated tahnks</text>, <text>Thanks Marcus,\nWill await your response.\n\n~ Bimal.</text>, <text>Hi Bimal, \nAre you available to assist with a VM ? Working on VM : mdtools02.c01.bpa.hulu.clt1.prod.mlbam.net\n\nTHere's no progress on the last command i've ran......:\n\n[root@hyp25 ~]# virsh -r list |grep mdtools02.c01.bpa.hulu.clt1.prod.mlbam.net\n 4     mdtools02.c01.bpa.hulu.clt1.prod.mlbam.net running\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578'\n    (100.00/100%)\nImage committed.\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578.meta.orig'\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578.lease.orig'\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578 /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/1e29e060-8ad1-4a84-8825-7b0f654a0578.orig'\n[root@hyp25 ~]#\n[root@hyp25 ~]# su vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/cde481a9-923e-4007-b3b7-0fe6edfb85b5 -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec'\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]# su vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/cde481a9-923e-4007-b3b7-0fe6edfb85b5.meta"\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]# lvchange -ay /dev/ovirt-local/cde481a9-923e-4007-b3b7-0fe6edfb85b5\n[root@hyp25 ~]# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/cde481a9-923e-4007-b3b7-0fe6edfb85b5 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec'\n    (100.00/100%)\nImage committed.\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec.meta.orig'\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec.lease.orig'\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]# su vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/86ea7362-863a-42a3-9830-eb9c9f4057ab/0bfc7619-dcfd-476e-9d32-acb2aacb48ec.orig'\n[root@hyp25 ~]#\n[root@hyp25 ~]#\n[root@hyp25 ~]# su vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/d3b3dd57-2306-480c-ad06-7e3f789433b7 -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/922899f6-49f9-4fad-b37e-cd11761d51e8/43a3f816-c6d9-49d1-aaf0-96d0e196b47e'\n    (0.00/100%)\n    (0.00/100%)\n    (0.00/100%)\n    (0.00/100%)\n    (0.00/100%)\n    (0.00/100%)\n    (0.00/100%)\n    (0.00/100%)</text>, <text>Request Management Escalation: Need assistance asap for production down VM please.</text>, <text>Update it finally started ticking, we're at 2% now after 5 minutes. I'm going to let it sit here and see if it progresses. Please hold off on manager escalation at the moment</text>, <text>Hi Marcus,\n\nIt's Jason, I see your update you have a Production VM down at them moment, is that the VM mdtools02.c01.bpa.hulu.clt1.prod.mlbam.net\n\n- To ensure we have all the right information please get me a logcollector from the RHV-M and a sosreport from the host it was last running on as well as a sosreport from the SPM.\n\nWe will work as quickly as we can to get it up for you once we have these details.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series was retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Working on the RME</text>, <text>Hello Marcus,\n\nMy Name is Himadri and I am an Escalation Manager for Red Hat's Customer Experience and Engagement Team. I have acknowledged the Management Escalation that you have requested. Thank you for bringing it to my attention.\n\nI do see that Jason has already responded to the ticket and has requested you to share the logcollector from the RHV-M and a sosreport from the host it was last running on as well as a sosreport from the SPM.\n\nAwaiting for you to share the logs for jason to analyse.\n\nShould you have any further questions or concerns please don't hesitate to update the ticket and we would be glad to assist.\n\nRegards,\nHimadri.\nEscalation Manager.\nRed Hat-CEE.</text>, <text>Hi Jason,\nLooks like the 'stall' is due to the underlying disk being a 200GB thick provisioned disk. We're at 85% complete so i think it's just slow going. No action needed on your part at the moment thank you</text>, <text>Hi Marcus,\n\nThat is great to hear that its not actually down. We will stand down, please keep us posted if you do need help.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series was retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Hi Bimal,\nCan you check the commands for VM:  mf08.c01.web.wwe.clt1.prod.bamtech.co\n\nSeems to have been cutoff towards the bottom of the command list</text>, <text>Hi Marcus,\n\nSorry about that.\n\nHere is the full AP.\n\nFIXING VM:  mf08.c01.web.wwe.clt1.prod.bamtech.co:\n===========================================================================\n \n~~~\n1.  Shutdown the VM.\n \n2.  Run the following commands on the host:  hyp90.clt1.prod.mlbam.net \n\nGlobal RHVM Commands:\n=====================\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM snapshots WHERE vm_id = '0d9a33e2-12e5-4313-83af-435420e24910' AND snapshot_id != '69311d07-45a4-4296-96d4-51024cc8edaa';"\n\nFIXING DISK 261a89ed-6cd0-4319-99b1-b3138bf8a1f3, CHAIN ['c6d07f99-c7ec-444e-acef-7a177e083b9b', 'c9477d51-6a03-41f3-b6ae-3f5826327f5d']\n\nHost Commands:\n==============\n\nsu vdsm -s /bin/sh -c 'qemu-img rebase -f qcow2 -t none -u -b /dev/ovirt-local/c6d07f99-c7ec-444e-acef-7a177e083b9b -F raw /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d'\n\nsu vdsm -s /bin/sh -c "sed -i 's/VOLTYPE=INTERNAL/VOLTYPE=LEAF/g' /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c6d07f99-c7ec-444e-acef-7a177e083b9b.meta"\n\nlvchange -ay /dev/ovirt-local/c6d07f99-c7ec-444e-acef-7a177e083b9b\n\nsu vdsm -s /bin/sh -c 'qemu-img commit -f qcow2 -t none -b /dev/ovirt-local/c6d07f99-c7ec-444e-acef-7a177e083b9b -d -p /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d.meta /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d.meta.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d.lease /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d.lease.orig'\n\nsu vdsm -s /bin/sh -c 'mv /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d /rhev/data-center/78a89f5e-e88a-40dc-9467-9ccbb77f59d6/c2ba9087-43d9-44b5-a862-32f3a4dafb95/images/261a89ed-6cd0-4319-99b1-b3138bf8a1f3/c9477d51-6a03-41f3-b6ae-3f5826327f5d.orig'\n\nRHVM Commands:\n==============\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "DELETE FROM images WHERE image_guid IN ('c9477d51-6a03-41f3-b6ae-3f5826327f5d');"\n\n/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c "UPDATE images SET imagestatus='1', active='t', vm_snapshot_id='69311d07-45a4-4296-96d4-51024cc8edaa' WHERE image_guid ='c6d07f99-c7ec-444e-acef-7a177e083b9b';"\n\n=======================================================================\n~~~</text>, <text>Great thank you!\n\n\n\n\nOn 6/6/18, 12:42 PM, "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>Hi Marcus,\n\nLet us know on the progress.\n\nThanks\nBimal</text>]
ham	[[<description>Can we use Red Hat Enterprise Linux for Virtual Datacenters subscription in Oracle VM server hosts? If it is possible, what is the procedure to subscribe them through satellite server?</description>], <text>This comment has been generated via an automated case assistant.\n\n \n\nThe following resources may be helpful for working with virt-who and virtual Subscriptions (including Virtual Data Center):\n\n \n\n    https://access.redhat.com/labs/virtwhoconfig/ Red Hat Virtualization Agent (virt-who) Configuration Helper - virt-who is required when working with any hypervisor if your subscription name includes the words 'Virtual Datacenters', '4 Guests', or 'Unlimited Guests'. This app helps users configure virt-who. When started, the app requests information about your virt-who usage and generates a configuration file based on he answers. It also provides instructions for updating your virt-who configuration.\n    https://access.redhat.com/blogs/1169563/posts/2190731 Subscription-manager for the former Red Hat Network User: Part 3 - Understanding virt-who \n    https://access.redhat.com/blogs/1169563/posts/2630111 Subscription-manager for the former Red Hat Network user - part 5 - Working with subscriptions that require virt-who\n    https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/virtual_instances_guide/ - Virtual Instances Guide for Satellite 6\n    https://access.redhat.com/documentation/en-us/red_hat_subscription_management/1/html/virtual_instances_guide/ - Virtual Instances Guide for use with Red Hat Subscription Management (RHSM) but no Satellite</text>, <text>*PRIVATE UPDATE, DO NOT MAKE PUBLIC*\n\n    The previous public update is part of a pilot to improve auto-solve and self-solve, by commenting on cases that match certain rules upon creation, to provide answers to common questions/issues.  You can find more information about this project[1], the rule match for this case[2], and how to provide feedback (whether the rule is helpful, suggestions for improvement) here:\n\n    [1] https://mojo.redhat.com/groups/air-automatic-issue-resolution/projects/diagnostics-20\n    [2] https://projects.engineering.redhat.com/browse/CPSEARCH-1503\n    [3] https://mojo.redhat.com/thread/942968</text>, <text>Hello,\n\nMy name is Diogo Henrique, I'm from Satellite engineering team and will work on your request.\n\nRed Hat Enterprise Linux for Virtual Data Centers is a subscription that needs the use of virt-who daemon, and virt-who is not supported in Oracle VM.\n\nAny additional question, please let me know.\n\nBest Regards/ Atenciosamente,\nDiogo Henrique | S\xeanior Technical Support Engineer\nRed Hat Global Support Services\n\n"RHN UI is no longer available.  Please transition to RHSM in order to continue receiving support for your systems. Learn How. https://access.redhat.com/products/red-hat-subscription-management/#migration "</text>, <text>Dear customer,\n\nThank you for continuing to use Red Hat Support and we hope that you had a great experience working towards issue resolution, this case is now being closed.\n\nPlease, if you need additional information about it, you can re-open it in 72 hours using our portal at https://access.redhat.com/support.\n\nYou'll receive an e-mail asking you to review the support that you received.\n\nPlease use this opportunity to provide us the feedback about our support service.\n\nKind regards,\n---\nRed Hat Customer Experience &amp; Engagement (CEE)\nhttps://access.redhat.com</text>]
ham	[[<description>What problem/issue/behavior are you having trouble with?  What do you expect to see?\n\nVMs crash with the following output:\n\n[18120.095501] INFO: task kworker/7:0:15034 blocked for more than 120 seconds.\n[18120.096821] "echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs" disables this message.\n[18120.098284] kworker/7:0     D ffff88043feaad10     0 15034      2 0x00000080\n[18120.099654] Workqueue: xfs-cil/vda1 xlog_cil_push_work [xfs]\n[18120.100896]  ffff880313ebb7b0 0000000000000046 ffff880420bf1fa0 ffff880313ebbfd8\n[18120.102292]  ffff880313ebbfd8 ffff880313ebbfd8 ffff880420bf1fa0 ffff88043fdd6cc0\n[18120.103685]  0000000000000000 7fffffffffffffff ffff88043fddec80 ffff88043feaad10\n[18120.105087] Call Trace:\n[18120.106122]  [&lt;ffffffff816a94c9&gt;] schedule+0x29/0x70\n[18120.107311]  [&lt;ffffffff816a6fd9&gt;] schedule_timeout+0x239/0x2c0\n[18120.108534]  [&lt;ffffffff81062efe&gt;] ? kvm_clock_get_cycles+0x1e/0x20\n[18120.109837]  [&lt;ffffffff816a8b4d&gt;] io_schedule_timeout+0xad/0x130\n[18120.111106]  [&lt;ffffffff816a8be8&gt;] io_schedule+0x18/0x20\n[18120.112289]  [&lt;ffffffff81305fe5&gt;] bt_get+0x135/0x1c0\n[18120.113479]  [&lt;ffffffff810b1910&gt;] ? wake_up_atomic_t+0x30/0x30\n[18120.114762]  [&lt;ffffffff81306525&gt;] blk_mq_get_tag+0x45/0xe0\n[18120.115975]  [&lt;ffffffff8130158b&gt;] __blk_mq_alloc_request+0x1b/0x200\n[18120.117188]  [&lt;ffffffff813030ec&gt;] blk_mq_map_request+0x18c/0x1e0\n[18120.118448]  [&lt;ffffffff81303370&gt;] blk_sq_make_request+0x80/0x3d0\n[18120.119610]  [&lt;ffffffff812f706f&gt;] ? generic_make_request_checks+0x24f/0x380\n[18120.120829]  [&lt;ffffffff812f8b45&gt;] generic_make_request+0x105/0x310\n[18120.122053]  [&lt;ffffffff8123b439&gt;] ? bvec_alloc+0x59/0x120\n[18120.123233]  [&lt;ffffffff812f8dc0&gt;] submit_bio+0x70/0x150\n[18120.124339]  [&lt;ffffffff8123b713&gt;] ? bio_alloc_bioset+0x213/0x310\n[18120.125552]  [&lt;ffffffffc0206313&gt;] _xfs_buf_ioapply+0x2f3/0x460 [xfs]\n[18120.126748]  [&lt;ffffffffc0227d5b&gt;] ? xlog_bdstrat+0x2b/0x60 [xfs]\n[18120.127984]  [&lt;ffffffffc0207b70&gt;] xfs_buf_submit+0x70/0x1f0 [xfs]\n[18120.129235]  [&lt;ffffffffc0227d5b&gt;] xlog_bdstrat+0x2b/0x60 [xfs]\n[18120.130456]  [&lt;ffffffffc0229c7e&gt;] xlog_sync+0x2fe/0x420 [xfs]\n[18120.131643]  [&lt;ffffffffc0229e1b&gt;] xlog_state_release_iclog+0x7b/0xd0 [xfs]\n[18120.132911]  [&lt;ffffffffc022a8fa&gt;] xlog_write+0x5da/0x720 [xfs]\n[18120.134081]  [&lt;ffffffffc022c1d8&gt;] xlog_cil_push+0x2a8/0x430 [xfs]\n[18120.135349]  [&lt;ffffffffc022c375&gt;] xlog_cil_push_work+0x15/0x20 [xfs]\n[18120.136586]  [&lt;ffffffff810a881a&gt;] process_one_work+0x17a/0x440\n[18120.137792]  [&lt;ffffffff810a9638&gt;] worker_thread+0x278/0x3c0\n[18120.138988]  [&lt;ffffffff810a93c0&gt;] ? manage_workers.isra.24+0x2a0/0x2a0\n[18120.140228]  [&lt;ffffffff810b098f&gt;] kthread+0xcf/0xe0\n[18120.141307]  [&lt;ffffffff8108ddeb&gt;] ? do_exit+0x6bb/0xa40\n[18120.142421]  [&lt;ffffffff810b08c0&gt;] ? insert_kthread_work+0x40/0x40\n[18120.143559]  [&lt;ffffffff816b4f18&gt;] ret_from_fork+0x58/0x90\n[18120.144652]  [&lt;ffffffff810b08c0&gt;] ? insert_kthread_work+0x40/0x40\n\nFull log output can be found here: https://projects.engineering.redhat.com/secure/attachment/44475/console.txt\n\nWhere are you experiencing the behavior?  What environment?\n\nProduction\n\nWhen does the behavior occur? Frequently?  Repeatedly?   At certain times?\n\nfrequently\n\nWhat information can you provide around timeframes and the business impact?\n\nHigh. Production tasks are failing</description>], <text>Greetings,\n\nThank you for contacting Red Hat Technical Support.\n\nMy name is Ganesh Gore, I have taken the ownership of this case and will further assist you on this case.\n\nCould you please provide us below details ? \n\n1. Were there any recent hardware/software changes made in system ?\n\n2. Were there any power, networking, or storage related outages reported near the time that this system crashed ?\n\n3. What is the approximate date and time stamp of the server reboot, so that we can check the logs accordingly ?\n\n4. If kdump is configured on the system then please check if vmcore is generated or not. \n   If yes then please provide it for further analysis by referring article\n   # https://access.redhat.com/solutions/2112#ftp\n\nPlease provide sosreport of the system for further analysis.\nRefer: https://access.redhat.com/solutions/3592\n\nI am not able access the link: https://projects.engineering.redhat.com/secure/attachment/44475/console.txt\n\nAwaiting your reply.\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Here are some answers:\n\n1- Yes, RHEL 7.4 update\n2- These are Openstack VMs, there might be some network issues\n3- Thu Jul 6 19:56:57 EDT 2017\n4- Unfortunately kdump was not setup\n\nUploading the console output.</text>, <text>Hello,\n\nThank you for the update.\n\nBoth the file shows that hung tasks were detected and they were waiting for IO to complete and all of them were running in xfs codepath.\n~~~\n[18120.095501] INFO: task kworker/7:0:15034 blocked for more than 120 seconds.\n[18120.096821] "echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs" disables this message.\n[18120.098284] kworker/7:0     D ffff88043feaad10     0 15034      2 0x00000080\n[18120.099654] Workqueue: xfs-cil/vda1 xlog_cil_push_work [xfs]\n[18120.100896]  ffff880313ebb7b0 0000000000000046 ffff880420bf1fa0 ffff880313ebbfd8\n[18120.102292]  ffff880313ebbfd8 ffff880313ebbfd8 ffff880420bf1fa0 ffff88043fdd6cc0\n[18120.103685]  0000000000000000 7fffffffffffffff ffff88043fddec80 ffff88043feaad10\n[18120.105087] Call Trace:\n[18120.106122]  [&lt;ffffffff816a94c9&gt;] schedule+0x29/0x70\n[18120.107311]  [&lt;ffffffff816a6fd9&gt;] schedule_timeout+0x239/0x2c0\n[18120.108534]  [&lt;ffffffff81062efe&gt;] ? kvm_clock_get_cycles+0x1e/0x20\n[18120.109837]  [&lt;ffffffff816a8b4d&gt;] io_schedule_timeout+0xad/0x130\n[18120.111106]  [&lt;ffffffff816a8be8&gt;] io_schedule+0x18/0x20\n[18120.112289]  [&lt;ffffffff81305fe5&gt;] bt_get+0x135/0x1c0\n[18120.113479]  [&lt;ffffffff810b1910&gt;] ? wake_up_atomic_t+0x30/0x30\n[18120.114762]  [&lt;ffffffff81306525&gt;] blk_mq_get_tag+0x45/0xe0\n[18120.115975]  [&lt;ffffffff8130158b&gt;] __blk_mq_alloc_request+0x1b/0x200\n[18120.117188]  [&lt;ffffffff813030ec&gt;] blk_mq_map_request+0x18c/0x1e0\n[18120.118448]  [&lt;ffffffff81303370&gt;] blk_sq_make_request+0x80/0x3d0\n[18120.119610]  [&lt;ffffffff812f706f&gt;] ? generic_make_request_checks+0x24f/0x380\n[18120.120829]  [&lt;ffffffff812f8b45&gt;] generic_make_request+0x105/0x310\n[18120.122053]  [&lt;ffffffff8123b439&gt;] ? bvec_alloc+0x59/0x120\n[18120.123233]  [&lt;ffffffff812f8dc0&gt;] submit_bio+0x70/0x150\n[18120.124339]  [&lt;ffffffff8123b713&gt;] ? bio_alloc_bioset+0x213/0x310\n[18120.125552]  [&lt;ffffffffc0206313&gt;] _xfs_buf_ioapply+0x2f3/0x460 [xfs]\n[18120.126748]  [&lt;ffffffffc0227d5b&gt;] ? xlog_bdstrat+0x2b/0x60 [xfs]\n[18120.127984]  [&lt;ffffffffc0207b70&gt;] xfs_buf_submit+0x70/0x1f0 [xfs]\n[18120.129235]  [&lt;ffffffffc0227d5b&gt;] xlog_bdstrat+0x2b/0x60 [xfs]\n[18120.130456]  [&lt;ffffffffc0229c7e&gt;] xlog_sync+0x2fe/0x420 [xfs]\n[18120.131643]  [&lt;ffffffffc0229e1b&gt;] xlog_state_release_iclog+0x7b/0xd0 [xfs]\n[18120.132911]  [&lt;ffffffffc022a8fa&gt;] xlog_write+0x5da/0x720 [xfs]\n[18120.134081]  [&lt;ffffffffc022c1d8&gt;] xlog_cil_push+0x2a8/0x430 [xfs]\n[18120.135349]  [&lt;ffffffffc022c375&gt;] xlog_cil_push_work+0x15/0x20 [xfs]\n[18120.136586]  [&lt;ffffffff810a881a&gt;] process_one_work+0x17a/0x440\n[18120.137792]  [&lt;ffffffff810a9638&gt;] worker_thread+0x278/0x3c0\n[18120.138988]  [&lt;ffffffff810a93c0&gt;] ? manage_workers.isra.24+0x2a0/0x2a0\n[18120.140228]  [&lt;ffffffff810b098f&gt;] kthread+0xcf/0xe0\n[18120.141307]  [&lt;ffffffff8108ddeb&gt;] ? do_exit+0x6bb/0xa40\n[18120.142421]  [&lt;ffffffff810b08c0&gt;] ? insert_kthread_work+0x40/0x40\n[18120.143559]  [&lt;ffffffff816b4f18&gt;] ret_from_fork+0x58/0x90\n[18120.144652]  [&lt;ffffffff810b08c0&gt;] ? insert_kthread_work+0x40/0x40\n~~~\n\nUnfortunately this data is insufficient to troubleshoot the issue further. A vmcore is required for further analysis.\n\nNext steps:\n===========\n- Configure kdump by referring below link:\n~~~\nHow do I configure kexec/kdump on RHEL?\n# https://access.redhat.com/solutions/6038\n# https://access.redhat.com/solutions/6038#testing\n~~~\n\n- Make sure you allocate the required free disc space for the dump path of kdump.\nThe only sure way to guarantee a successful dump is to have free space on disk at least equal to physical RAM.\n\n- Test the kdump configuration for successful vmcore generation. This is to make sure that we have a working kdump configuration which will be helpful in issues occurring in future.\n\n- Configure kernel.hung_task_panic to capture the vmcore automatically in case of hung tasks detected in the system:\n~~~\nHow to automatically collect vmcore when "hung_task_timeout_secs" messages are logged? \n# https://access.redhat.com/solutions/382913\n~~~\n\n- Once vmcore is generated provide it for further analysis by referring below link:\n# https://access.redhat.com/solutions/2112#ftp\n\nLet me know if you have any queries.\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>We are still waiting for kdump to do its job, will let you know when we have one.</text>, <text>Hello,\n\nThank you for the update.\n\nPlease make sure you have tested the kdump for successful vmcore generation. Without testing it is not guaranteed that a vmcore will capture in case of system crash.\n\nA vmcore will only be captured when system is crashing due to kernel panic.\n\nKeep us updated with the status of this case.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Can we please keep this case open a bit longer?\n\nOn 30 Sep 2017 12:04 p.m., "Red Hat Support" &lt;support@redhat.com&gt; wrote:</text>, <text>Hello,\n\nThank you for the update.\n\nWe can keep this case open for some more time.\n\nIf this case gets closed then you can always reopen it if the same issue is occurring or you can have a new case opened.\n\nLet me know if you have any queries.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Hi,\n\nwe're having trouble with generating kdumps. I have tried multiple times to tune /etc/kdump.conf but so far we have been unsuccessful. The current configuration of /etc/kdump.conf is:\n\nforce_rebuild 1\npath /var/crash\ndracut_args --mount "10.8.243.1:/var/crash /var/crash nfs4 defaults" -L 5\ncore_collector makedumpfile -l --message-level 1 -d 31\n\nThen:\n\n-bash-4.2# cat /etc/sysctl.conf \n# sysctl settings are defined through files in\n# /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/.\n#\n# Vendors settings live in /usr/lib/sysctl.d/.\n# To override a whole file, create a new file with the same in\n# /etc/sysctl.d/ and put new settings there. To override\n# only specific settings, add a file with a lexically later\n# name in /etc/sysctl.d/ and put new settings there.\n#\n# For more information, see sysctl.conf(5) and sysctl.d(5).\nkernel.sysrq = 1\nkernel.softlockup_panic = 1\nkernel.hung_task_panic = 1\n\nWhen I try to manually trigger kernel panic by doing echo c &gt; /proc/sysrq-trigger the file is saved. But when the problem occurs the machine is only being rebooted with nothing written into the nfs.\n\nI have also tried to save kdumps to the local machine with no luck and the same for ssh. Everytime I tested my configurartion by \n\necho c &gt; /proc/sysrq-trigger \n\nand it worked. So I'm sure that it's not a configuration issue. Can you please give me some advice here?\n\nThanks,\nL.</text>, <text>Hello Ladislav,\n\nThank you for the update.\n\nPlease provide the fresh sosreport from the server so that I can check the logs.\n\nDo you see any messages on the console when issue is occurring ? If so, can you share the same ?\n\nWe will get a vmcore only in case of kernel panic (automatic or manual) events and not in any other case.\n\nAwaiting your reply.\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Hi Ganesh,\n\nI have instructed our users to lock their instances after the failure occurs so we should be able to get as much data as possible from there. We're waiting for issue reoccurence ATM. We should have it in a day or two.</text>, <text>Hello Ladislav,\n\nThank you for the update\n\nWe will wait for an update from your side.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>So far, I have only a stack trace that's attached.</text>, <text>Hello Ladislav,\n\nThank you for the update.\n\nThe logs you have provided are dmesg logs and it shows that hung tasks are detected in the system.\n\nApart from call traces it does not give any more helpful information.\n\nQuestions:\n==========\n- Is kdump tested for successful vmcore generation ?\n\n- Did you configure kernel.hung_task_panic parameter as suggested earlier ?\n============\n- Configure kernel.hung_task_panic to capture the vmcore automatically in case of hung tasks detected in the system:\n~~~\nHow to automatically collect vmcore when "hung_task_timeout_secs" messages are logged? \n# https://access.redhat.com/solutions/382913\n~~~\n============\n\nAwaiting your reply.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>For this particular tenant this has not been configured yet. Note that the stack traces are happening in OpenStack environment and so far we have configured this for one tenant who suffered with these failures a lot.\n\nUnder that particular tenant that uses modified RHEL images containing all /etc/sysctl.conf and /etc/kdump.conf tweaks I successfully tested vmcore generation by running echo c &gt; /proc/sysrq-trigger. However, every time the actual failure happened the vmcore save failed . I have also changed the default kernel to the one that contains debug symbols.\n\nYou can check the current configuration yourself. I have spawned a test instance running this modified image running at 10.8.246.194. The access credentials are root/redhat. The NFS server runs at 10.8.243.1 also accessible as root/redhat. You can do whatever tests you find appropriate with instance 10.8.246.194 and you should see that the vmcore generation works when you test it.</text>, <text>Hello Ladislav,\n\nThank you for the update.\n\nFrom your last update I understand below points:\n~~~\n1. For this particular tenant this has not been configured yet. ==&gt; This seems to be a different host having similar issue.\n\n2. Vmcore is not being generated for the host where you have done the changes.\n~~~\n\nCould you please provide the fresh sosreport of the server on which vmcore is not getting captured even after doing the required changes ?\n\nAwaiting your reply.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>1. OK, let's focus on the configured tenant.\n2. I'd say that the kdump is triggered but the vmcore is not being saved for some reason. High I/O or I/O blocked?\n\nThe fresh sos report is attached. Please note that stack trace most probably haven't occurred on this host at all. So it's configured and ready but I don't expect it has some data for analysis. The problem is that instances are very short lived and they're basically destroyed after tests are finished. Sometimes we manage to lock the instance before provisioner destroys it. Unfortunately I don't know about any instance ATM that had suffered from stack trace and it's still on. However users of this tenant were educated to lock these instances and let me know immediately.</text>, <text>Hello Ladislav,\n\nThank you for the update.\n\nI have reviewed the sosreport (w.r.t kdump configuration only) and it looks fine to me.\n~~~\nKDUMP CONFIG\n  kexec-tools rpm version:\n    kexec-tools-2.0.14-17.el7.x86_64\n  Service enablement:\n    UNIT           STATE\n    kdump.service  enabled\n  kdump initrd/initramfs:\n    16972260 Sep 25 03:56 initramfs-3.10.0-693.2.1.el7.x86_64kdump.img\n    18212913 Oct 17 04:31 initramfs-3.10.0-693.2.2.el7.x86_64.debugkdump.img\n    16973783 Sep 25 04:15 initramfs-3.10.0-693.el7.x86_64kdump.img\n  Memory reservation config:\n    /proc/cmdline { crashkernel=auto }\n    GRUB default  { crashkernel=auto }\n  Actual memory reservation per /proc/iomem:\n      2b000000-350fffff : Crash kernel\n  kdump.conf:\n    force_rebuild 1\n    path /var/crash\n    dracut_args --mount "10.8.243.1:/var/crash /var/crash nfs4 defaults" -L 5\n    core_collector makedumpfile -l --message-level 1 -d 31\n  kdump.conf "path" available space:\n    System MemTotal (uncompressed core size) { 7.62 GiB }\n    Available free space on target path's fs { 34.50 GiB }  (fs=/)\n  Panic sysctls:\n    kernel.sysrq [bitmask] =  "1"  (all SysRqs allowed)\n~~~\n\nSince you already have tested kdump and a vmcore is being generated, we need to setup serial console logs to why its failing when system crashes.\n\nPlease setup serial console logging on this instance/VM:\n~~~\nHow does one set up a serial terminal and/or console in Red Hat Enterprise Linux?\n# https://access.redhat.com/node/7212\n~~~\n\nPlease note if this same is being removed/destroyed then it will make troubleshooting process difficult and long running. Once this node gets crashed when next time same issue occurs, then we need the serial console logs for analysis.\n\nWithout data it is difficult to progress further with the case.\n\nIf you are able to setup test environment where you can reproduce the issue, then it will be of great help as we will not need to depend on the same VM/instance.\n\nJust to let you know, I have no knowledge of OpenStack and I can help you with system getting crashed and kdump issue here.\n\nLet me know if you have any queries.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Hi Ganesh,\n\nwe already have serial console on, see\n\n# cat /proc/cmdline \nBOOT_IMAGE=/boot/vmlinuz-3.10.0-693.2.2.el7.x86_64.debug root=UUID=42ea7d3e-dafb-4059-b580-bb217b3838e6 ro console=ttyS0,115200n8 console=tty0 crashkernel=auto rhgb quiet\n\nThe output of the serial console is saved in the console.log of the VM however I didn't find a reason of why vmcore save fails. I can post you this file once we hit the problem again.</text>, <text>This is output from a console that failed to save a vmcore. Do you want to sosreport from this machine?</text>, <text>Hello,\n\nI have checked the console.log, and did not find anything registered which indicates failure during generation of vmcore.\n\nIt would be a good idea to drop the "rhgb quiet" parameter as well and observe for the issue occurence and share the updated logs.\n\n\nRegards,\nEliza Allience\nGlobal Support Services\nRed Hat, Inc.</text>, <text>https://access.redhat.com/solutions/277793</text>, <text>OK, rhgb quiet has been removed from kernel cmdline. Openstack image snapshot updated and we're waiting for problem to reoccur.\n\nBTW this bug [1] seems to be very similar to issues we're having.\n\n[1] https://bugzilla.redhat.com/show_bug.cgi?id=1375117</text>, <text>Here's the console.log after crash with removed rhgb quiet options.</text>, <text>Hello Ladislav,\n\nThank you for providing the console logs.\n\nCurrently I am reviewing them and I will get back to you with an update soon.\n\nI do not think below bugzilla is applicable here.\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1375117\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Hello Ladislav,\n\nSorry it took more time for me to update. Was stuck in another case.\n\nI reviewed the latest console logs and it does not look like it captured all the logs after kernel panic.\n\nI only see logs till time [    1.950925] which is hardly 2 seconds after panic.\n\nNext steps:\n===========\n- Set kernel command line parameters for serial console logs in below order:\n~~~\nconsole=tty0 console=ttyS0,115200n8\n~~~\n\nCurrently they are set in reverse order.\n\n- I would like to check what is exactly going on when system crashes via a remote session, since we are not getting full serial console logs.\n\nCould you please let me know your preferred time for remote session tomorrow ?\n\nAlso confirm if contact number +420608737620 is correct to reach you. Else provide your contact number with country code.\n\nAwaiting your reply.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>I can try to do that. However I don't see our customer online at the moment so I cannot ask him to provide me the VM for this change.\n\nMy timezone is CEST and I'm usually available from 06:00 - 15:00 UTC. But if necessary I can be available outside of these hours.\n\nThe phone number is correct, however we can use Bluejeans. Actually, I would prefer that since it's much comfortable for me - I have both hands free.</text>, <text>Hello Ladislav,\n\nThank you for the update.\n\nI will be available from 12:30PM to 9:30PM UTC time. WIll your customer be available to provide remote session for VM with hostname "ci-rhos-rhel7-pr-builder-1062" ?\n\nIf so then I need a remote session to what exactly happens when vmcore is being captured.\n\nPlease let me know the your (and customer's) availability for remote session.\n\nBluejeans can also be used here.\n\nAwaiting your reply.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Ganesh,\n\nI have another console.log but unfortunately still the same result.\n\nAs for VM's - those are dynamically provisioned and this one you requested is already destroyed. But I have two another still alive with the error present but with no ability to log in, it's just frozen and I have already sent you the output from the serial console.\n\nMy customer will go home soon today. I'm not able to run any new jobs myself. But even if I did it wouldn't be helpful. They are spawning around 40 VM's at a time and from time to time one of them suffers with this error. So we'll not be able to watch this in the real time anyway. And once it happens the VM is dead. We can of course restart it but the error will be gone.\n\nIf you want to make a remote session we can do it.</text>, <text>Hello Ladislav,\n\nThank you for the update.\n\nConsole logs are jumbled up and there is no clue about the issue.\n\nWere you able to capture a vmcore for any of the below two VMs ?\n~~~\nBut I have two another still alive with the error present but with no ability to log in, it's just frozen and I have already sent you the output from the serial console.\n~~~\n\nCan we have a problematic system consistently without destroying/removing it ?\n\nIf problematic system is getting destroyed/removed then it will make our troubleshooting really difficult since we are not actually making any progress at all.\n\nIf you are available now, then we can schedule a remote session.\n\nAwaiting your reply.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Unfortunately, no vmcores captured. :-(\n\nI have the system with problem up and running. And yes, I'm available, we can make the remote session.\n\nCheers,\nL.</text>, <text>Hello Ladislav,\n\nPlease use below details to join the remote session.\n\n\nSession Key: 4228935 \n\nURL: https://remotesupport.redhat.com/?ak=4e9bdff610da9e70b26e90ff2466e392 \n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Hello Ladislav,\n\nThank you for your time over the remote session. Below is the short summary of what we did in the session:\n\n- We crashed an instance from image/snapshot bxms-rhel7.4-snapshot-kdump and we were able to get a vmcore.\n\n- We also confirmed that we are getting the serial console logs captured.\n\n- You already have tested kdump using below methods and you get a vmcore captured while testing. \n~~~\nlocal\nvia ssh\nvia nfs\n~~~\n\n- However when issue occurs vmcore is never captured and console logs are also not captured completely.\n\nCurrently I am reviewing this case with my senior colleagues and I will get back to you with an update as soon as I have anything.\n\nPlease allow me a day or two for the same.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Hi Ganesh,\n\nthanks for remote session as well. I'm sending you requested console output of successful *test crash* issued by echo c &gt; /proc/sysrq-trigger. \n\nCheers,\nL.</text>, <text>//INTERNAL:\n\nAs seen from all the attachments and the case history, the issue most probably/repetitively seems to occur right at the login prompt once the system boots up.\n\nA couple of hung_tasks are observed having their traces in xfs codepath eventually making the system freeze. We also have kept the panic parameter enabled to panic the system when this occurs.\n\nA fact to share:\n\nIn RHEL7, the kdump service takes a few seconds to activate completely even when after the system boots up. This is because of the sanity check that kdumpctl does at the backend. Some of the things include selinux relabeling along with initrd verification.\n\nNow on top of that, customer has 'force_rebuild' enabled here [dont know why]. This will forcefully rebuilt kdump's initrd upon every service restart. To be precise, at every boot kdump will be doing that. This will directly impact the time taken by the service to activate. Those few seconds will now turn into a few minutes because of this.\n\nWhat I suspect here is that, at the time of the issue when system hangs and eventually crashes (because of the configured sysctl), kdump is still in the 'activating' state. Due to this the system gets deadlocked (as it happens in a no-kdump scenario).\n\nIn this case I would either recommend to remove force_rebuild and check if we still get a core or best way run #virsh dump from the compute node on which the instance resides/runs at the time of the issue.\n\ncompute node is kind of equivalent to the host (hypervisor) where the vm is running.\n\nOn the hypervisor:\n\n# virsh list --all //will give the list of instances hosted\n# virsh dump //(with the --memory-only option to avoid unreadable vmcore)\n\n- R.Raghavendra\n  Red Hat Global Support</text>, <text>We have at least 5 other OpenStack tenants hit by this. Increasing Severity.</text>, <text>Greetings team,\n\nThis is Amit Anjarlekar and I am part of the Kernel team in Global Support Services. I am taking ownership of this case during APAC hours.\n\nPlease allow me min. 2 hrs to go through the case history &amp; answer any queries or analysis you are expecting. Meantime keeping case as "waiting on redhat".\n\nWe highly appreciate your patience.\n\nBest Regards\nAmit Anjarlekar\nRedhat Global Support.</text>, <text>Hello,\n\nThanks for your patience. We are still internally working on the reported issue which may take more time if we have to reproduce the issue.\n\nWe highly appreciate your patience. Meantime could you please confirm on availability of remote session if required from our end?\n\nPlease feel free to let us know if you have any further queries or concerns.\n\nBest Regards\nAmit Anjarlekar\nRedhat Global Support.</text>, <text>### Internal Note\n\nChanged internal status to Waiting On Owner as per discussion with Ajit as collab status was not exactly justifying the current situation.</text>, <text>Hi,\n\nI'm available today until circa 15:00 UTC, then I can be online between 17:00 - 21:00 UTC.\n\nCheers,\nL.</text>, <text>### Internal Note Not To Publish\n\n - VM's in openstack environment's are failing in XFS codepath.\n\n - Unable to generate vmcore for which need to perform more troubleshooting.\n\n### Actions Taken\n\n - Reviewed all console logs where traces of xfs codepath are present.\n\n - Below is the brief update results in remote session taken by Ganesh :\n\n~~~~~~~~~~~~~\n\n- We crashed an instance from image/snapshot bxms-rhel7.4-snapshot-kdump and we were able to get a vmcore.\n\n- We also confirmed that we are getting the serial console logs captured.\n\n- You already have tested kdump using below methods and you get a vmcore captured while testing. \n~~~\nlocal\nvia ssh\nvia nfs\n~~~\n\n- However when issue occurs vmcore is never captured and console logs are also not captured completely.\n\n~~~~~~~~~~~~~\n\n### Next Action\n\n - Reproducer of the issue can help to further troubleshoot the issue which may take time.\n\n - Suggestion as per R.Raghavendra can be tested in case to isolate the issue of kdump.\n\n - Remote session with above tests can be used as per customer time zone &amp; availability.\n\n\nAfter reviewing video How To Reproduce :\n\n - Customer have the option to reproduce the issue using "execution of jenkins jobs that triggers multiple spawns of VM based on this image.\n\n - Manual kdump works fine &amp; generates vmcore too.\n\n - When issue is produced then no vmcore is generating &amp; even the console logging stops.</text>, <text>Hello Ladislav,\n\nI have discussed this case with Amit who was working in APAC shift and currently reviewing the case summary.\n\nKindly confirm if +420608737620 is the best number to reach you. I will get back to you in next 30 minutes.\n\n\nRegards,\nRakesh Pagare\nRed Hat</text>, <text>Hi Rakesh,\n\nthe number is correct however the best option would be Bluejeans as I'd have both hands free.\n\nCheers,\nL.</text>, <text>Hello Ladislav,\n\nKindly join the call for further discussion through below bluejeans link \n\nhttps://bluejeans.com/7045861877\n\nOR\n\nFrom the invite I sent you on email.\n\n\n\nRegards,\nRakesh Pagare\nRed Hat</text>, <text>// Internal\nSetting NEP to 2 hours as waiting for  Ladislav  to join Bluejeans call.</text>, <text>// Internal\n\nOn BJ session with customer. Putting WoC atm.</text>, <text>Hello Ladislav,\n\nThank you for your time over the bluejeans session.\n\nAs of now we asked to do below steps:\n1. Remove forec_rebuild option from /etc/kdump.conf. Make sure you restart the kdump service.\n2.  Get console log using command \n# nova console-log &lt;instance_id&gt;\n\nAs of now you are waiting for your colleague to start the jobs and see if the issue occurs or not.\n\nYou will let us know once you have got instance up and running.\n\nI will share the bluejeans session link again.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Hi Ganesh,\n\nI did the requested change to kdump.cfg and restarted the service.\n\nPlease find output of nova console-log ea1e36fb-c433-4d04-a982-5f5103e12127  in the attachment.\n\nLadislav</text>, <text>Hello Ladislav,\n\nThank you for the update and your time over the call.\n\nThere is not difference in nova console logs as well.\n\nAs suggested please do below steps:\n\n1. Set sysctl "kernel.panic = 0" in /etc/sysctl.conf. **DO NOT disable kernel.hung_task_panic parameter**.\n\n2. Disable kdump i.e stop the kdump service and disable it to start on boot.\n\n3. When issue is reproduced get nova console logs.\n\n4. With first two points system will remain in kernel panic state when the issue is reproduced/reoccurs. Then get snapshot of VM instance using virsh dump:\n\n   4.1 List the VMs\n   # virsh list --all\n\n   4.2 Generate the dump:\n   # virsh dump &lt;VM_name&gt; &lt;path_to_copy_core&gt; --memory-only --verbose\n\nOnce the core is captured please upload the vmcore to our FTP dropbox by referring below article:\n# https://access.redhat.com/solutions/2112#ftp\n\nPlease make sure you add the case number at the start of generated file before you upload it to FTP dropbox.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Hi Ganesh,\n\nthanks for suggestions. We managed to reproduce the error and I'm now running memory dump however I'm not sure if it finishes. Some facts:\n\n[stack@director ~]$ openstack server show     80f78e91-32d4-4661-b1a5-6decb0dddf2e\n+--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Field                                | Value                                                                                                                                                                    |\n+--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| OS-DCF:diskConfig                    | MANUAL                                                                                                                                                                   |\n| OS-EXT-AZ:availability_zone          | nova                                                                                                                                                                     |\n| OS-EXT-SRV-ATTR:host                 | overcloud-compute-38.localdomain                                                                                                                                         |\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | overcloud-compute-38.localdomain                                                                                                                                         |\n| OS-EXT-SRV-ATTR:instance_name        | instance-000b42cd                                                                                                                                                        |\n| OS-EXT-STS:power_state               | Running                                                                                                                                                                  |\n| OS-EXT-STS:task_state                | None                                                                                                                                                                     |\n| OS-EXT-STS:vm_state                  | active                                                                                                                                                                   |\n| OS-SRV-USG:launched_at               | 2017-10-23T18:27:04.000000                                                                                                                                               |\n| OS-SRV-USG:terminated_at             | None                                                                                                                                                                     |\n| accessIPv4                           |                                                                                                                                                                          |\n| accessIPv6                           |                                                                                                                                                                          |\n| addresses                            | bxms-qe-jenkins-2=172.16.127.63, 10.8.247.115                                                                                                                            |\n| config_drive                         |                                                                                                                                                                          |\n| created                              | 2017-10-23T18:26:11Z                                                                                                                                                     |\n| flavor                               | m1.large (431ac1fb-1463-4527-b3d1-79245dd698e1)                                                                                                                          |\n| hostId                               | efa9157318876605b7c7dfa7c6c4e4a1d214641efdecac1c2ebf6157                                                                                                                 |\n| id                                   | 80f78e91-32d4-4661-b1a5-6decb0dddf2e                                                                                                                                     |\n| image                                | bxms-rhel7.4-snapshot-kdump (6e5b77f9-baa7-4675-bbbb-4bd8e8c4800a)                                                                                                       |\n| key_name                             | bxms-qe-jenkins                                                                                                                                                          |\n| name                                 | ci-rhos-rhel7-9906                                                                                                                                                       |\n| os-extended-volumes:volumes_attached | []                                                                                                                                                                       |\n| progress                             | 0                                                                                                                                                                        |\n| project_id                           | 8db1c56fc8b94d218c9d13e791cc678f                                                                                                                                         |\n| properties                           | jenkins-cloud-name='ci-rhos', jenkins-instance='https://bxms-qe-jenkins.rhev-ci-vms.eng.rdu2.redhat.com/', jenkins-scope='node:ci-rhos-rhel7-9906', jenkins-template-    |\n|                                      | name='ci-rhos-rhel7'                                                                                                                                                     |\n| security_groups                      | [{u'name': u'default'}]                                                                                                                                                  |\n| status                               | ACTIVE                                                                                                                                                                   |\n| updated                              | 2017-10-23T18:45:42Z                                                                                                                                                     |\n| user_id                              | 4cc1cd0299384f42ba970524ef910b1c                                                                                                                                         |\n+--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nSo the instance is running on overcloud-compute-38.localdomain as instance-000b42cd. Running the suggested commands there:\n\n[stack@director ~]$ ssh overcloud-compute-38\nWarning: Permanently added 'overcloud-compute-38,172.16.105.57' (ECDSA) to the list of known hosts.\nLast login: Sat Oct 21 15:14:26 2017 from 172.16.105.1\n[admin@overcloud-compute-38 ~]$ sudo -i\n[root@overcloud-compute-38 ~]# virsh dump instance-000b42cd instance-000b42cd.dump --memory-only --verbose\n\nRunning for 10 minutes and nothing.\n[root@overcloud-compute-38 ~]# ps -ef | grep virsh\nroot      460437  460179  0 18:48 pts/15   00:00:00 virsh dump instance-000b42cd instance-000b42cd.dump --memory-only --verbose\n\n[root@overcloud-compute-38 ~]# strace -p 460437\nstrace: Process 460437 attached\nrestart_syscall(&lt;... resuming interrupted poll ...&gt;) = 0\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x55d570f3de1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x55d570f3de1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x55d570f3de1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x55d570f3de1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x55d570f3de1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x55d570f3de1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x55d570f3de1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\n\nThus I'm in doubt if it ever finishes. However I was able to capture another console output for you. Attaching to the case. IS it better? Can you see something there?</text>, <text>Hello Ladislav,\n\nThank you for the update.\n\nFrom strace output I see that write\n~~~\nfutex(0x55d570f3eda0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1   &lt;====\n~~~\n\nIt seems like dump is still being captured.\n\nWhat was the RAM size of instance-000b42cd and how much is the size of file instance-000b42cd.dump ?\n\nDo you see increase in size of file instance-000b42cd.dump ?\n\nAwaiting your reply.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Didn't see .dump file created at all. I was too impatient and cancelled it prematurely. We're going to run it again and I'll leave running over night. The instance has 8GB of RAM.</text>, <text>Hello Ladislav,\n\nThank you for the update.\n\nKeep us updated with your findings.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>OK, another instance stuck. Running memory dump over night. I'll update you how it went tomorrow morning CEST.\n\nThanks,\nL.</text>, <text>Hello Ladislav,\n\nThank you for the update.\n\nWe will wait for an update from your side.\n\n\nRegards,\nGanesh Gore\nGSS, Red Hat.</text>, <text>Hi Ganesh,\n\nso as I expected, nothing has happened over night. The dump is still running and .dump file doesn't exist on a file system. What will be the next steps?\n\nThanks,\nL.</text>, <text>Hello,\n\nGreetings from Red Hat Technical Support. I am Amit, I am a member of the Kernel team of Global Support Services. I have acknowledged this case and will assist you with this issue during APAC hours.\n\nPlease allow me to go through the yesterdays case activity &amp; answer your fresh queries.\n\nWe highly appreciate your patience.\n\nPlease feel free if you have any questions or concerns.\n\n\nBest Regards\nAmit Anjarlekar\nRedhat India Support.</text>, <text>### Internal Note - Not to publish\n\nUnable to get time to review case because of high FTS volume. Kindly review comments since yesterday &amp; take the case forward. Only updating next action plan as per customer's latest update.</text>, <text>Hello,\n\nThanks for your patience. APAC hours are getting over, so i am handing over the case to next engineer. The action plan is updated accordingly.\n\nPlease have patience before next engineer review the case &amp; update the next action plan.\n\nWe highly appreciate your patience.\n\nBest Regards\nAmit Anjarlekar\nRedhat India Support.</text>, <text>Hello Team,\n\nApologies for the delay in the response. My name is Akshay and I am one of the Engineers working on this case. Currently I am checking with domain experts about this case.\n\nI will get back to you soon.\n\nFeel free to update case for any queries/concerns in the meantime.\n\nBest Regards.\n\n- Akshay R.\nRed Hat India \nGSS</text>, <text>[collab-shell] https://gitlab.cee.redhat.com/gss-tools/collab-shell\n\n    # ssh your_kerb@collab-shell.usersys.redhat.com\n    # cd /cases/01932613\n\nthe following files have been downloaded and extracted:\n--------------------------------\n\t1M\t201-09-11-stuck_instance.log\n\t1M\tconsole.log\n\t1M\tconsole.log\n\t1M\tconsole.log\n\t1M\tconsole.log\n\t1M\tconsole.log\n\t1M\tconsole.log\n\t1M\tconsole.log\n\t1M\tconsole.log\n\t1M\tconsole.log\n\t1M\tconsole.txt\n\t1M\tnova-console.log\n\t1M\tnova-console.log\n\t1M\tnova-console.log\n\t1M\tnova-console.log\n\t10M\tsosreport-ci-rhos-rhel7-pr-builder-1062.01932613-20171017141213.tar.xz\n\t1M\tstacktrace.txt\n\t1M\tvmcore-dmesg.txt\n--------------------------------\n\nview attachments here: http://collab-shell.usersys.redhat.com/01932613/</text>, <text>//INTERNAL\n\n\n\nAs Pablo points, 120 seconds is like a standard timeout for IO waiting on device\n\nAlso, the virsh issues could be better diagnosed within sbr-virt\n\nstill checking other information\n\n\nhttps://access.redhat.com/solutions/2456711 ? The sosreport contains higher kernel but not sure it's the hypervisor one</text>, <text>Hi,\nThis is Pablo Iranzo from the EMEA ECS Team.\n\n\nI see that there's a sosreport attached to the case but I think it comes from the instance.\n\nI would like to clarify with you in order to update actual status:\n\n- Does this affect all vm's either repeatedly or randomly?\n- Does this happens on all computes or a subset of them?\n- Can you upload a sosreport from one of the affected computes  as the current attached one doesn't seem to pertain to them?\n\nWe would like to check some data about the system status (compute) that might be showing inside the vm (like memory pressure, disk utilization, etc).\n\nIf needed to continue with the kdump/virsh dump, we'll move this to be worked together with our Virtualization specialist team.\n\nRegards,\nPablo</text>, <text>Hi Pablo,\n\nit affects RHEL-7.4 based images and some of our tenants have reliable reproducers to to trigger this. When I ask them we always have one or two VM's out of ~40 spawned that get into this state. We don't have reports from all the tenants. But those tenants that reported the issue see it quite regularly - my theory is that they're breaking it by specific combination of tests.\n\nFailures are spread evenly on all compute nodes. Every time I looked where the instance failed it was always a different node.\n\nYes, I can. The instance is gone - the users needed to delete it but I can provide you data from compute-15 where I attempted to get memory dump for around 18 hours.\n\nYes, please ask your Virtualization specialist how we can get those memory dumps. BTW when I want to get to the instance in broken state via Horizon even VNC doesn't work and it freezes in handshake phase.\n\nThanks,\nLadislav</text>, <text>[collab-shell] https://gitlab.cee.redhat.com/gss-tools/collab-shell\n\n    # ssh your_kerb@collab-shell.usersys.redhat.com\n    # cd /cases/01932613\n\nthe following files have been downloaded and extracted:\n--------------------------------\n\t37M\tsosreport-overcloud-compute-15.localdomain.01932613-20171024140528.tar.xz\n--------------------------------\n\nview attachments here: http://collab-shell.usersys.redhat.com/01932613/</text>, <text>Thanks Ladislav,\n\nWe'll be reviewing the provided sosreport, and in parallel set the case to be reviewed by our Virtualization team for the vmcore generation.\n\nRegards,\nPablo</text>, <text>//COLLAB REQUEST for sbr-virt\n\n\nCan you please check why virsh dump doesn't generate vmcore so that we can analyze the vm crashes?\n\nThanks,\nPablo</text>, <text>Have the permissions been verified for the dump process?\n\nhttps://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Administration_Guide/sect-vish-dump.html\n\nAlso please have them run the following after the virsh dump command to check the status:\n\n#virsh domjobinfo</text>, <text>Hello,\n\nMy name is Jason from the virtualization team and I am assisting my counterparts from our OSP team.\n\nHave the permissions been verified for the dump process?\n\nhttps://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Administration_Guide/sect-vish-dump.html\n\nAlso please have them run the following after the virsh dump command to check the status:\n\n#virsh domjobinfo\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Hi Jason,\n\nwe managed to trigger stacktrace once again. In order to mitigate permission problem I'm attempting to dump the file to /tmp as:\n\n[admin@overcloud-compute-11 ~]$ sudo -i\n[root@overcloud-compute-11 ~]# cd /tmp/\n[root@overcloud-compute-11 tmp]# virsh dump instance-000bd507 /tmp/instance-000bd507.dump --memory-only --verbose\n\nPermissions on /tmp:\n\n[root@overcloud-compute-11 ~]# ls -ld /tmp/\ndrwxrwxrwt. 11 root root 4096 Oct 24 11:24 /tmp/\n\n[root@overcloud-compute-11 ~]# lsattr -d /tmp/\n---------------- /tmp/\n\ndomjobinfo shows Job type: None, see the following:\n\n[root@overcloud-compute-11 ~]# virsh domjobinfo instance-000bd507\nJob type:         None       \n\n[root@overcloud-compute-11 ~]# ps -ef | grep virsh\nroot      589484  589384  0 19:02 pts/18   00:00:00 virsh dump instance-000bd507 /tmp/instance-000bd507.dump --memory-only --verbose\n\nStrace still the same.\n\n[root@overcloud-compute-11 ~]# strace -p 589484\nstrace: Process 589484 attached\nrestart_syscall(&lt;... resuming interrupted poll ...&gt;) = 0\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x557f07146e1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x557f07146e1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x557f07146e1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x557f07146e1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x557f07146e1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x557f07146e1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\npoll([{fd=9, events=POLLIN}, {fd=0, events=POLLIN}], 2, 500) = 0 (Timeout)\nrt_sigprocmask(SIG_BLOCK, [INT], [], 8) = 0\nwrite(8, "\\1", 1)                       = 1\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x557f07146e1c, FUTEX_WAIT_PRIVATE, 1, NULL) = 0\nfutex(0x557f07147dd0, FUTEX_WAKE_PRIVATE, 1) = 0\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\n\nWhat else we can try?\n\nThanks,\nL.</text>, <text>BTW the /tmp/instance-000bd507.dump isn't created.</text>, <text>Hello,\n\nThank you for the update, please allow some additional time while we look into the additional data you provided.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>Pinged in sbr-virt to the SME's to verify what could be causing virsh dump to not create the dump file.</text>, <text>Hi,\n\nIf the main priority is to get the guest vmcore, I suggest bypassing libvirt's virsh dump and just do a manual gcore of the qemu-kvm process of the VM:\n# gcore &lt;qemu-kvm PID&gt;\n\nThen we can extract the VM memory from the gcore in order to generate an ELF Core to be used in optimus in order to proceed with the case.\nhttps://access.redhat.com/solutions/2292431\n\nAnd in case the vmcore points to something wrong at the virt layer (which this virsh hanging may indicate), we already have a coredump of the qemu-kvm process.\n\nRegards,\nGermano Veit Michel\nSenior Software Maintenance Engineer - Virtualization\nRed Hat Asia Pacific - Brisbane, Australia</text>, <text>Hello,\n\nAt this time it appears the main priority is to get the guest vmcore, so we suggest bypassing libvirt's virsh dump and just do a manual gcore of the qemu-kvm process of the VM:\n# gcore &lt;qemu-kvm PID&gt;\n\nThen we can extract the VM memory from the gcore in order to generate an ELF Core to be used in optimus in order to proceed with the case.\nhttps://access.redhat.com/solutions/2292431\n\nAnd in case the vmcore points to something wrong at the virt layer (which this virsh hanging may indicate), we already have a coredump of the qemu-kvm process.\n\nI'll be handing this over to our APAC/Pune teams at this time to be continue to be worked, but if you can get this core please let us know and upload it to our public FTP. Large files can be uploaded to Red Hat's public FTP site "Dropbox" as follows: \n\nhttps://access.redhat.com/knowledge/solutions/2112\n\nMake sure to have the ticket # in the filename and then include that same filename here in the case.\n\nRegards,  \nRobert McSwain, RHCE, RHCVA \nSenior Technical Support Engineer\nCustomer Experience &amp; Engagement, Red Hat, Inc.\n\n~~~~~~~~~~~~~~~~~~~~IMPORTANT~~~~~~~~~~~~~~~~~~~~\nRHEV 3.x series has reached Product Retirement as of September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev\n\nPlease upgrade as soon as possible to retain supportability of your environment. Ready to upgrade to RHV 4.0 from RHEV 3.6? Double check our RHV Upgrade Helper!\nhttps://access.redhat.com/labs/rhevupgradehelper/\n\nHave you backed up your RHEV database recently? \nhttps://access.redhat.com/solutions/61993</text>, <text>Hi Robert,\n\nthanks for suggestion. This has finally worked! I have generated gcores from two stuck instances and attached them to the case. Hopefully you'll be able to extract ELF Cores and debug the root cause of the soft lockup.\n\nThanks,\nL.</text>, <text>But looking at the kvm process, we might have incomplete gcore, see:\n\nqemu      573779  2.8  3.1 9304248 8296408 ?     Sl   Oct24  20:39 /usr/libexec/qemu-kvm -name guest=instance-000bd507,debug-threads=on -S -object secret,id=masterKey0,format=raw,file=/var/lib/libvirt/qemu/domain-1305-instance-000bd507/master-key.aes -machine pc-i440fx-rhel7.4.0,accel=kvm,usb=off,dump-guest-core=off -cpu Skylake-Client,ss=on,vmx=on,hypervisor=on,tsc_adjust=on,pdpe1gb=on,mpx=off,xsavec=off,xgetbv1=off -m 8192 -realtime mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid 62c47f6f-a2e0-4083-8593-44ea225eea82 -smbios type=1,manufacturer=Red Hat,product=OpenStack Compute,version=14.0.8-2.el7ost,serial=38873f97-1eae-4dc9-a74a-71b078bb59c9,uuid=62c47f6f-a2e0-4083-8593-44ea225eea82,family=Virtual Machine -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-1305-instance-000bd507/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew -global kvm-pit.lost_tick_policy=delay -no-hpet -no-shutdown -boot strict=on -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/nova/instances/62c47f6f-a2e0-4083-8593-44ea225eea82/disk,format=qcow2,if=none,id=drive-virtio-disk0,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=54,id=hostnet0,vhost=on,vhostfd=58 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=fa:16:3e:cd:d5:3b,bus=pci.0,addr=0x3 -add-fd set=2,fd=60 -chardev file,id=charserial0,path=/dev/fdset/2,append=on -device isa-serial,chardev=charserial0,id=serial0 -chardev pty,id=charserial1 -device isa-serial,chardev=charserial1,id=serial1 -device usb-tablet,id=input0,bus=usb.0,port=1 -vnc 172.16.108.51:28 -k en-us -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on\n\n\nThe dump-guest-core is set to off.  :-/ Now we need to find a way to turn this on in config or tell OpenStack to spawn new VM's with dump-guest-core=on.</text>, <text>If I understand it correctly, the kvm options are set in /etc/nova/nova.conf under [libvirt] section?\n\nHowever I cannot find any configuration reference that deals with dump-guest-core. Any suggestions here?</text>, <text>Hello,\n\n  Thanks for the update. I am checking it internally regarding 'dump-guest-core option. I will get back to you with more info on it shortly.\n\nBest Regards,\nSachin</text>, <text>Hello,\n\n\n   I have checked locally in KVM/rhev environment and we need to make following changes in qemu.conf file so that vms will start with "dump-guest-core=on".\n\n\nNOTE: Please DO NOT execute below steps before we get back to you with confirmed steps for openstack environment.\n\n\n- Open /etc/libvirt/qemu.conf\n\n- Search option 'dump_guest_core',\n\n# dump_guest_core = 1\n\nchange it to \n\ndump_guest_core = 1\n\n# restart the "libvirtd" service. (we do not recommend to restart libvirtd service on host while vms are running on it)\n\n# then restart the vm and check.\n\n---\n\nThe above procedure worked in RHEV but need to verify under openstack environment.\n\nI will request stack team to check this once locally and get back to you with final steps to enable 'dump_guest_core option.\n\nBest Regards,\nSachin</text>, <text>Hello,\n\nPlease follow the steps suggested by Sachin in his previous update.\n\nHowever  before restarting libvirtd please shutoff or migrate any running instances as per your requirement.\n\nAfter that perform the changes in /etc/libvirt/qemu.conf and then restart libvirtd and also openstack-nova-compute.service.\n~~~\n# systemctl restart libvirtd ; systemctl restart openstack-nova-compute\n~~~\n\n** Also note the VM in concern needs to be restarted. \n\nBelow is from my test environment, notice dump-guest-core in the qemu-kvm process :\n~~~\n$ nova list\n+--------------------------------------+-------------------+---------+------------+-------------+-------------------------------------+\n| ID                                   | Name              | Status  | Task State | Power State | Networks                            |\n+--------------------------------------+-------------------+---------+------------+-------------+-------------------------------------+\n| 62bd9275-ee0d-4901-81a5-f05692396820 | cirros_instance00 | SHUTOFF | -          | NOSTATE     | private=10.10.10.6, 192.168.200.206 |\n| 2a8d66c6-3749-464d-8396-ceec7afdc7e2 | rhel7             | ACTIVE  | -          | Running     | test2=10.1.2.108, 192.168.200.207   |\n~~~\n# ps aux|grep -i qemu\nqemu        3976  0.6  9.3 1573196 558124 ?      Rl   Oct24  11:22 /usr/libexec/qemu-kvm -name guest=instance-0000001d,debug-threads=on -S -object secret,id=masterKey0,format=raw,file=/var/lib/libvirt/qemu/domain-2-instance-0000001d/master-key.aes -machine pc-i440fx-rhel7.4.0,accel=tcg,usb=off,dump-guest-core=off -m 500 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 2a8d66c6-3749-464d-8396-ceec7afdc7e2 -smbios type=1,manufacturer=Red Hat,product=OpenStack Compute,version=14.0.8-2.el7ost,serial=d3aa9953-d538-4846-9c87-3eda1d785713,uuid=2a8d66c6-3749-464d-8396-ceec7afdc7e2,family=Virtual Machine -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-2-instance-0000001d/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -boot strict=on -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/nova/instances/2a8d66c6-3749-464d-8396-ceec7afdc7e2/disk,format=qcow2,if=none,id=drive-virtio-disk0,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=25,id=hostnet0 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=fa:16:3e:dc:23:fe,bus=pci.0,addr=0x3 -add-fd set=1,fd=28 -chardev file,id=charserial0,path=/dev/fdset/1,append=on -device isa-serial,chardev=charserial0,id=serial0 -chardev pty,id=charserial1 -device isa-serial,chardev=charserial1,id=serial1 -device usb-tablet,id=input0,bus=usb.0,port=1 -vnc 172.20.0.10:0 -k en-us -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on\n~~~\n\nHowever after changing config restarting services does not actually restart the qemu-process :\n~~~\n# systemctl restart libvirtd ; systemctl restart openstack-nova-compute\n[root@overcloud-compute-0 ~]# ps aux|grep -i remu\nroot      238554  0.0  0.0 112660   988 pts/1    S+   11:55   0:00 grep --color=auto -i remu\n[root@overcloud-compute-0 ~]# ps aux|grep -i qemu\n[root@overcloud-compute-0 ~]# ps aux|grep -i qemu\nqemu        3976  0.6  9.4 1573196 561440 ?      Sl   Oct24  11:00 /usr/libexec/qemu-kvm -name guest=instance-0000001d,debug-threads=on -S -object secret,id=masterKey0,format=raw,file=/var/lib/libvirt/qemu/domain-2-instance-0000001d/master-key.aes -machine pc-i440fx-rhel7.4.0,accel=tcg,usb=off,dump-guest-core=off -m 500 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 2a8d66c6-3749-464d-8396-ceec7afdc7e2 -smbios type=1,manufacturer=Red Hat,product=OpenStack Compute,version=14.0.8-2.el7ost,serial=d3aa9953-d538-4846-9c87-3eda1d785713,uuid=2a8d66c6-3749-464d-8396-ceec7afdc7e2,family=Virtual Machine -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-2-instance-0000001d/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -boot strict=on -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/nova/instances/2a8d66c6-3749-464d-8396-ceec7afdc7e2/disk,format=qcow2,if=none,id=drive-virtio-disk0,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=25,id=hostnet0 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=fa:16:3e:dc:23:fe,bus=pci.0,addr=0x3 -add-fd set=1,fd=28 -chardev file,id=charserial0,path=/dev/fdset/1,append=on -device isa-serial,chardev=charserial0,id=serial0 -chardev pty,id=charserial1 -device isa-serial,chardev=charserial1,id=serial1 -device usb-tablet,id=input0,bus=usb.0,port=1 -vnc 172.20.0.10:0 -k en-us -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on\nroot      238562  0.0  0.0 112660   988 pts/1    R+   11:55   0:00 grep --color=auto -i qemu\n~~~\n\nAfter that I rebooted the instance :\n~~~\n nova reboot rhel7\n\nRequest to reboot server &lt;Server: rhel7&gt; has been accepted\n\n# # ps aux|grep -i qemu\nqemu      238788 99.1  6.0 1425248 359728 ?      Rl   11:56   0:16 /usr/libexec/qemu-kvm -name guest=instance-0000001d,debug-threads=on -S -object secret,id=masterKey0,format=raw,file=/var/lib/libvirt/qemu/domain-3-instance-0000001d/master-key.aes -machine pc-i440fx-rhel7.4.0,accel=tcg,usb=off,dump-guest-core=on -m 500 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 2a8d66c6-3749-464d-8396-ceec7afdc7e2 -smbios type=1,manufacturer=Red Hat,product=OpenStack Compute,version=14.0.8-2.el7ost,serial=d3aa9953-d538-4846-9c87-3eda1d785713,uuid=2a8d66c6-3749-464d-8396-ceec7afdc7e2,family=Virtual Machine -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-3-instance-0000001d/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -boot strict=on -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/nova/instances/2a8d66c6-3749-464d-8396-ceec7afdc7e2/disk,format=qcow2,if=none,id=drive-virtio-disk0,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=25,id=hostnet0 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=fa:16:3e:dc:23:fe,bus=pci.0,addr=0x3 -add-fd set=1,fd=28 -chardev file,id=charserial0,path=/dev/fdset/1,append=on -device isa-serial,chardev=charserial0,id=serial0 -chardev pty,id=charserial1 -device isa-serial,chardev=charserial1,id=serial1 -device usb-tablet,id=input0,bus=usb.0,port=1 -vnc 172.20.0.10:0 -k en-us -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on\n\nNow change the difference in dump-guest-core option.\n~~~\n\n\nLet us know if it helps collecting the gcore containing guest memory.\n\n\nBest Regards,\nShatadru B.\nRHCE, RHCVA, RHCA.\nTechnical Support Engg.\nCustomer Experience &amp; Engagement, Red Hat.</text>, <text>Hi Shatadru,\n\nI have already changed libvirtd configuration and restarted the daemon without migrating instances. The restart is safe from my experience and recent restart only confirmed that. The requested core file has been generated, it's being compressed by the red-hat-support tool at the moment and will be sent to you shortly. The dump-guest-core option looks good now. See:\n\n[admin@overcloud-compute-8 ~]$ ps auxwf | grep instance-000c34a1 | grep dump-guest                                                                                                                                 \nqemu      255248 95.5  3.1 9305096 8264548 ?     Sl   09:41  66:10 /usr/libexec/qemu-kvm -name guest=instance-000c34a1,debug-threads=on -S -object secret,id=masterKey0,format=raw,file=/var/lib/libvirt/qemu/domain-2690-instance-000c34a1/master-key.aes -machine pc-i440fx-rhel7.4.0,accel=kvm,usb=off,dump-guest-core=on -cpu Skylake-Client,ss=on,vmx=on,hypervisor=on,tsc_adjust=on,pdpe1gb=on,mpx=off,xsavec=off,xgetbv1=off -m 8192 -realtime mlock=off -smp 4,sockets=4,cores=1,threads=1 -uuid e1d664b7-b07e-4905-a70c-567cb3f73e9e -smbios type=1,manufacturer=Red Hat,product=OpenStack Compute,version=14.0.8-2.el7ost,serial=38873f97-1eae-4dc9-a74a-71b078bb59c9,uuid=e1d664b7-b07e-4905-a70c-567cb3f73e9e,family=Virtual Machine -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-2690-instance-000c34a1/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew -global kvm-pit.lost_tick_policy=delay -no-hpet -no-shutdown -boot strict=on -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/nova/instances/e1d664b7-b07e-4905-a70c-567cb3f73e9e/disk,format=qcow2,if=none,id=drive-virtio-disk0,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=67,id=hostnet0,vhost=on,vhostfd=70 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=fa:16:3e:d0:6b:ef,bus=pci.0,addr=0x3 -add-fd set=2,fd=72 -chardev file,id=charserial0,path=/dev/fdset/2,append=on -device isa-serial,chardev=charserial0,id=serial0 -chardev pty,id=charserial1 -device isa-serial,chardev=charserial1,id=serial1 -device usb-tablet,id=input0,bus=usb.0,port=1 -vnc 172.16.108.48:21 -k en-us -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on\n\nBTW restart of the instance would destroy the data we'd like to dump. So I have changed configuration of the libvirtd, restarted the process, triggered the problem again, generated core file which is now being compressed.</text>, <text>[RHST] The following attachment was uploaded to dropbox.redhat.com:\n\n    01932613-core.255248.gz</text>, <text>One more question - are '-debuginfo' versions of kvm-qemu necessary for extraction of kernel ELF binary?</text>, <text>//Internal; auto-assistant rule reference #: DropboxNotify05.12-0539\n\n    The following files matching case id '01932613' have been found in dropbox.\n\nfilename                                                                                                         size        Attached   type      format     flags\n--------------------------------------------------------------------------------|------------------------------  ----------- ---------- --------- ---------- ----------------------------------\n01932613-core.255248.gz                                                                                           4440775447 No         core      gz         \n                                                                                                                  4440775447                                  \n\n\nFor additional information on this comment and its content see:\n    * "Diag2.0/CEE-AIR DropboxNotify - file notification processing"\n       https://access.redhat.com/articles/3136611</text>, <text>&gt;&gt;One more question - are '-debuginfo' versions of kvm-qemu necessary for extraction of kernel ELF binary?\n\nIt is not required unless we are trying to get the debug info for the kernel mode, also the question is not clear.\n\n\nThanks &amp; Regards,\nNirav Dave\nSoftware Maintenance Engineer (Virtualization)\nGlobal Support Delivery, Red Hat</text>, <text>Hello Ladislav.\n\nFormally the Red Hat Enterprise Linux kernel packages do not include debug symbols, as the symbols are stripped from binary files at build time. GDB needs those debug symbols to assist programmers when  kernel debugging, so the short answer is yes but before I want to be sure in what direction we are going (hopefully not South but North)\n\nhttps://access.redhat.com/blogs/766093/posts/2690881\n\nJust to be in the same page, do you need any assistance on that part? specially for involving our colleague from Virtualization if any specific question if our goal is debbugging the kernel guest level or anything else.\n\nBut most important I want to be sure we are going in the right direction for this ticket, as marked urgent we don't want people trying to jump on earlier conclusions or switching to running headless chicken just because SLA.\n\nIf you are available at irc, let's sync to be sure makes sense keeping the ticket as 24x7, or limiting the collaboration to some specific engineers including one case owner as main speaker.\n\n \n\n\nThanks,\nPablo Caruana\nSoftware Maintenance Engineer\nEnterprise Cloud Support\nRed Hat, Inc.</text>, <text>The kernel was booted with debug symbols that's OK. I already arranged that, but what we don't have is debug symbols for the qemu-kvm-[rhev] RPM  package and I'm not sure if you'll be able to extract kernel info from that core file without debug symbols in qemu-kvm binary.\n\nNo assistance needed. I just need to know if provided core file is sufficient for you. If not, then I'm going to install qemu-kvm-[rhev] -debuginfo package on all compute nodes, restart libvirtd and trigger the problem again.</text>, <text>Pablo just confirmed that kernel with debug symbols should be enough for you. So please analyze 01932613-core.255248.gz. It should have all the information you're looking for. There was an instance booted with debuginfo kernel that has crashed and I created a gtrace file from that. Please confirm that the file is of use for you.\n\nThanks,\nL.</text>, <text>// Internal  //\n\nWas looking at the case\n\nI cant extract the core based on https://access.redhat.com/solutions/2292431 in optimus \n\n$hostname \noptimus.gsslab.rdu2.redhat.com\n\n $pwd\n/cores/exceptions/01948887\n[sbandyop@optimus 01948887]$ ls\n01932613-core.255248  qemu-kvm-rhev-2.9.0-10.el7.x86_64.rpm  usr\n\n$ gdb usr/libexec/qemu-kvm 01932613-core.255248\n\n--- gave error about missing debuginfo ---\nMissing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.5-7.el6_0.x86_64 cyrus-sasl-lib-2.1.23-15.el6_6.2.x86_64 glib2-2.28.8-5.el6.x86_64 keyutils-libs-1.4-5.el6.x86_64 krb5-libs-1.10.3-57.el6.x86_64 libacl-2.2.49-6.el6.x86_64 libaio-0.3.107-10.el6.x86_64 libattr-2.4.44-7.el6.x86_64 libblkid-2.17.2-12.24.el6_8.1.x86_64 libcap-2.16-5.5.el6.x86_64 libcom_err-1.41.12-22.el6.x86_64 libgcc-4.4.7-17.el6.x86_64 libgcrypt-1.4.5-12.el6_8.x86_64 libgpg-error-1.7-4.el6.x86_64 libidn-1.18-2.el6.x86_64 libselinux-2.0.94-7.el6.x86_64 libuuid-2.17.2-12.24.el6_8.1.x86_64 nspr-4.11.0-1.el6.x86_64 nss-softokn-freebl-3.14.3-23.3.el6_8.x86_64 openldap-2.4.40-12.el6.x86_64 util-linux-ng-2.17.2-12.24.el6_8.1.x86_64\n\n--------------------------------------\n\n(gdb) source ./usr/share/qemu-kvm/dump-guest-memory.py\n(gdb) dump-guest-memory vmcore\nusage: dump-guest-memory FILE ARCH\n(gdb) dump-guest-memory vmcore x86_64\nNo valid arch type specified.\nCurrently supported types:\naarch64-be, aarch64-le, X86_64, 386, s390, ppc64-be, ppc64-le\n(gdb) dump-guest-memory vmcore X86_64\nguest RAM blocks:\ntarget_start     target_end       host_addr        message count\n---------------- ---------------- ---------------- ------- -----\nTraceback (most recent call last):\n  File "./usr/share/qemu-kvm/dump-guest-memory.py", line 520, in invoke\n    self.guest_phys_blocks = get_guest_phys_blocks()\n  File "./usr/share/qemu-kvm/dump-guest-memory.py", line 372, in get_guest_phys_blocks\n    current_map_p = gdb.parse_and_eval("address_space_memory.current_map")\nRuntimeError: Attempt to extract a component of a value that is not a structure.\nError occurred in Python command: Attempt to extract a component of a value that is not a structure.\n(gdb) \n(gdb) ./usr/share/qemu-kvm/dump-guest-memory.py^C(gdb) Quit\n(gdb) ^C(gdb) Quit\n\nI cant install these packages in optimus ;)\n\nIts better the vmcore is extracted in the system where it was collected based on https://access.redhat.com/solutions/2292431 and then uploaded in FTP\n\nAlso install all required debuginfo, gdb command will ask to install missing debuginfos</text>, <text>Thanks for the quick response at Red Hat IRC\n\nWe will take a look at the compute 15 from the stack side , so we are no red alerts on that part specially Sar file (the one from  10/24/17  is showing a quiet hypervisor in general).\n\nThe Original trace was showing  XFS codepath but typically 120 seconds is like a standard timeout for IO waiting on device.\n\nFrom your feedback you are not aware of happening with ext4,  as the hypervisor is holding a mix of ceph ephemeral ones.. you are spawning some additional ones to confirm  no external backend cinder, and meaning using  the local storage on compute nodes to isolate conditions.\n\nPreviously with Pablo Iranzo, we were thinking if any virsh issues could be better diagnosed within sbr-virt, but let's keep the general analysis for  01932613-core.255248.gz  also from kernel point of view.\n\n gcore &lt;qemu-kvm PID&gt;\n\nThen we can extract the VM memory from the gcore in order to generate an ELF Core to be used in optimus in order to proceed with the case.\nhttps://access.redhat.com/solutions/2292431\n\nAnd in case the vmcore points to something wrong at the virt layer (which this virsh hanging may indicate), we already have a coredump of the qemu-kvm process.\n\nI will be out until next Monday, just feel free to reach by ticket or IRC if some recurrence or any additional feedback.\n\nKeeping the severity but using a more extended time for the updates. We understand this is being considered at  blocker for some of  your tenant  to migrate from rhos7 to rhos10. Those particular user are feeling blocked and  project management looking for answers (sounds valid business reason). \n\n-edit- As I we have a feedback from the file  01932613-core.255248\n\n$ gdb usr/libexec/qemu-kvm 01932613-core.255248\n\n--- gave error about missing debuginfo ---\nMissing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.5-7.el6_0.x86_64 cyrus-sasl-lib-2.1.23-15.el6_6.2.x86_64 glib2-2.28.8-5.el6.x86_64 keyutils-libs-1.4-5.el6.x86_64 krb5-libs-1.10.3-57.el6.x86_64 libacl-2.2.49-6.el6.x86_64 libaio-0.3.107-10.el6.x86_64 libattr-2.4.44-7.el6.x86_64 libblkid-2.17.2-12.24.el6_8.1.x86_64 libcap-2.16-5.5.el6.x86_64 libcom_err-1.41.12-22.el6.x86_64 libgcc-4.4.7-17.el6.x86_64 libgcrypt-1.4.5-12.el6_8.x86_64 libgpg-error-1.7-4.el6.x86_64 libidn-1.18-2.el6.x86_64 libselinux-2.0.94-7.el6.x86_64 libuuid-2.17.2-12.24.el6_8.1.x86_64 nspr-4.11.0-1.el6.x86_64 nss-softokn-freebl-3.14.3-23.3.el6_8.x86_64 openldap-2.4.40-12.el6.x86_64 util-linux-ng-2.17.2-12.24.el6_8.1.x86_64\n\n--------------------------------------\n\n(gdb) source ./usr/share/qemu-kvm/dump-guest-memory.py\n(gdb) dump-guest-memory vmcore\nusage: dump-guest-memory FILE ARCH\n(gdb) dump-guest-memory vmcore x86_64\nNo valid arch type specified.\nCurrently supported types:\naarch64-be, aarch64-le, X86_64, 386, s390, ppc64-be, ppc64-le\n(gdb) dump-guest-memory vmcore X86_64\nguest RAM blocks:\ntarget_start     target_end       host_addr        message count\n---------------- ---------------- ---------------- ------- -----\nTraceback (most recent call last):\n  File "./usr/share/qemu-kvm/dump-guest-memory.py", line 520, in invoke\n    self.guest_phys_blocks = get_guest_phys_blocks()\n  File "./usr/share/qemu-kvm/dump-guest-memory.py", line 372, in get_guest_phys_blocks\n    current_map_p = gdb.parse_and_eval("address_space_memory.current_map")\nRuntimeError: Attempt to extract a component of a value that is not a structure.\nError occurred in Python command: Attempt to extract a component of a value that is not a structure.\n(gdb) \n(gdb) ./usr/share/qemu-kvm/dump-guest-memory.py^C(gdb) Quit\n(gdb) ^C(gdb) Quit\n\nLadislav, abusing of your patience, when are able to locate that broken instance or similar ones. \nIts  would be great if the vmcore is extracted at the same hypervosr where it was collected based on https://access.redhat.com/solutions/2292431 and then uploaded in FTP\n\nAlso install all required debuginfo, gdb command will ask to install missing debuginfos.\n\nAlso if you can include the virsh dumpxml of the instance, for documenting the devices used (ephemeral)\n\n\n\nThanks,\nPablo Caruana\nSoftware Maintenance Engineer\nEnterprise Cloud Support\nRed Hat, Inc.</text>, <text>Pablo, \n\nthe memory dump is ready and its being compressed by redhat-support-tool ATM. However what I'm not sure is where do you want kernel with debug symbols? From the hypervisor (the compute node) or from the  instance? The troubled instance is running kernel with debug symbols, hypervisor is not. The kernel version on hypervisor is:\n\n[root@overcloud-compute-31 ~]# uname -a\nLinux overcloud-compute-31 3.10.0-693.2.2.el7.x86_64 #1 SMP Sat Sep 9 03:55:24 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux\n\nThe failed instance:\n\n[stack@director ~]$ openstack server show 4bb17528-7075-4274-bab6-707d8c221f8f\n+--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Field                                | Value                                                                                                                                                                    |\n+--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| OS-DCF:diskConfig                    | MANUAL                                                                                                                                                                   |\n| OS-EXT-AZ:availability_zone          | nova                                                                                                                                                                     |\n| OS-EXT-SRV-ATTR:host                 | overcloud-compute-31.localdomain                                                                                                                                         |\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | overcloud-compute-31.localdomain                                                                                                                                         |\n| OS-EXT-SRV-ATTR:instance_name        | instance-000c3ef1                                                                                                                                                        |\n| OS-EXT-STS:power_state               | Running                                                                                                                                                                  |\n| OS-EXT-STS:task_state                | None                                                                                                                                                                     |\n| OS-EXT-STS:vm_state                  | active                                                                                                                                                                   |\n| OS-SRV-USG:launched_at               | 2017-10-25T14:56:45.000000                                                                                                                                               |\n| OS-SRV-USG:terminated_at             | None                                                                                                                                                                     |\n| accessIPv4                           |                                                                                                                                                                          |\n| accessIPv6                           |                                                                                                                                                                          |\n| addresses                            | bxms-qe-jenkins-2=172.16.127.65                                                                                                                                          |\n| config_drive                         |                                                                                                                                                                          |\n| created                              | 2017-10-25T14:56:38Z                                                                                                                                                     |\n| flavor                               | m1.large (431ac1fb-1463-4527-b3d1-79245dd698e1)                                                                                                                          |\n| hostId                               | a31533c1e5cb5a1973993d53b4145962fe0f55fe57687abe6f82dd3d                                                                                                                 |\n| id                                   | 4bb17528-7075-4274-bab6-707d8c221f8f                                                                                                                                     |\n| image                                | bxms-rhel7.4-snapshot-kdump (6e5b77f9-baa7-4675-bbbb-4bd8e8c4800a)                                                                                                       |\n| key_name                             | bxms-qe-jenkins                                                                                                                                                          |\n| name                                 | ci-rhos-rhel7-251                                                                                                                                                        |\n| os-extended-volumes:volumes_attached | []                                                                                                                                                                       |\n| progress                             | 0                                                                                                                                                                        |\n| project_id                           | 8db1c56fc8b94d218c9d13e791cc678f                                                                                                                                         |\n| properties                           | jenkins-cloud-name='ci-rhos', jenkins-instance='https://bxms-qe-jenkins.rhev-ci-vms.eng.rdu2.redhat.com/', jenkins-scope='node:ci-rhos-rhel7-251', jenkins-template-name |\n|                                      | ='ci-rhos-rhel7'                                                                                                                                                         |\n| security_groups                      | [{u'name': u'default'}]                                                                                                                                                  |\n| status                               | ACTIVE                                                                                                                                                                   |\n| updated                              | 2017-10-25T15:13:25Z                                                                                                                                                     |\n| user_id                              | 4cc1cd0299384f42ba970524ef910b1c                                                                                                                                         |\n+--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nAlso attaching console.log from the failed instance.</text>, <text>[RHST] The following attachment was uploaded to dropbox.redhat.com:\n\n    01932613-guest-memory.dump.gz</text>, <text>guest-memory.dump.gz is an output from:\n\n(gdb) dump-guest-memory &lt;output_file&gt;\n\nHopefully you have all the information now.</text>, <text>//Internal; auto-assistant rule reference #: DropboxNotify05.12-0539\n\n    The following files matching case id '01932613' have been found in dropbox.\n\nfilename                                                                                                         size        Attached   type      format     flags\n--------------------------------------------------------------------------------|------------------------------  ----------- ---------- --------- ---------- ----------------------------------\n01932613-guest-memory.dump.gz                                                                                     4200499603 No                   gz         \n                                                                                                                  4200499603                                  \n\n\nFor additional information on this comment and its content see:\n    * "Diag2.0/CEE-AIR DropboxNotify - file notification processing"\n       https://access.redhat.com/articles/3136611</text>, <text>\\\\ Internal \\\\\n\nCustomer converted as detailed in https://access.redhat.com/solutions/2292431 - 01932613-guest-memory.dump.gz\nI tried to load the converted core but this fails. https://optimus.gsslab.rdu2.redhat.com/manager/660867935</text>, <text>Hello Team,\n\nI am Vishal Agrawal from kernel team, I have checked the vmcore file.\n\ncrash&gt; sys\n      KERNEL: /cores/retrace/repos/kernel/x86_64/usr/lib/debug/lib/modules/3.10.0-693.2.2.el7.x86_64.debug/vmlinux\n    DUMPFILE: /cores/retrace/tasks/184687965/crash/vmcore  [PARTIAL DUMP]\n        CPUS: 4 [OFFLINE: 3]\n        DATE: Wed Oct 25 11:12:51 2017\n      UPTIME: 00:16:00\nLOAD AVERAGE: 7.36, 4.30, 1.91\n       TASKS: 297\n    NODENAME: ci-rhos-rhel7-251\n     RELEASE: 3.10.0-693.2.2.el7.x86_64.debug\n     VERSION: #1 SMP Sat Sep 9 04:01:42 EDT 2017\n     MACHINE: x86_64  (2599 Mhz)\n      MEMORY: 8 GB\n       PANIC: "Kernel panic - not syncing: hung_task: blocked tasks"\n\n\ncrash&gt; ps -m|grep UN|sort -n\n[0 00:00:00.093] [UN]  PID: 585    TASK: ffff8800bbbb8000  CPU: 2   COMMAND: "in:imjournal"\n[0 00:00:16.089] [UN]  PID: 586    TASK: ffff88022d614000  CPU: 3   COMMAND: "rs:main Q:Reg"\n[0 00:00:16.092] [UN]  PID: 2451   TASK: ffff88018cadc000  CPU: 3   COMMAND: "(tmpfiles)"\n[0 00:02:54.904] [UN]  PID: 1584   TASK: ffff880232ef0000  CPU: 1   COMMAND: "kworker/1:1"\n[0 00:03:20.551] [UN]  PID: 1754   TASK: ffff8800b6b48000  CPU: 1   COMMAND: "kworker/u8:1"\n[0 00:03:44.165] [UN]  PID: 1224   TASK: ffff88023484c000  CPU: 2   COMMAND: "dockerd-current"\n[0 00:03:45.528] [UN]  PID: 2445   TASK: ffff8800a871c000  CPU: 1   COMMAND: "java"\n[0 00:03:45.709] [UN]  PID: 2448   TASK: ffff880189294000  CPU: 2   COMMAND: "java"\n[0 00:03:45.799] [UN]  PID: 289    TASK: ffff8802310dc000  CPU: 1   COMMAND: "xfsaild/vda1"\ncrash&gt; bt 289\nPID: 289    TASK: ffff8802310dc000  CPU: 1   COMMAND: "xfsaild/vda1"\n #0 [ffff880231203c18] __schedule at ffffffff8177a891\n #1 [ffff880231203c88] schedule at ffffffff8177ae29\n #2 [ffff880231203c98] _xfs_log_force at ffffffffc020d357 [xfs]\n #3 [ffff880231203d10] xfs_log_force at ffffffffc020d47c [xfs]\n #4 [ffff880231203d40] xfsaild_push at ffffffffc021dd43 [xfs]\n #5 [ffff880231203df8] xfsaild at ffffffffc021e18c [xfs]\n #6 [ffff880231203e48] kthread at ffffffff810cb55d\n #7 [ffff880231203f50] ret_from_fork at ffffffff81788c58\ncrash&gt; bt 2448\nPID: 2448   TASK: ffff880189294000  CPU: 2   COMMAND: "java"\n #0 [ffff88018929f848] __schedule at ffffffff8177a891\n #1 [ffff88018929f8b8] schedule at ffffffff8177ae29\n #2 [ffff88018929f8c8] schedule_timeout at ffffffff81776f29\n #3 [ffff88018929f9a0] __down_common at ffffffff8177a11d\n #4 [ffff88018929fa10] __down at ffffffff8177a194\n #5 [ffff88018929fa20] down at ffffffff810d2a11\n #6 [ffff88018929fa40] xfs_buf_lock at ffffffffc01dea7c [xfs]\n #7 [ffff88018929fa68] _xfs_buf_find at ffffffffc01deeb0 [xfs]\n #8 [ffff88018929fac8] xfs_buf_get_map at ffffffffc01df44a [xfs]\n #9 [ffff88018929fb08] xfs_buf_read_map at ffffffffc01e0b40 [xfs]\n#10 [ffff88018929fb50] xfs_trans_read_buf_map at ffffffffc021fb59 [xfs]\n#11 [ffff88018929fb90] xfs_read_agi at ffffffffc01caabb [xfs]\n#12 [ffff88018929fbf0] xfs_ialloc_read_agi at ffffffffc01cac34 [xfs]\n#13 [ffff88018929fc28] xfs_dialloc at ffffffffc01cb888 [xfs]\n#14 [ffff88018929fc98] xfs_ialloc at ffffffffc01f95f1 [xfs]\n#15 [ffff88018929fd10] xfs_dir_ialloc at ffffffffc01f9bc8 [xfs]\n#16 [ffff88018929fda8] xfs_create at ffffffffc01fa5f6 [xfs]\n#17 [ffff88018929fe60] xfs_vn_mknod at ffffffffc01f5ccb [xfs]\n#18 [ffff88018929fec8] xfs_vn_mkdir at ffffffffc01f5e76 [xfs]\n#19 [ffff88018929fed8] vfs_mkdir at ffffffff812606c7\n#20 [ffff88018929ff10] sys_mkdirat at ffffffff8126746a\n#21 [ffff88018929ff70] sys_mkdir at ffffffff812674b9\n#22 [ffff88018929ff80] system_call_fastpath at ffffffff81788d09\n    RIP: 00007fdabc2ac537  RSP: 00007fdaaaa3afc0  RFLAGS: 00000293\n    RAX: 0000000000000053  RBX: ffffffff81788d09  RCX: ffffffffffffffff\n    RDX: 00007fda2cde6aa0  RSI: 00000000000001ff  RDI: 00007fda2cde6aa0\n    RBP: 00007fdaaaa3b050   R8: 00007fda2cde6b5c   R9: 00007fdabc30c087\n    R10: 000000000000000e  R11: 0000000000000206  R12: ffffffff812674b9\n    R13: ffff88018929ff78  R14: 00007fda6827f800  R15: 00007fda6827f9e8\n    ORIG_RAX: 0000000000000053  CS: 0033  SS: 002b\n\nFor further debugging I am moving this to filesystem team as codepath is coming from xfs tree.\n\nThank you,\n\nBest Regards,\nVishal Agrawal.\nGSS Red Hat</text>, <text>[Internal]\n\n\nhttps://optimus.gsslab.rdu2.redhat.com/manager/184687965\n\nvmlinux : 3.10.0-693.2.2.el7.x86_64.debug\n\nretrace-server-interact 184687965 crash\n\n\ncrash&gt; sys\n      KERNEL: /cores/retrace/repos/kernel/x86_64/usr/lib/debug/lib/modules/3.10.0-693.2.2.el7.x86_64.debug/vmlinux\n    DUMPFILE: /cores/retrace/tasks/184687965/crash/vmcore  [PARTIAL DUMP]\n        CPUS: 4 [OFFLINE: 3]\n        DATE: Wed Oct 25 11:12:51 2017\n      UPTIME: 00:16:00\nLOAD AVERAGE: 7.36, 4.30, 1.91\n       TASKS: 297\n    NODENAME: ci-rhos-rhel7-251\n     RELEASE: 3.10.0-693.2.2.el7.x86_64.debug\n     VERSION: #1 SMP Sat Sep 9 04:01:42 EDT 2017\n     MACHINE: x86_64  (2599 Mhz)\n      MEMORY: 8 GB\n       PANIC: "Kernel panic - not syncing: hung_task: blocked tasks"\n\n\ncrash&gt; ps -m|grep UN|sort -n\n[0 00:00:00.093] [UN]  PID: 585    TASK: ffff8800bbbb8000  CPU: 2   COMMAND: "in:imjournal"\n[0 00:00:16.089] [UN]  PID: 586    TASK: ffff88022d614000  CPU: 3   COMMAND: "rs:main Q:Reg"\n[0 00:00:16.092] [UN]  PID: 2451   TASK: ffff88018cadc000  CPU: 3   COMMAND: "(tmpfiles)"\n[0 00:02:54.904] [UN]  PID: 1584   TASK: ffff880232ef0000  CPU: 1   COMMAND: "kworker/1:1"\n[0 00:03:20.551] [UN]  PID: 1754   TASK: ffff8800b6b48000  CPU: 1   COMMAND: "kworker/u8:1"\n[0 00:03:44.165] [UN]  PID: 1224   TASK: ffff88023484c000  CPU: 2   COMMAND: "dockerd-current"\n[0 00:03:45.528] [UN]  PID: 2445   TASK: ffff8800a871c000  CPU: 1   COMMAND: "java"\n[0 00:03:45.709] [UN]  PID: 2448   TASK: ffff880189294000  CPU: 2   COMMAND: "java"\n[0 00:03:45.799] [UN]  PID: 289    TASK: ffff8802310dc000  CPU: 1   COMMAND: "xfsaild/vda1"\ncrash&gt; bt 289\nPID: 289    TASK: ffff8802310dc000  CPU: 1   COMMAND: "xfsaild/vda1"\n #0 [ffff880231203c18] __schedule at ffffffff8177a891\n #1 [ffff880231203c88] schedule at ffffffff8177ae29\n #2 [ffff880231203c98] _xfs_log_force at ffffffffc020d357 [xfs]\n #3 [ffff880231203d10] xfs_log_force at ffffffffc020d47c [xfs]\n #4 [ffff880231203d40] xfsaild_push at ffffffffc021dd43 [xfs]\n #5 [ffff880231203df8] xfsaild at ffffffffc021e18c [xfs]\n #6 [ffff880231203e48] kthread at ffffffff810cb55d\n #7 [ffff880231203f50] ret_from_fork at ffffffff81788c58\ncrash&gt; bt 2448\nPID: 2448   TASK: ffff880189294000  CPU: 2   COMMAND: "java"\n #0 [ffff88018929f848] __schedule at ffffffff8177a891\n #1 [ffff88018929f8b8] schedule at ffffffff8177ae29\n #2 [ffff88018929f8c8] schedule_timeout at ffffffff81776f29\n #3 [ffff88018929f9a0] __down_common at ffffffff8177a11d\n #4 [ffff88018929fa10] __down at ffffffff8177a194\n #5 [ffff88018929fa20] down at ffffffff810d2a11\n #6 [ffff88018929fa40] xfs_buf_lock at ffffffffc01dea7c [xfs]\n #7 [ffff88018929fa68] _xfs_buf_find at ffffffffc01deeb0 [xfs]\n #8 [ffff88018929fac8] xfs_buf_get_map at ffffffffc01df44a [xfs]\n #9 [ffff88018929fb08] xfs_buf_read_map at ffffffffc01e0b40 [xfs]\n#10 [ffff88018929fb50] xfs_trans_read_buf_map at ffffffffc021fb59 [xfs]\n#11 [ffff88018929fb90] xfs_read_agi at ffffffffc01caabb [xfs]\n#12 [ffff88018929fbf0] xfs_ialloc_read_agi at ffffffffc01cac34 [xfs]\n#13 [ffff88018929fc28] xfs_dialloc at ffffffffc01cb888 [xfs]\n#14 [ffff88018929fc98] xfs_ialloc at ffffffffc01f95f1 [xfs]\n#15 [ffff88018929fd10] xfs_dir_ialloc at ffffffffc01f9bc8 [xfs]\n#16 [ffff88018929fda8] xfs_create at ffffffffc01fa5f6 [xfs]\n#17 [ffff88018929fe60] xfs_vn_mknod at ffffffffc01f5ccb [xfs]\n#18 [ffff88018929fec8] xfs_vn_mkdir at ffffffffc01f5e76 [xfs]\n#19 [ffff88018929fed8] vfs_mkdir at ffffffff812606c7\n#20 [ffff88018929ff10] sys_mkdirat at ffffffff8126746a\n#21 [ffff88018929ff70] sys_mkdir at ffffffff812674b9\n#22 [ffff88018929ff80] system_call_fastpath at ffffffff81788d09\n    RIP: 00007fdabc2ac537  RSP: 00007fdaaaa3afc0  RFLAGS: 00000293\n    RAX: 0000000000000053  RBX: ffffffff81788d09  RCX: ffffffffffffffff\n    RDX: 00007fda2cde6aa0  RSI: 00000000000001ff  RDI: 00007fda2cde6aa0\n    RBP: 00007fdaaaa3b050   R8: 00007fda2cde6b5c   R9: 00007fdabc30c087\n    R10: 000000000000000e  R11: 0000000000000206  R12: ffffffff812674b9\n    R13: ffff88018929ff78  R14: 00007fda6827f800  R15: 00007fda6827f9e8\n    ORIG_RAX: 0000000000000053  CS: 0033  SS: 002b</text>, <text>[Internal]\n\nI am removing stack and virt from sbr as case is currently waiting for vmcore analysis from filesystem\n\nPlease feel free to add them back if anything is needed from openstack/virt end.\n\n--Shatadru\n#sbr-stack</text>, <text>Hello,\n\nThank you for your patience.\n\nI have gone through case and I can see that vm is crashing and you have attached vmcore.\n\nNow I am putting this case in collaboration so that one of our senior engineer will do a detail analysis of the vmcore file and update the case with his findings as soon as possible.\n\nPlease note vmcore analysis is a time taking process.\n\nKindly bear with us.\n\n\nRegards,\nAsutosh\nRed Hat Global Support</text>, <text>// internal note\n\ncrash&gt; kmem -i\n                 PAGES        TOTAL      PERCENTAGE\n    TOTAL MEM  1996402       7.6 GB         ----\n         FREE    38223     149.3 MB    1% of TOTAL MEM\n         USED  1958179       7.5 GB   98% of TOTAL MEM\n       SHARED  1109877       4.2 GB   55% of TOTAL MEM\n      BUFFERS        0            0    0% of TOTAL MEM\n       CACHED  1160781       4.4 GB   58% of TOTAL MEM\n         SLAB   152190     594.5 MB    7% of TOTAL MEM\n\n   TOTAL HUGE        0            0         ----\n    HUGE FREE        0            0    0% of TOTAL HUGE\n\n   TOTAL SWAP        0            0         ----\n    SWAP USED        0            0    0% of TOTAL SWAP\n    SWAP FREE        0            0    0% of TOTAL SWAP\n\n COMMIT LIMIT   998201       3.8 GB         ----\n    COMMITTED  1155985       4.4 GB  115% of TOTAL LIMIT\n\n[  960.989651] INFO: task xfsaild/vda1:289 blocked for more than 120 seconds.\n[  960.991768] "echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs" disables this message.\n[  960.992401] xfsaild/vda1    D ffff88023254f000 11736   289      2 0x00000000\n[  960.993034]  ffff880231203c80 0000000000000046 ffff880231203fd8 ffff880231203fd8\n[  960.993680]  ffff880231203fd8 ffff880231203fd8 ffff8802327f8000 ffff8802310dc000\n[  960.994304]  ffff880036995200 0000000000000000 ffff8802310dc000 ffff88023f7ba1a8\n[  960.994979] Call Trace:\n[  960.995196]  [&lt;ffffffff8177ae29&gt;] schedule+0x29/0x70\n[  960.995622]  [&lt;ffffffffc020d357&gt;] _xfs_log_force+0x1d7/0x2d0 [xfs]\n[  960.996132]  [&lt;ffffffff810e4430&gt;] ? wake_up_state+0x20/0x20\n[  960.996593]  [&lt;ffffffffc021dd43&gt;] ? xfsaild_push+0x603/0x970 [xfs]\n[  960.997115]  [&lt;ffffffffc020d47c&gt;] xfs_log_force+0x2c/0x130 [xfs]\n[  960.997613]  [&lt;ffffffffc021e001&gt;] ? xfsaild_push+0x8c1/0x970 [xfs]\n[  960.998176]  [&lt;ffffffffc021dd43&gt;] xfsaild_push+0x603/0x970 [xfs]\n[  960.998671]  [&lt;ffffffff810ac9d0&gt;] ? internal_add_timer+0x70/0x70\n[  960.999171]  [&lt;ffffffffc021e001&gt;] ? xfsaild_push+0x8c1/0x970 [xfs]\n[  960.999713]  [&lt;ffffffffc021e18c&gt;] xfsaild+0xdc/0x280 [xfs]\n[  961.000156]  [&lt;ffffffffc021e0b0&gt;] ? xfsaild_push+0x970/0x970 [xfs]\n[  961.000658]  [&lt;ffffffff810cb55d&gt;] kthread+0xed/0x100\n[  961.001052]  [&lt;ffffffff810cb470&gt;] ? insert_kthread_work+0x80/0x80\n[  961.001555]  [&lt;ffffffff81788c58&gt;] ret_from_fork+0x58/0x90\n[  961.001987]  [&lt;ffffffff810cb470&gt;] ? insert_kthread_work+0x80/0x80\n[  961.002463] no locks held by xfsaild/vda1/289.\n\ncrash&gt; kmem -z|grep HUG\nNR_ANON_TRANSPARENT_HUGEPAGES: 0\nNR_ANON_TRANSPARENT_HUGEPAGES: 325\nNR_ANON_TRANSPARENT_HUGEPAGES: 566\n\ncrash&gt; dev -d\nMAJOR GENDISK            NAME       REQUEST_QUEUE      TOTAL ASYNC  SYNC   DRV\n  253 ffff880232672800   vda        ffff880231518000     428   405    23 N/A(MQ)\n  252 ffff8800361a9000   dm-0       ffff880222a01df0       0     0     0     0\ncrash&gt; sys\n      KERNEL: /cores/retrace/repos/kernel/x86_64/usr/lib/debug/lib/modules/3.10.0-693.2.2.el7.x86_64.debug/vmlinux\n    DUMPFILE: /cores/retrace/tasks/184687965/crash/vmcore  [PARTIAL DUMP]\n        CPUS: 4 [OFFLINE: 3]\n        DATE: Wed Oct 25 11:12:51 2017\n      UPTIME: 00:16:00\nLOAD AVERAGE: 7.36, 4.30, 1.91\n       TASKS: 297\n    NODENAME: ci-rhos-rhel7-251\n     RELEASE: 3.10.0-693.2.2.el7.x86_64.debug\n     VERSION: #1 SMP Sat Sep 9 04:01:42 EDT 2017\n     MACHINE: x86_64  (2599 Mhz)\n      MEMORY: 8 GB\n       PANIC: "Kernel panic - not syncing: hung_task: blocked tasks"\n\ncrash&gt; ps -S\n  RU: 7\n  IN: 281\n  UN: 9\ncrash&gt; ps -m|grep UN\n[0 00:00:00.093] [UN]  PID: 585    TASK: ffff8800bbbb8000  CPU: 2   COMMAND: "in:imjournal"\n[0 00:00:16.089] [UN]  PID: 586    TASK: ffff88022d614000  CPU: 3   COMMAND: "rs:main Q:Reg"\n[0 00:00:16.092] [UN]  PID: 2451   TASK: ffff88018cadc000  CPU: 3   COMMAND: "(tmpfiles)"\n[0 00:02:54.904] [UN]  PID: 1584   TASK: ffff880232ef0000  CPU: 1   COMMAND: "kworker/1:1"\n[0 00:03:20.551] [UN]  PID: 1754   TASK: ffff8800b6b48000  CPU: 1   COMMAND: "kworker/u8:1"\n[0 00:03:44.165] [UN]  PID: 1224   TASK: ffff88023484c000  CPU: 2   COMMAND: "dockerd-current"\n[0 00:03:45.528] [UN]  PID: 2445   TASK: ffff8800a871c000  CPU: 1   COMMAND: "java"\n[0 00:03:45.799] [UN]  PID: 289    TASK: ffff8802310dc000  CPU: 1   COMMAND: "xfsaild/vda1"\n[0 00:03:45.709] [UN]  PID: 2448   TASK: ffff880189294000  CPU: 2   COMMAND: "java"\ncrash&gt; bt 2448\nPID: 2448   TASK: ffff880189294000  CPU: 2   COMMAND: "java"\n #0 [ffff88018929f848] __schedule at ffffffff8177a891\n #1 [ffff88018929f8b8] schedule at ffffffff8177ae29\n #2 [ffff88018929f8c8] schedule_timeout at ffffffff81776f29\n #3 [ffff88018929f9a0] __down_common at ffffffff8177a11d\n #4 [ffff88018929fa10] __down at ffffffff8177a194\n #5 [ffff88018929fa20] down at ffffffff810d2a11\n #6 [ffff88018929fa40] xfs_buf_lock at ffffffffc01dea7c [xfs]\n #7 [ffff88018929fa68] _xfs_buf_find at ffffffffc01deeb0 [xfs]\n #8 [ffff88018929fac8] xfs_buf_get_map at ffffffffc01df44a [xfs]\n #9 [ffff88018929fb08] xfs_buf_read_map at ffffffffc01e0b40 [xfs]\n#10 [ffff88018929fb50] xfs_trans_read_buf_map at ffffffffc021fb59 [xfs]\n#11 [ffff88018929fb90] xfs_read_agi at ffffffffc01caabb [xfs]\n#12 [ffff88018929fbf0] xfs_ialloc_read_agi at ffffffffc01cac34 [xfs]\n#13 [ffff88018929fc28] xfs_dialloc at ffffffffc01cb888 [xfs]\n#14 [ffff88018929fc98] xfs_ialloc at ffffffffc01f95f1 [xfs]\n#15 [ffff88018929fd10] xfs_dir_ialloc at ffffffffc01f9bc8 [xfs]\n#16 [ffff88018929fda8] xfs_create at ffffffffc01fa5f6 [xfs]\n#17 [ffff88018929fe60] xfs_vn_mknod at ffffffffc01f5ccb [xfs]\n#18 [ffff88018929fec8] xfs_vn_mkdir at ffffffffc01f5e76 [xfs]\n#19 [ffff88018929fed8] vfs_mkdir at ffffffff812606c7\n#20 [ffff88018929ff10] sys_mkdirat at ffffffff8126746a\n#21 [ffff88018929ff70] sys_mkdir at ffffffff812674b9\n#22 [ffff88018929ff80] system_call_fastpath at ffffffff81788d09\n    RIP: 00007fdabc2ac537  RSP: 00007fdaaaa3afc0  RFLAGS: 00000293\n    RAX: 0000000000000053  RBX: ffffffff81788d09  RCX: ffffffffffffffff\n    RDX: 00007fda2cde6aa0  RSI: 00000000000001ff  RDI: 00007fda2cde6aa0\n    RBP: 00007fdaaaa3b050   R8: 00007fda2cde6b5c   R9: 00007fdabc30c087\n    R10: 000000000000000e  R11: 0000000000000206  R12: ffffffff812674b9\n    R13: ffff88018929ff78  R14: 00007fda6827f800  R15: 00007fda6827f9e8\n    ORIG_RAX: 0000000000000053  CS: 0033  SS: 002b</text>, <text>Probably storage:\n\ncrash&gt; dev -d\nMAJOR GENDISK            NAME       REQUEST_QUEUE      TOTAL ASYNC  SYNC   DRV\n  253 ffff880232672800   vda        ffff880231518000     428   405    23 N/A(MQ)\n  252 ffff8800361a9000   dm-0       ffff880222a01df0       0     0     0     0\n\ncrash&gt; crashinfo --workqueues\n -----------------------WorkQueues - Active only-----------------------\n   ------xfs-sync/vda1------   &lt;struct workqueue_struct 0xffff8802325f2800&gt;\n  &lt;struct pool_workqueue 0xffffe8ffff801200&gt; active=1 delayed=0\n   &lt;struct worker_pool 0xffff8802371d7180&gt; nr_workers=3 nr_idle=2\n      &lt;struct worker 0xffff88022eec5e80&gt;  kworker/1:1 xfs_log_worker\n   --------writeback--------   &lt;struct workqueue_struct 0xffff88023fd06200&gt;\n  &lt;struct pool_workqueue 0xffff88023fd06400&gt; active=1 delayed=0\n   &lt;struct worker_pool 0xffff88017fc52800&gt; nr_workers=3 nr_idle=2\n      &lt;struct worker 0xffff8800a89daa00&gt;  kworker/u8:1 bdi_writeback_workfn\n       -- Decoding work for func=bdi_writeback_workfn\n          &lt;struct bdi_writeback 0xffff880231518550&gt;\n          &lt;struct backing_dev_info 0xffff880231518270&gt;\n\ncrash&gt; ps -m kworker/u8:1 kworker/1:1\n[0 00:02:54.904] [UN]  PID: 1584   TASK: ffff880232ef0000  CPU: 1   COMMAND: "kworker/1:1"\n[0 00:03:20.551] [UN]  PID: 1754   TASK: ffff8800b6b48000  CPU: 1   COMMAND: "kworker/u8:1"\n\ncrash&gt; ps -m | egrep -ve swapper -e IN\n[0 00:00:00.162] [RU]  PID: 360    TASK: ffff880232494000  CPU: 1   COMMAND: "systemd-journal"\n[0 00:00:00.093] [UN]  PID: 585    TASK: ffff8800bbbb8000  CPU: 2   COMMAND: "in:imjournal"\n[0 00:00:00.238] [RU]  PID: 29     TASK: ffff88023fd08000  CPU: 1   COMMAND: "khungtaskd"\n[0 00:00:00.450] [RU]  PID: 2062   TASK: ffff8801d33fc000  CPU: 1   COMMAND: "java"\n[0 00:00:16.089] [UN]  PID: 586    TASK: ffff88022d614000  CPU: 3   COMMAND: "rs:main Q:Reg"\n[0 00:00:16.092] [UN]  PID: 2451   TASK: ffff88018cadc000  CPU: 3   COMMAND: "(tmpfiles)"\n[0 00:02:54.904] [UN]  PID: 1584   TASK: ffff880232ef0000  CPU: 1   COMMAND: "kworker/1:1"\n[0 00:03:20.551] [UN]  PID: 1754   TASK: ffff8800b6b48000  CPU: 1   COMMAND: "kworker/u8:1"\n[0 00:03:44.165] [UN]  PID: 1224   TASK: ffff88023484c000  CPU: 2   COMMAND: "dockerd-current"\n[0 00:03:45.528] [UN]  PID: 2445   TASK: ffff8800a871c000  CPU: 1   COMMAND: "java"\n[0 00:03:45.799] [UN]  PID: 289    TASK: ffff8802310dc000  CPU: 1   COMMAND: "xfsaild/vda1"\n[0 00:03:45.709] [UN]  PID: 2448   TASK: ffff880189294000  CPU: 2   COMMAND: "java"\n\n6 of these are waiting in io_schedule, including the two processing workqueues:\n\nPID: 1584   TASK: ffff880232ef0000  CPU: 1   COMMAND: "kworker/1:1"\n #0 [ffff880201b77718] __schedule at ffffffff8177a891\n #1 [ffff880201b77788] schedule at ffffffff8177ae29\n #2 [ffff880201b77798] schedule_timeout at ffffffff81776f29\n #3 [ffff880201b77868] io_schedule_timeout at ffffffff8177a3fd\n #4 [ffff880201b77898] io_schedule at ffffffff8177a498\n #5 [ffff880201b778a8] bt_get at ffffffff81377b47\n #6 [ffff880201b77918] blk_mq_get_tag at ffffffff813780e5\n #7 [ffff880201b77940] __blk_mq_alloc_request at ffffffff813722ab\n #8 [ffff880201b77970] blk_mq_map_request at ffffffff81374f0e\n #9 [ffff880201b779e8] blk_sq_make_request at ffffffff81375240\n#10 [ffff880201b77a70] generic_make_request at ffffffff813678df\n#11 [ffff880201b77ae0] submit_bio at ffffffff81367c50\n#12 [ffff880201b77b38] _xfs_buf_ioapply at ffffffffc01dfd63 [xfs]\n#13 [ffff880201b77be8] xfs_buf_submit at ffffffffc01dfff9 [xfs]\n#14 [ffff880201b77c10] xlog_bdstrat at ffffffffc0208e2e [xfs]\n#15 [ffff880201b77c30] xlog_sync at ffffffffc020c0e9 [xfs]\n#16 [ffff880201b77c78] xlog_state_release_iclog at ffffffffc020c37e [xfs]\n#17 [ffff880201b77ca0] _xfs_log_force at ffffffffc020d3c9 [xfs]\n#18 [ffff880201b77d18] xfs_log_force at ffffffffc020d47c [xfs]\n#19 [ffff880201b77d48] xfs_log_worker at ffffffffc020d5a4 [xfs]\n#20 [ffff880201b77d60] process_one_work at ffffffff810c2486\n#21 [ffff880201b77de8] worker_thread at ffffffff810c2aa6\n#22 [ffff880201b77e48] kthread at ffffffff810cb55d\n#23 [ffff880201b77f50] ret_from_fork at ffffffff81788c58\n\nPID: 1754   TASK: ffff8800b6b48000  CPU: 1   COMMAND: "kworker/u8:1"\n #0 [ffff880084ae7428] __schedule at ffffffff8177a891\n #1 [ffff880084ae7498] schedule at ffffffff8177ae29\n #2 [ffff880084ae74a8] schedule_timeout at ffffffff81776f29\n #3 [ffff880084ae7578] io_schedule_timeout at ffffffff8177a3fd\n #4 [ffff880084ae75a8] io_schedule at ffffffff8177a498\n #5 [ffff880084ae75b8] bt_get at ffffffff81377b47\n #6 [ffff880084ae7628] blk_mq_get_tag at ffffffff813780e5\n #7 [ffff880084ae7650] __blk_mq_alloc_request at ffffffff813722ab\n #8 [ffff880084ae7680] blk_mq_map_request at ffffffff81374f0e\n #9 [ffff880084ae76f8] blk_sq_make_request at ffffffff81375240\n#10 [ffff880084ae7780] generic_make_request at ffffffff813678df\n#11 [ffff880084ae77f0] submit_bio at ffffffff81367c50\n#12 [ffff880084ae7848] xfs_add_to_ioend at ffffffffc01d4c5e [xfs]\n#13 [ffff880084ae7880] xfs_do_writepage at ffffffffc01d6e78 [xfs]\n#14 [ffff880084ae7908] write_cache_pages at ffffffff811d1177\n#15 [ffff880084ae7a38] xfs_vm_writepages at ffffffffc01d4ec5 [xfs]\n#16 [ffff880084ae7ab0] do_writepages at ffffffff811d29e1\n#17 [ffff880084ae7ac0] __writeback_single_inode at ffffffff812873f0\n#18 [ffff880084ae7b00] writeback_sb_inodes at ffffffff81287f30\n#19 [ffff880084ae7bc0] __writeback_inodes_wb at ffffffff812882cf\n#20 [ffff880084ae7c08] wb_writeback at ffffffff81288653\n#21 [ffff880084ae7c90] bdi_writeback_workfn at ffffffff81288dc9\n#22 [ffff880084ae7d60] process_one_work at ffffffff810c2486\n#23 [ffff880084ae7de8] worker_thread at ffffffff810c2aa6\n#24 [ffff880084ae7e48] kthread at ffffffff810cb55d\n#25 [ffff880084ae7f50] ret_from_fork at ffffffff81788c58</text>, <text>.</text>, <text>_internal_\n\nsbr-storage, can you please check why we have so many commands on vda?\nIn particular if there are requests standing for a while.\nComment 116 has more details.\n\nI tried to look into it but my storage-fu is weak.\n\nThanks</text>, <text>Whats with the offline CPUS message here\n\n      KERNEL: /cores/retrace/repos/kernel/x86_64/usr/lib/debug/lib/modules/3.10.0-693.2.2.el7.x86_64.debug/vmlinux\n    DUMPFILE: /cores/retrace/tasks/184687965/crash/vmcore  [PARTIAL DUMP]\n        CPUS: 4 [OFFLINE: 3] **** Note\n\nWe do have 4 RUNQ's however\n\ncrash&gt; cd /cores/retrace/tasks/184687965/misc\nWorking directory /cores/retrace/tasks/184687965/misc.\ncrash&gt; runq\nCPU 0 RUNQUEUE: ffff880236fd7d00\n  CURRENT: PID: 0      TASK: ffffffff81af54e0  COMMAND: "swapper/0"\n  RT PRIO_ARRAY: ffff880236fd7ed8\n     [no tasks queued]\n  CFS RB_ROOT: ffff880236fd7df0\n     [no tasks queued]\n\nCPU 1 RUNQUEUE: ffff8802371d7d00\n  CURRENT: PID: 29     TASK: ffff88023fd08000  COMMAND: "khungtaskd"\n  RT PRIO_ARRAY: ffff8802371d7ed8\n     [no tasks queued]\n  CFS RB_ROOT: ffff8802371d7df0\n     [120] PID: 2062   TASK: ffff8801d33fc000  COMMAND: "java"\n     [120] PID: 360    TASK: ffff880232494000  COMMAND: "systemd-journal"\n\nCPU 2 RUNQUEUE: ffff8802373d7d00\n  CURRENT: PID: 0      TASK: ffff88017ce1c000  COMMAND: "swapper/2"\n  RT PRIO_ARRAY: ffff8802373d7ed8\n     [no tasks queued]\n  CFS RB_ROOT: ffff8802373d7df0\n     [no tasks queued]\n\nCPU 3 RUNQUEUE: ffff8802375d7d00\n  CURRENT: PID: 0      TASK: ffff88017ce24000  COMMAND: "swapper/3"\n  RT PRIO_ARRAY: ffff8802375d7ed8\n     [no tasks queued]\n  CFS RB_ROOT: ffff8802375d7df0\n     [no tasks queued]\n\ncrash&gt; dev -d\nMAJOR GENDISK            NAME       REQUEST_QUEUE      TOTAL ASYNC  SYNC   DRV\n  253 ffff880232672800   vda        ffff880231518000     428   405    23 N/A(MQ)    MQ enabled\n  252 ffff8800361a9000   dm-0       ffff880222a01df0       0     0     0     0\n\nWhen was I/O last active to the vda device\n\nstamp = 4295427282, \n    in_flight = {{\n        counter = 21\n      }, {\n        counter = 107\n      }}, \n\njiffies = $3 = 4295627695\ncrash&gt; eval 4295627695-4295427282 \n    decimal: 200413  \nOver 200s ago hence the hungtask\n\nValidate\n\ncrash&gt; \ncrash&gt; virtio_blk_vq 0xffff880231f6b780\nstruct virtio_blk_vq {\n  vq = 0xffff880036b87000, \n ..\n\ncrash&gt; virtqueue 0xffff880036b87000\nstruct virtqueue {\n  list = {\n    next = 0xffff8800369894f0, \n    prev = 0xffff8800369894f0\n  }, \n  callback = 0xffffffffc001f000 &lt;virtblk_done&gt;, \n  name = 0xffff880231f6b7d0 "req.0", \n  vdev = 0xffff880036989000, \n  index = 0, \n  num_free = 1, \n  priv = 0xffffc900013b6000\n}\n\nSo seems the virtual device was blocked here</text>, <text>I will continue research on trying to see why this is blocked.\nThe device state is online</text>, <text>3.10.0-693.2.2.el7.x86_64.debug, was this asked for by us to triage this</text>, <text>INTERNAL \nFor Stan\n\ncrash&gt; dev -d\nMAJOR GENDISK            NAME       REQUEST_QUEUE      TOTAL ASYNC  SYNC   DRV\n  253 ffff880232672800   vda        ffff880231518000     428   405    23 N/A(MQ)\n  252 ffff8800361a9000   dm-0       ffff880222a01df0       0     0     0     0\ncrash&gt; request_queue.queuedata ffff880231518000\n  queuedata = 0xffff880232544a00\n\ncrash&gt; struct virtio_blk 0xffff880232544a00\nstruct virtio_blk {\n..\n.. \n  vqs = 0xffff880231f6b780    Here we go\n}\n\nThen from\nstruct virtio_blk {\n        struct virtio_device *vdev;\n\n        /* The disk structure for the kernel. */\n        struct gendisk *disk;\n\n        /* Block layer tags. */\n        struct blk_mq_tag_set tag_set;\n\n        /* Process context for config space updates */\n        struct work_struct config_work;\n\n        /* What host tells us, plus 2 for header &amp; tailer. */\n        unsigned int sg_elems;\n\n        /* Ida index - used to track minor number allocations. */\n        int index;\n\n        /* num of vqs */\n        int num_vqs;\n        struct virtio_blk_vq *vqs;\n};\n\ncrash&gt; virtio_blk_vq 0xffff880231f6b780\nstruct virtio_blk_vq {\n  vq = 0xffff880036b87000, \n  lock = {\n    {\n      rlock = {\n        raw_lock = {\n          val = {\n            counter = 0\n          }\n        }, \n        magic = 3735899821, \n        owner_cpu = 4294967295, \n        owner = 0xffffffffffffffff, \n        dep_map = {\n          key = 0xffffffffc00224c0 &lt;__key.36266&gt;, \n          class_cache = {0xffffffff82c96550 &lt;lock_classes+385392&gt;, 0x0}, \n          name = 0xffffffffc002108a "&amp;(&amp;vblk-&gt;vqs[i].lock)-&gt;rlock", \n          cpu = 1, \n          ip = 18446744072635938501\n        }\n      }, \n      {\n        __padding = "\\000\\000\\000\\000\\255N\\255\\336\\377\\377\\377\\377\\000\\000\\000\\000\\377\\377\\377\\377\\377\\377\\377\\377", \n        dep_map = {\n          key = 0xffffffffc00224c0 &lt;__key.36266&gt;, \n          class_cache = {0xffffffff82c96550 &lt;lock_classes+385392&gt;, 0x0}, \n          name = 0xffffffffc002108a "&amp;(&amp;vblk-&gt;vqs[i].lock)-&gt;rlock", \n          cpu = 1, \n          ip = 18446744072635938501\n        }\n      }\n    }\n  }, \n  name = "req.0\\000\\000\\000\u0638\\366\\061\\002\\210\\377\\377"\n}\n\nAnd virtqueue comes from here\nstruct virtio_blk_vq {\n        struct virtqueue *vq;\n        spinlock_t lock;\n        char name[VQ_NAME_LEN];\n} ____cacheline_aligned_in_smp;\n\n\nSo \ncrash&gt; virtqueue 0xffff880036b87000\nstruct virtqueue {\n  list = {\n    next = 0xffff8800369894f0, \n    prev = 0xffff8800369894f0\n  }, \n  callback = 0xffffffffc001f000 &lt;virtblk_done&gt;, \n  name = 0xffff880231f6b7d0 "req.0", \n  vdev = 0xffff880036989000, \n  index = 0, \n  num_free = 1, \n  priv = 0xffffc900013b6000\n}</text>, <text>Also then the vdev in block is the virtio_device \n\nThis is the MQ device\n\ncrash&gt; struct virtio_blk 0xffff880232544a00\nstruct virtio_blk {\n  vdev = 0xffff880036989000, \n  disk = 0xffff880232672800, \n  tag_set = {\n    ops = 0xffffffffc0022160 &lt;virtio_mq_ops&gt;, \n    nr_hw_queues = 1, \n    queue_depth = 128, \n    reserved_tags = 0, \n    cmd_size = 5168, \n    numa_node = -1, \n    timeout = 0, \n    flags = 1, \n    driver_data = 0xffff880232544a00, \n    tags = 0xffff88023fdec500, \n    tag_list_lock = {\n      count = {\n        counter = 1\n      }, \n..\n..\n  num_vqs = 1, \n  vqs = 0xffff880231f6b780\n}\n\ncrash&gt; struct virtio_blk_vq 0xffff880231f6b780\n\ncrash&gt; virtqueue 0xffff880036b87000\nstruct virtqueue {\n  list = {\n    next = 0xffff8800369894f0, \n    prev = 0xffff8800369894f0\n  }, \n  callback = 0xffffffffc001f000 &lt;virtblk_done&gt;, \n  name = 0xffff880231f6b7d0 "req.0", \n  vdev = 0xffff880036989000, \n  index = 0, \n  num_free = 1, \n  priv = 0xffffc900013b6000\n}\n\nSo not sure why its hung here, perhaps the hypervisor had issues, have we checked the hypervisor logs at the time</text>, <text>/*\n * Wrapper function for waiting on a wait queue serialised against wakeups\n * by a spinlock. This matches the semantics of all the wait queues used in the\n * log code.\n */\nstatic inline void xlog_wait(wait_queue_head_t *wq, spinlock_t *lock)\n{\n        DECLARE_WAITQUEUE(wait, current);\n\n        add_wait_queue_exclusive(wq, &amp;wait);                    ***** Added to wait queue here\n        __set_current_state(TASK_UNINTERRUPTIBLE);\n        spin_unlock(lock);\n        schedule();                                             ***** Schedule here, and blocked\n        remove_wait_queue(wq, &amp;wait);\n}\n\nPerhaps disable MQ so we disable it also for for virtio_blk and see if this no longer happens.\n\ndm_mod.use_blk_mq=n \n\nWe have ming with us now, will chat with him.\n\nhttp://events.linuxfoundation.org/sites/events/files/slides/virtio-blk_qemu_v0.96.pdf</text>, <text>Emailed Ming Lei to see what he thinks</text>, <text>Good afternoon team,\n\nThese analysis can take a bit of time.  Thank you very much for you patience, we are still working on our analysis and hope to have an update for you soon.  In the mean time, please feel free to update this case with an questions or concerns.\n\nBest regards,\nNathan Weddle\nRed Hat NA Global Support Services</text>, <text>Good morning,\n\nI wanted to add a note to let you know we have transitioned this case back to our North American group, we have senior resources working on this analysis, and are diligently working to complete our analysis.  As always, feel free to update this case, at any time, with questions or concerns regarding this case.\n\nThank you,\nNathan Weddle\nRed Hat NA Global Support Services</text>, <text>Hi, my name is John and I am assisting on this case.  For this issue we have reached out to development engineering and are waiting to hear back.\n\nIf you are still seeing the issue, is it possible for you to try a temporary workaround until we can provide a permanent fix?\n\nCould you try the following parameter in your grub configuration:\n\ndm_mod.use_blk_mq=n \n\nThis will disable multiqueue support at boot time.  The below documentation shows how to implement.\n\nHow to manually modify the boot parameter in grub before the system boots\nhttps://access.redhat.com/solutions/32726\n\nPlease let us know.\n\nBest Regards,\n\nJohn Pittman\nCustomer Engagement and Experience\nRed Hat Inc.</text>, <text>INTERNAL\n\nI was working with Ming Lei this morning.\nHe had me dump some structs from the vmcore for him and he thinks the issue is rather in the Qemu stack.\nAnd not likely in block MQ.\n\n:\n&lt;mingl&gt; np, next time, you can get the qemu log\n&lt;mingl&gt; or just login to qemu console to see what is going on\n&lt;loberman_dbcall&gt; Yes , I have asked for that, its a customer site and difficult to get access\n&lt;mingl&gt; all 128 requests are submitted to vq\n&lt;mingl&gt; but not completed \n&lt;loberman_dbcall&gt; So nothing to do with MQ\n&lt;loberman_dbcall&gt; That was my only concern\n&lt;mingl&gt; from crash log, virtio-blk/mq is fine\n&lt;loberman_dbcall&gt; Awesome\n&lt;loberman_dbcall&gt; Thank you\n&lt;mingl&gt; even in upstream we don't see such kind of issue\n"\n\nWe should ask for the Qemu log for the last hungtask I guess.\n\nThanks\nLaurence</text>, <text>[Internal]\n\nAttempted to track down where in the virtio we are stuck, so far without success. Dumping out important structures\n\nAs was already mentioned earlier, the hold-up is over "vda" device\n\ncrash&gt; dev -d\nMAJOR GENDISK            NAME       REQUEST_QUEUE      TOTAL ASYNC  SYNC   DRV\n  253 ffff880232672800   vda        ffff880231518000     428   405    23 N/A(MQ)\n  252 ffff8800361a9000   dm-0       ffff880222a01df0       0     0     0     0\n\n\ncrash&gt; request_queue.queuedata ffff880231518000\n  queuedata = 0xffff880232544a00\n\n\ncrash&gt; struct virtio_blk 0xffff880232544a00\nstruct virtio_blk {\n  vdev = 0xffff880036989000, \n  disk = 0xffff880232672800, \n  tag_set = {\n    ops = 0xffffffffc0022160 &lt;virtio_mq_ops&gt;, \n    nr_hw_queues = 0x1, \n    queue_depth = 0x80, \n    reserved_tags = 0x0, \n    cmd_size = 0x1430, \n    numa_node = 0xffffffff, \n    timeout = 0x0, \n    flags = 0x1, \n    driver_data = 0xffff880232544a00, \n    tags = 0xffff88023fdec500, \n..\n..\n.. \n  vqs = 0xffff880231f6b780    \n}\n\n\ncrash&gt; virtio_blk_vq 0xffff880231f6b780\nstruct virtio_blk_vq {\n  vq = 0xffff880036b87000, \n ..\n\ncrash&gt; virtqueue 0xffff880036b87000\nstruct virtqueue {\n  list = {\n    next = 0xffff8800369894f0, \n    prev = 0xffff8800369894f0\n  }, \n  callback = 0xffffffffc001f000 &lt;virtblk_done&gt;, \n  name = 0xffff880231f6b7d0 "req.0", \n  vdev = 0xffff880036989000, \n  index = 0, \n  num_free = 1, \n  priv = 0xffffc900013b6000\n}\n\n\n#define to_vvq(_vq) container_of(_vq, struct vring_virtqueue, vq)\n\ncrash&gt; struct -o vring_virtqueue\nstruct vring_virtqueue {\n   [0x0] struct virtqueue vq;\n  [0x38] struct vring vring;\n  [0x58] bool weak_barriers;\n  [0x59] bool broken;\n  [0x5a] bool indirect;\n  [0x5b] bool event;\n  [0x5c] unsigned int free_head;\n  [0x60] unsigned int num_added;\n  [0x64] u16 last_used_idx;\n  [0x68] bool (*notify)(struct virtqueue *);\n  [0x70] bool we_own_ring;\n  [0x78] size_t queue_size_in_bytes;\n  [0x80] dma_addr_t queue_dma_addr;\n  [0x88] struct vring_desc_state desc_state[];\n}\nSIZE: 0x88\n\nstruct vitrqueue is at the start of its container struct vring_virtqueue, so use the same address to display it\n\n\ncrash&gt; struct vring_virtqueue 0xffff880036b87000\nstruct vring_virtqueue {\n  vq = {\n    list = {\n      next = 0xffff8800369894f0, \n      prev = 0xffff8800369894f0\n    }, \n    callback = 0xffffffffc001f000 &lt;virtblk_done&gt;, \n    name = 0xffff880231f6b7d0 "req.0", \n    vdev = 0xffff880036989000,       &lt;--- struct virtio_device\n    index = 0x0, \n    num_free = 0x1, \n    priv = 0xffffc900013b6000\n  }, \n  vring = {\n    num = 0x80,    &lt;-- 128 decimal\n    desc = 0xffff88023279a000,       &lt;-- struct vring_desc\n    avail = 0xffff88023279a800, \n    used = 0xffff88023279a940        &lt;-- struct vring_used\n  }, \n  weak_barriers = 0x1, \n  broken = 0x0, \n  indirect = 0x1, \n  event = 0x1, \n  free_head = 0x39, \n  num_added = 0x0, \n  last_used_idx = 0x42dd, \n  notify = 0xffffffffc0027a20 &lt;vp_notify&gt;, \n  we_own_ring = 0x1, \n  queue_size_in_bytes = 0xd46, \n  queue_dma_addr = 0x23279a000, \n  desc_state = 0xffff880036b87088\n}\n\n\ncrash&gt; struct vring_desc 0xffff88023279a000\nstruct vring_desc {\n  addr = 0x2263237c0, \n  len = 0x30, \n  flags = 0x4, \n  next = 0x34\n}\n\ncrash&gt; struct vring_used 0xffff88023279a940\nstruct vring_used {\n  flags = 0x0, \n  idx = 0x42dd, \n  ring = 0xffff88023279a944\n}\n\n\n\ncrash&gt; struct virtio_device 0xffff880036989000\nstruct virtio_device {\n  index = 0x1, \n  config_enabled = 0x1, \n  config_change_pending = 0x0, \n  ...\n  failed = 0x0, \n  dev = {\n    parent = 0xffff88023fbfa098, \n    p = 0xffff880232539400, \n    kobj = {\n      name = 0xffff88023fd58248 "virtio1",         &lt;---\n      entry = {\n        next = 0xffff880036988870, \n        prev = 0xffff88003698a070\n      }, \n      parent = 0xffff88023fbfa0a8, \n      kset = 0xffff88017cd02e40, \n      ktype = 0xffffffff81c02900 &lt;device_ktype&gt;, \n      sd = 0xffff8802325796e0,                       &lt;--- struct sysfs_dirent\n      kref = {\n        refcount = {\n          counter = 0x5\n        }\n      }, \n      state_initialized = 0x1, \n      state_in_sysfs = 0x1, \n      state_add_uevent_sent = 0x1, \n      state_remove_uevent_sent = 0x0, \n      uevent_suppress = 0x0\n    }, \n...\n  id = {\n    device = 0x2, \n    vendor = 0x1af4\n  }, \n  config = 0xffffffffc0029040 &lt;virtio_pci_config_ops&gt;, \n  vringh_config = 0x0, \n  vqs = {\n    next = 0xffff880036b87000, \n    prev = 0xffff880036b87000\n  }, \n  features = 0x130000e54, \n  priv = 0xffff880232544a00     &lt;--- same as struct virtio_blk above\n}\n\n\nI still cannot see where the hang in virt layer is. SBR-virt pushed thsi case to SBR-Storage, but I have a feeling ultimately it is still in SBR-virt area of expertise.\n\nLaurence shared a link to Ming Lei's material on virtio-blk_qemu at \nhttp://events.linuxfoundation.org/sites/events/files/slides/virtio-blk_qemu_v0.96.pdf\n\nbut there isn't enough detail to spot what's out of kilter in the above structures..</text>, <text>INTERNAL\n\nLast comment from Ming was:\n\n&lt;mingl&gt; virtio-blk is a bit simple, and it is used very common, I used/tested it every day\n&lt;mingl&gt; but the odd thing is that why timeout isn't triggered\n&lt;loberman_dbcall&gt; That is my puzzle, was blocked fr over 200s\n&lt;mingl&gt; I will take a close look at request_queue \n&lt;mingl&gt; you may need to dump the array of blk_mq_ctx in request_queue\n&lt;loberman_dbcall&gt; OK\n&lt;mingl&gt; especially blk_mq_ctx-&gt;rq_list\n&lt;mingl&gt; we need to check if the request is on blk_mq_ctx-&gt;rq_list\n\nI have not had a chance today to continue, asked Stan to see what he could find</text>, <text>[Internal]\n\nAdding SBR Virtualization back to the list of SBRs as we are clearly looking at a hang / lost IO inside the virtio layer.\nPri 1 case should stay visible to the right people.\n\nLeaving SBR-Storage engaged just in case as well.</text>, <text>I don't see any issue within the vmcore either.  It just seems that the I/O has not been answered by the host after requests were pushed to virtio.\n\nThe mq request queue structures showed the tags as fully in use with no tags available. So this wasn't another bug where mq is waiting when it should be returning tags.  From the mq state:\n\ncrash&gt; p ((struct blk_mq_hw_ctx *)0xffff880232673000)-&gt;tags-&gt;rqs\n$62 = (struct request **) 0xffff880232dcf400\n\ncrash&gt; rd 0xffff880232dcf400 128\nffff880232dcf400:  ffff880231520000 ffff880231521600   ..R1......R1....\nffff880232dcf410:  ffff880231522c00 ffff880231524200   .,R1.....BR1....\nffff880232dcf420:  ffff880231525800 ffff880231526e00   .XR1.....nR1....\n...\nffff880232dcf7d0:  ffff8802315d1600 ffff8802315d2c00   ..]1.....,]1....\nffff880232dcf7e0:  ffff8802315d4200 ffff8802315d5800   .B]1.....X]1....\nffff880232dcf7f0:  ffff8802315d6e00 ffff8802315d8400   .n]1......]1....\n\n\nAll slots for tracking the requests matching a tag were non-NULL.  And checking the bitmaps, all bits for the tags were set as used as well:\n\ncrash&gt; p ((struct blk_mq_hw_ctx *)0xffff880232673000)-&gt;tags-&gt;bitmap_tags.map[0]\n$66 = {\n  word = 4294967295, \n  depth = 32\n}\ncrash&gt; p ((struct blk_mq_hw_ctx *)0xffff880232673000)-&gt;tags-&gt;bitmap_tags.map[1]\n$67 = {\n  word = 4294967295, \n  depth = 32\n}\ncrash&gt; p ((struct blk_mq_hw_ctx *)0xffff880232673000)-&gt;tags-&gt;bitmap_tags.map[2]\n$68 = {\n  word = 4294967295, \n  depth = 32\n}\ncrash&gt; p ((struct blk_mq_hw_ctx *)0xffff880232673000)-&gt;tags-&gt;bitmap_tags.map[3]\n$69 = {\n  word = 4294967295, \n  depth = 32\n}\n\n\nChecking when some of the requests were allocated, they were quite old.\n\ncrash&gt; p ((struct blk_mq_hw_ctx *)0xffff880232673000)-&gt;tags-&gt;rqs[0]-&gt;start_time\n$50 = 4295403529\ncrash&gt; p ((struct blk_mq_hw_ctx *)0xffff880232673000)-&gt;tags-&gt;rqs[1]-&gt;start_time\n$51 = 4295401984\ncrash&gt; p ((struct blk_mq_hw_ctx *)0xffff880232673000)-&gt;tags-&gt;rqs[2]-&gt;start_time\n$52 = 4295403529\ncrash&gt; p ((struct blk_mq_hw_ctx *)0xffff880232673000)-&gt;tags-&gt;rqs[3]-&gt;start_time\n$53 = 4295403529\ncrash&gt; p ((struct blk_mq_hw_ctx *)0xffff880232673000)-&gt;tags-&gt;rqs[4]-&gt;start_time\n$54 = 4295403529\n...\n\ncrash&gt; p jiffies\njiffies = $60 = 4295627695\n\n\nThese active requests were over 3 minutes old.  Checking some of virtio_blk's state, I don't see any extra completions waiting in the ring, so there doesn't look to have been anything odd like lost interrupts making the system miss completions which were waiting.\n\n\ncrash&gt; p ((struct vring_virtqueue *)0xffff880036b87000)-&gt;last_used_idx\n$75 = 17117\ncrash&gt; p ((struct vring_virtqueue *)0xffff880036b87000)-&gt;vring.used-&gt;idx\n$76 = 17117\n\n\nIt does all look like the system was stuck waiting for I/O from the host.</text>, <text>Hi John,\n\nthanks for the suggestion. I'll instruct our users to modify the kernel parameter and ask them to watch occurrence of the problem. I have one user in particular with whom I cooperate more closely on this issue and I'll have him tested this on Monday. We found a solid way how to reproduce this. \n\nThanks,\nLadislav</text>, <text>Hello\n\nApologies.\n\nWhile at the time we thought trying without block MQ was a good idea, we no longer believe that to be the case here.\nA good few engineers have looked at this now we all believe we are waiting on the hypervisor/host to complete the I/O's.\n\nPlease disregard our prior request and let us know if you can investigate the host/qemu logs to see if anything on the hypervisor/host suggests why I/O is not completing here.\n\nMany Thanks\nRed Hat</text>, <text>Request Management Escalation: OK, I have enough of this.  I have provided you all the information you need including logs from hypervisors. What exactly do you want from me?</text>, <text>Hello Ladislav\n\nWe are actively working this for you. \nI apologize for what seems to be a runaround here and what may look like a disconnect on our side here.\nThis is not our intention. This happens sometimes when multiple different groups here have engineers investigating the issue in parallel.\n\nWhen the storage team here took the case we actively starte dlooking for a possible issue with the virtio block layer but after multiple efforts we don't see the block layer being the cause.\nWe are working with the virtualization folks to check the logs here.\nPlease give them time to come back to the storage team here with what they find.\n\nMany Thanks\nRed Hat</text>, <text>//collab sbr-virt\nPlease check the Qemu and Hypervisor logs</text>, <text>Hello Ladislav,\n\nMy name is Mike Fiedler, and I am an Escalation Manager for Red Hat's Customer Experience and Engagement Team. I see Laurence has already replied and looking into this further for you.  For future reference, we typically discourage internal Red Hat employees from using the RME button as that is reserved for external customer use. Perhaps reaching out directly to the case owner or key contributors might be a better approach. \n\nMike Fiedler \nEscalation Manager \nRed Hat - Customer Experience &amp; Engagement\nmfiedler@redhat.com</text>, <text>OK, thanks guys. I'll be watching another updates from you. You also need to understand that we have multiple tickets opened for this and it's being watched by many people for quite a long time being updated by me asking people for actions, then revering them.  I think that I have provided you so much information based on various attempts how to get the vmcore that I don't understand why you're asking for more and more. I can provide you all the information you need once again if you want to and if it helps you but asking for logs and then another logs is not a way in my opinion.  I think Pablo has already went through Sar's from compute-15 where the failure was spotted and didn't find anything with blocked I/O but this is already in comments. \n\nI hope that I have explained you why I'm bit annoyed and why I did escalated to you. But OK, next time I'll reach out directly to the case owners.\n\nThanks,\nL.</text>, <text>#sbr-virt request for an updated case summary:\n\n[14:56:57] &lt;loberman&gt; sbr-virt: This needs urgent eyes , went into MGMT escalation 01932613\n[14:56:57] &lt;unifiedbot1&gt; [01932613] VMs crashing with Call Trace : WoRH, WoCollab [2577] [Sev1,FTS,118] [Storage,Virtualization] | https://gss--c.na7.visual.force.com/apex/Case_View?sbstr=01932613 | https://access.redhat.com/support/cases/internal/case/01932613\n[14:57:29] &lt;loberman&gt; Storage looked at it and its seems to be stuck on the hypervisor/host/qemu layer and not in the virtio driver\n[14:59:13] &lt;randy&gt; ooof we are really light on SEG support today as most are sick or out.  ping MKU is at lunch but I will see if she can help out.\n[15:03:32] &lt;mcswizzle&gt; loberman: looks like we need a more complete set of case notes if that's possible. Could we get confirmation of which VMs we're working with since it's not in the description or case summary/action plan?\n[15:03:51] &lt;loberman&gt; mcswizzleL I struggled with the same sir\n[15:04:00] &lt;loberman&gt; came to us as here look at it its torage :)\n[15:09:58] &lt;loberman&gt; mcswizzle: Its Red Hat (the customer) so .... we shoudl be good to nag for what we need\n[15:11:06] &lt;loberman&gt; This came to storage out of left field with not a lot of detail other than the problem statement. I would start by looking at the virt layer, or perhaps this need to go to the Openstack folks\n...\n[15:55:40] &lt;mcswizzle&gt; ping sbr-virt - who, if any SMEs, are good with using crash? asking for this case that was sent to us by stor.age\n[15:56:08] &lt;mcswizzle&gt; loberman: ^ can we get an updated case summary and request for collab? there's no mention of which VMs it is and that's needed to even look into the right VMs qemu logs</text>, <text>Updated case summary as follows:\n---\nCase summary Update\nOctober 27 16:00 (EDT)\nThis case was sent to the storage-sbr because initial review of the guest panic seemed to suggest the virtio_blk driver was having issues.\n\nHowever after careful review it appears the issue may lie in the hypervisor layer, where I/O was shipped up to the Qemu process and never completed.\n\nI have not looked into what has been shared here (other than the vmcore) in the captured logs but the customer has apparently shared all the Hypervisor logs with the case.\n\nWhat this needs is for either sbr-virt or the Openstack team to review the layer that manges the I/O from the guests as its appears thet stopped responding</text>, <text>Within the vmcore\n\n    NODENAME: ci-rhos-rhel7-251\n     RELEASE: 3.10.0-693.2.2.el7.x86_64.debug</text>, <text>//collab\n\nThe storage-sbr did not have a lot of history on this case. We were asked to investigate what appeared to be a block in the virtio block driver and found that the I/O is being sent to the hypervisor and held up there.\n\nWhat we know:\nWe have a vmcore from the guest - ci-rhos-rhel7-251.\nretrace-server-interact 184687965 crash\n\nThe Hypervisor if we go back and look in the notes appears to be Openstack buts its not clear which hypervisor was the one where the vmcore was generated.\nIt appears to be overcloud-compute-31, if the case notes are correct.\n\nThe ask now is look into the logs of overcloud-compute-31, check the Qemu and hypervisor logs for any issues that may show why I/O is not completing here.\n\nThanks\nLaurence</text>, <text>Hi Ladislav,\n\nI was trying to check the sos reports from the host where the VM is running, from virt perspective, but I do not see any libvirt logs and also, in the qemu folder, I do not see anything that matches the VM we were analyzing the coredump from.\n\nWe are looking at coredump from VM ci-rhos-rhel7-251 running on overcloud-compute-15 hypervisor.\nBut under /var/log/libvirt/qemu/ I see only files for VMs named instance-*. Nothing that looks like ci-rhos-rhel7-251.\nAlso - I see no libvirt logs, since they are disabled by default.\n\nI suggest the following action plan:\n1. Enable libvirt logging [1].\n2. Reproduce the issue.\n3. Provide sos reports from the host where the VM is running.\n4. Also - maybe sos-reports from the guest as well, for the full picture.\n\nI am sorry if I misunderstood something, but the case has more then 100 comments by now and this is the information I have gathered from the last comments, and no logs that can help me with what I need to look at next.\n\n\nRegards,\nMarina Kalinin,\nRHEV SSME, CEE, Red Hat Inc.\n\n\n[1] Configure libvirt to debug level and dump into libvirt.log:\nOn /etc/libvirt/libvirtd.conf:\nlog_filters="1:libvirt 1:event 1:json 1:util 1:qemu 1:remote"\nlog_outputs="1:file:/var/log/libvirt/libvirtd.log"\n# systemctl restart libvirtd \nNote: I am not familiar with OpenStack environment and I am not sure how it will react when we restart libvirtd with running VMs. Maybe some maintenance is required for the hypervisor first?</text>, <text>Hi,\n\nWe haven't heard from you in nearly 24 hours, so I'm going to remove the 24x7 flag from this case. If the issue continues and you need continued assistance outside of normal operating hours, please call in to the support line and let us know to re-instate 24x7 so that we can resume assisting you. Otherwise, we can continue this Monday.\n\nThanks,\nAllan Voss, RHCE, RHCVA\nSenior Technical Support Engineer\nRed Hat, Inc.\n\nIf your systems are registered on Red Hat Network (RHN), visit https://access.redhat.com/products/red-hat-subscription-management/  to learn why you need to migrate to the Red Hat Subscription Management (RHSM) interface by July 2017.\n\nRHEV 3.x series was retired on September 30, 2017:\nhttps://access.redhat.com/support/policy/updates/rhev\n\nPreparing to upgrade? Try the RHEV Upgrade Helper tool:\nhttps://access.redhat.com/labs/rhevupgradehelper/\n\nHave you configured regular database backups? \nhttps://access.redhat.com/solutions/61993  (RHEV 3.0 to 3.2)\nhttps://access.redhat.com/solutions/797463 (RHEV 3.3 to 3.6)\n\nIf you're using Self Hosted Engine, perform you database backups as per the Hosted Engine Guide:\nhttps://access.redhat.com/documentation/en-us/red_hat_virtualization/4.1/html/self-hosted_engine_guide/chap-backing_up_and_restoring_a_rhel-based_self-hosted_environment</text>, <text>So, attempting to get complete picture now:\n\n[stack@director ~]$ openstack server show 048dac93-eea6-4150-b4db-85bb336c38ef                           \n+--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Field                                | Value                                                                                                                                                                    |\n+--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| OS-DCF:diskConfig                    | MANUAL                                                                                                                                                                   |\n| OS-EXT-AZ:availability_zone          | nova                                                                                                                                                                     |\n| OS-EXT-SRV-ATTR:host                 | overcloud-compute-27.localdomain                                                                                                                                         |\n| OS-EXT-SRV-ATTR:hypervisor_hostname  | overcloud-compute-27.localdomain                                                                                                                                         |\n| OS-EXT-SRV-ATTR:instance_name        | instance-000d58bb                                                                                                                                                        |\n| OS-EXT-STS:power_state               | Running                                                                                                                                                                  |\n| OS-EXT-STS:task_state                | None                                                                                                                                                                     |\n| OS-EXT-STS:vm_state                  | active                                                                                                                                                                   |\n| OS-SRV-USG:launched_at               | 2017-10-30T10:09:01.000000                                                                                                                                               |\n| OS-SRV-USG:terminated_at             | None                                                                                                                                                                     |\n| accessIPv4                           |                                                                                                                                                                          |\n| accessIPv6                           |                                                                                                                                                                          |\n| addresses                            | bxms-qe-jenkins-2=172.16.127.81                                                                                                                                          |\n| config_drive                         |                                                                                                                                                                          |\n| created                              | 2017-10-30T10:08:55Z                                                                                                                                                     |\n| flavor                               | m1.large (431ac1fb-1463-4527-b3d1-79245dd698e1)                                                                                                                          |\n| hostId                               | bbab96d7c503c294cc611c52e2030bf77884793b481d872b3faa1dff                                                                                                                 |\n| id                                   | 048dac93-eea6-4150-b4db-85bb336c38ef                                                                                                                                     |\n| image                                | bxms-rhel7.4-snapshot-kdump (6e5b77f9-baa7-4675-bbbb-4bd8e8c4800a)                                                                                                       |\n| key_name                             | bxms-qe-jenkins                                                                                                                                                          |\n| name                                 | ci-rhos-rhel7-4489                                                                                                                                                       |\n| os-extended-volumes:volumes_attached | []                                                                                                                                                                       |\n| progress                             | 0                                                                                                                                                                        |\n| project_id                           | 8db1c56fc8b94d218c9d13e791cc678f                                                                                                                                         |\n| properties                           | jenkins-cloud-name='ci-rhos', jenkins-instance='https://bxms-qe-jenkins.rhev-ci-vms.eng.rdu2.redhat.com/', jenkins-scope='node:ci-rhos-rhel7-4489', jenkins-template-    |\n|                                      | name='ci-rhos-rhel7'                                                                                                                                                     |\n| security_groups                      | [{u'name': u'default'}]                                                                                                                                                  |\n| status                               | ACTIVE                                                                                                                                                                   |\n| updated                              | 2017-10-30T10:49:23Z                                                                                                                                                     |\n| user_id                              | 4cc1cd0299384f42ba970524ef910b1c                                                                                                                                         |\n+--------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n/var/log/libvirt/libvirtd.log is compressing at the moment, you'll have it shortly. The same for fresh elf_core. I'm not sure if I'll be able to get you sosreport from crashed machine. I might attempt to force reboot it and get the sosreport then.</text>, <text>//Internal; auto-assistant rule reference #: DropboxNotify05.12-0558\n\n    The following files matching case id '01932613' have been found in dropbox.\n\nfilename                                                                                                         size        Attached   type      format     flags\n--------------------------------------------------------------------------------|------------------------------  ----------- ---------- --------- ---------- ----------------------------------\n01932613-libvirtd.log.gz                                                                                          2151534701 No                   log.gz     \n                                                                                                                  2151534701                                  \n\n\nFor additional information on this comment and its content see:\n    * "Diag2.0/CEE-AIR DropboxNotify - file notification processing"\n       https://access.redhat.com/articles/3136611</text>, <text>[RHST] The following attachment was uploaded to dropbox.redhat.com:\n\n    01932613-elf_core.xz</text>, <text>Unfortunately I wasn't able to get sosreport from the VM. The provisioner has deleted it when I attempted to hard reboot it. Let me know if its a blocker for you.</text>, <text>//Internal; auto-assistant rule reference #: DropboxNotify05.12-0558\n\n    The following files matching case id '01932613' have been found in dropbox.\n\nfilename                                                                                                         size        Attached   type      format     flags\n--------------------------------------------------------------------------------|------------------------------  ----------- ---------- --------- ---------- ----------------------------------\n01932613-elf_core.xz                                                                                              4009730420 No         core      xz         \n                                                                                                                  4009730420                                  \n\n\nFor additional information on this comment and its content see:\n    * "Diag2.0/CEE-AIR DropboxNotify - file notification processing"\n       https://access.redhat.com/articles/3136611</text>, <text>Hello,\n\nThank you for the update.\n\nI have gone through the vmcore file but I could not able to extract it and got below error.\n\n 2017-10-30 23:28:27 Unable to find cached vmlinux at path: /cores/retrace/repos/kernel/x86_64/usr/lib/debug/lib/modules/1.10.2-3.el7_4.1.x86_64/vmlinux\n    2017-10-30 23:28:27 Searching for kernel-debuginfo package for 1.10.2-3.el7_4.1.x86_64\n   2017-10-30 23:28:27 Parsing kernel version '1.10.2-3.el7_4.1.x86_64'\n   2017-10-30 23:28:27 Version: '1.10.2'; Release: '3.el7_4.1'; Arch: 'x86_64'; Flavour: 'None'; Realtime: False\n   2017-10-30 23:28:27 Trying debuginfo file: /cores/retrace/repos/kernel/Packages/kernel-debuginfo-1.10.2-3.el7_4.1.x86_64.rpm\n   2017-10-30 23:28:27 Trying debuginfo file: /cores/retrace/repos/kernel/kernel-debuginfo-1.10.2-3.el7_4.1.x86_64.rpm\n   2017-10-30 23:28:27 Trying debuginfo file: /cores/retrace/repos/download/Packages/kernel-debuginfo-1.10.2-3.el7_4.1.x86_64.rpm\n   2017-10-30 23:28:27 Trying debuginfo file: /cores/retrace/repos/download/kernel-debuginfo-1.10.2-3.el7_4.1.x86_64.rpm\n   2017-10-30 23:28:27 Trying debuginfo file: /mnt/brewroot/packages/kernel/1.10.2/3.el7_4.1/x86_64/kernel-debuginfo-1.10.2-3.el7_4.1.x86_64.rpm\n   2017-10-30 23:28:27 Trying debuginfo URL: http://download.devel.redhat.com/brewroot/packages/kernel/1.10.2/3.el7_4.1/x86_64/kernel-debuginfo-1.10.2-3.el7_4.1.x86_64.rpm\n   2017-10-30 23:28:27 prepare_debuginfo failed: Unable to find debuginfo package and no cached vmlinux file\n\nCould you please check once the vmcore file if it completed or is this the right one.\n\nNow regarding the livvert logs I will engage virtualization team to check, in the mean time can you check the vmcore file.\n\nRegards,\nAsutosh\nRed Hat Global Support</text>, <text>///Collab Note///\n\nAs per the last update from the #virt team they needed the libvirt logs to troubleshoot the issue further.\n\nCustomer attached the log file, can someone please check the logs. Also I am moving this case to #sbr-virt and if there will be anything require from #storage end please revert back.\n\nRegards,\nAsutosh\nRed Hat Global Support</text>, <text>I think you're missing the kernel debuginfo package. \n\nBut guys, please. Do I have to really teach you what you are doing wrong and where to get packages?? I'm really tired of this. Use https://brewweb.engineering.redhat.com/brew/.</text>, <text>Request Management Escalation: Hi,\nThis ticket has been open for a long time, we have provided all logs and access to the environment possible to troubleshoot the issue, however, there only been requests for more logs than workaround or solutions. We believe the issue to be with RHEL and not Openstack and need someone senior to investigate asap. This issue is affecting atleast 5 tenants on our environment and blocking us to finish a long running project to get everyone migrated to RHOS 10 system.</text>, <text>Working on the RME</text>, <text>Hello Mohan,\n\nI have acknowledged the Management Escalation that you requested. Thank you for bringing it to my attention. I had tried calling you on the number listed on the account (1-919-754-4950) but the call went unanswered. \n\nSergio is actively working on the ticket and is analyzing the logs. I understand the delay caused, but you can expect a response in around 2 hours. We appreciate your patience.\n\nSergio is one of our best resource and I am sure he would assist you with the needful.\n\nShould you have any further questions or concerns please don't hesitate to update the ticket and we would be glad to assist.\n\nBelow is a link to the support lines numbers in your area.\n\nhttps://access.redhat.com/support/contact/technicalSupport.html\n\nRegards,\nHimadri.\nEscalation Manager.\nRed Hat - Customer Experience &amp; Engagement.</text>, <text>Hello,\n\nMy name is Sergio Lopez, I'm a Software Maintenance Engineer for Virtualization, and I'll be helping in the analysis of this issue.\n\nI've been working on the analysis of a previous full gcore (01932613-core.255248.gz), as it contains both Guest and QEMU states. From Guest's perspective, the symptoms are the same as my colleagues already pointed out in the previous vmcore analysis, with the kernel waiting for completion of some I/Os that never seem to finish. In fact, the virtqueue vring is full with pending requests:\n\ncrash&gt; p *(struct vring_virtqueue *) 0xffff880232f74000\n$9 = {\n  vq = {\n    list = {\n      next = 0xffff880232f95cf0, \n      prev = 0xffff880232f95cf0\n    }, \n    callback = 0xffffffffc00b1000 &lt;virtblk_done&gt;, \n    name = 0xffff880231ba2750 "req.0", \n    vdev = 0xffff880232f95800, \n    index = 0, \n    num_free = 0,                                        &lt;-- no free slots in the vq\n    priv = 0xffffc900013d8000\n  }, \n  vring = {\n    num = 128, \n    desc = 0xffff88003616a000, \n    avail = 0xffff88003616a800, \n    used = 0xffff88003616a940\n  }, \n  weak_barriers = true, \n  broken = false, \n  indirect = true, \n  event = true, \n  free_head = 0, \n  num_added = 0, \n  last_used_idx = 42953, \n  notify = 0xffffffffc0020a20 &lt;vp_notify&gt;, \n  we_own_ring = true, \n  queue_size_in_bytes = 3398, \n  queue_dma_addr = 907452416, \n  desc_state = 0xffff880232f74088\n}\n\nNow let's take a look at this from QEMU's perspective:\n\n - There's nothing unusual on the thread backtraces, with 4 of them backing vCPUs, 1 main_loop, 1 RCU worker and 1 VNC worker:\n\nThread 7 (Thread 0x7f3b61237d00 (LWP 255248)):\n#0  0x00007f3b487c2aff in __GI_ppoll (fds=0x55eb83bf5300, nfds=8, timeout=&lt;optimized out&gt;, \n    timeout@entry=0x7ffd7f4bae50, sigmask=sigmask@entry=0x0) at ../sysdeps/unix/sysv/linux/ppoll.c:56\n#1  0x000055eb81f003d9 in ppoll (__ss=0x0, __timeout=0x7ffd7f4bae50, __nfds=&lt;optimized out&gt;, \n    __fds=&lt;optimized out&gt;) at /usr/include/bits/poll2.h:77\n#2  qemu_poll_ns (fds=&lt;optimized out&gt;, nfds=&lt;optimized out&gt;, timeout=timeout@entry=999000000)\n    at util/qemu-timer.c:334\n#3  0x000055eb81f011e8 in os_host_main_loop_wait (timeout=999000000) at util/main-loop.c:255\n#4  main_loop_wait (nonblocking=nonblocking@entry=0) at util/main-loop.c:517\n#5  0x000055eb81bf2c2c in main_loop () at vl.c:1909\n#6  main (argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, envp=&lt;optimized out&gt;) at vl.c:4733\n\nThread 6 (Thread 0x7f393bbff700 (LWP 255281)):\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\n#1  0x000055eb81f040f9 in qemu_cond_wait (cond=cond@entry=0x55eb83bf4c00, \n    mutex=mutex@entry=0x55eb83bf4c30) at util/qemu-thread-posix.c:133\n#2  0x000055eb81e423fb in vnc_worker_thread_loop (queue=queue@entry=0x55eb83bf4c00) at ui/vnc-jobs.c:205\n#3  0x000055eb81e42938 in vnc_worker_thread (arg=0x55eb83bf4c00) at ui/vnc-jobs.c:312\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f393bbff700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\nThread 5 (Thread 0x7f3b3de6c700 (LWP 255279)):\n#0  0x00007f3b487c4107 in ioctl () at ../sysdeps/unix/syscall-template.S:81\n#1  0x000055eb81c42fa5 in kvm_vcpu_ioctl (cpu=cpu@entry=0x55eb84116000, type=type@entry=44672)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2178\n#2  0x000055eb81c4306b in kvm_cpu_exec (cpu=cpu@entry=0x55eb84116000)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2016\n#3  0x000055eb81c30432 in qemu_kvm_cpu_thread_fn (arg=0x55eb84116000)\n    at /usr/src/debug/qemu-2.9.0/cpus.c:1118\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f3b3de6c700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\nThread 4 (Thread 0x7f3b3e66d700 (LWP 255278)):\n#0  0x00007f3b487c4107 in ioctl () at ../sysdeps/unix/syscall-template.S:81\n#1  0x000055eb81c42fa5 in kvm_vcpu_ioctl (cpu=cpu@entry=0x55eb840f6000, type=type@entry=44672)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2178\n#2  0x000055eb81c4306b in kvm_cpu_exec (cpu=cpu@entry=0x55eb840f6000)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2016\n#3  0x000055eb81c30432 in qemu_kvm_cpu_thread_fn (arg=0x55eb840f6000)\n    at /usr/src/debug/qemu-2.9.0/cpus.c:1118\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f3b3e66d700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\nThread 3 (Thread 0x7f3b3ee6e700 (LWP 255277)):\n#0  0x00007f3b487c4107 in ioctl () at ../sysdeps/unix/syscall-template.S:81\n#1  0x000055eb81c42fa5 in kvm_vcpu_ioctl (cpu=cpu@entry=0x55eb840d6000, type=type@entry=44672)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2178\n#2  0x000055eb81c4306b in kvm_cpu_exec (cpu=cpu@entry=0x55eb840d6000)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2016\n#3  0x000055eb81c30432 in qemu_kvm_cpu_thread_fn (arg=0x55eb840d6000)\n    at /usr/src/debug/qemu-2.9.0/cpus.c:1118\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f3b3ee6e700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\nThread 2 (Thread 0x7f3b3f66f700 (LWP 255276)):\n#0  0x00007f3b487c4107 in ioctl () at ../sysdeps/unix/syscall-template.S:81\n#1  0x000055eb81c42fa5 in kvm_vcpu_ioctl (cpu=cpu@entry=0x55eb8405c000, type=type@entry=44672)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2178\n#2  0x000055eb81c4306b in kvm_cpu_exec (cpu=cpu@entry=0x55eb8405c000)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2016\n#3  0x000055eb81c30432 in qemu_kvm_cpu_thread_fn (arg=0x55eb8405c000)\n    at /usr/src/debug/qemu-2.9.0/cpus.c:1118\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f3b3f66f700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\nThread 1 (Thread 0x7f3b40e8a700 (LWP 255272)):\n#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\n#1  0x000055eb81f04406 in qemu_futex_wait (val=&lt;optimized out&gt;, f=&lt;optimized out&gt;)\n    at /usr/src/debug/qemu-2.9.0/include/qemu/futex.h:26\n#2  qemu_event_wait (ev=ev@entry=0x55eb82917784 &lt;rcu_call_ready_event&gt;) at util/qemu-thread-posix.c:399\n#3  0x000055eb81f13a2e in call_rcu_thread (opaque=&lt;optimized out&gt;) at util/rcu.c:249\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f3b40e8a700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\n - The information of the VQ matches what we've seen in the Guest's kernel, we have 128 slots, and all of them are in use:\n\n(gdb) p ((VirtIODevice *)0x55eb85adc510)-&gt;vq[0]\n$126 = {\n  vring = {\n    num = 128,                    &lt;-- 128 slots in total\n    num_default = 128, \n    align = 4096, \n    desc = 907452416, \n    avail = 907454464, \n    used = 907454784, \n    caches = 0x55eb86ad8d80\n  }, \n  last_avail_idx = 43081, \n  shadow_avail_idx = 43081, \n  used_idx = 42953, \n  signalled_used = 42953, \n  signalled_used_valid = true, \n  notification = true, \n  queue_index = 0, \n  inuse = 128,                     &lt;-- 128 slots in use\n  vector = 1, \n  handle_output = 0x55eb81c5e320 &lt;virtio_blk_handle_output&gt;, \n  handle_aio_output = 0x55eb81c5e440 &lt;virtio_blk_data_plane_handle_output&gt;, \n  vdev = 0x55eb85adc510, \n  guest_notifier = {\n    rfd = 30, \n    wfd = 30\n  }, \n  host_notifier = {\n    rfd = 31, \n    wfd = 31\n  }, \n  node = {\n    le_next = 0x0, \n    le_prev = 0x55eb85245e48\n  }\n}\n\nIt's also interesting seeing this VQ corresponds to a virtio-blk-pci which uses ioeventfd. In the structure above we can see the file descriptors corresponding to guest (fd=30) and host (fd=31) notifications.\n\n - The activity on file descriptors is monitored by the main loop (if no iothreads are configured, as it happens in this case), using qemu_poll_ns and gpollfds, with the latter containing information about the FDs to poll. Let's take a look at it:\n\n(gdb) p gpollfds-&gt;len\n$111 = 8\n(gdb) p ((GPollFD *)gpollfds-&gt;data)[0]\n$113 = {fd = -1, events = 1, revents = 0}\n(gdb) p ((GPollFD *)gpollfds-&gt;data)[1]\n$114 = {fd = 4, events = 1, revents = 0}\n(gdb) p ((GPollFD *)gpollfds-&gt;data)[2]\n$115 = {fd = 6, events = 1, revents = 0}\n(gdb) p ((GPollFD *)gpollfds-&gt;data)[3]\n$116 = {fd = 8, events = 1, revents = 0}\n(gdb) p ((GPollFD *)gpollfds-&gt;data)[4]\n$117 = {fd = 9, events = 1, revents = 0}\n(gdb) p ((GPollFD *)gpollfds-&gt;data)[5]\n$118 = {fd = 24, events = 1, revents = 0}\n(gdb) p ((GPollFD *)gpollfds-&gt;data)[6]\n$119 = {fd = 25, events = 1, revents = 0}\n(gdb) p ((GPollFD *)gpollfds-&gt;data)[7]\n$120 = {fd = 31, events = 1, revents = 0}\n\nTwo things are worth noting here:\n\n 1. We have an invalid file descriptor (fd = -1).\n\n 2. The file descriptor corresponding to the guest_notifier (fd = 30) is not being polled.\n\nI don't know yet if those circumstances are indeed indicators of a bug, or can appear sometimes under normal conditions. But if the guest_notifier is not being polled, QEMU probably won't be able to signal to the Guest the completion of the I/Os, just as we're apparently seeing from Guest's perspective.\n\n&lt;snip&gt;\nstatic void virtio_queue_guest_notifier_read(EventNotifier *n)\n{\n    VirtQueue *vq = container_of(n, VirtQueue, guest_notifier);\n    if (event_notifier_test_and_clear(n)) {\n        virtio_irq(vq);\n    }\n}\n\nvoid virtio_queue_set_guest_notifier_fd_handler(VirtQueue *vq, bool assign,\n                                                bool with_irqfd)\n{\n    if (assign &amp;&amp; !with_irqfd) {\n        event_notifier_set_handler(&amp;vq-&gt;guest_notifier,\n                                   virtio_queue_guest_notifier_read);\n    } else {\n        event_notifier_set_handler(&amp;vq-&gt;guest_notifier, NULL);\n    }\n    if (!assign) {\n        /* Test and clear notifier before closing it,\n         * in case poll callback didn't have time to run. */\n        virtio_queue_guest_notifier_read(&amp;vq-&gt;guest_notifier);\n    }\n}\n&lt;/snip&gt;\n\nI'm going to open an internal bug report with all this information and, in parallel, I'll try to reproduce this issue in the lab.\n\nOn the other hand, it's possible that disabling ioeventfd would work around the issue for the time being. I'm going to check with my colleagues of OpenStack if this is possible and supported with NOVA.\n\nSergio.</text>, <text>### INTERNAL ###\n\nVM is using irqfd, so the fact that the guest_notifier FD is not present in gpollfds is potentially normal and expected:\n\n(gdb) p ((VirtIOPCIProxy *)0x55eb85ad4000)-&gt;vector_irqfd\n$148 = (VirtIOIRQFD *) 0x55eb8606b740\n(gdb) p *((VirtIOPCIProxy *)0x55eb85ad4000)-&gt;vector_irqfd\n$149 = {\n  msg = {\n    address = 0, \n    data = 0\n  }, \n  virq = 0, \n  users = 0\n}\n\nSergio.</text>, <text>### INTERNAL ###\n\nIn the BlockDriverState structure from virtio-blk-pci, we can see 381 (127*3?) "in_flight" requests:\n\n(gdb) p *((VirtIOBlock *)0x55eb85adc510)-&gt;blk-&gt;root-&gt;bs\n$161 = {\n  open_flags = 8226, \n  read_only = false, \n  encrypted = false, \n  valid_key = false, \n  sg = false, \n  probed = false, \n  drv = 0x55eb824ae9a0 &lt;bdrv_qcow2&gt;, \n  opaque = 0x55eb83bfa1a0, \n  aio_context = 0x55eb83bf9700, \n  aio_notifiers = {\n    lh_first = 0x0\n  }, \n  walking_aio_notifiers = false, \n  filename = "/var/lib/nova/instances/e1d664b7-b07e-4905-a70c-567cb3f73e9e/disk", '\\000' &lt;repeats 4030 times&gt;, \n  backing_file = "/var/lib/nova/instances/_base/6318d40bb249611c2e7e0a16b337970981bd5fbe", '\\000' &lt;repeats 4025 times&gt;, \n  backing_format = "raw", '\\000' &lt;repeats 12 times&gt;, \n  full_open_options = 0x55eb8403f200, \n  exact_filename = "/var/lib/nova/instances/e1d664b7-b07e-4905-a70c-567cb3f73e9e/disk", '\\000' &lt;repeats 4030 times&gt;, \n  backing = 0x55eb83bc3a40, \n  file = 0x55eb83bc3ae0, \n  bl = {\n    request_alignment = 1, \n    max_pdiscard = 0, \n    pdiscard_alignment = 65536, \n    max_pwrite_zeroes = 0, \n    pwrite_zeroes_alignment = 65536, \n    opt_transfer = 0, \n    max_transfer = 0, \n    min_mem_alignment = 512, \n    opt_mem_alignment = 4096, \n    max_iov = 1024\n  }, \n  supported_write_flags = 0, \n  supported_zero_flags = 4, \n  node_name = "#block166", '\\000' &lt;repeats 22 times&gt;, \n  node_list = {\n    tqe_next = 0x55eb84044000, \n    tqe_prev = 0x55eb83cc64c0\n  }, \n  bs_list = {\n    tqe_next = 0x55eb83cc3400, \n    tqe_prev = 0x55eb824ac7a0 &lt;all_bdrv_states&gt;\n  }, \n  monitor_list = {\n    tqe_next = 0x0, \n    tqe_prev = 0x0\n  }, \n  refcnt = 1, \n  op_blockers = {{\n      lh_first = 0x0\n    } &lt;repeats 16 times&gt;}, \n  job = 0x0, \n  inherits_from = 0x0, \n  children = {\n    lh_first = 0x55eb83bc3a40\n  }, \n  parents = {\n    lh_first = 0x55eb83bc39f0\n  }, \n  options = 0x55eb83ca6400, \n  explicit_options = 0x55eb83ca8800, \n  detect_zeroes = BLOCKDEV_DETECT_ZEROES_OPTIONS_OFF, \n  backing_blocker = 0x55eb83c6dd40, \n  copy_on_read = 0, \n  total_sectors = 83886080, \n  before_write_notifiers = {\n    notifiers = {\n      lh_first = 0x0\n    }\n  }, \n  in_flight = 381,                                     &lt;--- in_flight requests\n  serialising_in_flight = 0, \n  wakeup = false, \n  wr_highest_offset = 42321043456, \n  write_threshold_offset = 0, \n  write_threshold_notifier = {\n    notify = 0x0, \n    node = {\n      le_next = 0x0, \n      le_prev = 0x0\n    }\n  }, \n  io_plugged = 0, \n  tracked_requests = {\n    lh_first = 0x7f39268a8ec0\n  }, \n  flush_queue = {\n    entries = {\n      sqh_first = 0x0, \n      sqh_last = 0x55eb83cc3218\n    }\n  }, \n  active_flush_req = false, \n  write_gen = 97225, \n  flushed_gen = 94926, \n  dirty_bitmaps = {\n    lh_first = 0x0\n  }, \n  enable_write_cache = 0, \n  quiesce_counter = 0\n}\n\nI suspect that QEMU's AIO subsystem is getting stuck, and perhaps it's related with this "fd = -1" we have in gpollfds. I'd need more time to build a hypothesis.\n\nSergio.</text>, <text>Hello,\n\nWe're still working on building an hypothesis that could explain the scenario you're seeing, as a first step towards a reproducer.\n\nTo this end, it'd help us if you could provide us some more data:\n\n 1. A gcore of a working VM instance of the same type as the ones presenting the issue.\n\n 2. The output of "lsof -p $PID" of both a working and a stuck VM.\n\nThanks,\nSergio.</text>, <text>Hi Sergio,\n\nthanks for analysis, this is what I was looking for. I'm going to ask my colleague to fire another bunch of jobs and get you requested info. What I can tell you now is that I will have problem with gettign lsof -p $PID of a stuck VM. Once it gets to broken state, the console is unresponsive and I can only hard reset the instance. So perhaps we can run a bash loop with lsof plus logger and the we'll have at least part of the logs written on disk? Or maybe I can configure remote logging?\n\nThanks.</text>, <text>And also, please note that I had to turn off libvirt debugging, it was really space consuming so if you need it I can turn it on again for testing purposes but it cannot run for a long time. I don't want to have it running for more than 24 hrs. You have already saw that the libvirtd.log is quite huge.</text>, <text>And which PID are you interested in?</text>, <text>Hi Ladislav,\n\nI can see from the case history that VM ci-rhos-rhel7-251 is getting stuck, you need to take the lsof of the VM's pid, ie the qemu process and lsof of the VM which running fine. \n\n@Sergio, Please correct me if I am wrong.\n\nThanks &amp; Regards,\nNirav Dave\nSoftware Maintenance Engineer (Virtualization)\nGlobal Support Delivery, Red Hat</text>, <text>Hi Ladislav,\n\nSorry, I should\u2019ve explained this a bit better. The idea is running \u201clsof -p $PID\u201d on the compute node, with the PIDs of both a working and a stuck VM. The reason for needing this is that in the coredump we can see the numbers of the FDs in use, but we can\u2019t see their correspondence with nodes, because this state resides in the Host\u2019s kernel.\n\nAlso, while originally I\u2019ve only asked for the gcore of the working VM, I\u2019m thinking it\u2019d probably be better collecting both, from the sames PIDs as the one were lsof was ran on.\n\nSo, let\u2019s suppose we have $PID_WORKING and $PID_STUCK, corresponding to the PID of a working QEMU instance and a stuck one, respectively. Then we\u2019d need to run:\n\n - lsof -p $PID_WORKING &gt; lsof.working\n - gcore $PID_WORKING\n - xz core.$PID_WORKING\n\n - lsof -p $PID_STUCK &gt; lsof.stuck\n - gcore $PID_STUCK\n - xz core.$PID_STUCK\n\nAs for libvirtd debugging level, I think it\u2019s safe to turn it off. As of this moment, all evidence points towards a bug in QEMU. We can enable it later, if needed.\n\nThanks,\nSergio.</text>, <text>[RHST] The following attachment was uploaded to dropbox.redhat.com:\n\n    01932613-vmcore.working.xz</text>, <text>[RHST] The following attachment was uploaded to dropbox.redhat.com:\n\n    01932613-vmcore.stuck.xz</text>, <text>OK, thanks for explanation. Please find all requested files attached.</text>, <text>//Internal; auto-assistant rule reference #: DropboxNotify05.12-0560\n\n    The following files matching case id '01932613' have been found in dropbox.\n\nfilename                                                                                                         size        Attached   type      format     flags\n--------------------------------------------------------------------------------|------------------------------  ----------- ---------- --------- ---------- ----------------------------------\n01932613-vmcore.stuck.xz                                                                                          4051163064 No         vmcore    xz         \n01932613-vmcore.working.xz                                                                                        3537246760 No         vmcore    xz         \n                                                                                                                  7588409824                                  \n\n\nFor additional information on this comment and its content see:\n    * "Diag2.0/CEE-AIR DropboxNotify - file notification processing"\n       https://access.redhat.com/articles/3136611</text>, <text>//Internal; auto-assistant rule reference #: VmcoreStage01.02-0035-31102017\n\n$ retrace-server-task create --no-verify --caseno 01932613 "01932613-vmcore.working.xz" # [filesize=    3.29 GiB]\nTask created on https://optimus.gsslab.rdu2.redhat.com/manager/171417515\nAdded case number '01932613' to the task '171417515'\n\n\nFor additional information on this comment and its content see:\n    * "Diag2.0/CEE-AIR VmcoreStage - core file processing"\n       https://access.redhat.com/articles/3186731</text>, <text>//Internal; auto-assistant rule reference #: VmcoreStage01.02-0035-31102017\n\n$ retrace-server-task create --no-verify --caseno 01932613 "01932613-vmcore.stuck.xz" # [filesize=    3.77 GiB]\nTask created on https://optimus.gsslab.rdu2.redhat.com/manager/766436143\nAdded case number '01932613' to the task '766436143'\n\n\nFor additional information on this comment and its content see:\n    * "Diag2.0/CEE-AIR VmcoreStage - core file processing"\n       https://access.redhat.com/articles/3186731</text>, <text>Hi Ladislav,\n\nWere those files generated using "gcore"? They don't appear to have the corresponding ELF sections:\n\n[root@ibm-x3550m4-2 core2]# readelf -n 01932613-vmcore.stuck \n\nDisplaying notes found at file offset 0x000001c8 with length 0x0000001c:\n  Owner                 Data size\tDescription\n  NONE                 0x00000005\tUnknown note type: (0x00000000)\n\n[root@ibm-x3550m4-2 core2]# readelf -n 01932613-vmcore.working \n\nDisplaying notes found at file offset 0x000001c8 with length 0x0000001c:\n  Owner                 Data size\tDescription\n  NONE                 0x00000005\tUnknown note type: (0x00000000)\n\nAs a comparison, these are the sections of the previously analyzed gcore:\n\n[root@ibm-x3550m4-2 core]# readelf -n core.255248 \n\nDisplaying notes found at file offset 0x00009260 with length 0x000061c4:\n  Owner                 Data size\tDescription\n  CORE                 0x00000088\tNT_PRPSINFO (prpsinfo structure)\n  CORE                 0x00000150\tNT_PRSTATUS (prstatus structure)\n  CORE                 0x00000200\tNT_FPREGSET (floating point registers)\n  LINUX                0x00000340\tNT_X86_XSTATE (x86 XSAVE extended state)\n  CORE                 0x00000080\tNT_SIGINFO (siginfo_t data)\n  CORE                 0x00000150\tNT_PRSTATUS (prstatus structure)\n  CORE                 0x00000200\tNT_FPREGSET (floating point registers)\n  LINUX                0x00000340\tNT_X86_XSTATE (x86 XSAVE extended state)\n  CORE                 0x00000080\tNT_SIGINFO (siginfo_t data)\n  CORE                 0x00000150\tNT_PRSTATUS (prstatus structure)\n  CORE                 0x00000200\tNT_FPREGSET (floating point registers)\n  LINUX                0x00000340\tNT_X86_XSTATE (x86 XSAVE extended state)\n  CORE                 0x00000080\tNT_SIGINFO (siginfo_t data)\n  CORE                 0x00000150\tNT_PRSTATUS (prstatus structure)\n  CORE                 0x00000200\tNT_FPREGSET (floating point registers)\n  LINUX                0x00000340\tNT_X86_XSTATE (x86 XSAVE extended state)\n  CORE                 0x00000080\tNT_SIGINFO (siginfo_t data)\n  CORE                 0x00000150\tNT_PRSTATUS (prstatus structure)\n  CORE                 0x00000200\tNT_FPREGSET (floating point registers)\n  LINUX                0x00000340\tNT_X86_XSTATE (x86 XSAVE extended state)\n  CORE                 0x00000080\tNT_SIGINFO (siginfo_t data)\n  CORE                 0x00000150\tNT_PRSTATUS (prstatus structure)\n  CORE                 0x00000200\tNT_FPREGSET (floating point registers)\n  LINUX                0x00000340\tNT_X86_XSTATE (x86 XSAVE extended state)\n  CORE                 0x00000080\tNT_SIGINFO (siginfo_t data)\n  CORE                 0x00000150\tNT_PRSTATUS (prstatus structure)\n  CORE                 0x00000200\tNT_FPREGSET (floating point registers)\n  LINUX                0x00000340\tNT_X86_XSTATE (x86 XSAVE extended state)\n  CORE                 0x00000080\tNT_SIGINFO (siginfo_t data)\n  CORE                 0x00000140\tNT_AUXV (auxiliary vector)\n  CORE                 0x00002c1d\tNT_FILE (mapped files)\n\nSergio.</text>, <text>Yes, I followed this tutorial https://access.redhat.com/solutions/2292431. \n\nI still have full memory dumps and can provide them to you.</text>, <text>(In reply to Jozsa, Ladislav)\n&gt; I still have full memory dumps and can provide them to you.\n\nYes, please upload both full gcores. The script "dump-guest-memory.py" extracts the Guest's memory, but we're actually more interested in the QEMU's internal state.\n\nThanks,\nSergio.</text>, <text>[RHST] The following attachment was uploaded to dropbox.redhat.com:\n\n    01932613-core.342898.xz</text>, <text>[RHST] The following attachment was uploaded to dropbox.redhat.com:\n\n    01932613-core.736443.xz</text>, <text>//Internal; auto-assistant rule reference #: DropboxNotify05.12-0560\n\n    The following files matching case id '01932613' have been found in dropbox.\n\nfilename                                                                                                         size        Attached   type      format     flags\n--------------------------------------------------------------------------------|------------------------------  ----------- ---------- --------- ---------- ----------------------------------\n01932613-core.342898.xz                                                                                           3539288296 No         core      xz         \n01932613-core.736443.xz                                                                                           4055553664 No         core      xz         \n                                                                                                                  7594841960                                  \n\n\nFor additional information on this comment and its content see:\n    * "Diag2.0/CEE-AIR DropboxNotify - file notification processing"\n       https://access.redhat.com/articles/3136611</text>, <text>Sorry for misunderstanding Sergio. You should be all set now.\n\nCheers,\nL.</text>, <text>From Guest's perspective, when this bug appears, all IO operations get indefinitely stuck waiting for completion in the corresponding request queue.\n\nFrom QEMU's perspective, we can make the following observations:\n\n - We have 7 threads, 4 of them backing vCPUs, 1 main_loop, 1 RCU worker and 1 VNC worker. Even though we're running with 'aio=threads' and there are pending I/O operations, we have no worker_threads:\n\nThread 7 (Thread 0x7f3b61237d00 (LWP 255248)):\n#0  0x00007f3b487c2aff in __GI_ppoll (fds=0x55eb83bf5300, nfds=8, timeout=&lt;optimized out&gt;, \n    timeout@entry=0x7ffd7f4bae50, sigmask=sigmask@entry=0x0) at ../sysdeps/unix/sysv/linux/ppoll.c:56\n#1  0x000055eb81f003d9 in ppoll (__ss=0x0, __timeout=0x7ffd7f4bae50, __nfds=&lt;optimized out&gt;, \n    __fds=&lt;optimized out&gt;) at /usr/include/bits/poll2.h:77\n#2  qemu_poll_ns (fds=&lt;optimized out&gt;, nfds=&lt;optimized out&gt;, timeout=timeout@entry=999000000)\n    at util/qemu-timer.c:334\n#3  0x000055eb81f011e8 in os_host_main_loop_wait (timeout=999000000) at util/main-loop.c:255\n#4  main_loop_wait (nonblocking=nonblocking@entry=0) at util/main-loop.c:517\n#5  0x000055eb81bf2c2c in main_loop () at vl.c:1909\n#6  main (argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, envp=&lt;optimized out&gt;) at vl.c:4733\n\nThread 6 (Thread 0x7f393bbff700 (LWP 255281)):\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\n#1  0x000055eb81f040f9 in qemu_cond_wait (cond=cond@entry=0x55eb83bf4c00, \n    mutex=mutex@entry=0x55eb83bf4c30) at util/qemu-thread-posix.c:133\n#2  0x000055eb81e423fb in vnc_worker_thread_loop (queue=queue@entry=0x55eb83bf4c00) at ui/vnc-jobs.c:205\n#3  0x000055eb81e42938 in vnc_worker_thread (arg=0x55eb83bf4c00) at ui/vnc-jobs.c:312\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f393bbff700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\nThread 5 (Thread 0x7f3b3de6c700 (LWP 255279)):\n#0  0x00007f3b487c4107 in ioctl () at ../sysdeps/unix/syscall-template.S:81\n#1  0x000055eb81c42fa5 in kvm_vcpu_ioctl (cpu=cpu@entry=0x55eb84116000, type=type@entry=44672)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2178\n#2  0x000055eb81c4306b in kvm_cpu_exec (cpu=cpu@entry=0x55eb84116000)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2016\n#3  0x000055eb81c30432 in qemu_kvm_cpu_thread_fn (arg=0x55eb84116000)\n    at /usr/src/debug/qemu-2.9.0/cpus.c:1118\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f3b3de6c700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\nThread 4 (Thread 0x7f3b3e66d700 (LWP 255278)):\n#0  0x00007f3b487c4107 in ioctl () at ../sysdeps/unix/syscall-template.S:81\n#1  0x000055eb81c42fa5 in kvm_vcpu_ioctl (cpu=cpu@entry=0x55eb840f6000, type=type@entry=44672)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2178\n#2  0x000055eb81c4306b in kvm_cpu_exec (cpu=cpu@entry=0x55eb840f6000)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2016\n#3  0x000055eb81c30432 in qemu_kvm_cpu_thread_fn (arg=0x55eb840f6000)\n    at /usr/src/debug/qemu-2.9.0/cpus.c:1118\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f3b3e66d700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\nThread 3 (Thread 0x7f3b3ee6e700 (LWP 255277)):\n#0  0x00007f3b487c4107 in ioctl () at ../sysdeps/unix/syscall-template.S:81\n#1  0x000055eb81c42fa5 in kvm_vcpu_ioctl (cpu=cpu@entry=0x55eb840d6000, type=type@entry=44672)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2178\n#2  0x000055eb81c4306b in kvm_cpu_exec (cpu=cpu@entry=0x55eb840d6000)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2016\n#3  0x000055eb81c30432 in qemu_kvm_cpu_thread_fn (arg=0x55eb840d6000)\n    at /usr/src/debug/qemu-2.9.0/cpus.c:1118\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f3b3ee6e700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\nThread 2 (Thread 0x7f3b3f66f700 (LWP 255276)):\n#0  0x00007f3b487c4107 in ioctl () at ../sysdeps/unix/syscall-template.S:81\n#1  0x000055eb81c42fa5 in kvm_vcpu_ioctl (cpu=cpu@entry=0x55eb8405c000, type=type@entry=44672)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2178\n#2  0x000055eb81c4306b in kvm_cpu_exec (cpu=cpu@entry=0x55eb8405c000)\n    at /usr/src/debug/qemu-2.9.0/kvm-all.c:2016\n#3  0x000055eb81c30432 in qemu_kvm_cpu_thread_fn (arg=0x55eb8405c000)\n    at /usr/src/debug/qemu-2.9.0/cpus.c:1118\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f3b3f66f700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\nThread 1 (Thread 0x7f3b40e8a700 (LWP 255272)):\n#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\n#1  0x000055eb81f04406 in qemu_futex_wait (val=&lt;optimized out&gt;, f=&lt;optimized out&gt;)\n    at /usr/src/debug/qemu-2.9.0/include/qemu/futex.h:26\n#2  qemu_event_wait (ev=ev@entry=0x55eb82917784 &lt;rcu_call_ready_event&gt;) at util/qemu-thread-posix.c:399\n#3  0x000055eb81f13a2e in call_rcu_thread (opaque=&lt;optimized out&gt;) at util/rcu.c:249\n#4  0x00007f3b48a9fe25 in start_thread (arg=0x7f3b40e8a700) at pthread_create.c:308\n#5  0x00007f3b487cd34d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\n\n - In the device's VQ, we have 128 slots, and all of them are in use:\n\n(gdb) p ((VirtIODevice *)0x55eb85adc510)-&gt;vq[0]\n$126 = {\n  vring = {\n    num = 128,                    &lt;-- 128 slots in total\n    num_default = 128, \n    align = 4096, \n    desc = 907452416, \n    avail = 907454464, \n    used = 907454784, \n    caches = 0x55eb86ad8d80\n  }, \n  last_avail_idx = 43081, \n  shadow_avail_idx = 43081, \n  used_idx = 42953, \n  signalled_used = 42953, \n  signalled_used_valid = true, \n  notification = true, \n  queue_index = 0, \n  inuse = 128,                     &lt;-- 128 slots in use\n  vector = 1, \n  handle_output = 0x55eb81c5e320 &lt;virtio_blk_handle_output&gt;, \n  handle_aio_output = 0x55eb81c5e440 &lt;virtio_blk_data_plane_handle_output&gt;, \n  vdev = 0x55eb85adc510, \n  guest_notifier = {\n    rfd = 30, \n    wfd = 30\n  }, \n  host_notifier = {\n    rfd = 31, \n    wfd = 31\n  }, \n  node = {\n    le_next = 0x0, \n    le_prev = 0x55eb85245e48\n  }\n}\n\n\n - We have 381 "in flight" requests in the QCOW2 layer, and 1 in the file layer:\n\n(gdb) p ((VirtIOBlock *)0x557ea7ae8510)-&gt;blk-&gt;root-&gt;bs-&gt;in_flight\n$162 = 383\n(gdb) p ((VirtIOBlock *)0x557ea7ae8510)-&gt;blk-&gt;root-&gt;bs-&gt;file-&gt;bs-&gt;in_flight\n$163 = 1\n\n\n - The coroutine servicing the first request from the QCOW2 layer is waiting on a lock held by coroutine 0x557ea751ad00.\n\n\n(gdb) bt\n#0  qemu_coroutine_switch (from_=&lt;optimized out&gt;, to_=0x7fc2d134dbd8,\n    action=action@entry=COROUTINE_YIELD) at util/coroutine-ucontext.c:176\n#1  0x0000557ea4493e6c in qemu_coroutine_yield () at util/qemu-coroutine.c:167\n#2  0x0000557ea449422a in qemu_co_mutex_lock_slowpath (mutex=0x557ea4e97784 &lt;rcu_call_ready_event&gt;,\n    ctx=0x0) at util/qemu-coroutine-lock.c:246\n#3  qemu_co_mutex_lock (mutex=0x557ea4e97784 &lt;rcu_call_ready_event&gt;, mutex@entry=0x557ea5c06250)\n    at util/qemu-coroutine-lock.c:284\n#4  0x0000557ea43e7765 in qcow2_co_pwritev (bs=0x557ea5ccc000, offset=0, bytes=12288,\n    qiov=0x557ea5c12c90, flags=&lt;optimized out&gt;) at block/qcow2.c:1597\n#5  0x0000557ea4410811 in bdrv_driver_pwritev (bs=bs@entry=0x557ea5ccc000,\n    offset=offset@entry=31570137088, bytes=bytes@entry=12288, qiov=qiov@entry=0x557ea5c12c90,\n    flags=flags@entry=0) at block/io.c:888\n#6  0x0000557ea4411bc6 in bdrv_aligned_pwritev (req=req@entry=0x7fc0a59c7ec0,\n    offset=offset@entry=31570137088, bytes=bytes@entry=12288, align=align@entry=1, qiov=0x557ea5c12c90,\n    flags=flags@entry=0, child=0x557ea5bcf9f0) at block/io.c:1396\n#7  0x0000557ea4412702 in bdrv_co_pwritev (child=0x557ea5bcf9f0, offset=offset@entry=31570137088,\n    bytes=bytes@entry=12288, qiov=qiov@entry=0x557ea5c12c90, flags=0) at block/io.c:1647\n#8  0x0000557ea440404b in blk_co_pwritev (blk=0x557ea5c16000, offset=31570137088, bytes=12288,\n    qiov=0x557ea5c12c90, flags=&lt;optimized out&gt;) at block/block-backend.c:995\n#9  0x0000557ea44040da in blk_aio_write_entry (opaque=0x557ea7ce23f0) at block/block-backend.c:1186\n#10 0x0000557ea4494c1a in coroutine_trampoline (i0=&lt;optimized out&gt;, i1=&lt;optimized out&gt;)\n    at util/coroutine-ucontext.c:79\n#11 0x00007fc2b8831d40 in ?? () from /lib64/libc.so.6\n#12 0x00007fff432fb9c0 in ?? ()\n#13 0x0000000000000000 in ?? ()\n\n(gdb) p *(CoMutex *)0x557ea5c06250\n$174 = {locked = 80, ctx = 0x557ea5c05700, from_push = {slh_first = 0x7fc09d3aaee0}, to_pop = {\n    slh_first = 0x7fc0a4ac6b50}, handoff = 0, sequence = 0, holder = 0x557ea751ad00}\n\n\n - Coroutine 0x557ea751ad00, which is holding the lock mentioned above, is servicing the only request in the file layer, as it's waiting for completion:\n\n(gdb) bt\n#0  qemu_coroutine_switch (from_=&lt;optimized out&gt;, to_=0x7fc2d134dbd8,\n    action=action@entry=COROUTINE_YIELD) at util/coroutine-ucontext.c:176\n#1  0x0000557ea4493e6c in qemu_coroutine_yield () at util/qemu-coroutine.c:167\n#2  0x0000557ea447fd78 in thread_pool_submit_co (pool=0x557ea4e97784 &lt;rcu_call_ready_event&gt;, func=0x0,\n    arg=0x0) at util/thread-pool.c:290\n#3  0x0000557ea440ee90 in bdrv_driver_preadv (bs=bs@entry=0x557ea5ccf400, offset=offset@entry=25886720,\n    bytes=bytes@entry=65536, qiov=0x7fc0a5ccaa70, qiov@entry=0x7fc2b0fa0700, flags=0) at block/io.c:847\n#4  0x0000557ea441214d in bdrv_aligned_preadv (child=child@entry=0x557ea5bcfae0,\n    req=req@entry=0x7fc0a5cca950, offset=offset@entry=25886720, bytes=bytes@entry=65536,\n    align=140474169559488, align@entry=512, qiov=0x7fc2b0fa0700, qiov@entry=0x7fc0a5ccaa70,\n    flags=flags@entry=0) at block/io.c:1096\n#5  0x0000557ea4412557 in bdrv_co_preadv (child=&lt;optimized out&gt;, offset=25886720, bytes=65536,\n    qiov=0x7fc0a5ccaa70, flags=0) at block/io.c:1190\n#6  0x0000557ea4412e25 in bdrv_rw_co_entry (opaque=0x7fc0a5ccaa10) at block/io.c:604\n#7  0x0000557ea4412f58 in bdrv_prwv_co (child=child@entry=0x557ea5bcfae0, offset=offset@entry=25886720,\n    qiov=qiov@entry=0x7fc0a5ccaa70, is_write=is_write@entry=false, flags=flags@entry=0) at block/io.c:633\n#8  0x0000557ea4413256 in bdrv_preadv (qiov=0x7fc0a5ccaa70, offset=25886720, child=0x557ea5bcfae0)\n    at block/io.c:747\n#9  bdrv_pread (child=0x557ea5bcfae0, offset=offset@entry=25886720, buf=&lt;optimized out&gt;,\n    bytes=&lt;optimized out&gt;) at block/io.c:768\n#10 0x0000557ea43f5d2f in qcow2_cache_do_get (bs=bs@entry=0x557ea5ccc000, c=0x557ea5c79b60,\n    offset=offset@entry=25886720, table=table@entry=0x7fc0a5ccab48,\n    read_from_disk=read_from_disk@entry=true) at block/qcow2-cache.c:360\n#11 0x0000557ea43f5eab in qcow2_cache_get (bs=bs@entry=0x557ea5ccc000, c=&lt;optimized out&gt;,\n    offset=offset@entry=25886720, table=table@entry=0x7fc0a5ccab48) at block/qcow2-cache.c:384\n#12 0x0000557ea43f269a in l2_load (l2_table=0x7fc0a5ccab48, l2_offset=25886720, bs=0x557ea5ccc000)\n    at block/qcow2-cluster.c:161\n#13 get_cluster_table (bs=bs@entry=0x557ea5ccc000, offset=offset@entry=37855051776,\n    new_l2_table=new_l2_table@entry=0x7fc0a5ccac10, new_l2_index=new_l2_index@entry=0x7fc0a5ccac0c)\n    at block/qcow2-cluster.c:656\n#14 0x0000557ea43f343f in handle_copied (m=0x7fc0a5ccacc0, bytes=&lt;synthetic pointer&gt;,\n    host_offset=&lt;synthetic pointer&gt;, guest_offset=37855051776, bs=0x557ea5ccc000)\n    at block/qcow2-cluster.c:999\n#15 qcow2_alloc_cluster_offset (bs=bs@entry=0x557ea5ccc000, offset=offset@entry=37855051776,\n    bytes=bytes@entry=0x7fc0a5ccacb4, host_offset=host_offset@entry=0x7fc0a5ccacb8,\n    m=m@entry=0x7fc0a5ccacc0) at block/qcow2-cluster.c:1348\n#16 0x0000557ea43e7808 in qcow2_co_pwritev (bs=0x557ea5ccc000, offset=37855051776, bytes=16384,\n    qiov=0x557ea5c14b80, flags=&lt;optimized out&gt;) at block/qcow2.c:1612\n#17 0x0000557ea4410811 in bdrv_driver_pwritev (bs=bs@entry=0x557ea5ccc000,\n    offset=offset@entry=37855051776, bytes=bytes@entry=16384, qiov=qiov@entry=0x557ea5c14b80,\n    flags=flags@entry=0) at block/io.c:888\n#18 0x0000557ea4411bc6 in bdrv_aligned_pwritev (req=req@entry=0x7fc0a5ccaec0,\n    offset=offset@entry=37855051776, bytes=bytes@entry=16384, align=align@entry=1, qiov=0x557ea5c14b80,\n    flags=flags@entry=0, child=0x557ea5bcf9f0) at block/io.c:1396\n#19 0x0000557ea4412702 in bdrv_co_pwritev (child=0x557ea5bcf9f0, offset=offset@entry=37855051776,\n    bytes=bytes@entry=16384, qiov=qiov@entry=0x557ea5c14b80, flags=0) at block/io.c:1647\n#20 0x0000557ea440404b in blk_co_pwritev (blk=0x557ea5c16000, offset=37855051776, bytes=16384,\n    qiov=0x557ea5c14b80, flags=&lt;optimized out&gt;) at block/block-backend.c:995\n#21 0x0000557ea44040da in blk_aio_write_entry (opaque=0x557ea7ce3de0) at block/block-backend.c:1186\n#22 0x0000557ea4494c1a in coroutine_trampoline (i0=&lt;optimized out&gt;, i1=&lt;optimized out&gt;)\n    at util/coroutine-ucontext.c:79\n#23 0x00007fc2b8831d40 in ?? () from /lib64/libc.so.6\n#24 0x00007fff432fb9c0 in ?? ()\n#25 0x0000000000000000 in ?? ()\n\n\n - In the ThreadPool from the AIOContext corresponding to the file layer, we have one element in head, and request_list is empty:\n\n(gdb) p *((struct ThreadPoolReal *)0x557ea5c844e0)\n$128 = {ctx = 0x557ea5c05700, completion_bh = 0x557ea5c79b00, lock = {lock = {__data = {__lock = 0, \n        __count = 0, __owner = 0, __nusers = 0, __kind = 0, __spins = 0, __elision = 0, __list = {\n          __prev = 0x0, __next = 0x0}}, __size = '\\000' &lt;repeats 39 times&gt;, __align = 0}}, \n  worker_stopped = {cond = {__data = {{__wseq = 0, __wseq32 = {__low = 0, __high = 0}}, {__g1_start = 0, \n          __g1_start32 = {__low = 0, __high = 0}}, __g_refs = {0, 0}, __g_size = {0, 0}, \n        __g1_orig_size = 0, __wrefs = 0, __g_signals = {0, 0}}, __size = '\\000' &lt;repeats 47 times&gt;, \n      __align = 0}}, sem = {sem = {__size = '\\000' &lt;repeats 31 times&gt;, __align = 0}}, max_threads = 64, \n  new_thread_bh = 0x557ea5c79b30, head = {lh_first = 0x557ea5bdf570}, request_list = {tqh_first = 0x0, \n    tqh_last = 0x557ea5c84580}, cur_threads = 0, idle_threads = 0, new_threads = 0, pending_threads = 0, \n  stopping = false}\n\n\n - The only ThreadPoolElement in head is in state THREAD_DONE, with ret == 0:\n\n(gdb) p *((struct ThreadPoolReal *)0x557ea5c844e0)-&gt;head-&gt;lh_first\n$160 = {common = {aiocb_info = 0x557ea49e7570 &lt;thread_pool_aiocb_info&gt;, bs = 0x0, \n    cb = 0x557ea447f940 &lt;thread_pool_co_cb&gt;, opaque = 0x7fc0a5cca7b0, refcnt = 1}, \n  pool = 0x557ea5c844e0, func = 0x557ea4409cd0 &lt;aio_worker&gt;, arg = 0x557ea853aac0, state = THREAD_DONE, \n  ret = 0, reqs = {tqe_next = 0x0, tqe_prev = 0x0}, all = {le_next = 0x0, le_prev = 0x557ea5c84578}}\n\n\n - The ThreadPoolCo corresponding to the above ThreadPoolElement is still -EINPROGRESS (-115):\n\n(gdb) p *(ThreadPoolCo *) 0x7fc0a5cca7b0\n$178 = {co = 0x557ea751ad00, ret = -115}\n\n\n - ThreadPool's completion BH is not scheduled:\n\n(gdb) p *((struct ThreadPoolReal *)0x557ea5c844e0)-&gt;completion_bh\n$156 = {ctx = 0x557ea5c05700, cb = 0x557ea447fb50 &lt;thread_pool_completion_bh&gt;, opaque = 0x557ea5c844e0, \n  next = 0x557ea5c79620, scheduled = false, idle = false, deleted = false}\n\n\n########################################################\n########################################################\n\n\nThe fact that ThreadPoolCo.ret is still -EINPROGRESS indicates that its callback (thread_pool_co_cb) has not yet been called. This element is still in the head of the ThreadPool, but the completion BH is not scheduled, and all worker_threads seem to have expired, so the system can't make any progress.\n\nI suspect there may be a concurrency issue between worker_thread and thread_pool_completion_bh:\n\n&lt;snip from util/thread-pool.c&gt;\n 77 static void *worker_thread(void *opaque)\n 78 {\n 79     ThreadPool *pool = opaque;\n 80 \n 81     qemu_mutex_lock(&amp;pool-&gt;lock);\n 82     pool-&gt;pending_threads--;\n 83     do_spawn_thread(pool);\n 84 \n 85     while (!pool-&gt;stopping) {\n 86         ThreadPoolElement *req;\n 87         int ret;\n 88 \n 89         do {\n 90             pool-&gt;idle_threads++;\n 91             qemu_mutex_unlock(&amp;pool-&gt;lock);\n 92             ret = qemu_sem_timedwait(&amp;pool-&gt;sem, 10000);\n 93             qemu_mutex_lock(&amp;pool-&gt;lock);\n 94             pool-&gt;idle_threads--;\n 95         } while (ret == -1 &amp;&amp; !QTAILQ_EMPTY(&amp;pool-&gt;request_list));\n 96         if (ret == -1 || pool-&gt;stopping) {\n 97             break;\n 98         }\n 99 \n100         req = QTAILQ_FIRST(&amp;pool-&gt;request_list);\n101         QTAILQ_REMOVE(&amp;pool-&gt;request_list, req, reqs);\n102         req-&gt;state = THREAD_ACTIVE;\n103         qemu_mutex_unlock(&amp;pool-&gt;lock);\n104 \n105         ret = req-&gt;func(req-&gt;arg);\n106 \n107         req-&gt;ret = ret;\n108         /* Write ret before state.  */\n109         smp_wmb();\n110         req-&gt;state = THREAD_DONE;\n111 \n112         qemu_mutex_lock(&amp;pool-&gt;lock);\n113 \n114         qemu_bh_schedule(pool-&gt;completion_bh);\n115     }\n116 \n117     pool-&gt;cur_threads--;\n118     qemu_cond_signal(&amp;pool-&gt;worker_stopped);\n119     qemu_mutex_unlock(&amp;pool-&gt;lock);\n120     return NULL;\n121 }\n&lt;/snip&gt;\n\n&lt;snip from util/thread-pool.c&gt;\n163 static void thread_pool_completion_bh(void *opaque)\n164 {\n165     ThreadPool *pool = opaque;\n166     ThreadPoolElement *elem, *next;\n167 \n168     aio_context_acquire(pool-&gt;ctx);\n169 restart:\n170     QLIST_FOREACH_SAFE(elem, &amp;pool-&gt;head, all, next) {\n171         if (elem-&gt;state != THREAD_DONE) {\n172             continue;\n173         }\n174 \n175         trace_thread_pool_complete(pool, elem, elem-&gt;common.opaque,\n176                                    elem-&gt;ret);\n177         QLIST_REMOVE(elem, all);\n178 \n179         if (elem-&gt;common.cb) {\n180             /* Read state before ret.  */\n181             smp_rmb();\n182 \n183             /* Schedule ourselves in case elem-&gt;common.cb() calls aio_poll() to\n184              * wait for another request that completed at the same time.\n185              */\n186             qemu_bh_schedule(pool-&gt;completion_bh);\n187 \n188             aio_context_release(pool-&gt;ctx);\n189             elem-&gt;common.cb(elem-&gt;common.opaque, elem-&gt;ret);\n190             aio_context_acquire(pool-&gt;ctx);\n191 \n192             /* We can safely cancel the completion_bh here regardless of someone\n193              * else having scheduled it meanwhile because we reenter the\n194              * completion function anyway (goto restart).\n195              */\n196             qemu_bh_cancel(pool-&gt;completion_bh);\n197 \n198             qemu_aio_unref(elem);\n199             goto restart;\n200         } else {\n201             qemu_aio_unref(elem);\n202         }\n203     }\n204     aio_context_release(pool-&gt;ctx);\n205 }\n&lt;/snip&gt;\n\nThe BH pool-&gt;completion_bh is scheduled after executing the request's function in worker_thread:114. In thread_pool_completion_bh:196 this BH is cancelled, which is supposedly safe as it's going back at the start of the loop.\n\nBut I wonder if it's possible that, with large CPU caches, the elem-&gt;state comparison at thread_pool_completion_bh:171 may be still seeing THREAD_QUEUED, even after it has been set to THREAD_DONE at worker_thread:110.\n\nIf this is indeed possible, and there are no more requests in the queue, then thread_pool_completion_bh would cancel the execution of the completion_bh, leaving the last request waiting forever, with its coroutine perpetually yield.</text>, <text>Since this bug report was entered in Red Hat Bugzilla, the release flag has been set to ? to ensure that it is properly evaluated for this release.</text>, <text>I wasn't able to reproduce this issue in the lab, but the customer is able to reliably do it launching multiple VM instances from the same template (this is OSP10) at the same time. Almost everytime they do this operation, one of the VMs gets in this state, while the others work properly.\n\nI think that, if we're able to make a good guess at the issue, we should be able to build a test package for the customer to confirm the fix and aim for a sanity-only verification.\n\nI'm considering writing a patch to wrap elem-&gt;state accesses into atomic operations, but I'd like to hear from you first, to confirm this makes sense to you. I'm still wondering why we're not seeing this issue on every customer, as I can't find any differential aspect in their environment.\n\nSergio.</text>, <text>Created attachment 1347017\nXML of an instance where this issue has been observed</text>, <text>Hi Ladislav,\n\nI'm sorry that is taking too long for us to come with an answer, the issue appears to be quite complex.\n\nI've just filled a public bug ticket for qemu-kvm-rhev, which includes all the information gathered for the latest gcores:\n\n - https://bugzilla.redhat.com/show_bug.cgi?id=1508886\n\nAs for the comparison between the working and the stuck VM:\n\n - Both VMs present this "-1" file description. While it doesn't appear to have any relation to the actual issue, I'm still going to ask Engineering about it, because I'm pretty sure it shouldn't be there.\n\n - The working VM as a number of worker_threads, as expected from an instance with I/O activity running with aio=threads, while the stuck one doesn't have any.\n\n - On the stuck VM, we have 381 "in flight" requests on the QCOW2 layer, and 1 in the file layer, with the later apparently waiting for a completion from AIO's subsystem.\n\nThis ticket has been created with the maximum priority level. We'll keep you informed on its progress.\n\nSergio.</text>, <text>Stefan, Can you help triage? Thanks.</text>, <text>Hello Ladislav,\n\n   As per previous reply by Sergio, we are already working with engineering on this issue. We will keep you updated with the progress of it.\n\nBest Regards,\nSachin</text>, <text>To test the worker_thread vs. thread_pool_completion_bh concurrency issue hypothesis, I wrote this small patch:\n\nFrom 057f9ed07937886c4ac3f8b8a408e7597f3da93d Mon Sep 17 00:00:00 2001\nFrom: Sergio Lopez &lt;slp@redhat.com&gt;\nDate: Fri, 3 Nov 2017 07:57:44 +0100\nSubject: [PATCH] util/thread-pool: use atomic operations for TPE-&gt;state\n\n---\n util/thread-pool.c | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/util/thread-pool.c b/util/thread-pool.c\nindex 610646d..a264c12 100644\n--- a/util/thread-pool.c\n+++ b/util/thread-pool.c\n@@ -107,7 +107,7 @@ static void *worker_thread(void *opaque)\n         req-&gt;ret = ret;\n         /* Write ret before state.  */\n         smp_wmb();\n-        req-&gt;state = THREAD_DONE;\n+        atomic_set(&amp;req-&gt;state, THREAD_DONE);\n \n         qemu_mutex_lock(&amp;pool-&gt;lock);\n \n@@ -168,7 +168,7 @@ static void thread_pool_completion_bh(void *opaque)\n     aio_context_acquire(pool-&gt;ctx);\n restart:\n     QLIST_FOREACH_SAFE(elem, &amp;pool-&gt;head, all, next) {\n-        if (elem-&gt;state != THREAD_DONE) {\n+        if (atomic_read(&amp;elem-&gt;state) != THREAD_DONE) {\n             continue;\n         }\n \n-- \n2.13.5</text>, <text>There's also test build with this patch:\n\n - https://brewweb.engineering.redhat.com/brew/taskinfo?taskID=14455027\n\nAs the customer has a reliable reproducer for this issue, I'm going to ask them if they can give this a try. Will get you back with their feedback.\n\nSergio.</text>, <text>Hi Ladislav,\n\nI'm attaching to this case a tarball with patched QEMU packages, including a tentative fix for the worker_thread vs. thread_pool_completion_bh concurrency issue hypothesis. Those packages are unsupported, but the change is very small, and as you have a reliable reproducer for the issue, it'd very helpful for us to have your feedback for either confirming or discarding this hypothesis.\n\nYou need to install all three packages at once, with something like this:\n\n - yum install qemu-kvm-rhev-2.9.0-10.el7.sfdc01932613.x86_64.rpm qemu-kvm-common-rhev-2.9.0-10.el7.sfdc01932613.x86_64.rpm qemu-img-rhev-2.9.0-10.el7.sfdc01932613.x86_64.rpm\n\nYou can also downgrade to the supported version in a similar way:\n\n - yum downgrade qemu-kvm-rhev-2.9.0-10.el7.x86_64 qemu-kvm-common-rhev-2.9.0-10.el7.x86_64 qemu-img-rhev-2.9.0-10.el7.x86_64\n\nDo you think you can give this a try?\n\nThanks,\nSergio.</text>, <text>Hi Sergio,\n\nthanks for a quick fix. Yes, we can try it. What I'll need to do once I upgrade the packages? Restart libvirtd?\n\nThanks,\nL.</text>, <text>(In reply to Jozsa, Ladislav)\n&gt; thanks for a quick fix. Yes, we can try it. What I'll need to do once I upgrade the packages? Restart libvirtd?\n\nJust after installing the packages, newly instantiated VMs will be started with the patched QEMU version, so you can execute your reproducer right away.\n\nThanks,\nSergio.</text>, <text>OK, hotfix applied. Now waiting for response from my colleague who is going to run his jobs and we'll verify if it helped or not.\n\nThanks,\nL.</text>, <text>Hmm, unfortunately id didn't help. :-( Do you want another gcore from this one?</text>, <text>(In reply to Jozsa, Ladislav)\n&gt; Hmm, unfortunately id didn't help. :-( Do you want another gcore from this one?\n\nThanks for the feedback. Yes, a newer gcore would be useful to confirm the symptoms are still the same.\n\nSergio.</text>, <text>The customer has tested the patches packages, but the issue still persists, so we need to work on another hypothesis.\n\nSergio.</text>, <text>Hello,\n\n  Thanks for providing the new gcore and lsof output. I will follow up with Sergio and engineering to investigate further on provided data.\n\nMeanwhile let me know if you have any questions.\n\nBest Regards,\nSachin</text>, <text>//Internal Only\n\nI am not sure if 'sergio' working during weekends. So i am putting this case to "collab" so that someone may assist with provided gcore analysis.\n\n========= Collaboration Note =========\n\n### Problem Statement : Openestack vms hangs due to bug #1508886\n\n### Actions Taken : As per BZ #1508886 test package provided to customer but issue still persists.\n\n### Next Action : The new lsof output and gcore has collected and provided by customer after applying the patch. Need someone to analyze it further.</text>, <text>Hi Aihua,\n\nCould you please help reproduce this bug ?\n\nThanks.</text>, <text>Hi Ladislav,\n\nJust a quick update to inform you that I'm analyzing the latest gcore, and it shows the exact symptoms we've seen on the others.\n\nI'm currently looking at each coroutine in more detail. I'll keep you updated on the progress.\n\nSergio.</text>, <text>Apparently, the patch in comment #5 didn't help. We have a new coredump from QEMU, with the same symptoms we've seen before.\n\nLooking at each coroutine backtrace, we can see we have a total of 130 coroutines, with 51 waiting for another operation in the same cluster (qemu_co_queue_wait), and 78 waiting for a mutex (qemu_co_mutex_lock_slow) held by this one:\n\n#0  qemu_coroutine_switch (from_=&lt;optimized out&gt;, to_=0x7f1f311dfbd8, action=action@entry=COROUTINE_YIELD) at util/coroutine-ucontext.c:176\n#1  0x0000560f395e333c in qemu_coroutine_yield () at util/qemu-coroutine.c:167\n#2  0x0000560f395ced78 in thread_pool_submit_co (pool=0x560f39fe6784 &lt;rcu_call_ready_event&gt;, func=0x0, arg=0x0) at util/thread-pool.c:290\n#3  0x0000560f3955de90 in bdrv_driver_preadv (bs=bs@entry=0x560f3c441400, offset=offset@entry=4128768, bytes=bytes@entry=65536, qiov=0x7f1cf1045940, qiov@entry=0x7f1f10e31700, flags=0) at block/io.c:847\n#4  0x0000560f3956114d in bdrv_aligned_preadv (child=child@entry=0x560f3c341ae0, req=req@entry=0x7f1cf1045820, offset=offset@entry=4128768, bytes=bytes@entry=65536, align=139771404032448, align@entry=512, qiov=0x7f1f10e31700, qiov@entry=0x7f1cf1045940, flags=flags@entry=0) at block/io.c:1096\n#5  0x0000560f39561557 in bdrv_co_preadv (child=&lt;optimized out&gt;, offset=4128768, bytes=65536, qiov=0x7f1cf1045940, flags=0) at block/io.c:1190\n#6  0x0000560f39561e25 in bdrv_rw_co_entry (opaque=0x7f1cf10458e0) at block/io.c:604\n#7  0x0000560f39561f58 in bdrv_prwv_co (child=child@entry=0x560f3c341ae0, offset=offset@entry=4128768, qiov=qiov@entry=0x7f1cf1045940, is_write=is_write@entry=false, flags=flags@entry=0) at block/io.c:633\n#8  0x0000560f39562256 in bdrv_preadv (qiov=0x7f1cf1045940, offset=4128768, child=0x560f3c341ae0) at block/io.c:747\n#9  bdrv_pread (child=0x560f3c341ae0, offset=offset@entry=4128768, buf=&lt;optimized out&gt;, bytes=&lt;optimized out&gt;) at block/io.c:768\n#10 0x0000560f39544d2f in qcow2_cache_do_get (bs=bs@entry=0x560f3c43e000, c=0x560f3c3ebb60, offset=offset@entry=4128768, table=table@entry=0x7f1cf1045a10, read_from_disk=read_from_disk@entry=true) at block/qcow2-cache.c:360\n#11 0x0000560f39544eab in qcow2_cache_get (bs=bs@entry=0x560f3c43e000, c=&lt;optimized out&gt;, offset=offset@entry=4128768, table=table@entry=0x7f1cf1045a10) at block/qcow2-cache.c:384\n#12 0x0000560f39541c26 in l2_load (l2_table=0x7f1cf1045a10, l2_offset=4128768, bs=0x560f3c43e000) at block/qcow2-cluster.c:161\n#13 qcow2_get_cluster_offset (bs=bs@entry=0x560f3c43e000, offset=offset@entry=37883887616, bytes=bytes@entry=0x7f1cf1045ab4, cluster_offset=cluster_offset@entry=0x7f1cf1045ab8) at block/qcow2-cluster.c:531\n#14 0x0000560f395378a5 in qcow2_co_preadv (bs=0x560f3c43e000, offset=37883887616, bytes=49152, qiov=0x7f1cf1045b90, flags=&lt;optimized out&gt;) at block/qcow2.c:1444\n#15 0x0000560f39541944 in do_perform_cow (bytes=49152, offset_in_cluster=16384, cluster_offset=&lt;optimized out&gt;, src_cluster_offset=&lt;optimized out&gt;, bs=0x560f3c43e000) at block/qcow2-cluster.c:424\n#16 perform_cow (bs=bs@entry=0x560f3c43e000, r=r@entry=0x560f3e80e498, m=0x560f3e80e460, m=0x560f3e80e460) at block/qcow2-cluster.c:754\n#17 0x0000560f39542067 in qcow2_alloc_cluster_link_l2 (bs=bs@entry=0x560f3c43e000, m=0x560f3e80e460) at block/qcow2-cluster.c:793\n#18 0x0000560f395369e0 in qcow2_co_pwritev (bs=0x560f3c43e000, offset=37883871232, bytes=16384, qiov=0x560f3dcde040, flags=&lt;optimized out&gt;) at block/qcow2.c:1674\n#19 0x0000560f3955f811 in bdrv_driver_pwritev (bs=bs@entry=0x560f3c43e000, offset=offset@entry=37883871232, bytes=bytes@entry=16384, qiov=qiov@entry=0x560f3dcde040, flags=flags@entry=0) at block/io.c:888\n#20 0x0000560f39560bc6 in bdrv_aligned_pwritev (req=req@entry=0x7f1cf1045ec0, offset=offset@entry=37883871232, bytes=bytes@entry=16384, align=align@entry=1, qiov=0x560f3dcde040, flags=flags@entry=0, child=0x560f3c3419f0) at block/io.c:1396\n#21 0x0000560f39561702 in bdrv_co_pwritev (child=0x560f3c3419f0, offset=offset@entry=37883871232, bytes=bytes@entry=16384, qiov=qiov@entry=0x560f3dcde040, flags=0) at block/io.c:1647\n#22 0x0000560f3955304b in blk_co_pwritev (blk=0x560f3c388000, offset=37883871232, bytes=16384, qiov=0x560f3dcde040, flags=&lt;optimized out&gt;) at block/block-backend.c:995\n#23 0x0000560f395530da in blk_aio_write_entry (opaque=0x560f3eee3220) at block/block-backend.c:1186\n#24 0x0000560f395e40ea in coroutine_trampoline (i0=&lt;optimized out&gt;, i1=&lt;optimized out&gt;) at util/coroutine-ucontext.c:79\n#25 0x00007f1f186c2d40 in ?? () from /lib64/libc.so.6\n#26 0x00007ffe590d9be0 in ?? ()\n#27 0x0000000000000000 in ?? ()\n\nAnd, once again, the corresponding Thread Element is marked as THREAD_DONE, and the corresponding completion_bh is not pending of execution (scheduled = false):\n\n(gdb) p *((struct ThreadPoolReal *)0x560f3c3f64e0)-&gt;head-&gt;lh_first\n$14 = {common = {aiocb_info = 0x560f39b36570 &lt;thread_pool_aiocb_info&gt;, bs = 0x0, \n    cb = 0x560f395ce940 &lt;thread_pool_co_cb&gt;, opaque = 0x7f1cf1045680, refcnt = 0x1}, \n  pool = 0x560f3c3f64e0, func = 0x560f39558cd0 &lt;aio_worker&gt;, arg = 0x560f3de22bc0, state = THREAD_DONE, \n  ret = 0x0, reqs = {tqe_next = 0x0, tqe_prev = 0x0}, all = {le_next = 0x0, le_prev = 0x560f3c3f6578}}\n\n(gdb) p *((struct ThreadPoolReal *)0x560f3c3f64e0)-&gt;completion_bh\n$17 = {ctx = 0x560f3c377700, cb = 0x560f395ceb50 &lt;thread_pool_completion_bh&gt;, opaque = 0x560f3c3f64e0, \n  next = 0x560f3c3eb620, scheduled = false, idle = false, deleted = false}\n\nSo, basically, everything appears to be waiting for a thread_pool_completion_bh call that never happens.\n\nLooking at the history of util/thread-pool.c, I found this interesting and relatively recent (in downstream terms) commit (b7a745dc33a18377bb4a8dfe54d1df01ea60bf66):\n\n&lt;snip&gt;\n thread-pool: add missing qemu_bh_cancel in completion function\n\ncommit 3c80ca1 fixed a deadlock scenarion with nested aio_poll invocations.\n\nHowever, the rescheduling of the completion BH introcuded unnecessary spinning\nin the main-loop. On very fast file backends this can even lead to the\n"WARNING: I/O thread spun for 1000 iterations" message popping up.\n\nCallgrind reports about 3-4% less instructions with this patch running\nqemu-img bench on a ramdisk based VMDK file.\n\nFixes: 3c80ca1\nCc: qemu-stable@nongnu.org\nSigned-off-by: Peter Lieven &lt;pl@kamp.de&gt;\nSigned-off-by: Kevin Wolf &lt;kwolf@redhat.com&gt;\n&lt;/snip&gt;\n\nWhile I wasn't able to come up with a concrete scenario in which we can get the completion_bh cancelled without the element being immediately executed in the thread_pool_completion_bh loop, I think it's worth a try to revert this commit and see if the issue is still reproducible.</text>, <text>Created attachment 1348525\nBacktraces from all coroutines backing traced requests</text>, <text>Created attachment 1348526\ncoredump from an affected QEMU process</text>, <text>Hi Ladislav,\n\nI think you're subscribed to the updates of the bugzilla ticket, so you probably have seen my latest comment there. I've just attached another tarball with patched packages reverting upstream commit b7a745dc33a18377bb4a8dfe54d1df01ea60bf66 (https://github.com/qemu/qemu/commit/b7a745dc33a18377bb4a8dfe54d1df01ea60bf66).\n\nWe still don't have a reproducer, but if you can give it a try, just as you did previously, it'd help us determine if we're at least looking at the proper places.\n\nThis time the release suffix is sfdc01932613v2 (please note the "v2" at the end), and can be installed the same way as before:\n\n - yum install qemu-img-rhev-2.9.0-10.el7.sfdc01932613v2.x86_64.rpm qemu-kvm-common-rhev-2.9.0-10.el7.sfdc01932613v2.x86_64.rpm qemu-kvm-rhev-2.9.0-10.el7.sfdc01932613v2.x86_64.rpm\n\nThanks a lot,\nSergio.</text>, <text>I'm continuing my monologue by informing you that we have a test built with upstream's commit b7a745dc33a18377bb4a8dfe54d1df01ea60bf66 manually reverted:\n\n - https://brewweb.engineering.redhat.com/brew/taskinfo?taskID=14471321\n\nI've asked the customer if they can give it a try. I'll keep you posted on their feedback.</text>, <text>Hi Sergio,\n\nyes, I read the update in Bugzilla. So as we have packages available I'm going to apply them to all the compute nodes and let's try it again. I'll let you know once I have results of another batch of Jenkins jobs that keep triggering this problem.\n\nCheers,\nL.</text>, <text>Yes, I believe the following scenario is possible:\n\n    I/O thread                             worker thread\n --------------------------------------------------------------------------------\n                                           speculatively read req-&gt;state\n    req-&gt;state = THREAD_DONE;\n    qemu_bh_schedule(pool-&gt;completion_bh)\n       bh-&gt;scheduled = 1;\n                                           qemu_bh_cancel(pool-&gt;completion_bh)\n                                             bh-&gt;scheduled = 0;\n                                           if (req-&gt;state == THREAD_DONE)\n                                             // sees THREAD_QUEUED\n\nApart from reverting the fix, we could also:\n\n1) add an smp_mb after qemu_bh_cancel\n\n2) change qemu_bh_cancel to use atomic_mb_set(&amp;bh-&gt;scheduled, 0)\n\nboth of which forbid the reordering.</text>, <text>Reverting the patch, not the fix, of course.</text>, <text>Sergio: Great analysis.  I find it interesting that atomic ops are used in util/async.c:aio_bh_poll():\n\n  /* The atomic_xchg is paired with the one in qemu_bh_schedule.  The\n   * implicit memory barrier ensures that the callback sees all writes\n   * done by the scheduling thread.  It also ensures that the scheduling                           \n   * thread sees the zero before bh-&gt;cb has run, and thus will call                                \n   * aio_notify again if necessary.                                                                \n   */\n  if (atomic_xchg(&amp;bh-&gt;scheduled, 0)) {\n\nBut not in:\n\n  void qemu_bh_cancel(QEMUBH *bh)\n  {       \n      bh-&gt;scheduled = 0;\n  }\n\nStill, I can't come up with a scenario where bh-&gt;scheduled is clobbered and the BH function returns without seeing that the element is done.</text>, <text>(In reply to Stefan Hajnoczi from comment #15)\n&gt; Sergio: Great analysis.  I find it interesting that atomic ops are used in\n&gt; util/async.c:aio_bh_poll():\n&gt; \n&gt;   /* The atomic_xchg is paired with the one in qemu_bh_schedule.  The\n&gt;    * implicit memory barrier ensures that the callback sees all writes\n&gt;    * done by the scheduling thread.  It also ensures that the scheduling    \n&gt; \n&gt;    * thread sees the zero before bh-&gt;cb has run, and thus will call         \n&gt; \n&gt;    * aio_notify again if necessary.                                         \n&gt; \n&gt;    */\n&gt;   if (atomic_xchg(&amp;bh-&gt;scheduled, 0)) {\n&gt; \n&gt; But not in:\n&gt; \n&gt;   void qemu_bh_cancel(QEMUBH *bh)\n&gt;   {       \n&gt;       bh-&gt;scheduled = 0;\n&gt;   }\n&gt; \n&gt; Still, I can't come up with a scenario where bh-&gt;scheduled is clobbered and\n&gt; the BH function returns without seeing that the element is done.\n\nCollision with Paolo's comment, but he has given a case where it matters :).</text>, <text>Hello,\n\n--&gt;\n\nyes, I read the update in Bugzilla. So as we have packages available I'm going to apply them to all the compute nodes and let's try it again. I'll let you know once I have results of another batch of Jenkins jobs that keep triggering this problem.\n--&gt;\n\nDo you able to run the above test ? Can you please update us with your findings ?\n\nBest Regards,\nSachin</text>, <text>Hi folks,\n\nI had to wait for my colleague to finish his testing as he attempted to trigger the problem multiple times. All looks much better now and it seems that you have identified the root cause. I'm watching communication in Bugzilla so if you want to implement further changes and have them tested just let me know. But ATM everything's seems to be working fine.\n\nThanks for superb analysis, Sergio!\n\nCheers,\nL.</text>, <text>Hello,\n\n   Thanks for the update and good to know that provided patch by sergio helped. I will pass on this information to sergio so that he will update you further if any further action needs to be executed.\n\nMeanwhile if you have any questions feel free to update us back.\n\nBest Regards,\nSachin</text>, <text>Hi,\n\n(In reply to Paolo Bonzini from comment #13)\n&gt; Yes, I believe the following scenario is possible:\n&gt; \n&gt;     I/O thread                             worker thread\n&gt;  ----------------------------------------------------------------------------\n&gt; ----\n&gt;                                            speculatively read req-&gt;state\n&gt;     req-&gt;state = THREAD_DONE;\n&gt;     qemu_bh_schedule(pool-&gt;completion_bh)\n&gt;        bh-&gt;scheduled = 1;\n&gt;                                           \n&gt; qemu_bh_cancel(pool-&gt;completion_bh)\n&gt;                                              bh-&gt;scheduled = 0;\n&gt;                                            if (req-&gt;state == THREAD_DONE)\n&gt;                                              // sees THREAD_QUEUED\n&gt; \n&gt; Apart from reverting the fix, we could also:\n&gt; \n&gt; 1) add an smp_mb after qemu_bh_cancel\n&gt; \n&gt; 2) change qemu_bh_cancel to use atomic_mb_set(&amp;bh-&gt;scheduled, 0)\n&gt; \n&gt; both of which forbid the reordering.\n\nAfter reading this, I was wondering why the patch at comment #5 didn't work. Now I see that I wrongly assumed atomic_[read|set] would emit HW memory barriers (I should've checked the generated code).\n\nSo, IIUC (please correct if I'm wrong), we have an implicit MB at worker_thread due to qemu_bh_schedule-&gt;atomic_xchg-&gt;(Intel ASM)xchg, and your proposal is to add another one on thread_pool_completion_bh at or after qemu_bh_cancel.\n\nI think we should prepare another test build, any preference between options (1) and (2)? Personally, I'm more inclined towards (2), as in Intel and s390x it'd be optimized to just emit a single xchg instruction:\n\n(gdb) disassemble/m qemu_bh_cancel \nDump of assembler code for function qemu_bh_cancel:\n176\t{\n\n177\t    atomic_mb_set(&amp;bh-&gt;scheduled, 0);\n   0x0000000000586650 &lt;+0&gt;:\txor    %eax,%eax\n   0x0000000000586652 &lt;+2&gt;:\txchg   %al,0x20(%rdi)\n\n178\t}\n   0x0000000000586655 &lt;+5&gt;:\tretq   \n   0x0000000000586656:\tnopw   %cs:0x0(%rax,%rax,1)\n\nOn the other hand, upstream is also affected, so probably the patch should be sent there too.</text>, <text>Hi Ladislav,\n\n(In reply to Jozsa, Ladislav)\n&gt; I had to wait for my colleague to finish his testing as he attempted to trigger the problem multiple times. All looks much better now and it seems that you have identified the root cause.\n\nNice, happy to read this :-)\n\n&gt; I'm watching communication in Bugzilla so if you want to implement further changes and have them tested just let me know. But ATM everything's seems to be working fine.\n\nThanks, if you can test a new package with a proper fix (one suitable for committing upstream), that'd be great. Apparently, the initial hypothesis was right, but the implementation of the fix didn't help because I wrongly assumed atomic_[read|set] would emit HW memory barriers.\n\nSergio.</text>, <text>Yes, I am not sure which of the two would be harder to follow and/or more likely to introduce bugs in the future.  I now think (2) is not just more efficient but also clearer; as Stefan said, it matches the way aio_bh_poll resets bh-&gt;scheduled.</text>, <text>Hi Ladislav,\n\nThis is to inform you that our Engineering team is actively working on the Bugzilla 1508886.\n\nWe will soon provide you more information once It becomes available.\n\nRegards,\nChetan Nagarkar,\nRed Hat Global Support Services</text>, <text>&gt; So, IIUC (please correct if I'm wrong), we have an implicit MB at worker_thread \n&gt; due to qemu_bh_schedule-&gt;atomic_xchg-&gt;(Intel ASM)xchg, and your proposal is to \n&gt; add another one on thread_pool_completion_bh at or after qemu_bh_cancel.\n\nCorrect.\n\nAnother way to see it is that qemu_bh_schedule is a "release" operation (they happen after everything before them), and aio_bh_poll instead is an "acquire" operation (it happens before everything after it).  The only way to order stores before everything that follows (that is, to order store-load and store-store pairs) is a full memory barrier, and that's what aio_bh_poll uses.\n\nCommit b7a745dc33a18377bb4a8dfe54d1df01ea60bf66 used qemu_bh_cancel in such a way that it must be an "acquire" operation too, and needs a full memory barrier too.</text>, <text>Thanks Paolo and Stefan,\n\nI'm going to prepare a test build with this oneliner:\n\n&lt;snip&gt;\ncommit 8dff3eeaf741722759afb5013b4722fbc45868ae (HEAD -&gt; master)\nAuthor: Sergio Lopez &lt;slp@redhat.com&gt;\nDate:   Tue Nov 7 14:30:16 2017 +0100\n\n    util/async: use atomic_mb_set on qemu_bh_cancel\n    \n    Commit b7a745d added a qemu_bh_cancel call to the completion function,\n    as an optimization to prevent it from unnecessarily rescheduling itself.\n    \n    This completion function is scheduled from worker_thread, after setting\n    the state of a ThreadPoolElement to THREAD_DONE.\n    \n    This was considered to be safe, as the completion function restarts the\n    loop just after the call to qemu_bh_cancel. But, under certain access\n    patterns and scheduling conditions, the loop may wrongly use a\n    pre-fetched elem-&gt;state value, reading it as THREAD_QUEUED, and ending\n    the completion function without having processed a pending TPE linked at\n    pool-&gt;head.\n    \n    In some situations, if there are no other independent requests in the\n    same aio context that could eventually trigger the scheduling of the\n    completion function, the omitted TPE and all operations pending on it\n    will get stuck forever.\n\ndiff --git a/util/async.c b/util/async.c\nindex 355af73..0e1bd87 100644\n--- a/util/async.c\n+++ b/util/async.c\n@@ -174,7 +174,7 @@ void qemu_bh_schedule(QEMUBH *bh)\n  */\n void qemu_bh_cancel(QEMUBH *bh)\n {\n-    bh-&gt;scheduled = 0;\n+    atomic_mb_set(&amp;bh-&gt;scheduled, 0);\n }\n \n /* This func is async.The bottom half will do the delete action at the finial\n&lt;/snip&gt;</text>, <text>We now have a test build with the patch on comment #20:\n\nhttps://brewweb.engineering.redhat.com/brew/taskinfo?taskID=14485205</text>, <text>Hi Ladislav,\n\nI'm attaching to the case the third iteration of test packages, this one including the latest patch posted in bugzilla.\n\nPlease give them a try if you have the opportunity. The release tag is "sfdc01932613v3" on this one.\n\nThanks,\nSergio.</text>, <text>Thanks, I have reviewed it upstream.  Please let me know when the customer has tested it.  It will probably be merged into QEMU 2.11-rc1.</text>, <text>Thanks Sergio! If I manage to catch my colleague still at work, we'll test it today. If not, we'll do that tomorrow morning CET.\n\nCheers,\nL.</text>, <text>Thank you for the update, we will await your reply.\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>The test results still look good. The v3 is usable as well. Thanks a lot!\n\nCheers,\nL.</text>, <text>Hi L.\n\nThat is great news. Are we ok to archive this case now?\n\nBest Regards,\n\nJason Bryant\nRHCE, RHCVA\nRed Hat Inc.\nGlobal Support Services - North America\n\n***********Note***********\n\nIf your systems are registered on Red Hat Network (RHN), \nvisit https://access.redhat.com/products/red-hat-subscription-management/  \nto learn why you need to migrate to the Red Hat Subscription Management (RHSM) \ninterface by July 2017.\n\nRHV 3.x series will be retired on September 30th:\nhttps://access.redhat.com/support/policy/updates/rhev</text>, <text>The customer has confirmed that the third build, including the "util/async: use atomic_mb_set on qemu_bh_cancel" patch, fixes the issue for them too.\n\nDo you think we can move this to "POST" and aim for a sanity only verification?\n\nI'd like to start the Accelerated Fix process ASAP. I'm a bit worried, while a total lockdown like this appears to be hard to reproduce, and probably depends on a very concrete access pattern to the disk image, many customers are in the process of moving to RHEL 7.4 based hypervisors, so chances of hitting the same issue somewhere else increase day by day.</text>, <text>Hi Jason,\n\nis the hotfix you provided us Red Hat supported? If not, are you going to provide us supported hotfix?\n\nI would also prefer to keep this case opened until the package is delivered officially via z-stream. We can lower priority, though.\n\nCheers,\nL.</text>, <text>Hi Ladislav,\n\nThanks a lot for your feedback. The patch has been submitted to upstream [1], so I expect that the BZ will move to "POST" state soon. When this transition takes place, we'll start the Accelerated Fix process, so you can get a supported HotFix until the patched packages are available for RHEL 7.4.\n\nWe'll keep you updated on the status of this process.\n\nSergio.\n\n[1] http://lists.nongnu.org/archive/html/qemu-devel/2017-11/msg01323.html</text>, <text>(In reply to Sergio Lopez from comment #23)\n&gt; The customer has confirmed that the third build, including the "util/async:\n&gt; use atomic_mb_set on qemu_bh_cancel" patch, fixes the issue for them too.\n&gt; \n&gt; Do you think we can move this to "POST" and aim for a sanity only\n&gt; verification?\n&gt; \n&gt; I'd like to start the Accelerated Fix process ASAP. I'm a bit worried, while\n&gt; a total lockdown like this appears to be hard to reproduce, and probably\n&gt; depends on a very concrete access pattern to the disk image, many customers\n&gt; are in the process of moving to RHEL 7.4 based hypervisors, so chances of\n&gt; hitting the same issue somewhere else increase day by day.\n\nThe usual workflow is to set POST when backports are posted to virt-devel@redhat.com.\n\nIn order to do a backport the upstream patch must be merged (this gives us a stable git SHA1 to include in the backport commit).\n\nI have merged your patch upstream and will send a pull request for QEMU 2.11-rc1.  Once it is merged I'll post a backport and change the BZ status to POST.  This should happen by Friday.</text>, <text>(In reply to Stefan Hajnoczi from comment #24)\n\nSorry, this part was unclear:\n\n&gt; I have merged your patch upstream and will send a pull request for QEMU\n&gt; 2.11-rc1.\n\nI have merged your patch in my *sub-maintainer tree*.  Sub-maintainers send pull requests to the qemu.git maintainer, who then does the qemu.git merge.  We must wait with the backport until the qemu.git merge.</text>, <text>(In reply to Stefan Hajnoczi from comment #24)\n&gt; The usual workflow is to set POST when backports are posted to\n&gt; virt-devel@redhat.com.\n&gt; \n&gt; In order to do a backport the upstream patch must be merged (this gives us a\n&gt; stable git SHA1 to include in the backport commit).\n&gt; \n&gt; I have merged your patch upstream and will send a pull request for QEMU\n&gt; 2.11-rc1.  Once it is merged I'll post a backport and change the BZ status\n&gt; to POST.  This should happen by Friday.\n\nAck, perfect.\n\nThanks Stefan.</text>, <text>Hi Ladislav,\n\nMy name is Shivraj and I am member of Virtualization support team.\n\n\nNote: There is no timeline or SLA fixed for the RFE's and bugs. The impact of the bugs will help the team to prioritize it. \n\nWe will update you as soon as we get any further update on the it as this process takes time. For now we are keeping the case with 2 days of time frame.\n\nIn the interim if you have any queries please feel free to ask them in this ticket.\n\nRegards,\nShivraj\nGlobal Support Services\nPune, India | Red Hat\n+++++++++++++++++++++++\n\u25cf Please upgrade to RHV4.x as soon as possible. RHV 3.x series support retired on September 30th, 2017.\n\xa0https://access.redhat.com/support/policy/updates/rhev.</text>, <text>Hello,\n\nI see the engineering team is working on this bug.  I will update you as soon as I get any further update on the it as this process takes time. I will be setting a time frame of 5 days for this case.\n\nIn the interim if you have any queries please feel free to ask them in this ticket.\n\nRegards,\nGajanan Chakkarwar, RHCE, RHCSA-RHOS,RHCVA\nVirtualization &amp; containerization Team\nGlobal Support Services\nRed Hat India.</text>, <text>Note to QE:\n\nThis race condition is hard to reproduce.  Please verify this BZ by code inspection if there is no reproducer available.  The util/async.c:qemu_bh_cancel() function must look like this:\n\n  void qemu_bh_cancel(QEMUBH *bh)\n  {\n      atomic_mb_set(&amp;bh-&gt;scheduled, 0);\n  }</text>, <text>This bug has been addressed by Stefan Hajnoczi &lt;stefanha@redhat.com&gt;, with the following series:\n[RHV7.5 qemu-kvm-rhev PATCH 0/1] util/async: use atomic_mb_set in\n\tqemu_bh_cancel\n\nhttp://patchwork.virt.bos.redhat.com/patch/77664</text>, <text>Hi Aihua,\n\nI mark qa_ack+ for this bz according to comment27 and it's a little bit urgent.\n\nCould you help fill in the QA whiteboard?\n\nBest Regards,\nJunyi</text>, <text>### INTERNAL - ACCELERATED FIX REQUEST ###\n\n1. Customer impact or business case.\n\nVMs get randomly stuck, impacting the production environment. Also, this issue can't be linked with a particularity of this customer's environment, so it can potentially impact other customers.\n\n2. In the case of Production Phase 2 or 3, does the request meet the corresponding Inclusion Criteria?\n\nThis is Production Phase 1\n\n3. When does the customer need the fix? Is there a deadline?\n\nNo deadlines, but ASAP as usual.\n\n4. Which type of release is needed?\n\nInterim Fix and zstream.\n\n5. Which version of RHEL and/or the full package version is being used? Provide full kernel release (uname -r) and architecture. If the request is for Satelite, provide the Satellite version/update level (e.g., 6.2.8), using this kbase if needed: What version of Satellite am I running? - Red Hat Customer Portal\n\nqemu-kvm-rhev-2.9.0-10.el7.x86_64\nkernel-3.10.0-693.2.2.el7.x86_64\n\n6. Provide a link to the associated bugzilla.\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1508886</text>, <text>Hi,\nI am requesting z-stream for 7.4 as well as hotfix for this bugzilla.\n\nAlthough the cusomter is Red Hat INC, so it is Red HAt internal account. The VMs get randomly stuck, impacting the production environment. Also, this issue can't be linked with a particularity of this customer's environment, so it can potentially impact other customers.\n\nRoman</text>, <text>z-stream and hostfix was requested. I am currently waiting for response form the engineering.</text>, <text>Stefan, Sergio, Paolo?\n\nAre you confident in this fix for z-stream and a hotfix? \n\nIs it low risk? \n\nAre there other tests you would recommend for QE to exercise the code? Perhaps iothreads and noiothreads? Both virtio-blk and scsi? \n\nThanks!</text>, <text>Devel ack is needed to complete 3ACK process and approval.\n\n--------------------------------------------------------------------------------\n\nIf you as an assigned engineer don't know what to do with the Z-stream bugzilla\nlook at the EUS mini HOWTO at:\n\n * Developer howto:\n https://home.corp.redhat.com/wiki/working-z-streams\n\n * QE - EUS test guidelines:\n http://wiki.test.redhat.com/RhelQe/ZStreamEusTestGuidelines\n\n * EUS FAQ:\n https://home.corp.redhat.com/node/33973\n\n * If you want to learn more about EUS and z-stream you can read training\n   presentation located here:\n https://mojo.redhat.com/docs/DOC-82911</text>, <text>This bug has been copied as 7.4 z-stream (EUS) bug #1513362 and now must be\nresolved in the current update release, set blocker flag.</text>, <text>(In reply to Karen Noel from comment #31)\n&gt; Stefan, Sergio, Paolo?\n&gt; \n&gt; Are you confident in this fix for z-stream and a hotfix? \n\nYes, the customer has verified the fix for Sergio.\n\n&gt; Is it low risk? \n\nYes, it is low risk.\n\n&gt; Are there other tests you would recommend for QE to exercise the code?\n&gt; Perhaps iothreads and noiothreads? Both virtio-blk and scsi? \n\nYes, those test sets are appropriate.  Note this bug is most likely to occur with -drive aio=threads because that uses the thread pool heavily.</text>, <text>This bug has been addressed by Stefan Hajnoczi &lt;stefanha@redhat.com&gt;, with the following series:\n[RHV-7.4.z qemu-kvm-rhev PATCH 0/1] util/async: use atomic_mb_set in\n\tqemu_bh_cancel\n\nhttp://patchwork.virt.bos.redhat.com/patch/77680</text>, <text>Fix included in qemu-kvm-rhev-2.9.0-16.el7_4.12</text>, <text>Hi Aihua,\n\nCould you please verify this bug asap ?\n\n1. code inspection if there is no reproducer available: https://bugzilla.redhat.com/show_bug.cgi?id=1508886#c27\n\n2. sanity testing\n\nThanks.</text>, <text>Hello,\n\nThe engineering team is still working on this bug.  I will update you as soon as I get any further update on the it as this process takes time. I will be setting a time frame of 7 days for this case.\n\nIn the interim if you have any queries please feel free to ask them in this ticket.\n\nBest Regards,\nSachin</text>, <text>Hello,\n\n   Just wanted to update that engineering still working on the issue and will keep you posted with further progress of it.\n\nBest Regards,\nSachin</text>, <text>Fix included in qemu-kvm-rhev-2.10.0-7.el7</text>, <text>Bug report changed to ON_QA status by Errata System.\nA QE request has been submitted for advisory RHBA-2017:30729-01\nhttps://errata.devel.redhat.com/advisory/30729</text>, <text>Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n\nSanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+ 3.10.0-693.2.1.el7.x86_64,PASS.</text>, <text>(In reply to aihua liang from comment #5)\n&gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; \n&gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n\nHi Stefan,\n\nQE could not reproduce this bug, sanity testing and code inspection based on BZ1508886#c27 pass, do you think is it ok or still need ask customer to have a try ?\n\nThanks.</text>, <text>(In reply to CongLi from comment #6)\n&gt; (In reply to aihua liang from comment #5)\n&gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; \n&gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; \n&gt; Hi Stefan,\n&gt; \n&gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; a try ?\n\nThe customer confirmed that a custom build with the patch solves the problem.\n\nThanks for doing the code inspection.  I would consider this BZ good to go.</text>, <text>*** Bug 1510728 has been marked as a duplicate of this bug. ***</text>, <text>(In reply to Stefan Hajnoczi from comment #7)\n&gt; (In reply to CongLi from comment #6)\n&gt; &gt; (In reply to aihua liang from comment #5)\n&gt; &gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; &gt; \n&gt; &gt; Hi Stefan,\n&gt; &gt; \n&gt; &gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; &gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; &gt; a try ?\n&gt; \n&gt; The customer confirmed that a custom build with the patch solves the problem.\n&gt; \n&gt; Thanks for doing the code inspection.  I would consider this BZ good to go.\n\nThanks Stefan for the confirmation.\n\nBased on comment 5 and comment 7, set this bug to 'VERIFIED'.\n\nThanks.</text>, <text>Hello,\n\n   Just wanted to update that engineering still working on the issue and will keep you posted with further progress of it.\n\nBest Regards,\nSachin</text>, <text>Code check on qemu-kvm-rhev-2.10.0-7.el7: PASS\nSanity Test on qemu-kvm-rhev-2.10.0-7.el7 + 3.10.0-796.el7.x86_64: PASS\n\nSo, set bug's status to "Verified".</text>, <text>Hello,\n\n   Just wanted to update that the bug #1508886 status is now 'verified' that means the patch has been tested successfully. Awaiting to get next updates from engineering now. I will keep you updated with further progress.\n\nBest Regards,\nSachin</text>, <text>+++ This bug was initially created as a clone of Bug #1513362 +++\n\nThis bug has been copied from bug #1508886 and has been proposed to be backported to 7.4 z-stream (EUS).\n\n--- Additional comment from Oneata Mircea Teodor on 2017-11-15 04:53:24 EST ---\n\nDevel ack is needed to complete 3ACK process and approval.\n\n--------------------------------------------------------------------------------\n\nIf you as an assigned engineer don't know what to do with the Z-stream bugzilla\nlook at the EUS mini HOWTO at:\n\n * Developer howto:\n https://home.corp.redhat.com/wiki/working-z-streams\n\n * QE - EUS test guidelines:\n http://wiki.test.redhat.com/RhelQe/ZStreamEusTestGuidelines\n\n * EUS FAQ:\n https://home.corp.redhat.com/node/33973\n\n * If you want to learn more about EUS and z-stream you can read training\n   presentation located here:\n https://mojo.redhat.com/docs/DOC-82911\n\n--- Additional comment from Danilo Cesar de Paula on 2017-11-15 08:41:09 EST ---\n\n\nThis bug has been addressed by Stefan Hajnoczi &lt;stefanha@redhat.com&gt;, with the following series:\n[RHV-7.4.z qemu-kvm-rhev PATCH 0/1] util/async: use atomic_mb_set in\n\tqemu_bh_cancel\n\nhttp://patchwork.virt.bos.redhat.com/patch/77680\n\n--- Additional comment from Miroslav Rezanina on 2017-11-16 03:25:49 EST ---\n\nFix included in qemu-kvm-rhev-2.9.0-16.el7_4.12\n\n--- Additional comment from CongLi on 2017-11-16 20:55:39 EST ---\n\nHi Aihua,\n\nCould you please verify this bug asap ?\n\n1. code inspection if there is no reproducer available: https://bugzilla.redhat.com/show_bug.cgi?id=1508886#c27\n\n2. sanity testing\n\nThanks.\n\n--- Additional comment from aihua liang on 2017-11-22 22:05:29 EST ---\n\n\nCode checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n\nSanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+ 3.10.0-693.2.1.el7.x86_64,PASS.\n\n--- Additional comment from CongLi on 2017-11-22 22:11:51 EST ---\n\n(In reply to aihua liang from comment #5)\n&gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; \n&gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n\nHi Stefan,\n\nQE could not reproduce this bug, sanity testing and code inspection based on BZ1508886#c27 pass, do you think is it ok or still need ask customer to have a try ?\n\nThanks.\n\n--- Additional comment from Stefan Hajnoczi on 2017-11-23 06:27:45 EST ---\n\n(In reply to CongLi from comment #6)\n&gt; (In reply to aihua liang from comment #5)\n&gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; \n&gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; \n&gt; Hi Stefan,\n&gt; \n&gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; a try ?\n\nThe customer confirmed that a custom build with the patch solves the problem.\n\nThanks for doing the code inspection.  I would consider this BZ good to go.\n\n--- Additional comment from CongLi on 2017-11-23 20:16:48 EST ---\n\n(In reply to Stefan Hajnoczi from comment #7)\n&gt; (In reply to CongLi from comment #6)\n&gt; &gt; (In reply to aihua liang from comment #5)\n&gt; &gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; &gt; \n&gt; &gt; Hi Stefan,\n&gt; &gt; \n&gt; &gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; &gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; &gt; a try ?\n&gt; \n&gt; The customer confirmed that a custom build with the patch solves the problem.\n&gt; \n&gt; Thanks for doing the code inspection.  I would consider this BZ good to go.\n\nThanks Stefan for the confirmation.\n\nBased on comment 5 and comment 7, set this bug to 'VERIFIED'.\n\nThanks.</text>, <text>BZ to track inclusion to OSP12</text>, <text>+++ This bug was initially created as a clone of Bug #1513362 +++\n\nThis bug has been copied from bug #1508886 and has been proposed to be backported to 7.4 z-stream (EUS).\n\n--- Additional comment from Oneata Mircea Teodor on 2017-11-15 04:53:24 EST ---\n\nDevel ack is needed to complete 3ACK process and approval.\n\n--------------------------------------------------------------------------------\n\nIf you as an assigned engineer don't know what to do with the Z-stream bugzilla\nlook at the EUS mini HOWTO at:\n\n * Developer howto:\n https://home.corp.redhat.com/wiki/working-z-streams\n\n * QE - EUS test guidelines:\n http://wiki.test.redhat.com/RhelQe/ZStreamEusTestGuidelines\n\n * EUS FAQ:\n https://home.corp.redhat.com/node/33973\n\n * If you want to learn more about EUS and z-stream you can read training\n   presentation located here:\n https://mojo.redhat.com/docs/DOC-82911\n\n--- Additional comment from Danilo Cesar de Paula on 2017-11-15 08:41:09 EST ---\n\n\nThis bug has been addressed by Stefan Hajnoczi &lt;stefanha@redhat.com&gt;, with the following series:\n[RHV-7.4.z qemu-kvm-rhev PATCH 0/1] util/async: use atomic_mb_set in\n\tqemu_bh_cancel\n\nhttp://patchwork.virt.bos.redhat.com/patch/77680\n\n--- Additional comment from Miroslav Rezanina on 2017-11-16 03:25:49 EST ---\n\nFix included in qemu-kvm-rhev-2.9.0-16.el7_4.12\n\n--- Additional comment from CongLi on 2017-11-16 20:55:39 EST ---\n\nHi Aihua,\n\nCould you please verify this bug asap ?\n\n1. code inspection if there is no reproducer available: https://bugzilla.redhat.com/show_bug.cgi?id=1508886#c27\n\n2. sanity testing\n\nThanks.\n\n--- Additional comment from aihua liang on 2017-11-22 22:05:29 EST ---\n\n\nCode checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n\nSanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+ 3.10.0-693.2.1.el7.x86_64,PASS.\n\n--- Additional comment from CongLi on 2017-11-22 22:11:51 EST ---\n\n(In reply to aihua liang from comment #5)\n&gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; \n&gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n\nHi Stefan,\n\nQE could not reproduce this bug, sanity testing and code inspection based on BZ1508886#c27 pass, do you think is it ok or still need ask customer to have a try ?\n\nThanks.\n\n--- Additional comment from Stefan Hajnoczi on 2017-11-23 06:27:45 EST ---\n\n(In reply to CongLi from comment #6)\n&gt; (In reply to aihua liang from comment #5)\n&gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; \n&gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; \n&gt; Hi Stefan,\n&gt; \n&gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; a try ?\n\nThe customer confirmed that a custom build with the patch solves the problem.\n\nThanks for doing the code inspection.  I would consider this BZ good to go.\n\n--- Additional comment from CongLi on 2017-11-23 20:16:48 EST ---\n\n(In reply to Stefan Hajnoczi from comment #7)\n&gt; (In reply to CongLi from comment #6)\n&gt; &gt; (In reply to aihua liang from comment #5)\n&gt; &gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; &gt; \n&gt; &gt; Hi Stefan,\n&gt; &gt; \n&gt; &gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; &gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; &gt; a try ?\n&gt; \n&gt; The customer confirmed that a custom build with the patch solves the problem.\n&gt; \n&gt; Thanks for doing the code inspection.  I would consider this BZ good to go.\n\nThanks Stefan for the confirmation.\n\nBased on comment 5 and comment 7, set this bug to 'VERIFIED'.\n\nThanks.</text>, <text>BZ to track inclusion to OSP11</text>, <text>+++ This bug was initially created as a clone of Bug #1513362 +++\n\nThis bug has been copied from bug #1508886 and has been proposed to be backported to 7.4 z-stream (EUS).\n\n--- Additional comment from Oneata Mircea Teodor on 2017-11-15 04:53:24 EST ---\n\nDevel ack is needed to complete 3ACK process and approval.\n\n--------------------------------------------------------------------------------\n\nIf you as an assigned engineer don't know what to do with the Z-stream bugzilla\nlook at the EUS mini HOWTO at:\n\n * Developer howto:\n https://home.corp.redhat.com/wiki/working-z-streams\n\n * QE - EUS test guidelines:\n http://wiki.test.redhat.com/RhelQe/ZStreamEusTestGuidelines\n\n * EUS FAQ:\n https://home.corp.redhat.com/node/33973\n\n * If you want to learn more about EUS and z-stream you can read training\n   presentation located here:\n https://mojo.redhat.com/docs/DOC-82911\n\n--- Additional comment from Danilo Cesar de Paula on 2017-11-15 08:41:09 EST ---\n\n\nThis bug has been addressed by Stefan Hajnoczi &lt;stefanha@redhat.com&gt;, with the following series:\n[RHV-7.4.z qemu-kvm-rhev PATCH 0/1] util/async: use atomic_mb_set in\n\tqemu_bh_cancel\n\nhttp://patchwork.virt.bos.redhat.com/patch/77680\n\n--- Additional comment from Miroslav Rezanina on 2017-11-16 03:25:49 EST ---\n\nFix included in qemu-kvm-rhev-2.9.0-16.el7_4.12\n\n--- Additional comment from CongLi on 2017-11-16 20:55:39 EST ---\n\nHi Aihua,\n\nCould you please verify this bug asap ?\n\n1. code inspection if there is no reproducer available: https://bugzilla.redhat.com/show_bug.cgi?id=1508886#c27\n\n2. sanity testing\n\nThanks.\n\n--- Additional comment from aihua liang on 2017-11-22 22:05:29 EST ---\n\n\nCode checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n\nSanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+ 3.10.0-693.2.1.el7.x86_64,PASS.\n\n--- Additional comment from CongLi on 2017-11-22 22:11:51 EST ---\n\n(In reply to aihua liang from comment #5)\n&gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; \n&gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n\nHi Stefan,\n\nQE could not reproduce this bug, sanity testing and code inspection based on BZ1508886#c27 pass, do you think is it ok or still need ask customer to have a try ?\n\nThanks.\n\n--- Additional comment from Stefan Hajnoczi on 2017-11-23 06:27:45 EST ---\n\n(In reply to CongLi from comment #6)\n&gt; (In reply to aihua liang from comment #5)\n&gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; \n&gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; \n&gt; Hi Stefan,\n&gt; \n&gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; a try ?\n\nThe customer confirmed that a custom build with the patch solves the problem.\n\nThanks for doing the code inspection.  I would consider this BZ good to go.\n\n--- Additional comment from CongLi on 2017-11-23 20:16:48 EST ---\n\n(In reply to Stefan Hajnoczi from comment #7)\n&gt; (In reply to CongLi from comment #6)\n&gt; &gt; (In reply to aihua liang from comment #5)\n&gt; &gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; &gt; \n&gt; &gt; Hi Stefan,\n&gt; &gt; \n&gt; &gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; &gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; &gt; a try ?\n&gt; \n&gt; The customer confirmed that a custom build with the patch solves the problem.\n&gt; \n&gt; Thanks for doing the code inspection.  I would consider this BZ good to go.\n\nThanks Stefan for the confirmation.\n\nBased on comment 5 and comment 7, set this bug to 'VERIFIED'.\n\nThanks.</text>, <text>BZ to track inclusion to OSP10</text>, <text>+++ This bug was initially created as a clone of Bug #1513362 +++\n\nThis bug has been copied from bug #1508886 and has been proposed to be backported to 7.4 z-stream (EUS).\n\n--- Additional comment from Oneata Mircea Teodor on 2017-11-15 04:53:24 EST ---\n\nDevel ack is needed to complete 3ACK process and approval.\n\n--------------------------------------------------------------------------------\n\nIf you as an assigned engineer don't know what to do with the Z-stream bugzilla\nlook at the EUS mini HOWTO at:\n\n * Developer howto:\n https://home.corp.redhat.com/wiki/working-z-streams\n\n * QE - EUS test guidelines:\n http://wiki.test.redhat.com/RhelQe/ZStreamEusTestGuidelines\n\n * EUS FAQ:\n https://home.corp.redhat.com/node/33973\n\n * If you want to learn more about EUS and z-stream you can read training\n   presentation located here:\n https://mojo.redhat.com/docs/DOC-82911\n\n--- Additional comment from Danilo Cesar de Paula on 2017-11-15 08:41:09 EST ---\n\n\nThis bug has been addressed by Stefan Hajnoczi &lt;stefanha@redhat.com&gt;, with the following series:\n[RHV-7.4.z qemu-kvm-rhev PATCH 0/1] util/async: use atomic_mb_set in\n\tqemu_bh_cancel\n\nhttp://patchwork.virt.bos.redhat.com/patch/77680\n\n--- Additional comment from Miroslav Rezanina on 2017-11-16 03:25:49 EST ---\n\nFix included in qemu-kvm-rhev-2.9.0-16.el7_4.12\n\n--- Additional comment from CongLi on 2017-11-16 20:55:39 EST ---\n\nHi Aihua,\n\nCould you please verify this bug asap ?\n\n1. code inspection if there is no reproducer available: https://bugzilla.redhat.com/show_bug.cgi?id=1508886#c27\n\n2. sanity testing\n\nThanks.\n\n--- Additional comment from aihua liang on 2017-11-22 22:05:29 EST ---\n\n\nCode checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n\nSanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+ 3.10.0-693.2.1.el7.x86_64,PASS.\n\n--- Additional comment from CongLi on 2017-11-22 22:11:51 EST ---\n\n(In reply to aihua liang from comment #5)\n&gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; \n&gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n\nHi Stefan,\n\nQE could not reproduce this bug, sanity testing and code inspection based on BZ1508886#c27 pass, do you think is it ok or still need ask customer to have a try ?\n\nThanks.\n\n--- Additional comment from Stefan Hajnoczi on 2017-11-23 06:27:45 EST ---\n\n(In reply to CongLi from comment #6)\n&gt; (In reply to aihua liang from comment #5)\n&gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; \n&gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; \n&gt; Hi Stefan,\n&gt; \n&gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; a try ?\n\nThe customer confirmed that a custom build with the patch solves the problem.\n\nThanks for doing the code inspection.  I would consider this BZ good to go.\n\n--- Additional comment from CongLi on 2017-11-23 20:16:48 EST ---\n\n(In reply to Stefan Hajnoczi from comment #7)\n&gt; (In reply to CongLi from comment #6)\n&gt; &gt; (In reply to aihua liang from comment #5)\n&gt; &gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; &gt; \n&gt; &gt; Hi Stefan,\n&gt; &gt; \n&gt; &gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; &gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; &gt; a try ?\n&gt; \n&gt; The customer confirmed that a custom build with the patch solves the problem.\n&gt; \n&gt; Thanks for doing the code inspection.  I would consider this BZ good to go.\n\nThanks Stefan for the confirmation.\n\nBased on comment 5 and comment 7, set this bug to 'VERIFIED'.\n\nThanks.</text>, <text>BZ to track inclusion to OSP9</text>, <text>+++ This bug was initially created as a clone of Bug #1513362 +++\n\nThis bug has been copied from bug #1508886 and has been proposed to be backported to 7.4 z-stream (EUS).\n\n--- Additional comment from Oneata Mircea Teodor on 2017-11-15 04:53:24 EST ---\n\nDevel ack is needed to complete 3ACK process and approval.\n\n--------------------------------------------------------------------------------\n\nIf you as an assigned engineer don't know what to do with the Z-stream bugzilla\nlook at the EUS mini HOWTO at:\n\n * Developer howto:\n https://home.corp.redhat.com/wiki/working-z-streams\n\n * QE - EUS test guidelines:\n http://wiki.test.redhat.com/RhelQe/ZStreamEusTestGuidelines\n\n * EUS FAQ:\n https://home.corp.redhat.com/node/33973\n\n * If you want to learn more about EUS and z-stream you can read training\n   presentation located here:\n https://mojo.redhat.com/docs/DOC-82911\n\n--- Additional comment from Danilo Cesar de Paula on 2017-11-15 08:41:09 EST ---\n\n\nThis bug has been addressed by Stefan Hajnoczi &lt;stefanha@redhat.com&gt;, with the following series:\n[RHV-7.4.z qemu-kvm-rhev PATCH 0/1] util/async: use atomic_mb_set in\n\tqemu_bh_cancel\n\nhttp://patchwork.virt.bos.redhat.com/patch/77680\n\n--- Additional comment from Miroslav Rezanina on 2017-11-16 03:25:49 EST ---\n\nFix included in qemu-kvm-rhev-2.9.0-16.el7_4.12\n\n--- Additional comment from CongLi on 2017-11-16 20:55:39 EST ---\n\nHi Aihua,\n\nCould you please verify this bug asap ?\n\n1. code inspection if there is no reproducer available: https://bugzilla.redhat.com/show_bug.cgi?id=1508886#c27\n\n2. sanity testing\n\nThanks.\n\n--- Additional comment from aihua liang on 2017-11-22 22:05:29 EST ---\n\n\nCode checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n\nSanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+ 3.10.0-693.2.1.el7.x86_64,PASS.\n\n--- Additional comment from CongLi on 2017-11-22 22:11:51 EST ---\n\n(In reply to aihua liang from comment #5)\n&gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; \n&gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n\nHi Stefan,\n\nQE could not reproduce this bug, sanity testing and code inspection based on BZ1508886#c27 pass, do you think is it ok or still need ask customer to have a try ?\n\nThanks.\n\n--- Additional comment from Stefan Hajnoczi on 2017-11-23 06:27:45 EST ---\n\n(In reply to CongLi from comment #6)\n&gt; (In reply to aihua liang from comment #5)\n&gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; \n&gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; \n&gt; Hi Stefan,\n&gt; \n&gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; a try ?\n\nThe customer confirmed that a custom build with the patch solves the problem.\n\nThanks for doing the code inspection.  I would consider this BZ good to go.\n\n--- Additional comment from CongLi on 2017-11-23 20:16:48 EST ---\n\n(In reply to Stefan Hajnoczi from comment #7)\n&gt; (In reply to CongLi from comment #6)\n&gt; &gt; (In reply to aihua liang from comment #5)\n&gt; &gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; &gt; \n&gt; &gt; Hi Stefan,\n&gt; &gt; \n&gt; &gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; &gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; &gt; a try ?\n&gt; \n&gt; The customer confirmed that a custom build with the patch solves the problem.\n&gt; \n&gt; Thanks for doing the code inspection.  I would consider this BZ good to go.\n\nThanks Stefan for the confirmation.\n\nBased on comment 5 and comment 7, set this bug to 'VERIFIED'.\n\nThanks.</text>, <text>BZ to track inclusion to OSP8</text>, <text>+++ This bug was initially created as a clone of Bug #1513362 +++\n\nThis bug has been copied from bug #1508886 and has been proposed to be backported to 7.4 z-stream (EUS).\n\n--- Additional comment from Oneata Mircea Teodor on 2017-11-15 04:53:24 EST ---\n\nDevel ack is needed to complete 3ACK process and approval.\n\n--------------------------------------------------------------------------------\n\nIf you as an assigned engineer don't know what to do with the Z-stream bugzilla\nlook at the EUS mini HOWTO at:\n\n * Developer howto:\n https://home.corp.redhat.com/wiki/working-z-streams\n\n * QE - EUS test guidelines:\n http://wiki.test.redhat.com/RhelQe/ZStreamEusTestGuidelines\n\n * EUS FAQ:\n https://home.corp.redhat.com/node/33973\n\n * If you want to learn more about EUS and z-stream you can read training\n   presentation located here:\n https://mojo.redhat.com/docs/DOC-82911\n\n--- Additional comment from Danilo Cesar de Paula on 2017-11-15 08:41:09 EST ---\n\n\nThis bug has been addressed by Stefan Hajnoczi &lt;stefanha@redhat.com&gt;, with the following series:\n[RHV-7.4.z qemu-kvm-rhev PATCH 0/1] util/async: use atomic_mb_set in\n\tqemu_bh_cancel\n\nhttp://patchwork.virt.bos.redhat.com/patch/77680\n\n--- Additional comment from Miroslav Rezanina on 2017-11-16 03:25:49 EST ---\n\nFix included in qemu-kvm-rhev-2.9.0-16.el7_4.12\n\n--- Additional comment from CongLi on 2017-11-16 20:55:39 EST ---\n\nHi Aihua,\n\nCould you please verify this bug asap ?\n\n1. code inspection if there is no reproducer available: https://bugzilla.redhat.com/show_bug.cgi?id=1508886#c27\n\n2. sanity testing\n\nThanks.\n\n--- Additional comment from aihua liang on 2017-11-22 22:05:29 EST ---\n\n\nCode checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n\nSanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+ 3.10.0-693.2.1.el7.x86_64,PASS.\n\n--- Additional comment from CongLi on 2017-11-22 22:11:51 EST ---\n\n(In reply to aihua liang from comment #5)\n&gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; \n&gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n\nHi Stefan,\n\nQE could not reproduce this bug, sanity testing and code inspection based on BZ1508886#c27 pass, do you think is it ok or still need ask customer to have a try ?\n\nThanks.\n\n--- Additional comment from Stefan Hajnoczi on 2017-11-23 06:27:45 EST ---\n\n(In reply to CongLi from comment #6)\n&gt; (In reply to aihua liang from comment #5)\n&gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; \n&gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; \n&gt; Hi Stefan,\n&gt; \n&gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; a try ?\n\nThe customer confirmed that a custom build with the patch solves the problem.\n\nThanks for doing the code inspection.  I would consider this BZ good to go.\n\n--- Additional comment from CongLi on 2017-11-23 20:16:48 EST ---\n\n(In reply to Stefan Hajnoczi from comment #7)\n&gt; (In reply to CongLi from comment #6)\n&gt; &gt; (In reply to aihua liang from comment #5)\n&gt; &gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; &gt; \n&gt; &gt; Hi Stefan,\n&gt; &gt; \n&gt; &gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; &gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; &gt; a try ?\n&gt; \n&gt; The customer confirmed that a custom build with the patch solves the problem.\n&gt; \n&gt; Thanks for doing the code inspection.  I would consider this BZ good to go.\n\nThanks Stefan for the confirmation.\n\nBased on comment 5 and comment 7, set this bug to 'VERIFIED'.\n\nThanks.</text>, <text>BZ to track inclusion to OSP7</text>, <text>+++ This bug was initially created as a clone of Bug #1513362 +++\n\nThis bug has been copied from bug #1508886 and has been proposed to be backported to 7.4 z-stream (EUS).\n\n--- Additional comment from Oneata Mircea Teodor on 2017-11-15 04:53:24 EST ---\n\nDevel ack is needed to complete 3ACK process and approval.\n\n--------------------------------------------------------------------------------\n\nIf you as an assigned engineer don't know what to do with the Z-stream bugzilla\nlook at the EUS mini HOWTO at:\n\n * Developer howto:\n https://home.corp.redhat.com/wiki/working-z-streams\n\n * QE - EUS test guidelines:\n http://wiki.test.redhat.com/RhelQe/ZStreamEusTestGuidelines\n\n * EUS FAQ:\n https://home.corp.redhat.com/node/33973\n\n * If you want to learn more about EUS and z-stream you can read training\n   presentation located here:\n https://mojo.redhat.com/docs/DOC-82911\n\n--- Additional comment from Danilo Cesar de Paula on 2017-11-15 08:41:09 EST ---\n\n\nThis bug has been addressed by Stefan Hajnoczi &lt;stefanha@redhat.com&gt;, with the following series:\n[RHV-7.4.z qemu-kvm-rhev PATCH 0/1] util/async: use atomic_mb_set in\n\tqemu_bh_cancel\n\nhttp://patchwork.virt.bos.redhat.com/patch/77680\n\n--- Additional comment from Miroslav Rezanina on 2017-11-16 03:25:49 EST ---\n\nFix included in qemu-kvm-rhev-2.9.0-16.el7_4.12\n\n--- Additional comment from CongLi on 2017-11-16 20:55:39 EST ---\n\nHi Aihua,\n\nCould you please verify this bug asap ?\n\n1. code inspection if there is no reproducer available: https://bugzilla.redhat.com/show_bug.cgi?id=1508886#c27\n\n2. sanity testing\n\nThanks.\n\n--- Additional comment from aihua liang on 2017-11-22 22:05:29 EST ---\n\n\nCode checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n\nSanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+ 3.10.0-693.2.1.el7.x86_64,PASS.\n\n--- Additional comment from CongLi on 2017-11-22 22:11:51 EST ---\n\n(In reply to aihua liang from comment #5)\n&gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; \n&gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n\nHi Stefan,\n\nQE could not reproduce this bug, sanity testing and code inspection based on BZ1508886#c27 pass, do you think is it ok or still need ask customer to have a try ?\n\nThanks.\n\n--- Additional comment from Stefan Hajnoczi on 2017-11-23 06:27:45 EST ---\n\n(In reply to CongLi from comment #6)\n&gt; (In reply to aihua liang from comment #5)\n&gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; \n&gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; \n&gt; Hi Stefan,\n&gt; \n&gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; a try ?\n\nThe customer confirmed that a custom build with the patch solves the problem.\n\nThanks for doing the code inspection.  I would consider this BZ good to go.\n\n--- Additional comment from CongLi on 2017-11-23 20:16:48 EST ---\n\n(In reply to Stefan Hajnoczi from comment #7)\n&gt; (In reply to CongLi from comment #6)\n&gt; &gt; (In reply to aihua liang from comment #5)\n&gt; &gt; &gt; Code checking done in qemu-kvm-rhev-2.9.0-16.el7_4.12,PASS.\n&gt; &gt; &gt; \n&gt; &gt; &gt; Sanity test on qemu-kvm-rhev-2.9.0-16.el7_4.12+\n&gt; &gt; &gt; 3.10.0-693.2.1.el7.x86_64,PASS.\n&gt; &gt; \n&gt; &gt; Hi Stefan,\n&gt; &gt; \n&gt; &gt; QE could not reproduce this bug, sanity testing and code inspection based on\n&gt; &gt; BZ1508886#c27 pass, do you think is it ok or still need ask customer to have\n&gt; &gt; a try ?\n&gt; \n&gt; The customer confirmed that a custom build with the patch solves the problem.\n&gt; \n&gt; Thanks for doing the code inspection.  I would consider this BZ good to go.\n\nThanks Stefan for the confirmation.\n\nBased on comment 5 and comment 7, set this bug to 'VERIFIED'.\n\nThanks.</text>, <text>BZ to track inclusion to OSP6</text>, <text>Karen,\n\nWould you be the right contact as from the OSP HF process in [1] to give\nthe hot_fix_requested + ack that we can sign and deliver qemu-kvm-rhev-2.9.0-16.el7_4.12 to customer Monaco Telecom via SFDC case 01982914 ?\n\nThanks,\nMartin\n\nhttps://mojo.redhat.com/docs/DOC-1102250</text>, <text>(In reply to Stefan Hajnoczi from comment #33)\n&gt; (In reply to Karen Noel from comment #31)\n&gt; &gt; Stefan, Sergio, Paolo?\n&gt; &gt; \n&gt; &gt; Are you confident in this fix for z-stream and a hotfix? \n&gt; \n&gt; Yes, the customer has verified the fix for Sergio.\n&gt; \n&gt; &gt; Is it low risk? \n&gt; \n&gt; Yes, it is low risk.\n&gt; \n&gt; &gt; Are there other tests you would recommend for QE to exercise the code?\n&gt; &gt; Perhaps iothreads and noiothreads? Both virtio-blk and scsi? \n&gt; \n&gt; Yes, those test sets are appropriate.  Note this bug is most likely to occur\n&gt; with -drive aio=threads because that uses the thread pool heavily.\n\nI approve the hotfix request from engineering. \n\nRoman, Who can approve the hotfix and set the flag? Should we also approve this on the 7.4.z Bugzilla? Thanks.</text>, <text>(In reply to Martin Schuppert from comment #2)\n&gt; Karen,\n&gt; \n&gt; Would you be the right contact as from the OSP HF process in [1] to give\n&gt; the hot_fix_requested + ack that we can sign and deliver\n&gt; qemu-kvm-rhev-2.9.0-16.el7_4.12 to customer Monaco Telecom via SFDC case\n&gt; 01982914 ?\n&gt; \n&gt; Thanks,\n&gt; Martin\n&gt; \n&gt; https://mojo.redhat.com/docs/DOC-1102250\n\nI added my approval on BZ 1508886. But, I can't set the flag.\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1508886#c38\n\nThe flag and approval might need to be set on the 7.4.z bug:\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1513362</text>, <text>due to comment3 set hot_fix_requested + for OSP delivery for customer Monaco Telecom via via SFDC case 01982914.\n\nHotfix signed package at:\nhttp://download-node-02.eng.bos.redhat.com/brewroot/packages/qemu-kvm-rhev/2.9.0/16.el7_4.12/data/signed/f21541eb/</text>, <text>Remote Support Invitation\n\nClick on the link below and follow the directions.\n\nhttps://remotesupport.redhat.com/?ak=cfc7b3137b2c4c4936f80bbb2ffcc320\n\nBomgar(TM) enables a support representative to view your screen in order to assist you.\nSession traffic is fully encrypted to protect your system's data.\nOnce a session has begun, you will be able to end it at any time</text>, <text>(In reply to Karen Noel from comment #38)\n&gt; (In reply to Stefan Hajnoczi from comment #33)\n&gt; &gt; (In reply to Karen Noel from comment #31)\n&gt; &gt; &gt; Stefan, Sergio, Paolo?\n&gt; &gt; &gt; \n&gt; &gt; &gt; Are you confident in this fix for z-stream and a hotfix? \n&gt; &gt; \n&gt; &gt; Yes, the customer has verified the fix for Sergio.\n&gt; &gt; \n&gt; &gt; &gt; Is it low risk? \n&gt; &gt; \n&gt; &gt; Yes, it is low risk.\n&gt; &gt; \n&gt; &gt; &gt; Are there other tests you would recommend for QE to exercise the code?\n&gt; &gt; &gt; Perhaps iothreads and noiothreads? Both virtio-blk and scsi? \n&gt; &gt; \n&gt; &gt; Yes, those test sets are appropriate.  Note this bug is most likely to occur\n&gt; &gt; with -drive aio=threads because that uses the thread pool heavily.\n&gt; \n&gt; I approve the hotfix request from engineering. \n&gt; \n&gt; Roman, Who can approve the hotfix and set the flag? Should we also approve\n&gt; this on the 7.4.z Bugzilla? Thanks.\n\nDo you mean "hot_fix_requested" ? This one is just for tracking purposes. I will set that to + as soon as the build is available.</text>, <text>Bug report changed to ON_QA status by Errata System.\nA QE request has been submitted for advisory RHSA-2017:31978-01\nhttps://errata.devel.redhat.com/advisory/31978</text>, <text>Bug report changed to ON_QA status by Errata System.\nA QE request has been submitted for advisory RHSA-2017:31979-01\nhttps://errata.devel.redhat.com/advisory/31979</text>, <text>Bug report changed to ON_QA status by Errata System.\nA QE request has been submitted for advisory RHSA-2017:31980-01\nhttps://errata.devel.redhat.com/advisory/31980</text>, <text>Bug report changed to ON_QA status by Errata System.\nA QE request has been submitted for advisory RHSA-2017:31981-01\nhttps://errata.devel.redhat.com/advisory/31981</text>, <text>Bug report changed to ON_QA status by Errata System.\nA QE request has been submitted for advisory RHSA-2017:31982-01\nhttps://errata.devel.redhat.com/advisory/31982</text>, <text>Bug report changed to ON_QA status by Errata System.\nA QE request has been submitted for advisory RHSA-2017:31983-01\nhttps://errata.devel.redhat.com/advisory/31983</text>, <text>Bug report changed to ON_QA status by Errata System.\nA QE request has been submitted for advisory RHSA-2017:31984-01\nhttps://errata.devel.redhat.com/advisory/31984</text>, <text>*** Bug 1524770 has been marked as a duplicate of this bug. ***</text>, <text>Based on HF ACK in https://bugzilla.redhat.com/show_bug.cgi?id=1525504#c3 set hot_fix_requested + for OSP delivery for customer ERICSSON via SFDC case 01995790.\n\nHotfix signed package at:\nhttp://download-node-02.eng.bos.redhat.com/brewroot/packages/qemu-kvm-rhev/2.9.0/16.el7_4.12/data/signed/f21541eb/</text>, <text>delivery qemu-kvm-rhev-2.9.0-16.el7_4.12 for customer Alcatel - Canada (5620 SAM) via SFDC case 01988424.</text>, <text>Hello,\n\n   Just wanted to update that there is no new updates available at the moment. Awaiting to get next updates from engineering now. I will keep you updated with further progress.\n\nBest Regards,\nSachin</text>, <text>Passing CI and verification from RHEL https://bugzilla.redhat.com/show_bug.cgi?id=1513362</text>, <text>Passing CI and verification from RHEL https://bugzilla.redhat.com/show_bug.cgi?id=1513362</text>, <text>Passing CI and verification from RHEL https://bugzilla.redhat.com/show_bug.cgi?id=1513362</text>, <text>Passing CI and verification from RHEL https://bugzilla.redhat.com/show_bug.cgi?id=1513362</text>, <text>Passing CI and verification from RHEL https://bugzilla.redhat.com/show_bug.cgi?id=1513362</text>, <text>Passing CI and verification from RHEL https://bugzilla.redhat.com/show_bug.cgi?id=1513362</text>, <text>Passing CI and verification from RHEL https://bugzilla.redhat.com/show_bug.cgi?id=1513362</text>, <text>Advisory RHSA-2017:31978-03 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory</text>, <text>Advisory RHSA-2017:31979-03 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory</text>, <text>Advisory RHSA-2017:31980-03 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory</text>, <text>Advisory RHSA-2017:31981-03 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory</text>, <text>Advisory RHSA-2017:31982-03 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory</text>, <text>Advisory RHSA-2017:31983-03 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory</text>, <text>Advisory RHSA-2017:31984-03 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory</text>, <text>Bug report changed to RELEASE_PENDING status by Errata System.\nAdvisory RHSA-2017:31978-03 has been changed to PUSH_READY status.\nhttps://errata.devel.redhat.com/advisory/31978</text>, <text>Bug report changed to RELEASE_PENDING status by Errata System.\nAdvisory RHSA-2018:0056-03 has been changed to PUSH_READY status.\nhttps://errata.devel.redhat.com/advisory/31980</text>, <text>Bug report changed to RELEASE_PENDING status by Errata System.\nAdvisory RHSA-2018:0059-03 has been changed to PUSH_READY status.\nhttps://errata.devel.redhat.com/advisory/31983</text>, <text>Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHSA-2018:0056</text>, <text>Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHSA-2018:0059</text>, <text>Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHSA-2018:0058</text>, <text>Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHSA-2018:0057</text>, <text>Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHSA-2018:0055</text>, <text>Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHSA-2018:0054</text>, <text>Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHSA-2018:0060</text>, <text>Shouldn't this bug be closed, as the fix is in 'qemu-kvm-rhev-2.9.0-16.el7_4.13.x86_64' ?;\n\n$ rpm -qp --changelog qemu-kvm-rhev-2.9.0-16.el7_4.13.x86_64.rpm |less\n* Thu Dec 14 2017 Miroslav Rezanina &lt;mrezanin@redhat.com&gt; - rhev-2.9.0-16.el7_4.13\n- Fix CVE-2017-5715\n\n* Thu Nov 16 2017 Miroslav Rezanina &lt;mrezanin@redhat.com&gt; - rhev-2.9.0-16.el7_4.12\n- kvm-util-async-use-atomic_mb_set-in-qemu_bh_cancel.patch [bz#1513362]\n- Resolves: bz#1513362\n  (QEMU's AIO subsystem gets stuck inhibiting all I/O operations on virtio-blk-pci devices [rhel-7.4.z])</text>, <text>Hi Gordon, due to the urgent CVE bugs, this bug didn't get chance to enter errata so its bugs is still in VERIFIED, but you are right, customer should already get the fix through qemu-kvm-rhev-2.9.0-16.el7_4.13.x86_64.rpm.</text>, <text>Hello,\n\n   This issue has been fixed with updated version of qemu-kvm-rhev for Red Hat OpenStack Platform 12.0 (Pike). Please refer the following errata for more details, https://access.redhat.com/errata/RHSA-2018:0060\n\nLet me know if you have any questions.\n\nBest Regards,\nSachin</text>, <text>Hi Sachin,\n\nand what about RHOS10? This support ticket has been opened for RHOS10.\n\nThanks,\nL.</text>, <text>Hello,\n\n   Following errata released for RHOS-10 fix, https://access.redhat.com/errata/RHSA-2018:0058\n\nBest Regards,\nSachin</text>, <text>OK, thanks a lot! Feel free to archive the case.\n\nCheers,\nL.</text>, <text>Hello,\n\nI really appreciate for your confirmation on the issue resolution. Its our pleasure to assist you. With your permission, I am moving forward to archive this service request. \nIf you have any additional questions or doubts, please open a new support case, referring this case number for us to be able to assist you.\n\nBest Regards,\nSachin</text>, <text>We are facing the same issue in OSP 10 deployment where the spawning of a huge VM gets stuck. We would want to know how to get the custom build with the patch mentioned in this bugzilla. \n\nWe have downloaded a source RPM (.src.rpm) from http://ftp.redhat.com/pub/redhat/linux/enterprise/7Server/en/RHOS/SRPMS/, specifically qemu-kvm-rhev-2.9.0-16.el7_4.13.src.rpm.\n\nSince this is a source RPM, we are yet to build the RPM from this file. We followed through the steps https://wiki.centos.org/HowTos/RebuildSRPM on how to build source RPMs, including installing dependencies, such as gcc and kernel-headers, but there are a ton of dependencies. \n\nWe have used the 'yum-builddep &lt;src rpm&gt;' command to install some of the dependencies, but there are yet other packages that aren't available. These are the ff.:\n\u2022\tbluez-libs-devel\n\u2022\tbrlapi-devel\n\u2022\tgperftools-devel\n\u2022\tlibfdt-devel &gt;= 1.4.3\n\u2022\tlbiscsi-devel\n\u2022\tlibseccomp-devel &gt;= 2.3.0\n\u2022\tlibssh2-devel\n\u2022\tlzo-devel\n\u2022\tpciutils-devel\n\u2022\tsnapp-devel\n\nCan you guide us on how to add these dependencies on RHEL OSP and let us know if we are missing any repositories?</text>, <text>Shivapriya, I believe you need to get the official binary rpm through subscription channel.\n@Oneata, do you have any guidance about that? Thanks.</text>, <text>Thank you, Miya. \n\nSince this needs to be done on OSP compute nodes where we don't enable any RHEL subscription, we will not be able to download from official channel.\nCan you please guide us to obtain the binary rpm elsewhere, so that we can install it on compute node directly?</text>, <text>(In reply to shivapriya.o.hiremath from comment #14)\n&gt; Thank you, Miya. \n&gt; \n&gt; Since this needs to be done on OSP compute nodes where we don't enable any\n&gt; RHEL subscription, we will not be able to download from official channel.\n&gt; Can you please guide us to obtain the binary rpm elsewhere, so that we can\n&gt; install it on compute node directly?\n\nqemu-kvm-rhev package for OSP10 was tracked in BZ1525502 and got released via\nhttps://access.redhat.com/errata/RHSA-2018:0058 . You could load it from the \nerrata.</text>, <text>(In reply to shivapriya.o.hiremath from comment #14)\n&gt; Thank you, Miya. \n&gt; \n&gt; Since this needs to be done on OSP compute nodes where we don't enable any\n&gt; RHEL subscription, we will not be able to download from official channel.\n\nwe strongly recommend to enable the needed channels [1] either direct to the RHN \nor using e.g. a satellite server and use the official procedure as described in [1]\nto keep the environment up to date.\n\n[1] https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/10/html-single/director_installation_and_usage/index#sect-Repository_Requirements\n[2] https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/10/html-single/upgrading_red_hat_openstack_platform/index#sect-Updating_the_Environment</text>, <text>Thank you Miya and Martin. Updating the qemu-kvm-rhev package to the latest through official subscription channel fixed the issue.</text>, <text>Advisory RHSA-2018:0060-03 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory</text>, <text>Advisory RHBA-2017:30729-01 has been changed to REL_PREP status. This indicates QE successfully finished testing the advisory</text>, <text>Bug report changed to RELEASE_PENDING status by Errata System.\nAdvisory RHSA-2017:30729-11 has been changed to PUSH_READY status.\nhttps://errata.devel.redhat.com/advisory/30729</text>, <text>Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHSA-2018:1104</text>]
ham	[[<description>Hi Team,\nWe need urgent help from Redhat team regarding  VM spanwing issue.\nWhen we do the deployment,  VM's are not spawning up in some of the compute nodes. \n\nCase1. VM's are stuck in build state for 30 min and at the end it will be up and running. \nCase 2. VM's are stuck in build state and remain in Build status \nCase 3. VM's stuck initially for sometime and goes to error state.\n\nThis is important because it is blocking the deployment and it is a production setup. We need someone from Redhat to jump in to webex https://cisco.webex.com/join/garisty\n\nRegards,\nAnand TS</description>], <text>Hello Redhat Team,\n\nCould you join the webex for active troubleshooting. \nhttps://cisco.webex.com/join/garisty .</text>, <text>Hello Team,\n\nReminder!\nWe need someone from Redhat team to join us on the webex immediately. We are in a critical issue with the deployment. \nConsider this as priority. \nhttps://cisco.webex.com/join/garisty\n\n-Anand  TS</text>, <text>Request Management Escalation: This is a severity 1 case where our deployment is stuck and need urgent support for the compute node VM spawning issue. Could you help some redhat engineer join the webex \nhttps://cisco.webex.com/join/garisty</text>, <text>Ack RME.</text>, <text>Hello Team,\n\nThank you for raising management attention to this case. My name is Fernanda and I am an Escalation Manager with Red Hat's Customer Experience &amp; Engagement team. I acknowledge your request for management escalation. I am coordinating with SBR Manager Ather to have an Engineer assisting you as soon as possible.\nI apologize for any inconvenience and will continue to monitor this case.\nPlease let me know if I can be of any more help.\n\nRegards,\nFernanda</text>, <text>Hi,\n\n Thank you for contacting Red Hat Global Support Services.\n\nI am Chaitanya, and I would be assisting you with this case in Indian business hours.\n\nFrom the case description, I can understand that you are facing issues while spawning instances in your environment.\n\nCan you let us know the following things before we start the remote session:\n\n- OSP version\n\n- How many computes and controllers are you running?\n\n- sosreport from a controller node and a compute node on which the instance is failing to spawn.\n\n- Output of following command:\n\n~~~\n$source overcloudrc\n\n$nova service-list\n~~~\n\nRegards,\nChaitanya</text>, <text>Hello Team,\n\nEngineer Chaitanya will assist you shortly.\n\nRegards,\nFernanda</text>, <text>*Internal*\n\nReceived a call on hot line for this case, informed customer that We now have an Engineer assisting on it and We should have a feedback soon.</text>, <text>Hello Chaitanya, \n\nWe will be giving all the details to you. We are running the OSPD 10. \n\nI will attach the SOS report to the SR soon. \n\nBut could you join the webex now. \nhttps://cisco.webex.com/join/garisty</text>, <text>Hi,\n\n   While trying to join the webex, I am getting below message:\n\n"You can join the meeting after the host admits you."\n\nRegards,\nChaitanya</text>, <text>Hi,\n\n  Thanks for your time on the webex session.\n\nSession summary:\n\n- While trying to spawn multiple instances at a time (40), some of them are getting stuck at build state, some are getting into the error state and some are spawning correctly. This issue is happening randomly on any compute node (not on any specific compute node).\n\n- There are 14 compute node in the environment and the output of 'nova service-list' looks good as seen on the session.\n\n- The output of 'pcs status' looks clean with no failed actions.\n\n- You saw one of the below errors in the logs:\n\n~~~\n00:25:07,436 23-Nov-2017 WARN Status Msg: VIM Driver: VM booted in ERROR state in Openstack: Build of instance c3cd4693-9112-4cc3-a481-3801ac98ca8d aborted: Failed to allocate the network(s), not rescheduling.\n~~~\n\nKindly attach the sosreport from the controller node while I go through the attached compute node sosreport.\n\nRegards,\nChaitanya</text>, <text>Hi,\n\n  Going through the compute sosreports, I see the below logs with respect to instance 'c3cd4693-9112-4cc3-a481-3801ac98ca8d':\n\n~~~\n2017-11-23 00:27:47.533 640224 WARNING nova.virt.libvirt.driver [req-d907e22c-9dfa-4313-9514-5b662de2abff bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d] Timeout waiting for vif plugging callback for instance c3cd4693-9112-4cc3-a481-3801ac98ca8d\n[...]\n\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [req-d907e22c-9dfa-4313-9514-5b662de2abff bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d] Instance failed to spawn\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d] Traceback (most recent call last):\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 2083, in _build_resources\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d]     yield resources\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1924, in _build_and_run_instance\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d]     block_device_info=block_device_info)\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 2618, in spawn\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d]     destroy_disks_on_failure=True)\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4915, in _create_domain_and_network\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d]     raise exception.VirtualInterfaceCreateException()\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d] VirtualInterfaceCreateException: Virtual Interface creation failed\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d] \n2017-11-23 00:27:48.084 640224 INFO nova.compute.manager [req-d907e22c-9dfa-4313-9514-5b662de2abff bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d] Terminating instance\n~~~\n\nIt seems that instance creation timed out waiting for neutron. Let me go through the attached controller node sosreport and will get back to you soon.\n\nRegards,\nChaitanya</text>, <text>the problem seems to have started ~nov-23 00:26\n00:25:03,153 23-Nov-2017 WARN  Status Msg: VIM Driver: VM booted in ERROR state in Openstack: Build of instance 3c8722fa-8551-49c1-957f-76f85707e2e9 aborted: Failed to allocate the network(s), not rescheduling.\nWe see this failed to allocate error</text>, <text>When can we expect update from Red Hat ?\n\nI still need to upload 2 files.</text>, <text>Hello,\n\nCan you please join Webex again ?  We need to discuss how to move forward. \n\nhttps://cisco.webex.com/join/garisty\n\nRegards\nNebojsa</text>, <text>Hi,\n\n Thanks for the update.\n\nI see many errors reported in the neutron server.log file though the timestamp where it starts is '2017-11-23 03:28:04'. The logs may have got rotated.\n\nThe hearbeats of neutron agents are seen flipping many times:\n\n~~~\n2017-11-23 06:17:01.660 6040 WARNING neutron.db.agents_db [req-d7db5b4d-5928-4303-994d-2452c78bb3a8 - - - - -] Agent healthcheck: found 7 dead agents out of 34\n:\n                Type       Last heartbeat host\n  Open vSwitch agent  2017-11-23 06:15:45 wsstackovs-compute-16.localdomain\n            L3 agent  2017-11-23 06:15:44 wsstackovs-controller-0.localdomain\n          DHCP agent  2017-11-23 06:15:42 wsstackovs-controller-0.localdomain\n      Metadata agent  2017-11-23 06:15:31 wsstackovs-controller-0.localdomain\n  Open vSwitch agent  2017-11-23 06:15:42 wsstackovs-compute-2.localdomain\n      Metadata agent  2017-11-23 06:15:33 wsstackovs-controller-2.localdomain\n            L3 agent  2017-11-23 06:15:44 wsstackovs-controller-1.localdomain\n\n$ grep -r "dead agents" wsstackovs-controller-1.localdomain/var/log/neutron/server.log | wc -l\n75\n~~~\n\nAlso, the common error seen is the below:\n\n~~~\n2017-11-23 06:13:09.558 5923 ERROR oslo_db.sqlalchemy.exc_filters [req-f7d44db3-39d6-440f-b16e-8bcf38ebcf44 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] DBAPIError exception wrapped from (pymysql.err.InternalError) (1206, u'The total number of locks exceeds the lock table size') [SQL: u'INSERT INTO resourcedeltas (resource, reservation_id, amount) VALUES (%(resource)s, %(reservation_id)s, %(amount)s)'] [parameters: {'reservation_id': 'cf2206df-df55-4c83-86f0-859e197449fe', 'amount': 1, 'resource': 'port'}]\n2017-11-23 06:13:09.558 5923 ERROR oslo_db.sqlalchemy.exc_filters Traceback (most recent call last):\n2017-11-23 06:13:09.558 5923 ERROR oslo_db.sqlalchemy.exc_filters   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1139, in _execute_context\n2017-11-23 06:13:09.558 5923 ERROR oslo_db.sqlalchemy.exc_filters     context)\n2017-11-23 06:13:09.558 5923 ERROR oslo_db.sqlalchemy.exc_filters   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py", line 450, in do_execute\n[...]\n\n2017-11-23 06:13:09.561 5923 ERROR neutron.api.v2.resource DBError: (pymysql.err.InternalError) (1206, u'The total number of locks exceeds the lock table size') [SQL: u'INSERT INTO resourcedeltas (resource, reservation_id, amount) VALUES (%(resource)s, %(reservation_id)s, %(amount)s)'] [parameters: {'reservation_id': 'cf2206df-df55-4c83-86f0-859e197449fe', 'amount': 1, 'resource': 'port'}]\n2017-11-23 06:13:09.561 5923 ERROR neutron.api.v2.resource\n~~~\n\nI am checking internally and will get back to you soon.\n\nIn the meantime, can you get us the output of below command:\n\n~~~\n$source overcloudrc\n\n$neutron agent-list\n\n[wait for 2-3 min]\n\n$neutron agent-list\n\n[again wait for 2-3 min]\n\n$neutron agent-list\n~~~\n\nRegards,\nChaitanya</text>, <text>Request Management Escalation: Hello, \n\nCan you please join Webex again ? \n\nWe need to discuss how to move forward. \n\nhttps://cisco.webex.com/join/garisty</text>, <text>Request Management Escalation: Hello, \n\nCan you please join Webex again ? \n\nWe need to discuss how to move forward. \n\nhttps://cisco.webex.com/join/garisty</text>, <text>Ack RME.</text>, <text>I informed Webex Host to keep an eye on new Webex connections.\n\nPlease try to reconnect.</text>, <text>Hello Team,\n\nThis case is already under EMT attention, for any request you can just update without requirement of a new RME.\nPlease note that the RME button should not be used to share an update or to get an update.\nPlease allow me some time, to that I can request the Engineer to join the call. Thank you for your Patience.\n\nRegards,\nFernanda</text>, <text>*Internal*\n\nI spoke to customer and informed that engineer will be in the call ASAP.</text>, <text>Hi Pavan,\n\n   Thanks for your time on the call.\n\nCall summary:\n\n- The issue is occurring from the neutron end: some instances are failing to spawn while waiting for a response from neutron while the vif is plugged.\n\n- These set of instances were undeployed and were tried to re-deploy yesterday when some of the instances did not come up.\n\n- Kindly attach the '/var/log/neutron' directory from the controller node as we do not have the logs from the time when the issue occured (logs were rotated).\n\n- Let me check internally if I get any pointers from the provided logs.\n\n- I will update the case with the procedure to enable debug logs for neutron on the controllers shortly.\n\nRegards,\nChaitanya</text>, <text>- Kindly attach the '/var/log/neutron' directory from the controller node as we do not have the logs from the time when the issue occured (logs were rotated).  PROVIDED\n\nAction plan pending on RH:\n\n- Let me check internally if I get any pointers from the provided logs. \n\n- I will update the case with the procedure to enable debug logs for neutron on the controllers shortly.</text>, <text>When can we expect update from RH regarding this action points ?</text>, <text>Hi,\n\n  Thanks for providing the logs.\n\nYou may follow this knowledgebase article to enable debug logging for neutron: https://access.redhat.com/solutions/1391343\n\nKindly note that "This will cause a momentary loss in network connectivity to your instances".\n\nI will be transferring this case to an Engineer in next geo location and will get back to you soon.\n\nRegards,\nChaitanya</text>, <text>//FTS Handover Note\n\n- Customer tried to spawn 40 instances at a time. Some instances were spawned successfully, some were stuck for a long time in build state and were timed out while creating.\n\n- 'pcs status' and all the services on controllers look healthy. compute service also look healthy. controllers are ntp synchronized correctly.\n\n- There are "VirtualInterfaceCreateException: Virtual Interface creation failed" errors in compute logs while creating the instances:\n\n~~~\n2017-11-23 00:27:47.533 640224 WARNING nova.virt.libvirt.driver [req-d907e22c-9dfa-4313-9514-5b662de2abff bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d] Timeout waiting for vif plugging callback for instance c3cd4693-9112-4cc3-a481-3801ac98ca8d\n[...]\n\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d] VirtualInterfaceCreateException: Virtual Interface creation failed\n2017-11-23 00:27:48.083 640224 ERROR nova.compute.manager [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d] \n2017-11-23 00:27:48.084 640224 INFO nova.compute.manager [req-d907e22c-9dfa-4313-9514-5b662de2abff bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: c3cd4693-9112-4cc3-a481-3801ac98ca8d] Terminating instance\n~~~\n\n- On neutron side, there are agents flipping and heartbeats misses frequently. In addition, below errors are seen frequently:\n\n~~~\n2017-11-23 06:13:09.561 5923 ERROR neutron.api.v2.resource DBError: (pymysql.err.InternalError) (1206, u'The total number of locks exceeds the lock table size') [SQL: u'INSERT INTO resourcedeltas (resource, reservation_id, amount) VALUES (%(resource)s, %(reservation_id)s, %(amount)s)'] [parameters: {'reservation_id': 'cf2206df-df55-4c83-86f0-859e197449fe', 'amount': 1, 'resource': 'port'}]\n2017-11-23 06:13:09.561 5923 ERROR neutron.api.v2.resource\n~~~\n\n- We have sosreport from controller-1 and compute nodes where instance timed out while spawning. Also '/var/log/neutron' directory from all the 3 controller nodes.\n\n- Customer is reluctant to provide debug logs as it requires restarting of neutron services (they say that they will have to schedule a maintenance window to enable debug logging)</text>, <text>Thank you for providing the procedure, we will check it in order to see will we have any questions. \n\nWe still are waiting from you analysis of provided neutron controller logs.</text>, <text>Hello,\n  As Chaitanya is at the end of his office hours, I'll take care of the case during the remaining time of EMEA office.\n\n  I understand that the deployment of the instances is now finished, can you please confirm that? As I understand, the action needed on our side is tha analysis on the provided logs. I would still request you to enable debug as recommended previously, during your maintenance window, and provide the debug logs _after the issue is reproduced_.\n\n\nBest regards\nPetr Barta\nRed Hat Global Support Services</text>, <text>// by mistake sent out twice the previous update. Scrapping the copy.</text>, <text>Hello,\n\nDeployment is not finished since we face issue with Neutron. \nAll SOSreports and Neutron logs are provided to RH for analysis. \n\nWe are reviewing procedure to enable debug logs for Neutron, when we finish we will get back to you, maybe there will be some questions before running this on production device.</text>, <text>Hello,\n  Thank you for the info.\n\n  In the logs available I can see following type of errors:\n\ngrep "9e59c9c9-c511-427b-a796-e3e89b78c40e" sosreport-20171123-065934/wsstackovs-compute-6.localdomain/var/log/nova/nova-compute.log\n2017-11-23 00:17:06.257 640224 INFO nova.compute.claims [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Attempting claim: memory 14336 MB, disk 100 GB, vcpus 10 CPU\n2017-11-23 00:17:06.257 640224 INFO nova.compute.claims [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Total memory: 262032 MB, used: 200608.00 MB\n2017-11-23 00:17:06.257 640224 INFO nova.compute.claims [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] memory limit not specified, defaulting to unlimited\n2017-11-23 00:17:06.258 640224 INFO nova.compute.claims [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Total disk: 1116 GB, used: 400.00 GB\n2017-11-23 00:17:06.258 640224 INFO nova.compute.claims [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] disk limit not specified, defaulting to unlimited\n2017-11-23 00:17:06.258 640224 INFO nova.compute.claims [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Total vcpu: 56 VCPU, used: 40.00 VCPU\n2017-11-23 00:17:06.258 640224 INFO nova.compute.claims [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] vcpu limit not specified, defaulting to unlimited\n2017-11-23 00:17:06.259 640224 INFO nova.compute.claims [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Claim successful\n2017-11-23 00:17:06.936 640224 INFO nova.virt.libvirt.driver [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Creating image\n2017-11-23 00:17:11.279 640224 INFO nova.virt.libvirt.driver [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Using config drive\n2017-11-23 00:17:11.542 640224 INFO nova.virt.libvirt.driver [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Creating config drive at /var/lib/nova/instances/9e59c9c9-c511-427b-a796-e3e89b78c40e/disk.config\n2017-11-23 00:17:12.178 640224 INFO nova.compute.manager [req-18077c81-e7b0-4645-9a02-5cf5ef7ee330 - - - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] VM Started (Lifecycle Event)\n2017-11-23 00:17:12.214 640224 INFO nova.compute.manager [req-18077c81-e7b0-4645-9a02-5cf5ef7ee330 - - - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] VM Paused (Lifecycle Event)\n2017-11-23 00:17:12.301 640224 INFO nova.compute.manager [req-18077c81-e7b0-4645-9a02-5cf5ef7ee330 - - - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] During sync_power_state the instance has a pending task (spawning). Skip.\n2017-11-23 00:22:12.179 640224 WARNING nova.virt.libvirt.driver [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Timeout waiting for vif plugging callback for instance 9e59c9c9-c511-427b-a796-e3e89b78c40e\n2017-11-23 00:22:12.732 640224 INFO nova.virt.libvirt.driver [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Deleting instance files /var/lib/nova/instances/9e59c9c9-c511-427b-a796-e3e89b78c40e_del\n2017-11-23 00:22:12.733 640224 INFO nova.virt.libvirt.driver [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Deletion of /var/lib/nova/instances/9e59c9c9-c511-427b-a796-e3e89b78c40e_del complete\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Instance failed to spawn\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Traceback (most recent call last):\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 2083, in _build_resources\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]     yield resources\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1924, in _build_and_run_instance\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]     block_device_info=block_device_info)\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 2618, in spawn\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]     destroy_disks_on_failure=True)\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4915, in _create_domain_and_network\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]     raise exception.VirtualInterfaceCreateException()\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] VirtualInterfaceCreateException: Virtual Interface creation failed\n2017-11-23 00:22:12.818 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] \n2017-11-23 00:22:12.818 640224 INFO nova.compute.manager [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Terminating instance\n2017-11-23 00:22:12.822 640224 INFO nova.virt.libvirt.driver [-] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] During wait destroy, instance disappeared.\n2017-11-23 00:22:12.892 640224 INFO nova.virt.libvirt.driver [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Deletion of /var/lib/nova/instances/9e59c9c9-c511-427b-a796-e3e89b78c40e_del complete\n2017-11-23 00:22:13.034 640224 INFO nova.compute.manager [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Took 0.21 seconds to destroy the instance on the hypervisor.\n2017-11-23 00:22:13.223 640224 ERROR nova.compute.manager [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Failed to allocate network(s)\n2017-11-23 00:22:13.223 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Traceback (most recent call last):\n2017-11-23 00:22:13.223 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1924, in _build_and_run_instance\n2017-11-23 00:22:13.223 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]     block_device_info=block_device_info)\n2017-11-23 00:22:13.223 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 2618, in spawn\n2017-11-23 00:22:13.223 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]     destroy_disks_on_failure=True)\n2017-11-23 00:22:13.223 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]   File "/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py", line 4915, in _create_domain_and_network\n2017-11-23 00:22:13.223 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]     raise exception.VirtualInterfaceCreateException()\n2017-11-23 00:22:13.223 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] VirtualInterfaceCreateException: Virtual Interface creation failed\n2017-11-23 00:22:13.223 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] \n2017-11-23 00:22:13.225 640224 ERROR nova.compute.manager [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Build of instance 9e59c9c9-c511-427b-a796-e3e89b78c40e aborted: Failed to allocate the network(s), not rescheduling.\n2017-11-23 00:22:13.225 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Traceback (most recent call last):\n2017-11-23 00:22:13.225 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1783, in _do_build_and_run_instance\n2017-11-23 00:22:13.225 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]     filter_properties)\n2017-11-23 00:22:13.225 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1964, in _build_and_run_instance\n2017-11-23 00:22:13.225 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e]     reason=msg)\n2017-11-23 00:22:13.225 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] BuildAbortException: Build of instance 9e59c9c9-c511-427b-a796-e3e89b78c40e aborted: Failed to allocate the network(s), not rescheduling.\n2017-11-23 00:22:13.225 640224 ERROR nova.compute.manager [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] \n2017-11-23 00:22:14.844 640224 INFO nova.compute.manager [req-2b36c4c8-92f5-429b-a0df-a345a7cf8e18 bb446e1bfeb54eb39fa9a4241e541aaf 141cd8795b9c47da8574ffd52445e66a - - -] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] Took 1.62 seconds to deallocate network for instance.\n2017-11-23 00:22:27.392 640224 INFO nova.compute.manager [-] [instance: 9e59c9c9-c511-427b-a796-e3e89b78c40e] VM Stopped (Lifecycle Event)\n\n\n  All I see so far is that there was really timeout during plugging VIF. I don't see yet what was the reason for it.\n  Can you please really enable debug logs on neutron and nova on both controllers and compute nodes, and provide fresh sosreports of the nodes after the issue is reproduced?\n\n\nBest regards\nPetr Barta\nRed Hat Global Support Services</text>, <text>Hello,\n  I can see as well that there is sysstat supposed to be running, but on the controller node (the one provided) I can see sar logs only till Nov 12. Can you please therefore restart sysstat service and make sure that the sar logs are created, before reproducing? This would show us that the issue is not in memory on controller nodes not being available.\n\n  I'm updating the action plan.\n\n  My colleagues from NA will take over, for now.\n\n\nBest regards\nPetr Barta\nRed Hat Global Support Services</text>, <text>Hello,\n\nDue to inactivity in this case for more than 12hrs, the 24x7 flag is no longer set in this case. This means we will be assisting you primarily during the support hours of your region when you are available. This can be readjusted at any time, pending your availability. Please feel free to give us a call on the numbers listed on our contact page, or update the case if you have any questions or concerns.\n\n   Contacting Technical Support\n   https://access.redhat.com/support/contact/technicalSupport.html\n\nThank you for choosing Red Hat! We appreciate your support.\n\nRegards,\nNikhil Gupta\nSupport Operations Lead, Enterprise Cloud Support\nRed Hat India Pvt Ltd\nwww.redhat.com | TRIED. TESTED. TRUSTED. | redhat.com/trusted</text>, <text>Thanks for the update. \n\nI don't think we have sysstat configured and which is enabled on the controller nodes. Please provide us with the steps to do it and how to create the sar logs.\n\nWhat is the impact if we restart this service? Compute nodes 13-16 carry live traffic and it should not be impacted. If there is no impact we can go ahead and restart the service and give the logs to find out for any memory issue in controller nodes.\n\nAlso if you enable debug=true for neutron logs, it could possibly create huge log files from info to higher. It is not recommended in production system until you confirm that it won't do any harm to the running nodes. \nAlso after changing the debug settings, neutron has to be restarted, by doing so any running VM's are impacted ?\n\nSo from whatever logs shared you could identify issue as " some instances are failing to spawn while waiting for a response from neutron while the vif is plugged" and this is not sufficient to provide the RCA of the issue. Is there any further analysis is happening at your end based on the logs shared. \n\nWe are working with the customer to get the maintenance window to enable debug=true in neutron.\n\nThanks,\nAnand TS</text>, <text>Hi Anand,\n\n   sysstat is installed on the controller nodes and we do see the logs captured until 12th Nov 2017 and the service is also running:\n\n~~~\n$ less wsstackovs-controller-1.localdomain/var/log/sa/sa\nsa01   sa02   sa03   sa04   sa05   sa06   sa07   sa08   sa09   sar01  sar02  sar03  sar04  sar05  sar06  sar07  sar08  sar09  sar10  sar11  sar12\n~~~\n\nYou can just restart the sysstat service running on the controller nodes using below command:\n\n~~~\n$ sudo systemctl restart sysstat\n~~~\n\nOnce restarted, sysstat will start capturing the sar logs under '/var/log/sa' directory. I confirm that there would be no impact of restarting the sysstat service on the controllers.\n\n&gt;&gt; Also if you enable debug=true for neutron logs, it could possibly create huge log files from info to higher. It is not recommended in production system until you confirm that it won't do any harm to the running nodes\n\nSo, the plan is to enable debug logs for neutron on all the 3 controller nodes, reproduce the issue and disable the debug logging for neutron. This will indeed create additional logs for neutron which will help us in debugging the issue. I don't think it will create the logs in quantity that the controllers would not be able to handle. From controller-1 sosreport, I see that the root disk (/) is only 10% utilized.\n\nIn addition to the above, we will also need nova debug logs from the compute node on which the instance fails to spawn. You can follow the same KCS article to enable nova debug logs on the compute node. I confirm that there would not be any downtime on the running instances while restarting the openstack-nova-compute service on the compute nodes.\n\n&gt;&gt; Also after changing the debug settings, neutron has to be restarted, by doing so any running VM's are impacted ?\n\nAs stated in my earlier update, "This will cause a momentary loss in network connectivity to your instances". \n\n&gt;&gt; Is there any further analysis is happening at your end based on the logs shared.\n\nAs stated by my colleague Petr in an earlier update, we really need debug logs to check what's going on when instances are being spawned.\n\n&gt;&gt; We are working with the customer to get the maintenance window to enable debug=true in neutron.\n\nThanks. Kindly get back to us if you have any queries regarding the same.\n\nRegards,\nChaitanya</text>, <text>Hi,\n\n To add further, following is my observation from the controller sosreport provided:\n\n- Controllers are running with a 56 CPU configuration.\n\n- From neutron.conf file, I see that the workers are set to 56:\n\n~~~\n$ less wsstackovs-controller-1.localdomain/etc/neutron/neutron.conf | grep -i workers\n#api_workers = &lt;None&gt;\napi_workers=56\n#rpc_workers = 1\nrpc_workers=56\n#rpc_state_report_workers = 1\n~~~\n\n- There are way too many neutron-server processes running on the controllers:\n\n~~~\n$ egrep -ic neutron-server wsstackovs-controller-1.localdomain/ps\n115\n~~~\n\n- rabbit_qos_prefetch_count is set to 0 in neutron.conf file:\n\n~~~\n# Specifies the number of messages to prefetch. Setting to zero allows\n# unlimited messages. (integer value)\n#rabbit_qos_prefetch_count = 0\n~~~ \n\nI am not completely sure if tuning the above values would help in this case. This can be confirmed when we get the debug logs to the case.\n\nCurrently 24*7 flag is not enabled on this case. Would you be available 24*7 to work on this case? Also, as the weekend is approaching, would you like us to enable 24*7 support on this case so that it gets highlighted throughout the weekend and you will get support on the weekends.\n\nIn any case if you have queries with this support request, you can dial in the support line: https://access.redhat.com/support/contact/technicalSupport/\n\nRegards,\nChaitanya</text>, <text>RH Team,\n\n\nThere is a Maintenance Window activity scheduled for tonight at 1130pm PST, and there will be a webex session for RH to join to support this activity.  Cisco will update this case again within the next few hours with the webex details</text>, <text>Dear Cisco support,\n\nIn order to guarantee that adequate resources are available and are prepared, it would be best if important maintenance operations were announced with longer delays. This case is opened as US business hours, hence East Coast resources are already past their shift and West Coast resources are soon to leave - which means that with such a short notice, this case could easily have slipped through the cracks. This case was not marked as 24x7 any more, and it literally is the last email which I checked before signing off for the rest of the day. Please keep this in mind for the future, so that we can provide you with an optimal support experience. \n\nFrom the case history, it seems that you are always active over night: Do you want us to transition this case into APAC  business hours? Thank you very much for your understanding and for specifying your timezone of choice.\n\nWith the above said, I am flagging this case as 24x7 so that the teams in APAC can handle this. A Red Hat support engineer from this region will contact you shortly via this ticket. \n\nShould the transition not work out smoothly, please dial our the support line: https://access.redhat.com/support/contact/technicalSupport/\n\nThank you very much and best regards,\n\nAndreas</text>, <text>Hello,\n\nThanks for letting us know your plan. We will have an engineer ready for the session at 11:30 PST. And we will be awaiting the session details.\n\n\nRegards\n\nJames Biao RHCE\nSenior Technical Support Engineer\nCustomer Experience and Engagement - APAC \nRed Hat Asia-Pacific, Level 1, 193 North Quay, Brisbane, Australia, 4000\nPh:1800 888 297\nRed Hat Technical Support \u2013 contact numbers and availability: https://access.redhat.com/support/contact/technicalSupport.html</text>, <text>// Handover to EMEA\n\nThe maintenance window specified in #46 specify the time to be 11:30pm which is EMEA timezone.\n\n11:30 pm Tuesday, in Pacific Time is\n5:30 pm Wednesday, in Brisbane QLD\n\nThank you,\nKind Regards,\nRobin \u010cern\xedn</text>, <text>Hello James Biao, Team,\n\nPlease find the WebEx Information of the MW scheduled at 28th Nov 11:30 PST time.\n\nMeeting link:  https://cisco.webex.com/ciscosales/j.php?MTID=me0682af9e87439de191c5da41fa412af\nMeeting number: 207 817 096\nMeeting password: 6701112965\n\nI have also attached the outlook invitation. Please join 15 min early to start the MW on time.\n\nIt would be good if you share the person name and contact no who will be attending the MW from RedHat.\n\nThanks.</text>, <text>psahoo++ will be joining the call at the time that they have mentioned</text>, <text>Hello Redhat Team,\n\nGentle reminder!\nHope you have received the webex invite. MW is scheduled at 11:30 PST time which is one hour from now. \nWe need RH to join the webex at the same time. \n\nThanks,\nAnand TS</text>, <text>Hi Anand,\n\nThank you for the update. My name is Pradipta and I will assist you in this case during Indian business hour.\nI will available in below webex meeting at 11:30 PST.\n\nMeeting link:  https://cisco.webex.com/ciscosales/j.php?MTID=me0682af9e87439de191c5da41fa412af\nMeeting number: 207 817 096\nMeeting password: 6701112965\n\nRegards,\nPradipta Sahoo\n---------------------------------------------------------------\nRed Hat India OpenStack Team\nAvailability: Mon-Fri (9AM-6PM IST)\nTRIED. TESTED. TRUSTED.\nTechnical Support Contact Details: https://access.redhat.com/support/contact/technicalSupport</text>, <text>Hi Cisco Team,\n\nIn the remote session, due to below Galera issue you were unable to enable the debug for neutron and reproduce the issue which has discussed earlier.\n\nRemote session Summary:\n- During precheck in overcloud environment, both Nova and neutron agent services were UP.\n- After enabling debug option in neutron, we noticed the most neutron agents status are DOWN. In this step, no neutron services were restarted.\n- We tried to restart ovs agent, but there was no luck.\n- At the same time, the horizon dashboard also not accessible for one region.\n- During the investigation, it seems OpenStack client unable to contact keystone public endpoint, suspecting MySQL buffer issue.\n\t#echo "show variables like 'innodb_buffer_pool_size'; " | mysql\n\tVariable_name Value  \n\tinnodb_buffer_pool_size 134217728\n- Referring below KCS you recovered the Galera service and set the master to controller-1. After this action, OSP services are getting operational.\n https://access.redhat.com/articles/2332651\n- But few ovs services shows down(xxx) for controller-0 and other 2 compute node. (neutron agent-list)\n- To isolate the issue, I raised another case 01983028, to troubleshoot the ovs issue for controller-0.\n- As per your confirmation, I escalate case 01983028 to NA region for active support.\n\nAction Plan:\nAfter fixing the above ovs issue(01983028), Please schedule another maintenance work-window to enable the debug option in neutron service and reproduce the actual issue which described in the problem description.  \n\nRegards,\nPradipta Sahoo\n---------------------------------------------------------------\nRed Hat India OpenStack Team\nAvailability: Mon-Fri (9AM-6PM IST)\nTRIED. TESTED. TRUSTED.\nTechnical Support Contact Details: https://access.redhat.com/support/contact/technicalSupport</text>, <text>#Internal Note\nRemote session Summary: (Participant from RH: Pradipta and Jaison)\n\nDue to below Galera issue customer unable to enable the debug and reproduce the issue during maintenance work-window.\n\n- During precheck in overcloud environment, both Nova and neutron agent services are UP.\n- After enabling debug option in neutron, we noticed the most neutron agents status are DOWN. In this step, no neutron services were restarted.\n- We tried to restart ovs agent, but there was no luck.\n- At the same time, the horizon dashboard also not accessible for one region.\n- During the investigation, it seems OpenStack client unable to contact keystone public endpoint, suspecting MySQL issue where keystone unable to reach.\n\t#echo "show variables like 'innodb_buffer_pool_size'; " | mysql\n\tVariable_name Value  \n\tinnodb_buffer_pool_size 134217728\n- Referring below KCS we recovered the Galera service and set the master to controller-1. After OSP services are getting operational.\nhttps://access.redhat.com/articles/2332651\n- But few ovs service shows down(xxx) in controller-0 and other 2 compute node.\n- TO isolate the issue, I raised another case 01983028, to troubleshoot the ovs issue for controller-0.\n- As per customer confirmation, I escalate case 01983028 NA region for active support. Please start to investigate once customer upload the SOS report of controller-0 and affected compute node.\n\nAction Plan:\nAfter fixing the above ovs issue(01983028), customer schedule another maintenance work-window to enable the debug option in neutron service and reproduce the actual issue described in problem description.</text>, <text>Hello,\n\nLet us know when you have the next maintenance window to fix this issue.\n\nDue to inactivity in this case for more than 12hrs, the 24x7 flag is no longer set in this case. This means we will be assisting you primarily during the support hours of your region when you are available. This can be readjusted at any time, pending your availability. Please feel free to give us a call on the numbers listed on our contact page, or update the case if you have any questions or concerns.\n\n   Contacting Technical Support\n   https://access.redhat.com/support/contact/technicalSupport.html\n\nThank you for choosing Red Hat! We appreciate your support.\n\nRegards,\nNikhil Gupta\nSupport Operations Lead, Enterprise Cloud Support\nRed Hat India Pvt Ltd\nwww.redhat.com | TRIED. TESTED. TRUSTED. | redhat.com/trusted</text>, <text>##Internal update of child case -- 01983028 (Issue fixed)\nThe openvswitch issue has fixed in the controller and compute nodes.\nhttps://access.redhat.com/support/cases/internal/case/01983028/discussion/a0aA000000L60GPIAZ\n\nAction Item:\nIn WebEx meeting as got the confirmation, the customer will set another maintenance work window to enable the debug in neutron for the controller and reproduce the issue and then will proceed to upload the updated SOS report to this case.</text>]
ham	[[<description>What problem/issue/behavior are you having trouble with?  What do you expect to see?\n\nEnvironment is Satellite 6.2.12 on RHEL7.4\n\nInvestigating whether could be related to https://access.redhat.com/solutions/3153361\n\n[root@gbl15217 hornetq]# pwd\n/var/lib/candlepin/hornetq\n[root@gbl15217 hornetq]# du -sh *\n2.0M    bindings\n121M    journal\n140K    largemsgs\n39G     paging\n[root@gbl15217 hornetq]# find paging -print|wc -l\n40417\n[root@gbl15217 hornetq]#\n\nWhere are you experiencing the behavior?  What environment?\n\nProduction\n\nWhen does the behavior occur? Frequently?  Repeatedly?   At certain times?\n\nRepeatedly.  Tried clearing 1GB of space and doing katello-service restart, but hornetq paging directory continues to grow in size.\n\nWhat information can you provide around timeframes and the business impact?\n\nWill attach sosreport when it completes.\n\n[root@gbl15217 hornetq]# qpid-stat --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 -q katello_event_queue;sleep 10;qpid-stat --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 -q katello_event_queue\nProperties:\n  Name                 Durable  AutoDelete  Exclusive  FlowStopped  FlowStoppedCount  Consumers  Bindings\n  =========================================================================================================\n  katello_event_queue  Y        N           N          N            0                 1          6\n\nOptional Properties:\n  Property      Value\n  =====================\n  arguments     {}\n  alt-exchange\n\nStatistics:\n  Statistic                   Messages  Bytes\n  ==================================================\n  queue-depth                 9         137,653\n  total-enqueues              3,164     56,417,995\n  total-dequeues              3,155     56,280,342\n  persistent-enqueues         3,164     56,417,995\n  persistent-dequeues         3,155     56,280,342\n  transactional-enqueues      0         0\n  transactional-dequeues      0         0\n  flow-to-disk-depth          0         0\n  flow-to-disk-enqueues       0         0\n  flow-to-disk-dequeues       0         0\n  acquires                    3,170\n  releases                    6\n  discards-ttl-expired        0\n  discards-limit-overflow     0\n  discards-ring-overflow      0\n  discards-lvq-replace        0\n  discards-subscriber-reject  0\n  discards-purged             0\n  reroutes                    0\nProperties:\n  Name                 Durable  AutoDelete  Exclusive  FlowStopped  FlowStoppedCount  Consumers  Bindings\n  =========================================================================================================\n  katello_event_queue  Y        N           N          N            0                 1          6\n\nOptional Properties:\n  Property      Value\n  =====================\n  arguments     {}\n  alt-exchange\n\nStatistics:\n  Statistic                   Messages  Bytes\n  ==================================================\n  queue-depth                 9         137,653\n  total-enqueues              3,176     56,666,031\n  total-dequeues              3,167     56,528,378\n  persistent-enqueues         3,176     56,666,031\n  persistent-dequeues         3,167     56,528,378\n  transactional-enqueues      0         0\n  transactional-dequeues      0         0\n  flow-to-disk-depth          0         0\n  flow-to-disk-enqueues       0         0\n  flow-to-disk-dequeues       0         0\n  acquires                    3,182\n  releases                    6\n  discards-ttl-expired        0\n  discards-limit-overflow     0\n  discards-ring-overflow      0\n  discards-lvq-replace        0\n  discards-subscriber-reject  0\n  discards-purged             0\n  reroutes                    0\n[root@gbl15217 hornetq]#</description>], <text>Hello,\n\nWelcome to Red Hat Technical Support!!!\n\nMy name is Prashant Waghmare and I am assisting you on this case.\n\n1) In order to investigate further could you please provide me the SOS report of satellite server? To generate it, please refer below article:\n==================\nhttps://access.redhat.com/solutions/3592\n===================\n\n2) Also, could you Add following parameter to /etc/qpid/qpidd.conf:\n\ndefault-queue-limit=0\n\nApply the change:\n\nservice qpidd restart\n\n3) Also, from satellite server , provide me the out-put of following commands:\n\n# hammer ping\n\n# katello-service status\n\n# su - postgres -c "psql -d foreman -c 'select label,count(label),state from foreman_tasks_tasks where state &lt;&gt; '\\''stopped'\\'' group by label,state;'"\n\n4) Yes, article: https://access.redhat.com/solutions/3153361 could be related.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>(1) sosreport now being uploaded.\n(2) now actioned ok\n(3) attached</text>, <text>Hi Prashant,\n\nThe requested information is now all attached to the case.  We continue to see foreman_tasks looking a little erratic from 'hammer ping' and hornetq remains at 40GB in size, and qpidd.conf default-queue-limit remains at 0.\n\nAppreciate your thoughts on next steps.\n\nThanks,\n\nSteve.\n\n[root@gbl15217 ~]# hammer ping\ncandlepin:\n    Status:          ok\n    Server Response: Duration: 16ms\ncandlepin_auth:\n    Status:          ok\n    Server Response: Duration: 19ms\npulp:\n    Status:          ok\n    Server Response: Duration: 25ms\nforeman_tasks:\n    Status:          FAIL\n    Server Response:\n\n[root@gbl15217 ~]#\n\n[root@gbl15217 ~]# du -sh /var/lib/candlepin/hornetq\n40G     /var/lib/candlepin/hornetq\n[root@gbl15217 ~]#</text>, <text>Hello,\n\nCould you do the following steps:\n\n# cd /var/lib/qpidd/.qpidd/qls/\n\n# ls -la\n\n# du -ch *\n\n1) If the size of /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/*jrnl is more than any other folders then perform below steps:\n\nservice qpidd stop\nrm -f /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/returned/*\nkatello-service restart\n\n===============================\n\n2)If there are many under /var/lib/qpidd/.qpidd/qls/jrnl2/katello_event_queue/ , then you can:\n\nservice tomcat stop\n\nservice foreman-tasks stop\n\nqpid-config --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 del queue katello_event_queue\n\nqpid-config --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 add queue katello_event_queue\n\nfor key in compliance.created entitlement.created entitlement.deleted pool.created pool.deleted; do\n    qpid-config --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 bind event katello_event_queue $key\ndone\n\nservice qpidd stop\n\nrm -f /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/returned/*\n\nkatello-service restart\n\n\nIn both cases, services will be restarts so it is recommended to do so when Satellite is idle / not running repo sync or Content View publish/promote or a remote task or so.\n\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>The size of /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/*jrnl is 3.9GB, while the in_use+returned size comes to 15GB.\n\nI've not taken any further action yet.  Could you review and advise ?  Also attached the outputs in case useful.\n\n[root@gbl15217 2048k]# du -ch *jrnl|tail\n2.1M    ff59acdc-56c6-441f-93a2-0327995694ea.jrnl\n2.1M    ff5f1d7b-e855-43fb-9640-d85c5a31728a.jrnl\n2.1M    ff7dfc17-7b47-43f5-bf37-d04a5f672394.jrnl\n2.1M    ff834eea-b635-44ae-ab6d-486641b42a3e.jrnl\n2.1M    ff973e0c-bb29-49d4-b8d8-8c9a18c2b1d2.jrnl\n2.1M    ff98b3cc-0926-4374-8647-b614a17021eb.jrnl\n2.1M    ffa8dfd5-4d46-496c-b25b-8887b7a611b0.jrnl\n2.1M    ffb4ec7e-e0a4-4c6c-8d78-4deda188670b.jrnl\n2.1M    fff2b405-2f04-4b85-866e-3c1d26d10b5b.jrnl\n3.9G    total\n[root@gbl15217 2048k]#\n\n[root@gbl15217 2048k]# du -ch in_use returned\n15G     in_use\n4.1M    returned\n15G     total\n[root@gbl15217 2048k]#</text>, <text>Hello,\n\nThank you for update.  \n\nkatello-service stop\n\nrm -rf /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/*jrnl\n\nkatello-service start\n\njust to forcefully delete them\n\nAnd for /var/lib/qpidd/.qpidd/qls/jrnl2/katello_event_queue/ , run :\n\nservice tomcat stop\n\nservice foreman-tasks stop\n\nqpid-config --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 del queue katello_event_queue\n\nqpid-config --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 add queue katello_event_queue\n\nfor key in compliance.created entitlement.created entitlement.deleted pool.created pool.deleted; do\n    qpid-config --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 bind event katello_event_queue $key\ndone\n\nservice qpidd stop\n\nrm -f /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/returned/*\n\nkatello-service restart\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hello,\n\nI am transferring this case to NA team for further assistance.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>FTS Handover:\n\nCu is facing issue of : /var/lib/candlepin/hornetq getting filled.\n\n\nAsked to perform few steps , need to observe after performing the steps whether performance is stable or still facing issue?</text>, <text>Hello,\n \nSince there are no updates from your side on this case for more than 12 hours we are dropping this case from 24x7 monitoring.You will get support on this service request during your business hours. \n\nYou can always re-establish 24x7 monitoring by simply calling our support numbers or update the case.\n~~~\nContacting Technical Support\n# https://access.redhat.com/support/contact/technicalSupport\n~~~\n\n\n\n\n\nThanks &amp; Regards\nManeesh Verma</text>, <text>/var/lib/candlepin/hornetq/paging full again at 07:00 GMT.\n\n/var/lib/qpidd/.qpidd/qls/p001/efp/2048k/*jrnl removed as per instructions.\n\n[root@gbl15217 hornetq]# du -sh *\n2.0M    bindings\n101M    journal\n140K    largemsgs\n42G     paging\n[root@gbl15217 hornetq]#\n\nHowever, queue katello_event_queue was unable to be removed.\n\n[root@gbl15217 ~]# service tomcat stop\nRedirecting to /bin/systemctl stop tomcat.service\n[root@gbl15217 ~]# service foreman-tasks stop\nRedirecting to /bin/systemctl stop foreman-tasks.service\n[root@gbl15217 ~]# qpid-config --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 del queue katello_event_queue\nFailed: Exception: Exception from Agent: {u'error_code': 7, u'error_text': 'precondition-failed: Cannot delete queue katello_event_queue; queue not empty (/builddir/build/BUILD/qpid-cpp-0.34/src/qpid/broker/Broker.cpp:1071)'}\n[root@gbl15217 ~]#\n\nPlease advise/confirm on next steps.</text>, <text>Hello,\n\nCould you perform below steps on satellite server:\n\n# katello-service stop\n\n# cd /var/lib/candlepin/hornetq/paging\n\n# tar -cvf page.tar.gz /var/lib/candlepin/hornetq/paging/*\n\n# mv page.tar.gz /tmp/\n\n# rm -rf /var/lib/candlepin/hornetq/paging/\n\n# katello-service start\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>/var/lib/candlepin/hornetq/paging/ has been removed and katello services restarted successfully. queue-depth for katello_event_queue is now showing 0:\n\n[root@gbl15217 hornetq]# qpid-stat --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 -q katello_event_queue\nProperties:\n  Name                 Durable  AutoDelete  Exclusive  FlowStopped  FlowStoppedCount  Consumers  Bindings\n  =========================================================================================================\n  katello_event_queue  Y        N           N          N            0                 1          6\n\nOptional Properties:\n  Property      Value\n  =====================\n  arguments     {}\n  alt-exchange\n\nStatistics:\n  Statistic                   Messages  Bytes\n  ==================================================\n  queue-depth                 0         0\n  total-enqueues              1,357     23,846,867\n  total-dequeues              1,357     23,846,867\n  persistent-enqueues         1,357     23,846,867\n  persistent-dequeues         1,357     23,846,867\n  transactional-enqueues      0         0\n  transactional-dequeues      0         0\n  flow-to-disk-depth          0         0\n  flow-to-disk-enqueues       0         0\n  flow-to-disk-dequeues       0         0\n  acquires                    1,357\n  releases                    0\n  discards-ttl-expired        0\n  discards-limit-overflow     0\n  discards-ring-overflow      0\n  discards-lvq-replace        0\n  discards-subscriber-reject  0\n  discards-purged             0\n  reroutes                    0\n[root@gbl15217 hornetq]#\n\nHowever, "hammer ping" is still inconsistently reporting foreman_tasks as FAILED. Can we investigate why?\n\n[root@gbl15217 ~]# hammer ping\ncandlepin:\n    Status:          ok\n    Server Response: Duration: 29ms\ncandlepin_auth:\n    Status:          ok\n    Server Response: Duration: 24ms\npulp:\n    Status:          ok\n    Server Response: Duration: 45ms\nforeman_tasks:\n    Status:          ok\n    Server Response: Duration: 1128ms\n\n[root@gbl15217 ~]# hammer ping\ncandlepin:\n    Status:          ok\n    Server Response: Duration: 26ms\ncandlepin_auth:\n    Status:          ok\n    Server Response: Duration: 30ms\npulp:\n    Status:          ok\n    Server Response: Duration: 36ms\nforeman_tasks:\n    Status:          ok\n    Server Response: Duration: 64ms\n\n[root@gbl15217 ~]# hammer ping\ncandlepin:\n    Status:          ok\n    Server Response: Duration: 18ms\ncandlepin_auth:\n    Status:          ok\n    Server Response: Duration: 21ms\npulp:\n    Status:          ok\n    Server Response: Duration: 27ms\nforeman_tasks:\n    Status:          ok\n    Server Response: Duration: 47ms\n\n[root@gbl15217 ~]# hammer ping\ncandlepin:\n    Status:          ok\n    Server Response: Duration: 62ms\ncandlepin_auth:\n    Status:          ok\n    Server Response: Duration: 56ms\npulp:\n    Status:          ok\n    Server Response: Duration: 82ms\nforeman_tasks:\n    Status:          FAIL\n    Server Response:\n\n[root@gbl15217 ~]#\n\nAlso, do we leave "default-queue-limit=0" in /etc/qpid/qpidd.conf?</text>, <text>Hello,\n\nleave "default-queue-limit=0" in /etc/qpid/qpidd.conf\n\nCould you provide me out-put of below command:\n\n# service foreman-tasks restart\n\n# service foreman-tasks status -l\n\n# journalctl -xn\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>[root@gbl15217 ~]# service foreman-tasks restart\nRedirecting to /bin/systemctl restart foreman-tasks.service\n[root@gbl15217 ~]# service foreman-tasks status -l\nRedirecting to /bin/systemctl status  -l foreman-tasks.service\n\u25cf foreman-tasks.service - Foreman jobs daemon\n   Loaded: loaded (/usr/lib/systemd/system/foreman-tasks.service; enabled; vendor preset: disabled)\n   Active: active (running) since Thu 2017-11-16 10:46:13 GMT; 13s ago\n     Docs: https://github.com/theforeman/foreman-tasks\n  Process: 6303 ExecStop=/usr/bin/foreman-tasks stop (code=exited, status=0/SUCCESS)\n  Process: 6802 ExecStart=/usr/bin/foreman-tasks start (code=exited, status=0/SUCCESS)\n   CGroup: /system.slice/foreman-tasks.service\n           \u251c\u25006916 dynflow_executor_monitor\n           \u2514\u25006919 dynflow_executor\n\nNov 16 10:46:09 gbl15217.systems.uk.hsbc systemd[1]: Starting Foreman jobs daemon...\nNov 16 10:46:12 gbl15217.systems.uk.hsbc foreman-tasks[6802]: Dynflow Executor: start in progress\nNov 16 10:46:12 gbl15217.systems.uk.hsbc foreman-tasks[6802]: dynflow_executor: process with pid 6919 started.\nNov 16 10:46:13 gbl15217.systems.uk.hsbc systemd[1]: Started Foreman jobs daemon.\n[root@gbl15217 ~]# journalctl -xn\n-- Logs begin at Sat 2017-11-11 11:51:18 GMT, end at Thu 2017-11-16 10:46:33 GMT. --\nNov 16 10:46:33 gbl15217.systems.uk.hsbc pulp[5508]: py.warnings:WARNING: (5508-85952)\nNov 16 10:46:33 gbl15217.systems.uk.hsbc pulp[29035]: celery.worker.strategy:INFO: Received task: pulp.server.async.tasks._release_resource[a10d0973-ddff-4973-96bc-db492e63c09f]\nNov 16 10:46:33 gbl15217.systems.uk.hsbc pulp[28901]: celery.worker.job:INFO: Task pulp.server.async.tasks._queue_reserved_task[8f4e0a7a-dc96-43ba-9cc5-f8b55bff93aa] succeeded in 0.0\nNov 16 10:46:33 gbl15217.systems.uk.hsbc pulp[5508]: py.warnings:WARNING: (5508-85952) /usr/lib/python2.7/site-packages/mongoengine/document.py:367: DeprecationWarning: update is dep\nNov 16 10:46:33 gbl15217.systems.uk.hsbc pulp[5508]: py.warnings:WARNING: (5508-85952)   upsert=upsert, **write_concern)\nNov 16 10:46:33 gbl15217.systems.uk.hsbc pulp[5508]: py.warnings:WARNING: (5508-85952)\nNov 16 10:46:33 gbl15217.systems.uk.hsbc pulp[29035]: celery.worker.job:INFO: Task pulp.server.managers.consumer.applicability.regenerate_applicability_for_consumers[ad6d94c6-77d5-4e\nNov 16 10:46:33 gbl15217.systems.uk.hsbc pulp[7764]: py.warnings:WARNING: (7764-85952) /usr/lib64/python2.7/site-packages/pymongo/topology.py:74: UserWarning: MongoClient opened befo\nNov 16 10:46:33 gbl15217.systems.uk.hsbc pulp[7764]: py.warnings:WARNING: (7764-85952)   "MongoClient opened before fork. Create MongoClient "\nNov 16 10:46:33 gbl15217.systems.uk.hsbc pulp[7764]: py.warnings:WARNING: (7764-85952)\n[root@gbl15217 ~]#</text>, <text>Hello,\n\nAs per out-put:\n\nforeman-tasks.service - Foreman jobs daemon\n   Loaded: loaded (/usr/lib/systemd/system/foreman-tasks.service; enabled; vendor preset: disabled)\n   Active: active (running) since Thu 2017-11-16 10:46:13 GMT; 13s ago\n\nIt seems that foreman-tasks service is running and active at the moment.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hi, thanks for the assistance with this.  Service now seems fairly stable.  Some remaining questions if you could advise, before we finally close the case.\n\n(1) Foreman tasks continue to report a fail via 'hammer ping', about 33% of the time.  How can that be addressed ?\n(2) default-queue-limit remains at zero, need clarity on whether ok to remain set to that.\n(3) 16GB still used in /var/lib/qpidd/.qpidd/qls/p001/efp/2048k, is that normal ?\n(4) /var/lib/candlepin/hornetq/paging removed.  Needs reinstating ? \n(5) What was the root cause of this issue, and how can we avoid subsequent similar issues ?</text>, <text>Hello Steve, \n\nThank you for update. As we have seen that foeman-tasks service got start when restarted it, if its failing due to some of inconvenience then it should report while starting it, so unfortunately no known reason for it.\n\n(2) default-queue-limit remains at zero. Let it be as it is.\n\n(3) 16GB still used in /var/lib/qpidd/.qpidd/qls/p001/efp/2048k, is that normal ? =&gt; You can delete those *jrnl files as its the space where empty files get stores.\n\n(4) /var/lib/candlepin/hornetq/paging removed.  No need of reinstating, I asked you to create tar file for safety purpose , because we could restore it if something goes wrong, but as everything is going on smoothly, no need to reinstate it.\n\n5) Root cause of issue is following bugzillas:\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1503282\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1390373\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1469573\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1440235\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1202691\n\nI am transferring this case to NA Team if need any assistance.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>FTS Handover:\n\nAs per last update of cu: "Service now seems fairly stable" , provided info about few questions of cu, need to update further based on cu's update.</text>, <text>I was with the CU today... If we are telling them to set "default-queue-limit=0" in /etc/qpid/qpidd.conf can we get a BZ raised so the installer doesn't wipe that change when it is next ran (or get them to do it via the installer if it can be already)\n\nThanks!</text>, <text>Hello ,\n\nUpon review of your case for 24x7 support, it has been more than 12 hours since the last update.\nAt this time we are removing the 24x7 coverage from your case. This does not change the severity of the case but you will receive updates during your normal business hours.\n\nPlease feel free to call us for any queries.\n\nRegards,\nHarshad More\nGSS, Red Hat India.\n1.888.GO.REDHAT</text>, <text>Out of office, returning Monday 20th November.\n\n\n-----------------------------------------\n\nSAVE PAPER - THINK BEFORE YOU PRINT!\n\n\n\nThis E-mail is confidential.  \n\n\n\nIt may also be legally privileged. If you are not the addressee you may not copy,\n\nforward, disclose or use any part of it. If you have received this message in error,\n\nplease delete it and all copies from your system and notify the sender immediately by\n\nreturn E-mail.\n\n\n\nInternet communications cannot be guaranteed to be timely secure, error or virus-free.\n\nThe sender does not accept liability for any errors or omissions.</text>, <text>Hi - a few points needing advise please.\n\n(1) foreman_tasks continues to often report FAILED within 'hammer ping'.  Need to understand what is causing this and how to fix.  It doesn't just happen immediately after start, it happens repeatedly long after the service started.\n\n(2) default-queue-limit remains at zero.  What are the consequences of using this setting ?  The previous setting appears to have been the default, at 104,857,600.\n\n(3) /var/lib/qpidd/.qpidd/qls/p001/efp/2048k now using 24GB, see attached output.  The earlier comment suggested removing all .jrnl files.  Can absolutely all .jrnl files be removed, even the many created today ?  What should be housekeeping these - as we see around 2.5GB created daily.\n\n[root@gbl15217 2048k]# find . -name \\*jrnl -ls | awk '/Nov 17/{t+=$7}END{print t/1024/1024/1024,"gb"}'\n2.51858 gb\n[root@gbl15217 2048k]# find . -name \\*jrnl -ls | awk '/Nov 18/{t+=$7}END{print t/1024/1024/1024,"gb"}'\n2.88257 gb\n[root@gbl15217 2048k]# find . -name \\*jrnl -ls | awk '/Nov 19/{t+=$7}END{print t/1024/1024/1024,"gb"}'\n2.94324 gb\n[root@gbl15217 2048k]# find . -name \\*jrnl -ls | awk '/Nov 20/{t+=$7}END{print t/1024/1024/1024,"gb"}'\n1.81213 gb\n[root@gbl15217 2048k]#\n\n(4) Of the bugzillas cited as the root cause, what fixes can we apply to avoid a repeat of this outage ?  Is there a new RPM we should apply ?\n\nBug 1503282 - [RFE] Make qpidd queues more resilient / Simplify their deletion/recreation \nBug 1390373 - [6.2.z] Satellite Candlepin: Bulk delete content hosts error \nBug 1469573 - Error importing manifest.PG::Error: ERROR: null value in column "virt_who" violates not-null constraint \nBug 1440235 - candlepin event listener does not acknowledge every 100th message \nBug 1202691 - Qpid client throws weird error when boolean values are quoted in address string</text>, <text>Hello,\n\nI will discuss this case with our senior engineer and will get back to you with further details.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Issue:\n\nCu need RCA for /var/lib/qpidd/.qpidd/qls/p001/efp/2048k getting full , its now using 24GB, see attached output.  The earlier comment suggested removing all .jrnl files.  Can absolutely all .jrnl files be removed, even the many created today ?  What should be housekeeping these - as we see around 2.5GB created daily.\n\n[root@gbl15217 2048k]# find . -name \\*jrnl -ls | awk '/Nov 17/{t+=$7}END{print t/1024/1024/1024,"gb"}'\n2.51858 gb\n[root@gbl15217 2048k]# find . -name \\*jrnl -ls | awk '/Nov 18/{t+=$7}END{print t/1024/1024/1024,"gb"}'\n2.88257 gb\n[root@gbl15217 2048k]# find . -name \\*jrnl -ls | awk '/Nov 19/{t+=$7}END{print t/1024/1024/1024,"gb"}'\n2.94324 gb\n[root@gbl15217 2048k]# find . -name \\*jrnl -ls | awk '/Nov 20/{t+=$7}END{print t/1024/1024/1024,"gb"}'\n1.81213 gb\n[root@gbl15217 2048k]#\n\n======================================\n\nSteps tried till now are:\n\nkatello-service stop\n\nrm -rf /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/*jrnl\n\nkatello-service start\n\njust to forcefully delete them\n\nAnd for /var/lib/qpidd/.qpidd/qls/jrnl2/katello_event_queue/ , run :\n\nservice tomcat stop\n\nservice foreman-tasks stop\n\nqpid-config --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 del queue katello_event_queue\n\nqpid-config --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 add queue katello_event_queue\n\nfor key in compliance.created entitlement.created entitlement.deleted pool.created pool.deleted; do\n    qpid-config --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 bind event katello_event_queue $key\ndone\n\nservice qpidd stop\n\nrm -f /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/returned/*\n\nkatello-service restart\n\nCu still facing issue.</text>, <text>//private, collab\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1443470#c5 looks like this bz (or 1463819) and as per this comment, those that don't have symlinks should be safe to move of delete (maybe just move at first, to be really sure) \n\nPlease, \n1. stop qpid:\n# service qpidd stop\n\n2. identify which files in /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/in_use do not have a symlink from /var/lib/qpidd/.qpidd/qls/jrnl2/*/*  \n# ls -l /var/lib/qpidd/.qpidd/qls/jrnl2/*/*\nand move/delete such files\n\n3. start qpid again:\n# service qpidd start\n\nPlease take a snapshot/backup of your Satellite in prior to this procedure.</text>, <text>Hello,\nmy name is Pavel Moravec and I review the case as Subject Matter Expert for Satellite6 and esp. on qpid/foreman-tasks and around.\n\nI understood your current problem is /var/lib/qpidd filling one directory and hammer ping sometimes failing on foreman-tasks. That is quite probably related, foreman-tasks issues can trigger katello_event_queue problems that can create (currently) unused jrnl files under the directory. And one fact seems worryfying:\n\n[root@gbl15217 qpid]# su - postgres -c "psql -d foreman -c 'select label,count(label),state from foreman_tasks_tasks where state &lt;&gt; '\\''stopped'\\'' group by label,state;'"\n/etc/profile.d/hsbc_profile.sh: line 21: TMOUT: readonly variable\n                     label                     | count |   state\n-----------------------------------------------+-------+-----------\n Actions::Katello::Host::GenerateApplicability |     1 | planned\n Actions::Katello::EventQueue::Monitor         |     2 | running\n Actions::Candlepin::ListenOnCandlepinEvents   |     3 | running\n Actions::Insights::EmailPoller                |     1 | scheduled\n(4 rows)\n\n[root@gbl15217 qpid]#\n\nSee 3 ListenOnCandlepinEvents (LOCE) tasks running (and 2 EventQueue::Monitor ones) - there should be just one of each type. And since LOCE task connects to qpidd to fetch messages from the katello_event_queue (that seems to be affected by [1]), we shall have the root cause.\n\nI lack some logs, anyway let try fixing the above unexpected concurrent tasks and if it wont help, let provide some more data.\n\n0) In case you need RCA, provide task export (that timeouted during sosreport calling foreman-debug):\n\nforeman-rake foreman_tasks:export_tasks task_search=all\n\n(this can be time consuming but necessary to perform now, if RCA required - below we delete some tasks forever)\n\n\n1) Remove the tasks via rake console:\n\nforeman-rake console    # wait until it loads the console\n\nForemanTasks::Task.where(['label = ? AND state = ?', "Actions::Candlepin::ListenOnCandlepinEvents", "running"]).destroy_all\n\nForemanTasks::Task.where(['label = ? AND state = ?', "Actions::Katello::EventQueue::Monitor", "running"]).destroy_all\n\n(Ctrl+D to exit)\n\n\n2) Stop all services (bit overkill but better than solve potential consequences), clean not-required journal files, start everything back:\n\nkatello-service stop\n\nrm -f /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/*jrnl\n\nkatello-service start\n\n(this step will also restart foreman-tasks service, that is required also to "fix" _all_ LOCE tasks deleted)\n\n\n3) Try hammer ping several times and monitor disk usage, like:\n\nwhile true; do date; hammer ping; qpid-stat --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 -q; du -ks /var/lib/qpidd/.qpidd/qls/; find /var/lib/qpidd/.qpidd/qls/; echo; sleep 60; done &gt; 01974013-monitor.log 2&gt;&amp;1 &amp;\n\n(optionally put it as a cron job)\n\nIf hammer ping fails or disk usage grows again, provide:\n\ntar cvzf 01974013-logs.tar.gz 01974013-monitor.log /var/log/httpd/foreman-ssl_access_ssl.log /var/log/foreman/dynflow_executor.output /var/log/foreman/dynflow_executor.log /var/log/foreman/production.log /var/log/messages\n\nand upload the 01974013-logs.tar.gz together with a (new) task export per step 0)\n\n\n\nKind regards,\nPavel Moravec\nGSS SEG\n\n[1] https://access.redhat.com/solutions/3145381</text>, <text>* Currently showing 1 count for each task, as below.\n\n[root@gbl15217 steve]# su - postgres -c "psql -d foreman -c 'select label,count(label),state from    foreman_tasks_tasks where state &lt;&gt; '\\''stopped'\\'' group by label,state;'"\n/etc/profile.d/hsbc_profile.sh: line 21: TMOUT: readonly variable\n                    label                    | count |   state\n---------------------------------------------+-------+-----------\n Actions::Candlepin::ListenOnCandlepinEvents |     1 | running\n Actions::Insights::EmailPoller              |     1 | scheduled\n Actions::Katello::EventQueue::Monitor       |     1 | running\n(3 rows)\n\n[root@gbl15217 steve]# date\nMon 20 Nov 13:15:58 GMT 2017\n[root@gbl15217 steve]#\n\n* Foreman task export was run, but caused Satellite outage due to out-of-memory event.  \n  Have since manually stopped + started via katello-service.  \n  By restarting Satellite, swap usage has dropped from 100% down to 15%. \n  Have attached a sosreport.\n\nScript started on Mon 20 Nov 2017 12:59:28 GMT\n[root@gbl15217 steve]# foreman-rake foreman_tasks:export_tasks task_search=all\nGathering 180602 tasks.\n1/180602\n2/180602\n3/180602\n4/180602\n5/180602\n6/180602\n7/180602\n[...]\n24647/180602\n24648/180602\n24649/180602\n24650/180602\n24651/180602\n24652/180602\n24653/180602\n24654/180602\n/tmp/tmp.pVoKl98me4: line 1: 24800 Killed                  rake foreman_tasks:export_tasks task_search=all\n[root@gbl15217 steve]#\n\n[root@gbl15217 steve]# sar -S -s 11:00:00\nLinux 3.10.0-693.5.2.el7.x86_64 (gbl15217.systems.uk.hsbc)      20/11/17        _x86_64_        (4 CPU)\n\n11:00:01    kbswpfree kbswpused  %swpused  kbswpcad   %swpcad\n11:10:01      1253044   2941256     70.13    635692     21.61\n11:20:01      1235044   2959256     70.55    647184     21.87\n11:30:02      1204224   2990076     71.29    649016     21.71\n11:40:01      1200676   2993624     71.37    649828     21.71\n11:50:01      1207136   2987164     71.22    661700     22.15\n12:00:01      1207416   2986884     71.21    661900     22.16\n12:10:01      1200488   2993812     71.38    662144     22.12\n12:20:02      1157596   3036704     72.40    663988     21.87\n12:30:01      1082788   3111512     74.18    669960     21.53\n12:40:01       914724   3279576     78.19    677440     20.66\n12:50:02       870112   3324188     79.25    679276     20.43\n13:00:01       784976   3409324     81.28    678676     19.91\n13:10:01       271872   3922428     93.52    262556      6.69\n13:20:02      3207240    987060     23.53     48192      4.88\nAverage:      1199810   2994490     71.39    589110     19.67\n[root@gbl15217 steve]#\n\nExcerpt from /var/log/messages (full log is in attached sosreport)\n\nNov 20 13:14:47 gbl15217 kernel: qdrouterd invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=0\nNov 20 13:14:47 gbl15217 kernel: qdrouterd cpuset=/ mems_allowed=0\nNov 20 13:14:47 gbl15217 kernel: CPU: 3 PID: 24970 Comm: qdrouterd Not tainted 3.10.0-693.5.2.el7.x86_64 #1\nNov 20 13:14:47 gbl15217 kernel: Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 09/21/2015\nNov 20 13:14:47 gbl15217 kernel: ffff8802fefa1fa0 0000000006791747 ffff8801d7fdb700 ffffffff816a3e51\nNov 20 13:14:47 gbl15217 kernel: ffff8801d7fdb790 ffffffff8169f246 ffff8801d7fdb798 ffffffff812b7d1b\nNov 20 13:14:47 gbl15217 kernel: ffff8807dcf59618 0000000000000206 ffffffff00000206 fffeefff00000000\nNov 20 13:14:47 gbl15217 kernel: Call Trace:\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff816a3e51&gt;] dump_stack+0x19/0x1b\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff8169f246&gt;] dump_header+0x90/0x229\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff812b7d1b&gt;] ? cred_has_capability+0x6b/0x120\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff811863a4&gt;] oom_kill_process+0x254/0x3d0\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff812b7eec&gt;] ? selinux_capable+0x1c/0x40\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81186be6&gt;] out_of_memory+0x4b6/0x4f0\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff8169fd4a&gt;] __alloc_pages_slowpath+0x5d6/0x724\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff8118cdb5&gt;] __alloc_pages_nodemask+0x405/0x420\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff811d1078&gt;] alloc_pages_current+0x98/0x110\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff8118761e&gt;] __get_free_pages+0xe/0x40\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215bf0&gt;] __pollwait+0xa0/0xf0\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff815d9e6a&gt;] tcp_poll+0x5a/0x230\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff8156868d&gt;] sock_poll+0x9d/0xb0\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81217287&gt;] do_sys_poll+0x327/0x580\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215b50&gt;] ? poll_initwait+0x50/0x50\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215dc0&gt;] ? poll_select_copy_remaining+0x150/0x150\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215dc0&gt;] ? poll_select_copy_remaining+0x150/0x150\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215dc0&gt;] ? poll_select_copy_remaining+0x150/0x150\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215dc0&gt;] ? poll_select_copy_remaining+0x150/0x150\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215dc0&gt;] ? poll_select_copy_remaining+0x150/0x150\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215dc0&gt;] ? poll_select_copy_remaining+0x150/0x150\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215dc0&gt;] ? poll_select_copy_remaining+0x150/0x150\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215dc0&gt;] ? poll_select_copy_remaining+0x150/0x150\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff81215dc0&gt;] ? poll_select_copy_remaining+0x150/0x150\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff812175e4&gt;] SyS_poll+0x74/0x110\nNov 20 13:14:47 gbl15217 kernel: [&lt;ffffffff816b5089&gt;] system_call_fastpath+0x16/0x1b\nNov 20 13:14:47 gbl15217 kernel: Mem-Info:\nNov 20 13:14:47 gbl15217 kernel: active_anon:7341132 inactive_anon:551509 isolated_anon:0#012 active_file:6156 inactive_file:6361 isolated_file:383#012 unevictable:0 dirty:5 writeback:0 unstable:0#012 slab_reclaimable:96852 slab_unreclaimable:29594#012 mapped:17975 shmem:277336 pagetables:47024 bounce:0#012 free:49866 free_pcp:89 free_cma:0\nNov 20 13:14:47 gbl15217 kernel: Node 0 DMA free:15868kB min:32kB low:40kB high:48kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:15992kB managed:15908kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB slab_reclaimable:0kB slab_unreclaimable:8kB kernel_stack:0kB pagetables:0kB unstable:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:0 all_unreclaimable? yes\nNov 20 13:14:47 gbl15217 kernel: lowmem_reserve[]: 0 2815 31998 31998\nNov 20 13:14:47 gbl15217 kernel: Node 0 DMA32 free:122236kB min:5940kB low:7424kB high:8908kB active_anon:1976604kB inactive_anon:494184kB active_file:2040kB inactive_file:1852kB unevictable:0kB isolated(anon):0kB isolated(file):380kB present:3129280kB managed:2883272kB mlocked:0kB dirty:8kB writeback:0kB mapped:8780kB shmem:53220kB slab_reclaimable:240748kB slab_unreclaimable:15556kB kernel_stack:4288kB pagetables:14560kB unstable:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:20657 all_unreclaimable? yes\nNov 20 13:14:47 gbl15217 kernel: lowmem_reserve[]: 0 0 29182 29182\nNov 20 13:14:47 gbl15217 kernel: Node 0 Normal free:61360kB min:61608kB low:77008kB high:92412kB active_anon:27387924kB inactive_anon:1711852kB active_file:22584kB inactive_file:23592kB unevictable:0kB isolated(anon):0kB isolated(file):1152kB present:30408704kB managed:29882972kB mlocked:0kB dirty:12kB writeback:0kB mapped:63120kB shmem:1056124kB slab_reclaimable:146660kB slab_unreclaimable:102812kB kernel_stack:13376kB pagetables:173536kB unstable:0kB bounce:0kB free_pcp:356kB local_pcp:120kB free_cma:0kB writeback_tmp:0kB pages_scanned:97994 all_unreclaimable? yes\nNov 20 13:14:47 gbl15217 kernel: lowmem_reserve[]: 0 0 0 0\nNov 20 13:14:47 gbl15217 kernel: Node 0 DMA: 1*4kB (U) 1*8kB (U) 1*16kB (U) 1*32kB (U) 1*64kB (U) 1*128kB (U) 1*256kB (U) 0*512kB 1*1024kB (U) 1*2048kB (M) 3*4096kB (M) = 15868kB\nNov 20 13:14:47 gbl15217 kernel: Node 0 DMA32: 17004*4kB (UEM) 6829*8kB (UEM) 0*16kB 0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 0*4096kB = 122648kB\nNov 20 13:14:47 gbl15217 kernel: Node 0 Normal: 15313*4kB (U) 0*8kB 0*16kB 0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 0*4096kB = 61252kB\nNov 20 13:14:47 gbl15217 kernel: Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=1048576kB\nNov 20 13:14:47 gbl15217 kernel: Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB\nNov 20 13:14:47 gbl15217 kernel: 304230 total pagecache pages\nNov 20 13:14:47 gbl15217 kernel: 13733 pages in swap cache\nNov 20 13:14:47 gbl15217 kernel: Swap cache stats: add 4229686, delete 4215953, find 17214522/17417420\nNov 20 13:14:47 gbl15217 kernel: Free swap  = 0kB\nNov 20 13:14:47 gbl15217 kernel: Total swap = 4194300kB\nNov 20 13:14:47 gbl15217 kernel: 8388494 pages RAM\nNov 20 13:14:47 gbl15217 kernel: 0 pages HighMem/MovableOnly\nNov 20 13:14:47 gbl15217 kernel: 192956 pages reserved\nNov 20 13:14:47 gbl15217 kernel: [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name\nNov 20 13:14:47 gbl15217 kernel: [  574]     0   574    26738    10952      58       54             0 systemd-journal\nNov 20 13:14:47 gbl15217 kernel: [  596]     0   596   104080       84      38      913             0 lvmetad\nNov 20 13:14:47 gbl15217 kernel: [  611]     0   611    13182       88      26      568         -1000 systemd-udevd\nNov 20 13:14:47 gbl15217 kernel: [  919]     0   919    14166       39      27       95         -1000 auditd\nNov 20 13:14:47 gbl15217 kernel: [  946]     0   946    25741       12      46      408             0 VGAuthService\nNov 20 13:14:47 gbl15217 kernel: [  948]     0   948    77158      177      59      260             0 vmtoolsd\nNov 20 13:14:47 gbl15217 kernel: [  951]     0   951     7491      110      18       70             0 systemd-logind\nNov 20 13:14:47 gbl15217 kernel: [  952]   997   952   160441      178      61     1908             0 polkitd\nNov 20 13:14:47 gbl15217 kernel: [  956]    81   956     9583      118      19       52          -900 dbus-daemon\nNov 20 13:14:47 gbl15217 kernel: [  963]     0   963   120124      833      87      294             0 NetworkManager\nNov 20 13:14:47 gbl15217 kernel: [  964]     0   964     5406       62      16       42             0 irqbalance\nNov 20 13:14:47 gbl15217 kernel: [  967]     0   967    32937       42      20      139             0 crond\nNov 20 13:14:47 gbl15217 kernel: [  971]     0   971    28889       23      11       38             0 agetty\nNov 20 13:14:47 gbl15217 kernel: [ 1240]     0  1240   141440      878      96     2002             0 tuned\nNov 20 13:14:47 gbl15217 kernel: [ 1243]     0  1243   377516     3803     182     3251             0 python\nNov 20 13:14:47 gbl15217 kernel: [ 1252]     0  1252    26802       71      55      223         -1000 sshd\nNov 20 13:14:47 gbl15217 kernel: [ 1253]     0  1253    28945       39      13       19             0 rhsmcertd\nNov 20 13:14:47 gbl15217 kernel: [ 1270]    38  1270     8313       37      21      125             0 ntpd\nNov 20 13:14:47 gbl15217 kernel: [ 1275]     0  1275     7620       27      19       72             0 xinetd\nNov 20 13:14:47 gbl15217 kernel: [ 1428]     0  1428    44819       52      30      107             0 vnetd\nNov 20 13:14:47 gbl15217 kernel: [ 1475]     0  1475    64858     9111      81     1403             0 puppet\nNov 20 13:14:47 gbl15217 kernel: [ 1500]     0  1500    54479       57      49      232             0 bpcd\nNov 20 13:14:47 gbl15217 kernel: [ 1511]     0  1511    26500      144      23      476             0 python\nNov 20 13:14:47 gbl15217 kernel: [ 1517]     0  1517   247938     5816     102     3942             0 python\nNov 20 13:14:47 gbl15217 kernel: [ 1546]     0  1546    38118     7278      55      225             0 adclient\nNov 20 13:14:47 gbl15217 kernel: [ 1559]     0  1559    23963       43      16       66             0 cdcwatch\nNov 20 13:14:47 gbl15217 kernel: [ 1893]     0  1893    78526        8      37      220             0 mtstrmd\nNov 20 13:14:47 gbl15217 kernel: [ 2273]     0  2273    22689       40      42      247             0 master\nNov 20 13:14:47 gbl15217 kernel: [ 2304]    89  2304    22761       33      44      249             0 qmgr\nNov 20 13:14:47 gbl15217 kernel: [ 3044]     0  3044     1739       20       7       71             0 bvcontrold\nNov 20 13:14:47 gbl15217 kernel: [ 3318]     0  3318    53886    17561     109    10143             0 splunkd\nNov 20 13:14:47 gbl15217 kernel: [ 3328]     0  3328    16633      120      30     2367         -1000 splunkd\nNov 20 13:14:47 gbl15217 kernel: [ 3348]     0  3348    26980       36       9       10             0 rhnsd\nNov 20 13:14:47 gbl15217 kernel: [ 3451]     0  3451      909      107       5       54             0 discagnt\nNov 20 13:14:47 gbl15217 kernel: [10103]     0 10103    26986       17      10       10             0 tail\nNov 20 13:14:47 gbl15217 kernel: [13423]     0 13423    26986        0       9       25             0 tail\nNov 20 13:14:47 gbl15217 kernel: [17322]   177 17322    29340       58      58     3165             0 dhcpd\nNov 20 13:14:47 gbl15217 kernel: [22012]     0 22012    26986        1       9       26             0 tail\nNov 20 13:14:47 gbl15217 kernel: [26741]     0 26741    74846      303     113     3259          -900 rhsmd\nNov 20 13:14:47 gbl15217 kernel: [10851]     0 10851   173401     6178     170      123             0 rsyslogd\nNov 20 13:14:47 gbl15217 kernel: [28461]   184 28461 13808973    28152    8597     2773             0 mongod\nNov 20 13:14:47 gbl15217 kernel: [28506]    26 28506    58735      724      53      174         -1000 postgres\n\n\n[d3250498@gbl15217 ~]$ sar -S -s 11:00:00\nLinux 3.10.0-693.5.2.el7.x86_64 (gbl15217.systems.uk.hsbc)      20/11/17        _x86_64_        (4 CPU)\n\n11:00:01    kbswpfree kbswpused  %swpused  kbswpcad   %swpcad\n11:10:01      1253044   2941256     70.13    635692     21.61\n11:20:01      1235044   2959256     70.55    647184     21.87\n11:30:02      1204224   2990076     71.29    649016     21.71\n11:40:01      1200676   2993624     71.37    649828     21.71\n11:50:01      1207136   2987164     71.22    661700     22.15\n12:00:01      1207416   2986884     71.21    661900     22.16\n12:10:01      1200488   2993812     71.38    662144     22.12\n12:20:02      1157596   3036704     72.40    663988     21.87\n12:30:01      1082788   3111512     74.18    669960     21.53\n12:40:01       914724   3279576     78.19    677440     20.66\n12:50:02       870112   3324188     79.25    679276     20.43\n13:00:01       784976   3409324     81.28    678676     19.91\n13:10:01       271872   3922428     93.52    262556      6.69\n13:20:02      3207240    987060     23.53     48192      4.88\n13:30:01      3534052    660248     15.74     31880      4.83\nAverage:      1355426   2838874     67.68    551962     19.44\n[d3250498@gbl15217 ~]$</text>, <text>Appears the earlier export wrote over 3GB to /tmp, thus causing outage.  Possible to have it write to a different (larger) filesystem by passing some params ?\n\n[root@gbl15217 task-export20171120-24800-500n0g]# ls -ld .\ndrwx------. 2 foreman foreman 1404928 Nov 20 13:14 .\n[root@gbl15217 task-export20171120-24800-500n0g]# du -sh .\n3.0G    .\n[root@gbl15217 task-export20171120-24800-500n0g]# ls -ltr|head\ntotal 3117056\n-rwxr-x---. 1 foreman foreman   66924 Nov 20 13:00 bootstrap.js\n-rw-r-----. 1 foreman foreman   16682 Nov 20 13:00 run_prettify.js\n-rw-r-----. 1 foreman foreman  274080 Nov 20 13:00 jquery.js\n-rw-r-----. 1 foreman foreman     729 Nov 20 13:00 application.js\n-rwxr-x---. 1 foreman foreman  141796 Nov 20 13:00 bootstrap.css\n-rw-r-----. 1 foreman foreman    1603 Nov 20 13:00 application.css\n-rw-r-----. 1 foreman foreman  166084 Nov 20 13:00 4da3c57f-46e8-45c8-b277-6244cace030e.html\n-rw-r-----. 1 foreman foreman  113670 Nov 20 13:00 989685b0-0e86-489b-99f1-62a9c7261727.html\n-rw-r-----. 1 foreman foreman  166084 Nov 20 13:00 42032122-7bd9-40dc-ac9c-77555819c7c0.html\n[root@gbl15217 task-export20171120-24800-500n0g]# ls | wc -l\n24661\n[root@gbl15217 task-export20171120-24800-500n0g]#</text>, <text>Hello,\n\nCould you please refer below article to change the /tmp/ directory for generating the task export:\n======================\nhttps://access.redhat.com/solutions/2623091\n=======================\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>After making those changes, and rerunning the foreman-rake, it unfortunately still managed to consume all swap on the server. \n\n[root@gbl15217 ~]# du -sh /backup/foreman\n9.1G    /backup/foreman\n[root@gbl15217 ~]#\n\n[root@gbl15217 tasks]# pwd\n/opt/theforeman/tfm/root/usr/share/gems/gems/foreman-tasks-0.7.14.14/lib/foreman_tasks/tasks\n[root@gbl15217 tasks]# diff export_tasks.rake.orig export_tasks.rake\n250c250,252\n&lt;     export_filename = ENV['TASK_FILE'] || "/tmp/task-export-#{DateTime.now.to_i}.#{format == 'csv' ? 'csv' : 'tar.gz'}"\n---\n&gt;     # Changed 20-Nov-2017 Steve Kay, case 01974013\n&gt;     #export_filename = ENV['TASK_FILE'] || "/tmp/task-export-#{DateTime.now.to_i}.#{format == 'csv' ? 'csv' : 'tar.gz'}"\n&gt;     export_filename = ENV['TASK_FILE'] || "/backup/foreman/task-export-#{DateTime.now.to_i}.#{format == 'csv' ? 'csv' : 'tar.gz'}"\n[root@gbl15217 tasks]#\n\n[root@gbl15217 ruby]# pwd\n/opt/rh/rh-ruby22/root/usr/share/ruby\n[root@gbl15217 ruby]# diff tmpdir.rb.orig tmpdir.rb\n25c25,27\n&lt;       [ENV['TMPDIR'], ENV['TMP'], ENV['TEMP'], @@systmpdir, '/tmp', '.'].each do |dir|\n---\n&gt;       # Modified 20-Nov-2017 Steve Kay, case 01974013\n&gt;       # [ENV['TMPDIR'], ENV['TMP'], ENV['TEMP'], @@systmpdir, '/tmp', '.'].each do |dir|\n&gt;       [ '/backup/foreman', '.'].each do|dir|\n[root@gbl15217 ruby]#</text>, <text>sosreport attached, taken at 16:34 after the latest foreman-rake consumed all swap.  katello-service restart has since been run, in order to resume service.</text>, <text>Hello,\n\nCould you please let me know have you done below steps as suggested by Pavel:\n\n1) Stop all services (bit overkill but better than solve potential consequences), clean not-required journal files, start everything back:\n\nkatello-service stop\n\nrm -f /var/lib/qpidd/.qpidd/qls/p001/efp/2048k/*jrnl\n\nkatello-service start\n\n(this step will also restart foreman-tasks service, that is required also to "fix" _all_ LOCE tasks deleted)\n\n\n2) Try hammer ping several times and monitor disk usage, like:\n\nwhile true; do date; hammer ping; qpid-stat --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 -q; du -ks /var/lib/qpidd/.qpidd/qls/; find /var/lib/qpidd/.qpidd/qls/; echo; sleep 60; done &gt; 01974013-monitor.log 2&gt;&amp;1 &amp;\n\n(optionally put it as a cron job)\n\nIf hammer ping fails or disk usage grows again, provide:\n\ntar cvzf 01974013-logs.tar.gz 01974013-monitor.log /var/log/httpd/foreman-ssl_access_ssl.log /var/log/foreman/dynflow_executor.output /var/log/foreman/dynflow_executor.log /var/log/foreman/production.log /var/log/messages\n\nand upload the 01974013-logs.tar.gz together with a (new) task export per step 0)\n\nMay be you need to observe the situation till tomorrow to provide requested out-put.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>That stop / manual cleardown of jrnl files / start has now been actioned successfully,  08:30 UKT.\nHappy to report that disk usage now appears stable.\nforeman continues to report a fail in 'hammer ping'.  \nLogs attached as requested.  Have not run a task export yet.</text>, <text>Started running a task export, had to abort it and completely restart katello-service about 25% of the way through, as it had consumed 12GB of memory by that point, and was looking likely to cause a third service outage.  Generated 6GB of output files during that time, will attach all/some of that output, depending on compressed size.\n\n[root@gbl15217 backup]# head tasks201711210920.txt\nnohup: ignoring input\nGathering 183196 tasks.\n1/183196\n2/183196\n3/183196\n4/183196\n5/183196\n6/183196\n7/183196\n8/183196\n[root@gbl15217 backup]# tail tasks201711210920.txt\n42482/183196\n42483/183196\n42484/183196\n42485/183196\n42486/183196\n42487/183196\n42488/183196\n42489/183196\n42490/183196\n42491/183196\n[root@gbl15217 backup]#</text>, <text>Truncated task output attached.\n\n# ls -l tasks201711210920.tar.gz\n-rw-r--r--. 1 root root 356941180 Nov 21 09:43 tasks201711210920.tar.gz\n#</text>, <text>Unable to get that 350MB file through our proxy.  Truncated the output a little further, in order to give you a sample of the output generated.</text>, <text>Hello,\n\nThank you for update. I am looking into the task export provided and will get back to you with further details.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hello,\n\nAs per provided task export, it shows the logs of 26th oct 2017, where all system registration logs shows error : There was an issue with the backend service candlepin: 404 Resource Not Found (RuntimeError)\n\n, but as per recent out-put it does not shows the error related to candlepin service, so might you have run satellite-installer to fix that issue.\n\nSo, could you provide me the output of below commands from satellite server:\n===================\n# psql foreman\n\nforeman=# select count(*) from dynflow_actions;\n\nforeman=# select count(*) from dynflow_execution_plans;\n\nforeman=# select count(*) from dynflow_steps;\n\nforeman=# select count(*) from foreman_tasks_locks;\n\nforeman=# select count(*) from foreman_tasks_tasks;\n\nforeman=# \\q\n========================\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hi,\n\nSure, will run the psql as required.  Could you confirm the syntax please ?\n\nThanks,\n\nSteve\n\n[root@gbl15217 ~]# id\nuid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023\n[root@gbl15217 ~]# psql foreman\npsql: FATAL:  role "root" does not exist\n[root@gbl15217 ~]#\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 21 November 2017 12:21\nTo: Koon TANG &lt;koon.tang@hsbc.com&gt;; sokeeffe@redhat.com; Stephen G KAY &lt;stephenkay@hsbc.com&gt;\nSubject: (WoC) (SEV 1) Case #01974013 (Satellite outage due to /var full) ref:_00DA0HxWH._500A0Z2w2G:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01974013\nCase Title       : Satellite outage due to /var full\nCase Number      : 01974013\nCase Open Date   : 2017-11-15 09:46:32\nSeverity         : 1 (Urgent)\nProblem Type     : Defect / Bug\n\nMost recent comment: On 2017-11-21 12:20:53, Waghmare, Prashant commented:\n"Hello,\n\nAs per provided task export, it shows the logs of 26th oct 2017, where all system registration logs shows error : There was an issue with the backend service candlepin: 404 Resource Not Found (RuntimeError)\n\n, but as per recent out-put it does not shows the error related to candlepin service, so might you have run satellite-installer to fix that issue.\n\nSo, could you provide me the output of below commands from satellite server:\n===================\n# psql foreman\n\nforeman=# select count(*) from dynflow_actions;\n\nforeman=# select count(*) from dynflow_execution_plans;\n\nforeman=# select count(*) from dynflow_steps;\n\nforeman=# select count(*) from foreman_tasks_locks;\n\nforeman=# select count(*) from foreman_tasks_tasks;\n\nforeman=# \\q\n========================\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\n---------------------------------------\n\nThank you for your latest interaction with Red Hat Global Support Services. We are currently working to resolve your case.\n\nYour case has transitioned to "Waiting On Customer" status. This means that the Red Hat associate working on your case needs information or action from you to proceed. To help us resolve your case as quickly as possible, please update your case online on the Customer Portal at https://access.redhat.com.\n\nOnce you update the case to provide the requested information, we can continue working to resolve your issue.\n\nIf you wish to contact Red Hat, visit the Customer Portal at https://access.redhat.com to find phone and web contact information relevant to your region and support contract.\n\n\nThank you,\n\nRed Hat Global Support Services\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z2w2G:ref\n\n\n\n\n \n\n\n-----------------------------------------\nSAVE PAPER - THINK BEFORE YOU PRINT!\n\nThis E-mail is confidential.  \n\nIt may also be legally privileged. If you are not the addressee you may not copy,\nforward, disclose or use any part of it. If you have received this message in error,\nplease delete it and all copies from your system and notify the sender immediately by\nreturn E-mail.\n\nInternet communications cannot be guaranteed to be timely secure, error or virus-free.\nThe sender does not accept liability for any errors or omissions.</text>, <text>Now actioned ok...su - postgres.....\n\nSteve.\n\n[root@gbl15217 ~]# su - postgres\nLast login: Tue Nov 21 10:25:17 GMT 2017 on pts/4\n-bash: TMOUT: readonly variable\n-bash-4.2$ psql foreman\npsql (9.2.23)\nType "help" for help.\n\nforeman=# select count(*) from dynflow_actions;\n  count\n---------\n 2720519\n(1 row)\n\nforeman=# select count(*) from dynflow_execution_plans;\n count\n--------\n 778676\n(1 row)\n\nforeman=# select count(*) from dynflow_steps;\n  count\n---------\n 4143017\n(1 row)\n\nforeman=# select count(*) from foreman_tasks_locks;\n  count\n---------\n 1461796\n(1 row)\n\nforeman=# select count(*) from foreman_tasks_tasks;\n count\n--------\n 778555\n(1 row)\n\nforeman=# \\q\n-bash-4.2$\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 21 November 2017 12:30\nTo: Koon TANG &lt;koon.tang@hsbc.com&gt;; sokeeffe@redhat.com; Stephen G KAY &lt;stephenkay@hsbc.com&gt;\nSubject: (WoRH) (SEV 1) Case #01974013 (Satellite outage due to /var full) ref:_00DA0HxWH._500A0Z2w2G:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01974013\nCase Title       : Satellite outage due to /var full\nCase Number      : 01974013\nCase Open Date   : 2017-11-15 09:46:32\nSeverity         : 1 (Urgent)\nProblem Type     : Defect / Bug\n\nMost recent comment: On 2017-11-21 12:30:04, Kay, Steve commented:\n"Hi,\n\nSure, will run the psql as required.  Could you confirm the syntax please ?\n\nThanks,\n\nSteve\n\n[root@gbl15217 ~]# id\nuid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023\n[root@gbl15217 ~]# psql foreman\npsql: FATAL:  role "root" does not exist\n[root@gbl15217 ~]#\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com]\nSent: 21 November 2017 12:21\nTo: Koon TANG &lt;koon.tang@hsbc.com&gt;; sokeeffe@redhat.com; Stephen G KAY &lt;stephenkay@hsbc.com&gt;\nSubject: (WoC) (SEV 1) Case #01974013 (Satellite outage due to /var full) ref:_00DA0HxWH._500A0Z2w2G:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01974013\nCase Title       : Satellite outage due to /var full\nCase Number      : 01974013\nCase Open Date   : 2017-11-15 09:46:32\nSeverity         : 1 (Urgent)\nProblem Type     : Defect / Bug\n\nMost recent comment: On 2017-11-21 12:20:53, Waghmare, Prashant commented:\n"Hello,\n\nAs per provided task export, it shows the logs of 26th oct 2017, where all system registration logs shows error : There was an issue with the backend service candlepin: 404 Resource Not Found (RuntimeError)\n\n, but as per recent out-put it does not shows the error related to candlepin service, so might you have run satellite-installer to fix that issue.\n\nSo, could you provide me the output of below commands from satellite server:\n===================\n# psql foreman\n\nforeman=# select count(*) from dynflow_actions;\n\nforeman=# select count(*) from dynflow_execution_plans;\n\nforeman=# select count(*) from dynflow_steps;\n\nforeman=# select count(*) from foreman_tasks_locks;\n\nforeman=# select count(*) from foreman_tasks_tasks;\n\nforeman=# \\q\n========================\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\n---------------------------------------\n\nThank you for your latest interaction with Red Hat Global Support Services. We are currently working to resolve your case.\n\nYour case has transitioned to "Waiting On Customer" status. This means that the Red Hat associate working on your case needs information or action from you to proceed. To help us resolve your case as quickly as possible, please update your case online on the Customer Portal at https://access.redhat.com.\n\nOnce you update the case to provide the requested information, we can continue working to resolve your issue.\n\nIf you wish to contact Red Hat, visit the Customer Portal at https://access.redhat.com to find phone and web contact information relevant to your region and support contract.\n\n\nThank you,\n\nRed Hat Global Support Services\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z2w2G:ref\n\n\n\n\n \n\n\n-----------------------------------------\nSAVE PAPER - THINK BEFORE YOU PRINT!\n\nThis E-mail is confidential.  \n\nIt may also be legally privileged. If you are not the addressee you may not copy, forward, disclose or use any part of it. If you have received this message in error, please delete it and all copies from your system and notify the sender immediately by return E-mail.\n\nInternet communications cannot be guaranteed to be timely secure, error or virus-free.\nThe sender does not accept liability for any errors or omissions."\n\nhttps://access.redhat.com/support/cases/#/case/01974013?commentId=a0aA000000L3sL1IAJ\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z2w2G:ref\n\n\n\n\n \n\n\n-----------------------------------------\nSAVE PAPER - THINK BEFORE YOU PRINT!\n\nThis E-mail is confidential.  \n\nIt may also be legally privileged. If you are not the addressee you may not copy,\nforward, disclose or use any part of it. If you have received this message in error,\nplease delete it and all copies from your system and notify the sender immediately by\nreturn E-mail.\n\nInternet communications cannot be guaranteed to be timely secure, error or virus-free.\nThe sender does not accept liability for any errors or omissions.</text>, <text>Hello,\n\nFor RHEL6 64 bit, attaching the hotfix.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hello,\n\nPlease ignore my last reply and hotfix provided, mistakenly typed here. From the provided out-put , could you run below steps:\n=========================\n1. Stop the Satellite Service\n# katello-service stop\n\n2. Take a snapshot if you don't have a recent one.\n\n3. Start the postgresl service\n# systemctl start postgresql\n\n4. Switch to postgres user\n# su - postgres\n\n5. Let's take a dump of the db, adjust destination if needed\n$ pg_dump foreman &gt; foreman.sql\n\n6. Connect to foreman database\n$ psql foreman\n\n7. Truncate the task tables\nforeman=# TRUNCATE TABLE dynflow_actions CASCADE;\nforeman=# TRUNCATE TABLE dynflow_execution_plans CASCADE;\nforeman=# TRUNCATE TABLE dynflow_steps CASCADE;\nforeman=# TRUNCATE TABLE foreman_tasks_locks CASCADE;\nforeman=# TRUNCATE TABLE foreman_tasks_tasks CASCADE;\n\nNote:\n------------------------------------------------------\nAs a reference, here isu - postgres\ns the output I got in my setup:\n  foreman=# TRUNCATE TABLE dynflow_actions CASCADE;\n  NOTICE:  truncate cascades to table "dynflow_steps"\n  TRUNCATE TABLE\n  foreman=# TRUNCATE TABLE dynflow_execution_plans CASCADE;\n  NOTICE:  truncate cascades to table "dynflow_actions"\n  NOTICE:  truncate cascades to table "dynflow_steps"\n  NOTICE:  truncate cascades to table "dynflow_delayed_plans"\n  TRUNCATE TABLE\n\n=&gt; I had to press Enter a 2nd time after few minutes to see it completing\n  foreman=# \n  foreman=# TRUNCATE TABLE dynflow_steps CASCADE;\n  TRUNCATE TABLE\n  foreman=# TRUNCATE TABLE foreman_tasks_locks CASCADE;\n  TRUNCATE TABLE\n  foreman=# TRUNCATE TABLE foreman_tasks_tasks CASCADE;\n  TRUNCATE TABLE\n  foreman=# \n------------------------------------------------------\n\n8. Quit Postgres\nforeman=# \\q\n\n9. Restart Satellite services\n# katello-service restart\n===================================\n\nThen check the performance of foreman-tasks service.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Prashant, thanks for the advice.  As this requires some downtime I've scheduled a Change Request for Sunday 26th November, in order to action the suggestions.  Will be in touch afterwards with how it went.  In the meantime, by all means reduce the severity on this to a P3.</text>, <text>Hello,\n\nThank you for update. I have reduced the severity of case to sev3 as per your request. Meanwhile I am keeping this case as waiting on customer.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Out of office, returning Monday 27th November.\n\n\n-----------------------------------------\n\nSAVE PAPER - THINK BEFORE YOU PRINT!\n\n\n\nThis E-mail is confidential.  \n\n\n\nIt may also be legally privileged. If you are not the addressee you may not copy,\n\nforward, disclose or use any part of it. If you have received this message in error,\n\nplease delete it and all copies from your system and notify the sender immediately by\n\nreturn E-mail.\n\n\n\nInternet communications cannot be guaranteed to be timely secure, error or virus-free.\n\nThe sender does not accept liability for any errors or omissions.</text>, <text>Tables successfully truncated.  The FAIL against foreman_tasks is still often seen via 'hammer ping' though.\n\n-bash-4.2$ pg_dump foreman &gt;foreman.sql\n-bash-4.2$ ls -lh\n-rw-r-----. 1 postgres postgres 55G Nov 26 12:15 foreman.sql\n-bash-4.2$ psql foreman\npsql (9.2.23)\nType "help" for help.\n\nforeman=# TRUNCATE TABLE dynflow_actions CASCADE;\nNOTICE:  truncate cascades to table "dynflow_steps"\nTRUNCATE TABLE\nforeman=# TRUNCATE TABLE dynflow_execution_plans CASCADE;\nNOTICE:  truncate cascades to table "dynflow_actions"\nNOTICE:  truncate cascades to table "dynflow_steps"\nNOTICE:  truncate cascades to table "dynflow_delayed_plans"\nTRUNCATE TABLE\nforeman=# TRUNCATE TABLE dynflow_steps CASCADE;\nTRUNCATE TABLE\nforeman=# TRUNCATE TABLE foreman_tasks_locks CASCADE;\nTRUNCATE TABLE\nforeman=# TRUNCATE TABLE foreman_tasks_tasks CASCADE;\nTRUNCATE TABLE\nforeman=# \\q\n-bash-4.2$ pg_dump foreman &gt;foreman-after.sql\n-bash-4.2$ ls -lh\ntotal 58G\n-rw-r-----. 1 postgres postgres 2.6G Nov 26 12:21 foreman-after.sql\n-rw-r-----. 1 postgres postgres  55G Nov 26 12:15 foreman.sql\n-bash-4.2$</text>, <text>Hello,\n\nFor this issue Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1420651 is available.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hi, that BZ suggests the issue is present only for a short period after foreman-tasks startup\n\n    If you run "service foreman-tasks start", this will kick off foreman-tasks. However, "hammer ping" will return "fail" for foreman-tasks for a few dozen seconds after the systemctl script has returned.\n\nAm finding our issue remains long after foreman-tasks has started.  See attached example.  Appreciate your thoughts on this and how best to fix.</text>, <text>After removing all foreman tasks yesterday, did a new export today.  Appears to have generated many tasks in that time.  Unsure whether this is expected.  Attaching details.</text>, <text>Task file quite large, couldn't attach directly to case, so uploaded via your dropbox server.\n\n[root@gbl15217 foreman]# cp task-export-1511775775.tar.gz tasks-case-no-01974013\n[root@gbl15217 foreman]# curl -T tasks-case-no-01974013 -x http://43250498:TopBar789@uk-proxy-01:80 ftp://dropbox.redhat.com/incoming/\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 15.9M    0     0  100 15.9M      0   417k  0:00:39  0:00:39 --:--:--  361k226 File receive OK.\n100 15.9M    0    22  100 15.9M      0   415k  0:00:39  0:00:39 --:--:--  321k\n[root@gbl15217 foreman]#</text>, <text>Ignore the updates I've placed on this case today, they were intended for a (related?) case 01979232</text>, <text>So, to summarise, we've done the table truncates on 26-Nov.  The tasks/locks were removed ok, resulting in the DB now dumping to a size of just 2.6GB instead of 55GB.\n\n-bash-4.2$ pg_dump foreman &gt;foreman-after.sql\n-bash-4.2$ ls -lh\n-rw-r-----. 1 postgres postgres 2.6G Nov 26 12:21 foreman-after.sql\n-rw-r-----. 1 postgres postgres  55G Nov 26 12:15 foreman.sql\n-bash-4.2$\n\nRunning "foreman-rake foreman_tasks:export_tasks task_search=all" today (27-Nov) completes ok, finding 4,607 tasks. \nThe number of tasks appears to be increasing by about 200 each hour.\nSo there's a concern that left unchecked this will eventually reach the situation we had last week, with thousands of tasks present.\n\nCounts currently look as below.\n\nforeman=# select count(*) from dynflow_actions;\n count\n-------\n 15714\n(1 row)\n\nforeman=# select count(*) from dynflow_execution_plans;\n count\n-------\n  4620\n(1 row)\n\nforeman=# select count(*) from dynflow_steps;\n count\n-------\n 23722\n(1 row)\n\nforeman=# select count(*) from foreman_tasks_locks;\n count\n-------\n  8621\n(1 row)\n\nforeman=# select count(*) from foreman_tasks_tasks;\n count\n-------\n  4622\n(1 row)\n\nWhat other diagnostics should I attach ?</text>, <text>Hello,\n\nIf you will face same issue again and will find the foreman-tasks service failed, then that time provide me the out-put of below commands from satellite server:\n\n# service foreman-tasks status -l\n\n# journalctl -xn\n\nAlso, provide me the recent SOS report of satellite server.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Example of two current fails.  Will run a sosreport and upload now.\n\n[root@gbl15217 ~]# date;hammer ping;service foreman-tasks status -l;journalctl -xn;date\nMon 27 Nov 13:11:09 GMT 2017\ncandlepin:\n    Status:          ok\n    Server Response: Duration: 15ms\ncandlepin_auth:\n    Status:          ok\n    Server Response: Duration: 98ms\npulp:\n    Status:          ok\n    Server Response: Duration: 27ms\nforeman_tasks:\n    Status:          FAIL\n    Server Response:\n\nRedirecting to /bin/systemctl status  -l foreman-tasks.service\n\u25cf foreman-tasks.service - Foreman jobs daemon\n   Loaded: loaded (/usr/lib/systemd/system/foreman-tasks.service; enabled; vendor preset: disabled)\n   Active: active (running) since Sun 2017-11-26 12:45:20 GMT; 24h ago\n     Docs: https://github.com/theforeman/foreman-tasks\n  Process: 28943 ExecStop=/usr/bin/foreman-tasks stop (code=exited, status=0/SUCCESS)\n  Process: 30666 ExecStart=/usr/bin/foreman-tasks start (code=exited, status=0/SUCCESS)\n   CGroup: /system.slice/foreman-tasks.service\n           \u251c\u250030714 dynflow_executor\n           \u2514\u250030716 dynflow_executor_monitor\n\nNov 26 12:45:17 gbl15217.systems.uk.hsbc systemd[1]: Starting Foreman jobs daemon...\nNov 26 12:45:19 gbl15217.systems.uk.hsbc foreman-tasks[30666]: Dynflow Executor: start in progress\nNov 26 12:45:19 gbl15217.systems.uk.hsbc foreman-tasks[30666]: dynflow_executor: process with pid 30714 started.\nNov 26 12:45:20 gbl15217.systems.uk.hsbc systemd[1]: Started Foreman jobs daemon.\n-- Logs begin at Wed 2017-11-22 18:24:03 GMT, end at Mon 2017-11-27 13:11:10 GMT. --\nNov 27 13:10:42 gbl15217.systems.uk.hsbc pulp[19477]: py.warnings:WARNING: (19477-88928)   upsert=upsert, **write_concern)\nNov 27 13:10:42 gbl15217.systems.uk.hsbc pulp[19477]: py.warnings:WARNING: (19477-88928)\nNov 27 13:10:42 gbl15217.systems.uk.hsbc pulp[30224]: celery.worker.job:INFO: Task pulp.server.managers.consumer.applicability.regenerate_applicability_for_consumers[e21786fb-7a92-4d63-8bc9-782a3297c761] succeeded in 0.0403155\nNov 27 13:10:42 gbl15217.systems.uk.hsbc pulp[30224]: celery.worker.job:INFO: Task pulp.server.async.tasks._release_resource[c01b2249-3534-4a4b-8a0a-4a29a56f6d09] succeeded in 0.00422020797851s: None\nNov 27 13:10:46 gbl15217.systems.uk.hsbc pulp[30304]: kombu.transport.qpid:INFO: Connected to qpid with SASL mechanism ANONYMOUS\nNov 27 13:10:51 gbl15217.systems.uk.hsbc qpidd[29651]: 2017-11-27 13:10:51 [Protocol] error Error on attach: Node not found: pulp.agent.5981d93b-ae4f-4835-991a-b6f9cf6a1b82\nNov 27 13:10:51 gbl15217.systems.uk.hsbc qpidd[29651]: 2017-11-27 13:10:51 [Protocol] error Error on attach: Node not found: pulp.agent.5981d93b-ae4f-4835-991a-b6f9cf6a1b82\nNov 27 13:11:00 gbl15217.systems.uk.hsbc pulp[30306]: kombu.transport.qpid:INFO: Connected to qpid with SASL mechanism ANONYMOUS\nNov 27 13:11:06 gbl15217.systems.uk.hsbc pulp[30304]: kombu.transport.qpid:INFO: Connected to qpid with SASL mechanism ANONYMOUS\nNov 27 13:11:10 gbl15217.systems.uk.hsbc pulp[30304]: kombu.transport.qpid:INFO: Connected to qpid with SASL mechanism ANONYMOUS\nMon 27 Nov 13:11:14 GMT 2017\n[root@gbl15217 ~]#\n\n[root@gbl15217 ~]# date;hammer ping;service foreman-tasks status -l;journalctl -xn;date\nMon 27 Nov 13:12:47 GMT 2017\ncandlepin:\n    Status:          ok\n    Server Response: Duration: 14ms\ncandlepin_auth:\n    Status:          ok\n    Server Response: Duration: 15ms\npulp:\n    Status:          ok\n    Server Response: Duration: 24ms\nforeman_tasks:\n    Status:          FAIL\n    Server Response:\n\nRedirecting to /bin/systemctl status  -l foreman-tasks.service\n\u25cf foreman-tasks.service - Foreman jobs daemon\n   Loaded: loaded (/usr/lib/systemd/system/foreman-tasks.service; enabled; vendor preset: disabled)\n   Active: active (running) since Sun 2017-11-26 12:45:20 GMT; 24h ago\n     Docs: https://github.com/theforeman/foreman-tasks\n  Process: 28943 ExecStop=/usr/bin/foreman-tasks stop (code=exited, status=0/SUCCESS)\n  Process: 30666 ExecStart=/usr/bin/foreman-tasks start (code=exited, status=0/SUCCESS)\n   CGroup: /system.slice/foreman-tasks.service\n           \u251c\u250030714 dynflow_executor\n           \u2514\u250030716 dynflow_executor_monitor\n\nNov 26 12:45:17 gbl15217.systems.uk.hsbc systemd[1]: Starting Foreman jobs daemon...\nNov 26 12:45:19 gbl15217.systems.uk.hsbc foreman-tasks[30666]: Dynflow Executor: start in progress\nNov 26 12:45:19 gbl15217.systems.uk.hsbc foreman-tasks[30666]: dynflow_executor: process with pid 30714 started.\nNov 26 12:45:20 gbl15217.systems.uk.hsbc systemd[1]: Started Foreman jobs daemon.\n-- Logs begin at Wed 2017-11-22 18:24:03 GMT, end at Mon 2017-11-27 13:12:48 GMT. --\nNov 27 13:12:47 gbl15217.systems.uk.hsbc pulp[19565]: py.warnings:WARNING: (19565-88928)\nNov 27 13:12:47 gbl15217.systems.uk.hsbc pulp[19565]: py.warnings:WARNING: (19565-88928) /usr/lib/python2.7/site-packages/mongoengine/queryset/base.py:461: DeprecationWarning: update is deprecated. Use replace_one, update_one\nNov 27 13:12:47 gbl15217.systems.uk.hsbc pulp[19565]: py.warnings:WARNING: (19565-88928)   upsert=upsert, **write_concern)\nNov 27 13:12:47 gbl15217.systems.uk.hsbc pulp[19565]: py.warnings:WARNING: (19565-88928)\nNov 27 13:12:48 gbl15217.systems.uk.hsbc pulp[19565]: py.warnings:WARNING: (19565-88928) /usr/lib/python2.7/site-packages/mongoengine/document.py:367: DeprecationWarning: update is deprecated. Use replace_one, update_one or up\nNov 27 13:12:48 gbl15217.systems.uk.hsbc pulp[19565]: py.warnings:WARNING: (19565-88928)   upsert=upsert, **write_concern)\nNov 27 13:12:48 gbl15217.systems.uk.hsbc pulp[19565]: py.warnings:WARNING: (19565-88928)\nNov 27 13:12:48 gbl15217.systems.uk.hsbc pulp[30224]: celery.worker.job:INFO: Task pulp.server.managers.consumer.applicability.regenerate_applicability_for_consumers[a8228538-a65d-48e9-9617-d23eef42b08f] succeeded in 0.2117977\nNov 27 13:12:48 gbl15217.systems.uk.hsbc pulp[30224]: celery.worker.job:INFO: Task pulp.server.async.tasks._release_resource[8a28f565-2a44-485d-a4f2-9082f2682d93] succeeded in 0.00419944198802s: None\nNov 27 13:12:48 gbl15217.systems.uk.hsbc pulp[30306]: kombu.transport.qpid:INFO: Connected to qpid with SASL mechanism ANONYMOUS\nMon 27 Nov 13:12:56 GMT 2017\n[root@gbl15217 ~]#\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 27 November 2017 12:48\nTo: Koon TANG &lt;koon.tang@hsbc.com&gt;; sokeeffe@redhat.com; Stephen G KAY &lt;stephenkay@hsbc.com&gt;\nSubject: (WoC) (SEV 3) Case #01974013 (Satellite outage due to /var full) ref:_00DA0HxWH._500A0Z2w2G:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01974013\nCase Title       : Satellite outage due to /var full\nCase Number      : 01974013\nCase Open Date   : 2017-11-15 09:46:32\nSeverity         : 3 (Normal)\nProblem Type     : Defect / Bug\n\nMost recent comment: On 2017-11-27 12:46:41, Waghmare, Prashant commented:\n"Hello,\n\nIf you will face same issue again and will find the foreman-tasks service failed, then that time provide me the out-put of below commands from satellite server:\n\n# service foreman-tasks status -l\n\n# journalctl -xn\n\nAlso, provide me the recent SOS report of satellite server.\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\nhttps://access.redhat.com/support/cases/#/case/01974013?commentId=a0aA000000L4tI4IAJ\n\n---------------------------------------\n\nA comment has been added to the case.\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z2w2G:ref\n\n\n\n\n \n\n\n-----------------------------------------\nSAVE PAPER - THINK BEFORE YOU PRINT!\n\nThis E-mail is confidential.  \n\nIt may also be legally privileged. If you are not the addressee you may not copy,\nforward, disclose or use any part of it. If you have received this message in error,\nplease delete it and all copies from your system and notify the sender immediately by\nreturn E-mail.\n\nInternet communications cannot be guaranteed to be timely secure, error or virus-free.\nThe sender does not accept liability for any errors or omissions.</text>, <text>Hello,\n\nCould you run below command on satellite server:\n\n# su - postgres -c "psql -d foreman -c 'select label,count(label),state from foreman_tasks_tasks where state &lt;&gt; '\\''stopped'\\'' group by label,state;'"\n\nIf you will find tasks as below :\n\n                   label                    | count |   state   \n---------------------------------------------+-------+-----------\n Actions::Candlepin::ListenOnCandlepinEvents |     1 | running\n Actions::Insights::EmailPoller              |     1 | scheduled\n Actions::Katello::EventQueue::Monitor       |     1 | running\n(3 rows)\n\nMeans foreman-tasks service is running correctly but hammer ping is showing result wrongly.\n\n# rpm -qf /usr/bin/foreman-tasks\ntfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch\n\nMake sure "rhel-7-server-satellite-6.2-rpms" repository is enabled on satellite server via # yum repolist  because its the repository which provides that package.\n\nAs per provided logs it seems that hammer ping is showing the false result as foreman-tasks service is active:\n\nThen reinstall the package tfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch\n\n# yum reinstall tfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch\n\nThen you could re-run the satellite-installer and see if helps to resolve the issue.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Here's the output generated by psql.  Plus confirmation of the tfm-rubygem-foreman-tasks version installed and version presented via satellite repo.  The same.\n\nI can raise a CR to reinstall the RPM during a service outage window.  I need to know what this is actually achieving though - is there evidence the installed RPM is broken in some way ?  And what are the consequences of rerunning satellite-installer afterwards ?  Given the Satellite instability we've seen throughout this year, we're very wary of making changes here, so need to understand this completely.\n\nMany thanks,\n\nSteve.\n\n[root@gbl15217 ~]# date\nMon 27 Nov 14:40:40 GMT 2017\n [root@gbl15217 ~]# su - postgres -c "psql -d foreman -c 'select label,count(label),state from foreman_tasks_tasks where state &lt;&gt; '\\''stopped'\\'' group by label,state;'"\n/etc/profile.d/hsbc_profile.sh: line 21: TMOUT: readonly variable\n                    label                    | count |   state\n---------------------------------------------+-------+-----------\n Actions::Insights::EmailPoller              |     1 | scheduled\n Actions::Candlepin::ListenOnCandlepinEvents |     1 | running\n Actions::Katello::EventQueue::Monitor       |     1 | running\n(3 rows)\n\n[root@gbl15217 ~]#\n\n[root@gbl15217 ~]# rpm -qf /usr/bin/foreman-tasks\ntfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch\n[root@gbl15217 ~]# yum info tfm-rubygem-foreman-tasks\nLoaded plugins: enabled_repos_upload, package_upload, product-id, search-disabled-repos, subscription-manager\nInstalled Packages\nName        : tfm-rubygem-foreman-tasks\nArch        : noarch\nVersion     : 0.7.14.14\nRelease     : 1.el7sat\nSize        : 2.0 M\nRepo        : installed\nFrom repo   : rhel-7-server-satellite-6.2-rpms\nSummary     : Tasks support for Foreman with Dynflow integration\nURL         : http://github.com/theforeman/foreman-tasks\nLicence     : GPLv3\nDescription : The goal of this plugin is to unify the way of showing task statuses across\n            : the Foreman instance.  It defines Task model for keeping the information\n            : about the tasks and Lock for assigning the tasks to resources. The locking\n            : allows dealing with preventing multiple colliding tasks to be run on the\n            : same resource. It also optionally provides Dynflow infrastructure for using\n            : it for managing the tasks.\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 27 November 2017 13:37\nTo: Koon TANG &lt;koon.tang@hsbc.com&gt;; sokeeffe@redhat.com; Stephen G KAY &lt;stephenkay@hsbc.com&gt;\nSubject: (WoC) (SEV 3) Case #01974013 (Satellite outage due to /var full) ref:_00DA0HxWH._500A0Z2w2G:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01974013\nCase Title       : Satellite outage due to /var full\nCase Number      : 01974013\nCase Open Date   : 2017-11-15 09:46:32\nSeverity         : 3 (Normal)\nProblem Type     : Defect / Bug\n\nMost recent comment: On 2017-11-27 13:36:45, Waghmare, Prashant commented:\n"Hello,\n\nCould you run below command on satellite server:\n\n# su - postgres -c "psql -d foreman -c 'select label,count(label),state from foreman_tasks_tasks where state &lt;&gt; '\\''stopped'\\'' group by label,state;'"\n\nIf you will find tasks as below :\n\n                   label                    | count |   state   \n---------------------------------------------+-------+-----------\n Actions::Candlepin::ListenOnCandlepinEvents |     1 | running\n Actions::Insights::EmailPoller              |     1 | scheduled\n Actions::Katello::EventQueue::Monitor       |     1 | running\n(3 rows)\n\nMeans foreman-tasks service is running correctly but hammer ping is showing result wrongly.\n\n# rpm -qf /usr/bin/foreman-tasks\ntfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch\n\nMake sure "rhel-7-server-satellite-6.2-rpms" repository is enabled on satellite server via # yum repolist  because its the repository which provides that package.\n\nAs per provided logs it seems that hammer ping is showing the false result as foreman-tasks service is active:\n\nThen reinstall the package tfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch\n\n# yum reinstall tfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch\n\nThen you could re-run the satellite-installer and see if helps to resolve the issue.\n\nThank you.\n\nWith regards,\nPrashant Waghmare."\n\n---------------------------------------\n\nThank you for your latest interaction with Red Hat Global Support Services. We are currently working to resolve your case.\n\nYour case has transitioned to "Waiting On Customer" status. This means that the Red Hat associate working on your case needs information or action from you to proceed. To help us resolve your case as quickly as possible, please update your case online on the Customer Portal at https://access.redhat.com.\n\nOnce you update the case to provide the requested information, we can continue working to resolve your issue.\n\nIf you wish to contact Red Hat, visit the Customer Portal at https://access.redhat.com to find phone and web contact information relevant to your region and support contract.\n\n\nThank you,\n\nRed Hat Global Support Services\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z2w2G:ref\n\n\n\n\n \n\n\n-----------------------------------------\nSAVE PAPER - THINK BEFORE YOU PRINT!\n\nThis E-mail is confidential.  \n\nIt may also be legally privileged. If you are not the addressee you may not copy,\nforward, disclose or use any part of it. If you have received this message in error,\nplease delete it and all copies from your system and notify the sender immediately by\nreturn E-mail.\n\nInternet communications cannot be guaranteed to be timely secure, error or virus-free.\nThe sender does not accept liability for any errors or omissions.</text>, <text>Note that when the problem occurs, we see foreman reporting 'some executors are not responding', details below at 15:19:11.\n\n[root@gbl15217 ~]# date;time hammer ping\nMon 27 Nov 15:19:07 GMT 2017\ncandlepin:\n    Status:          ok\n    Server Response: Duration: 13ms\ncandlepin_auth:\n    Status:          ok\n    Server Response: Duration: 13ms\npulp:\n    Status:          ok\n    Server Response: Duration: 25ms\nforeman_tasks:\n    Status:          FAIL\n    Server Response:\n\n\nreal    0m3.456s\nuser    0m1.197s\nsys     0m0.155s\n[root@gbl15217 ~]#\n\n/var/log/foreman/production.log:\n\n2017-11-27 15:19:06 b678dd4c [app] [I]   Parameters: {"id"=&gt;"1969cb24-3075-412e-afae-c67049410796"}\n2017-11-27 15:19:06 b678dd4c [app] [I] Completed 200 OK in 87ms (Views: 2.3ms | ActiveRecord: 1.8ms)\n2017-11-27 15:19:08 e8ca673c [app] [I] Started GET "/rhsm/" for 130.160.226.122 at 2017-11-27 15:19:08 +0000\n2017-11-27 15:19:08 e8ca673c [app] [I] Processing by Katello::Api::V2::RootController#rhsm_resource_list as JSON\n2017-11-27 15:19:08 e8ca673c [app] [I]   Parameters: {"root"=&gt;{}}\n2017-11-27 15:19:08 e8ca673c [app] [I]   Rendered /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/views/katello/api/v2/root/resource_list.json.rabl within katello/api/v2/layouts/collection (1.4ms)\n2017-11-27 15:19:08 e8ca673c [app] [I] Completed 200 OK in 15ms (Views: 2.2ms | ActiveRecord: 0.0ms)\n2017-11-27 15:19:08 69f4d110 [app] [I] Started GET "/rhsm/" for 130.199.80.128 at 2017-11-27 15:19:08 +0000\n2017-11-27 15:19:08 69f4d110 [app] [I] Processing by Katello::Api::V2::RootController#rhsm_resource_list as JSON\n2017-11-27 15:19:08 69f4d110 [app] [I]   Parameters: {"root"=&gt;{}}\n2017-11-27 15:19:08 69f4d110 [app] [I]   Rendered /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/views/katello/api/v2/root/resource_list.json.rabl within katello/api/v2/layouts/collection (1.5ms)\n2017-11-27 15:19:08 69f4d110 [app] [I] Completed 200 OK in 15ms (Views: 2.3ms | ActiveRecord: 0.0ms)\n2017-11-27 15:19:08 3df50e82 [app] [I] Started GET "/rhsm/consumers/44655bc5-f492-4d82-9e08-c0361c162909/certificates/serials" for 130.160.226.122 at 2017-11-27 15:19:08 +0000\n2017-11-27 15:19:08 3df50e82 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#serials as JSON\n2017-11-27 15:19:08 3df50e82 [app] [I]   Parameters: {"id"=&gt;"44655bc5-f492-4d82-9e08-c0361c162909"}\n2017-11-27 15:19:08 3614029e [app] [I] Started GET "/rhsm/consumers/385bb47e-a986-4501-95f0-06c284b04604/owner" for 128.161.55.33 at 2017-11-27 15:19:08 +0000\n2017-11-27 15:19:08 3614029e [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#get as JSON\n2017-11-27 15:19:08 3614029e [app] [I]   Parameters: {"id"=&gt;"385bb47e-a986-4501-95f0-06c284b04604"}\n2017-11-27 15:19:08 3df50e82 [app] [I] Completed 200 OK in 80ms (Views: 0.3ms | ActiveRecord: 3.9ms)\n2017-11-27 15:19:09 3614029e [app] [I] Completed 200 OK in 47ms (Views: 0.2ms | ActiveRecord: 0.4ms)\n2017-11-27 15:19:09 dfc0d7a7 [app] [I] Started GET "/rhsm/" for 128.161.55.33 at 2017-11-27 15:19:09 +0000\n2017-11-27 15:19:09 dfc0d7a7 [app] [I] Processing by Katello::Api::V2::RootController#rhsm_resource_list as JSON\n2017-11-27 15:19:09 dfc0d7a7 [app] [I]   Parameters: {"root"=&gt;{}}\n2017-11-27 15:19:09 dfc0d7a7 [app] [I]   Rendered /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/views/katello/api/v2/root/resource_list.json.rabl within katello/api/v2/layouts/collection (1.4ms)\n2017-11-27 15:19:09 dfc0d7a7 [app] [I] Completed 200 OK in 14ms (Views: 2.2ms | ActiveRecord: 0.0ms)\n2017-11-27 15:19:09 1b3f8197 [app] [I] Started GET "/rhsm/consumers/385bb47e-a986-4501-95f0-06c284b04604" for 128.161.55.33 at 2017-11-27 15:19:09 +0000\n2017-11-27 15:19:09 1b3f8197 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2017-11-27 15:19:09 1b3f8197 [app] [I]   Parameters: {"id"=&gt;"385bb47e-a986-4501-95f0-06c284b04604"}\n2017-11-27 15:19:09 43e6ab17 [app] [I] Started GET "/rhsm/consumers/e57e311f-8372-423e-9531-d30e1c27160e/certificates/serials" for 130.199.80.128 at 2017-11-27 15:19:09 +0000\n2017-11-27 15:19:09 43e6ab17 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#serials as JSON\n2017-11-27 15:19:09 43e6ab17 [app] [I]   Parameters: {"id"=&gt;"e57e311f-8372-423e-9531-d30e1c27160e"}\n2017-11-27 15:19:09 1b3f8197 [app] [I] Completed 200 OK in 89ms (Views: 2.7ms | ActiveRecord: 2.0ms)\n2017-11-27 15:19:09 43e6ab17 [app] [I] Completed 200 OK in 78ms (Views: 0.3ms | ActiveRecord: 3.4ms)\n2017-11-27 15:19:09 1ced938c [app] [I] Started GET "/katello/api/ping" for 128.88.46.160 at 2017-11-27 15:19:09 +0000\n2017-11-27 15:19:09 d6a75d17 [app] [I] Started GET "/rhsm/status" for 130.160.226.122 at 2017-11-27 15:19:09 +0000\n2017-11-27 15:19:09 d6a75d17 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#server_status as JSON\n2017-11-27 15:19:09 1ced938c [app] [I] Processing by Katello::Api::V2::PingController#index as JSON\n2017-11-27 15:19:09 1ced938c [app] [I]   Parameters: {"api_version"=&gt;"v2", "ping"=&gt;{}}\n2017-11-27 15:19:09 d6a75d17 [app] [I] Completed 200 OK in 73ms (Views: 0.4ms | ActiveRecord: 0.0ms)\n2017-11-27 15:19:09 5b27f2d7 [app] [I] Started GET "/rhsm/status" for 130.199.80.128 at 2017-11-27 15:19:09 +0000\n2017-11-27 15:19:09 5b27f2d7 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#server_status as JSON\n2017-11-27 15:19:09 5b27f2d7 [app] [I] Completed 200 OK in 71ms (Views: 0.3ms | ActiveRecord: 0.0ms)\n2017-11-27 15:19:09 2f10a07e [app] [I] Started GET "/rhsm/consumers/959a3b6e-b734-436f-9430-11a6c59e4254" for 130.160.226.122 at 2017-11-27 15:19:09 +0000\n2017-11-27 15:19:09 2f10a07e [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2017-11-27 15:19:09 2f10a07e [app] [I]   Parameters: {"id"=&gt;"959a3b6e-b734-436f-9430-11a6c59e4254"}\n2017-11-27 15:19:09 5298e725 [app] [I] Started GET "/rhsm/consumers/44655bc5-f492-4d82-9e08-c0361c162909" for 130.160.226.122 at 2017-11-27 15:19:09 +0000\n2017-11-27 15:19:09 5298e725 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2017-11-27 15:19:09 5298e725 [app] [I]   Parameters: {"id"=&gt;"44655bc5-f492-4d82-9e08-c0361c162909"}\n2017-11-27 15:19:09 2f10a07e [app] [I] Completed 200 OK in 88ms (Views: 2.9ms | ActiveRecord: 2.0ms)\n2017-11-27 15:19:09 5298e725 [app] [I] Completed 200 OK in 84ms (Views: 2.8ms | ActiveRecord: 1.8ms)\n2017-11-27 15:19:09 947368e6 [app] [I] Started GET "/rhsm/consumers/e57e311f-8372-423e-9531-d30e1c27160e" for 130.199.80.128 at 2017-11-27 15:19:09 +0000\n2017-11-27 15:19:09 947368e6 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2017-11-27 15:19:09 947368e6 [app] [I]   Parameters: {"id"=&gt;"e57e311f-8372-423e-9531-d30e1c27160e"}\n2017-11-27 15:19:09 947368e6 [app] [I] Completed 200 OK in 87ms (Views: 3.2ms | ActiveRecord: 1.9ms)\n2017-11-27 15:19:10 659e5468 [app] [I] Started GET "/rhsm/consumers/dbda2139-d300-413e-8575-9b3e1e7397a8/certificates/serials" for 130.199.80.128 at 2017-11-27 15:19:10 +0000\n2017-11-27 15:19:10 659e5468 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#serials as JSON\n2017-11-27 15:19:10 659e5468 [app] [I]   Parameters: {"id"=&gt;"dbda2139-d300-413e-8575-9b3e1e7397a8"}\n2017-11-27 15:19:10 659e5468 [app] [I] Completed 200 OK in 83ms (Views: 0.3ms | ActiveRecord: 3.9ms)\n2017-11-27 15:19:10 9d584f44 [app] [I] Started GET "/rhsm/consumers/4875b353-a7da-4002-8da0-667b87f65c82" for 130.160.226.122 at 2017-11-27 15:19:10 +0000\n2017-11-27 15:19:10 9d584f44 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2017-11-27 15:19:10 9d584f44 [app] [I]   Parameters: {"id"=&gt;"4875b353-a7da-4002-8da0-667b87f65c82"}\n2017-11-27 15:19:10 9d584f44 [app] [I] Completed 200 OK in 86ms (Views: 2.8ms | ActiveRecord: 1.9ms)\n2017-11-27 15:19:10 b2bd680c [app] [I] Started GET "/rhsm/consumers/af3636ce-35e5-48f9-a48a-4d769029ae7b/owner" for 128.161.55.33 at 2017-11-27 15:19:10 +0000\n2017-11-27 15:19:10 b2bd680c [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#get as JSON\n2017-11-27 15:19:10 b2bd680c [app] [I]   Parameters: {"id"=&gt;"af3636ce-35e5-48f9-a48a-4d769029ae7b"}\n2017-11-27 15:19:10 b2bd680c [app] [I] Completed 200 OK in 46ms (Views: 0.2ms | ActiveRecord: 0.4ms)\n2017-11-27 15:19:10 43562045 [app] [I] Started GET "/rhsm/" for 128.161.55.33 at 2017-11-27 15:19:10 +0000\n2017-11-27 15:19:10 43562045 [app] [I] Processing by Katello::Api::V2::RootController#rhsm_resource_list as JSON\n2017-11-27 15:19:10 43562045 [app] [I]   Parameters: {"root"=&gt;{}}\n2017-11-27 15:19:10 43562045 [app] [I]   Rendered /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/views/katello/api/v2/root/resource_list.json.rabl within katello/api/v2/layouts/collection (1.4ms)\n2017-11-27 15:19:10 43562045 [app] [I] Completed 200 OK in 14ms (Views: 2.2ms | ActiveRecord: 0.0ms)\n2017-11-27 15:19:10 1e16f2c3 [app] [I] Started GET "/rhsm/consumers/af3636ce-35e5-48f9-a48a-4d769029ae7b" for 128.161.55.33 at 2017-11-27 15:19:10 +0000\n2017-11-27 15:19:10 1e16f2c3 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2017-11-27 15:19:10 1e16f2c3 [app] [I]   Parameters: {"id"=&gt;"af3636ce-35e5-48f9-a48a-4d769029ae7b"}\n2017-11-27 15:19:10 1e16f2c3 [app] [I] Completed 200 OK in 86ms (Views: 2.6ms | ActiveRecord: 1.9ms)\n2017-11-27 15:19:10 1a393559 [app] [I] Started GET "/rhsm/consumers/6bb53388-4171-4c94-80ec-2f6b544f4573/owner" for 128.161.55.33 at 2017-11-27 15:19:10 +0000\n2017-11-27 15:19:10 1a393559 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#get as JSON\n2017-11-27 15:19:10 1a393559 [app] [I]   Parameters: {"id"=&gt;"6bb53388-4171-4c94-80ec-2f6b544f4573"}\n2017-11-27 15:19:10 1a393559 [app] [I] Completed 200 OK in 46ms (Views: 0.2ms | ActiveRecord: 0.3ms)\n2017-11-27 15:19:10 6693bbfc [app] [I] Started GET "/rhsm/" for 128.161.55.33 at 2017-11-27 15:19:10 +0000\n2017-11-27 15:19:10 6693bbfc [app] [I] Processing by Katello::Api::V2::RootController#rhsm_resource_list as JSON\n2017-11-27 15:19:10 6693bbfc [app] [I]   Parameters: {"root"=&gt;{}}\n2017-11-27 15:19:10 3acfe6a1 [app] [I] Started GET "/rhsm/consumers/4875b353-a7da-4002-8da0-667b87f65c82/compliance" for 130.160.226.122 at 2017-11-27 15:19:10 +0000\n2017-11-27 15:19:10 6693bbfc [app] [I]   Rendered /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/views/katello/api/v2/root/resource_list.json.rabl within katello/api/v2/layouts/collection (1.4ms)\n2017-11-27 15:19:10 6693bbfc [app] [I] Completed 200 OK in 14ms (Views: 2.1ms | ActiveRecord: 0.0ms)\n2017-11-27 15:19:10 3acfe6a1 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#get as JSON\n2017-11-27 15:19:10 3acfe6a1 [app] [I]   Parameters: {"id"=&gt;"4875b353-a7da-4002-8da0-667b87f65c82"}\n2017-11-27 15:19:10 ace13908 [app] [I] Started GET "/rhsm/consumers/6bb53388-4171-4c94-80ec-2f6b544f4573" for 128.161.55.33 at 2017-11-27 15:19:10 +0000\n2017-11-27 15:19:10 ace13908 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#consumer_show as JSON\n2017-11-27 15:19:10 ace13908 [app] [I]   Parameters: {"id"=&gt;"6bb53388-4171-4c94-80ec-2f6b544f4573"}\n2017-11-27 15:19:10 3acfe6a1 [app] [I] Completed 200 OK in 57ms (Views: 0.2ms | ActiveRecord: 0.3ms)\n2017-11-27 15:19:10 ace13908 [app] [I] Completed 200 OK in 89ms (Views: 2.6ms | ActiveRecord: 2.1ms)\n2017-11-27 15:19:11 240df9fa [app] [I] Started GET "/rhsm/consumers/4875b353-a7da-4002-8da0-667b87f65c82/certificates/serials" for 130.160.226.122 at 2017-11-27 15:19:11 +0000\n2017-11-27 15:19:11 4607bf61 [app] [I] Started GET "/rhsm/consumers/959a3b6e-b734-436f-9430-11a6c59e4254/compliance" for 130.160.226.122 at 2017-11-27 15:19:11 +0000\n2017-11-27 15:19:11 240df9fa [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#serials as JSON\n2017-11-27 15:19:11 240df9fa [app] [I]   Parameters: {"id"=&gt;"4875b353-a7da-4002-8da0-667b87f65c82"}\n2017-11-27 15:19:11 4607bf61 [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#get as JSON\n2017-11-27 15:19:11 4607bf61 [app] [I]   Parameters: {"id"=&gt;"959a3b6e-b734-436f-9430-11a6c59e4254"}\n2017-11-27 15:19:11 4607bf61 [app] [I] Completed 200 OK in 59ms (Views: 0.2ms | ActiveRecord: 0.3ms)\n2017-11-27 15:19:11 240df9fa [app] [I] Completed 200 OK in 80ms (Views: 0.3ms | ActiveRecord: 6.2ms)\n2017-11-27 15:19:11 1ced938c [app] [W] some executors are not responding, check /foreman_tasks/dynflow/status\n | /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/models/katello/ping.rb:70:in `block in ping'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/models/katello/ping.rb:85:in `exception_watch'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/models/katello/ping.rb:59:in `ping'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/controllers/katello/api/v2/ping_controller.rb:13:in `index'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_controller/metal/implicit_render.rb:4:in `send_action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/abstract_controller/base.rb:189:in `process_action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_controller/metal/rendering.rb:10:in `process_action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/abstract_controller/callbacks.rb:20:in `block in process_action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:113:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:113:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:149:in `block in halting_and_conditional'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:299:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:299:in `block (2 levels) in halting'\n | /usr/share/foreman/app/controllers/api/v2/base_controller.rb:152:in `disable_json_root'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:424:in `block in make_lambda'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:298:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:298:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:229:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:229:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:299:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:299:in `block (2 levels) in halting'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/rails-observers-0.1.2/lib/rails/observers/action_controller/caching/sweeping.rb:73:in `around'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:447:in `public_send'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:447:in `block in make_lambda'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:298:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:298:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:299:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:299:in `block (2 levels) in halting'\n | /usr/share/foreman/app/controllers/concerns/application_shared.rb:13:in `set_timezone'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:424:in `block in make_lambda'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:298:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:298:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:149:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:149:in `block in halting_and_conditional'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:229:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:229:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:299:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:299:in `block (2 levels) in halting'\n | /usr/share/foreman/app/models/concerns/foreman/thread_session.rb:32:in `clear_thread'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:424:in `block in make_lambda'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:298:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:298:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:229:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:229:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:166:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:299:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:299:in `block (2 levels) in halting'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/rails-observers-0.1.2/lib/rails/observers/action_controller/caching/sweeping.rb:73:in `around'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:447:in `public_send'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:447:in `block in make_lambda'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:298:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:298:in `block in halting'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:149:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:149:in `block in halting_and_conditional'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:86:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:86:in `run_callbacks'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/abstract_controller/callbacks.rb:19:in `process_action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_controller/metal/rescue.rb:29:in `process_action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_controller/metal/instrumentation.rb:31:in `block in process_action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/notifications.rb:159:in `block in instrument'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/notifications/instrumenter.rb:20:in `instrument'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/notifications.rb:159:in `instrument'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_controller/metal/instrumentation.rb:30:in `process_action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_controller/metal/params_wrapper.rb:250:in `process_action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activerecord-4.1.5/lib/active_record/railties/controller_runtime.rb:18:in `process_action'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/controllers/katello/concerns/api/api_controller.rb:39:in `process_action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/abstract_controller/base.rb:136:in `process'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionview-4.1.5/lib/action_view/rendering.rb:30:in `process'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_controller/metal.rb:196:in `dispatch'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_controller/metal/rack_delegation.rb:13:in `dispatch'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_controller/metal.rb:232:in `block in action'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/routing/route_set.rb:80:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/routing/route_set.rb:80:in `dispatch'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/routing/route_set.rb:48:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/routing/mapper.rb:45:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/journey/router.rb:71:in `block in call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/journey/router.rb:59:in `each'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/journey/router.rb:59:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/routing/route_set.rb:676:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/railties-4.1.5/lib/rails/engine.rb:514:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/railties-4.1.5/lib/rails/railtie.rb:194:in `public_send'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/railties-4.1.5/lib/rails/railtie.rb:194:in `method_missing'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/journey/router.rb:71:in `block in call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/journey/router.rb:59:in `each'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/journey/router.rb:59:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/routing/route_set.rb:676:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/apipie-rails-0.3.6/lib/apipie/static_dispatcher.rb:65:in `call'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/apipie-rails-0.3.6/lib/apipie/extractor/recorder.rb:132:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/apipie-rails-0.3.6/lib/apipie/middleware/checksum_in_headers.rb:27:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/etag.rb:23:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/conditionalget.rb:25:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/head.rb:11:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/params_parser.rb:27:in `call'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/lib/katello/params_parser_wrapper.rb:12:in `call'\n | /usr/share/foreman/lib/middleware/catch_json_parse_errors.rb:9:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/flash.rb:254:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/session/abstract/id.rb:225:in `context'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/session/abstract/id.rb:220:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/cookies.rb:560:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activerecord-4.1.5/lib/active_record/query_cache.rb:36:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activerecord-4.1.5/lib/active_record/connection_adapters/abstract/connection_pool.rb:621:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/callbacks.rb:29:in `block in call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/callbacks.rb:82:in `run_callbacks'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/callbacks.rb:27:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/remote_ip.rb:76:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/debug_exceptions.rb:17:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/show_exceptions.rb:30:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/railties-4.1.5/lib/rails/rack/logger.rb:38:in `call_app'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/railties-4.1.5/lib/rails/rack/logger.rb:22:in `call'\n | /usr/share/foreman/lib/middleware/tagged_logging.rb:18:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/request_id.rb:21:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/methodoverride.rb:21:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/runtime.rb:17:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/activesupport-4.1.5/lib/active_support/cache/strategy/local_cache_middleware.rb:26:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/actionpack-4.1.5/lib/action_dispatch/middleware/static.rb:64:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/sendfile.rb:112:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/railties-4.1.5/lib/rails/engine.rb:514:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/railties-4.1.5/lib/rails/application.rb:144:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/railties-4.1.5/lib/rails/railtie.rb:194:in `public_send'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/railties-4.1.5/lib/rails/railtie.rb:194:in `method_missing'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/builder.rb:138:in `call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/urlmap.rb:65:in `block in call'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/urlmap.rb:50:in `each'\n | /opt/rh/rh-ror41/root/usr/share/gems/gems/rack-1.5.2/lib/rack/urlmap.rb:50:in `call'\n | /usr/share/gems/gems/passenger-4.0.18/lib/phusion_passenger/rack/thread_handler_extension.rb:77:in `process_request'\n | /usr/share/gems/gems/passenger-4.0.18/lib/phusion_passenger/request_handler/thread_handler.rb:140:in `accept_and_process_next_request'\n | /usr/share/gems/gems/passenger-4.0.18/lib/phusion_passenger/request_handler/thread_handler.rb:108:in `main_loop'\n | /usr/share/gems/gems/passenger-4.0.18/lib/phusion_passenger/request_handler.rb:441:in `block (3 levels) in start_threads'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'\n | /opt/theforeman/tfm/root/usr/share/gems/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'\n2017-11-27 15:19:11 4d1844df [app] [I] Started GET "/rhsm/consumers/44655bc5-f492-4d82-9e08-c0361c162909/content_overrides" for 130.160.226.122 at 2017-11-27 15:19:11 +0000\n2017-11-27 15:19:11 1ced938c [app] [I]   Rendered /opt/theforeman/tfm/root/usr/share/gems/gems/katello-3.0.0.157/app/views/katello/api/v2/ping/show.json.rabl within katello/api/v2/layouts/resource (1.9ms)\n2017-11-27 15:19:11 1ced938c [app] [I] Completed 200 OK in 2065ms (Views: 7.5ms | ActiveRecord: 0.0ms)\n2017-11-27 15:19:11 4d1844df [app] [I] Processing by Katello::Api::Rhsm::CandlepinProxiesController#get as JSON\n2017-11-27 15:19:11 4d1844df [app] [I]   Parameters: {"id"=&gt;"44655bc5-f492-4d82-9e08-c0361c162909"}\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 27 November 2017 14:49\nTo: Koon TANG &lt;koon.tang@hsbc.com&gt;; sokeeffe@redhat.com; Stephen G KAY &lt;stephenkay@hsbc.com&gt;\nSubject: (WoRH)</text>, <text>Now at 7,663 tasks.  Task export performed and uploaded to dropbox.\n\n[root@gbl15217 ~]# foreman-rake foreman_tasks:export_tasks task_search=all\nGathering 7663 tasks.\n1/7663\n[...]\n7663/7663\ntar cvzf /backup/foreman/task-export-1511859373.tar.gz /backup/foreman/task-export20171128-16085-1xrn3uo &gt; /dev/null\ntar: Removing leading `/' from member names\nCreated /backup/foreman/task-export-1511859373.tar.gz\n[root@gbl15217 ~]# cd /backup/foreman\n[root@gbl15217 foreman]# mv task-export-1511859373.tar.gz tasks27nov-case-no-01974013\n[root@gbl15217 foreman]# lftp -v -c 'open -e "set ftps:initial-prot "";set cmd:trace true;set ftp:ssl-force true;set ftp:ssl-protect-data true;set ssl:verify-certificate true;set ftp:proxy http://uk-proxy-01.systems.uk.hsbc:80;open anonymous:ftp@dropbox.redhat.com;cd incoming;put tasks27nov-case-no-01974013'\n+ set ftp:ssl-force true\n+ set ftp:ssl-protect-data true\n+ set ssl:verify-certificate true\n+ set ftp:proxy http://uk-proxy-01.systems.uk.hsbc:80\n+ open anonymous:ftp@dropbox.redhat.com\n+ cd incoming\n+ put tasks27nov-case-no-01974013\n`tasks27nov-case-no-01974013' at 296108 (1%) 73.4K/s eta:4m [Sending data]\n`tasks27nov-case-no-01974013' at 3080700 (15%) 356.8K/s eta:45s [Sending data]\n`tasks27nov-case-no-01974013' at 7529644 (38%) 408.4K/s eta:29s [Sending data]\n`tasks27nov-case-no-01974013' at 12705724 (65%) 430.7K/s eta:16s [Sending data]\n`tasks27nov-case-no-01974013' at 19531542 (100%) 423.0K/s eta:0s [Waiting for response...]\n[root@gbl15217 foreman]#\n\n\n-bash-4.2$ psql foreman\npsql (9.2.23)\nType "help" for help.\n\nforeman=# select count(*) from dynflow_actions;\n count\n-------\n 25883\n(1 row)\n\nforeman=# select count(*) from dynflow_execution_plans;\n count\n-------\n  7680\n(1 row)\n\nforeman=# select count(*) from dynflow_steps;\n count\n-------\n 42952\n(1 row)\n\nforeman=# select count(*) from foreman_tasks_locks;\n count\n-------\n 15973\n(1 row)\n\nforeman=# select count(*) from foreman_tasks_tasks;\n count\n-------\n  7680\n(1 row)\n\nforeman=# \\q\n-bash-4.2$</text>, <text>Attaching a further file to the case.\n\n\n-----Original Message-----\nFrom: Red Hat Support [mailto:support@redhat.com] \nSent: 27 November 2017 10:00\nTo: Koon TANG &lt;koon.tang@hsbc.com&gt;; sokeeffe@redhat.com; Stephen G KAY &lt;stephenkay@hsbc.com&gt;\nSubject: (WoRH) (SEV 3) Case #01974013 (Satellite outage due to /var full) ref:_00DA0HxWH._500A0Z2w2G:ref\n\n---------------------------------------\n|         Case Information            |\n---------------------------------------\nhttps://access.redhat.com/support/cases/#/case/01974013\nCase Title       : Satellite outage due to /var full\nCase Number      : 01974013\nCase Open Date   : 2017-11-15 09:46:32\nSeverity         : 3 (Normal)\nProblem Type     : Defect / Bug\n\n---------------------------------------\n\nThe following attachments have been recently added to this case:\ntask-export-27-Nov-2017-0945.txt\n\n---------------------------------------\n\nTo ensure the best support experience possible, please note the following:\n\n* Replying to this email should result in your comments being added to the case. However, we suggest adding comments to the case directly via the Customer Portal in case the email fails.\n* When replying to this email, do not change the subject.\n* Check to make sure you are replying to case emails from the email address that is listed as the case contact.\n* Attachments cannot be added to a case via email. Attachments must be uploaded to a case directly.\n\n---------------------------------------\n\nSupporting success. Exceeding expectations.\n\nRed Hat Support on Social Media: https://access.redhat.com/social/ Red Hat Customer Portal Discussions: https://access.redhat.com/discussions/\nRed Hat Access Labs: https://access.redhat.com/labs/\n\nIf you need immediate assistance, please refer to https://access.redhat.com/support/contact/technicalSupport/\n\n---------------------------------------\nref:_00DA0HxWH._500A0Z2w2G:ref\n\n\n\n\n \n\n\n-----------------------------------------\nSAVE PAPER - THINK BEFORE YOU PRINT!\n\nThis E-mail is confidential.  \n\nIt may also be legally privileged. If you are not the addressee you may not copy,\nforward, disclose or use any part of it. If you have received this message in error,\nplease delete it and all copies from your system and notify the sender immediately by\nreturn E-mail.\n\nInternet communications cannot be guaranteed to be timely secure, error or virus-free.\nThe sender does not accept liability for any errors or omissions.</text>, <text>Hello,\n\nThank you for update. Have you followed the steps:\n\nMake sure "rhel-7-server-satellite-6.2-rpms" repository is enabled on satellite server via # yum repolist  because its the repository which provides that package.\n\nThen reinstall the package tfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch\n\n# yum reinstall tfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch\n\nThen you could re-run the satellite-installer and see if helps to resolve the issue.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Hi Prashant,\n\nThat step has not been followed yet.  I need to schedule downtime, change request to action it.  Earliest opportunity to action that will be Sunday 3rd December.\n\nI am unclear what it will achieve though, could you help me understand what this will be fixing ?  That package version is already installed and 'rpm verify' confirms the package is ok.\n\n[root@gbl15217 ~]# rpm -V tfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch\nS.5....T.    /opt/theforeman/tfm/root/usr/share/gems/gems/foreman-tasks-0.7.14.14/lib/foreman_tasks/tasks/export_tasks.rake\n[root@gbl15217 ~]# ls /opt/theforeman/tfm/root/usr/share/gems/gems/foreman-tasks-0.7.14.14/lib/foreman_tasks/tasks/export_tasks.rake*\n/opt/theforeman/tfm/root/usr/share/gems/gems/foreman-tasks-0.7.14.14/lib/foreman_tasks/tasks/export_tasks.rake\n/opt/theforeman/tfm/root/usr/share/gems/gems/foreman-tasks-0.7.14.14/lib/foreman_tasks/tasks/export_tasks.rake.orig\n[root@gbl15217 ~]# diff /opt/theforeman/tfm/root/usr/share/gems/gems/foreman-tasks-0.7.14.14/lib/foreman_tasks/tasks/export_tasks.rake.orig /opt/theforeman/tfm/root/usr/share/gems/gems/foreman-tasks-0.7.14.14/lib/foreman_tasks/tasks/export_tasks.rake\n250c250,252\n&lt;     export_filename = ENV['TASK_FILE'] || "/tmp/task-export-#{DateTime.now.to_i}.#{format == 'csv' ? 'csv' : 'tar.gz'}"\n---\n&gt;     # Changed 20-Nov-2017 Steve Kay, case 01974013\n&gt;     #export_filename = ENV['TASK_FILE'] || "/tmp/task-export-#{DateTime.now.to_i}.#{format == 'csv' ? 'csv' : 'tar.gz'}"\n&gt;     export_filename = ENV['TASK_FILE'] || "/backup/foreman/task-export-#{DateTime.now.to_i}.#{format == 'csv' ? 'csv' : 'tar.gz'}"\n[root@gbl15217 ~]#</text>, <text>Hello,\n\nThank you for update. Could you please perform the requested steps and feel free to update the case once done with it.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>tfm-rubygem-foreman-tasks-0.7.14.14-1.el7sat.noarch was reinstalled ok, with satellite-installer run afterwards, on production environment yesterday.\n\nA run of 'foreman-rake foreman_tasks:export_tasks task_search=all' today yields 23,250 records.\n\nqpid-state run today yields this output.\n\n[root@gbl15217 tmp]# qpid-stat --ssl-certificate=/etc/pki/katello/qpid_client_striped.crt -b amqps://localhost:5671 -q katello_event_queue\nProperties:\n  Name                 Durable  AutoDelete  Exclusive  FlowStopped  FlowStoppedCount  Consumers  Bindings\n  =========================================================================================================\n  katello_event_queue  Y        N           N          N            0                 1          6\n\nOptional Properties:\n  Property      Value\n  =====================\n  arguments     {}\n  alt-exchange\n\nStatistics:\n  Statistic                   Messages  Bytes\n  =====================================================\n  queue-depth                 0         0\n  total-enqueues              63,079    1,254,467,944\n  total-dequeues              63,079    1,254,467,944\n  persistent-enqueues         63,079    1,254,467,944\n  persistent-dequeues         63,079    1,254,467,944\n  transactional-enqueues      0         0\n  transactional-dequeues      0         0\n  flow-to-disk-depth          0         0\n  flow-to-disk-enqueues       0         0\n  flow-to-disk-dequeues       0         0\n  acquires                    63,086\n  releases                    7\n  discards-ttl-expired        0\n  discards-limit-overflow     0\n  discards-ring-overflow      0\n  discards-lvq-replace        0\n  discards-subscriber-reject  0\n  discards-purged             0\n  reroutes                    0\n[root@gbl15217 tmp]#\n\nRow counts now report as below:\n\nforeman=# select count(*) from dynflow_actions;\n count\n--------\n 102566\n(1 row)\n\nforeman=# select count(*) from dynflow_execution_plans;\n count\n-------\n 25518\n(1 row)\n\nforeman=# select count(*) from dynflow_steps;\n count\n--------\n 181839\n(1 row)\n\nforeman=# select count(*) from foreman_tasks_locks;\n count\n-------\n 58277\n(1 row)\n\nforeman=# select count(*) from foreman_tasks_tasks;\n count\n-------\n 25518\n(1 row)\n\nThe numbers don't seem to be decreasing then.  Please advise thoughts on next steps.</text>, <text>Note that 'hammer ping' continues to regularly report foreman_tasks as failed, during which time we observe errors "[app] [W] some executors are not responding, check /foreman_tasks/dynflow/status" in foreman production.log.</text>, <text>//Internal; auto-assistant rule reference #: DropboxNotify05.12-0564\n\n    The following files matching case id '01974013' have been found in dropbox.\n\nfilename                                                                                                         size        Attached   type      format     flags\n--------------------------------------------------------------------------------|------------------------------  ----------- ---------- --------- ---------- ----------------------------------\ntasks-case-no-01974013                                                                                              16723610 No                              \ntasks27nov-case-no-01974013                                                                                         19531542 No                              \ntaskviasecureftp-case-no-01974013                                                                                   16723610 No                              \n                                                                                                                    52978762                                  \n\n\nFor additional information on this comment and its content see:\n    * "Diag2.0/CEE-AIR DropboxNotify - file notification processing"\n       https://access.redhat.com/articles/3136611</text>, <text>Hello Steve\n\nWe will check the logs and will get back to you.\n\n\nThanks &amp; Regards\nManeesh Verma</text>, <text>Hello,\n\nFrom the provided logs, few tasks got failed due to error :\n============================================\n Katello::Errors::PulpError\n\nPLP0000: insertDocument :: caused by :: 11000 E11000 duplicate key error index: pulp_database.repo_profile_applicability.$profile_hash_-1_repo_id_-1 dup key: { : "b04811bd9607a00970499b2b4687cf604349cc4a6d4f22b7053585b099d02ccc", : "HSBC-Red_Hat_Enterprise_Linux_Server-Red_Hat_Enterprise_Linux_7_Server_-_Optional_RPMs_x86_64_7Server" } \n============================================\n\nAnd few by: Required lock already taken.\n\nSo, in  order to fix the first error , could you please follow the below article:\n=========================\nhttps://access.redhat.com/solutions/2534911\n==========================\n\nNote: Please take snapshot of your satellite server before proceeding for steps mentioned in above article.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>Thanks.  We have a CR raised to action this fix on Sat 09-Dec.  Will update the case shortly after that.</text>, <text>Sorry, ignore last attachment, was intended for another case !</text>, <text>Moving case Waiting on Customer based on last response from Prashant and Customer.. Customer to come back with details.</text>, <text>[root@gbl15217 jd]# ./remove_dups_units_ostree.sh\nFri  8 Dec 10:53:44 GMT 2017: Detecting duplicates in units_ostree collection..\nFri  8 Dec 10:53:44 GMT 2017: Preparing commands to remove the duplicates..\nFri  8 Dec 10:53:44 GMT 2017: removing the duplicates from mongo (0 mongo commands)..\nFri  8 Dec 10:53:44 GMT 2017: detecting potential duplicates in repo_content_unit (can take several                                                                                                            minutes)..\nFri  8 Dec 11:00:07 GMT 2017: removing duplicates in repo_content_unit (0 mongo commands)..\nFri  8 Dec 11:00:07 GMT 2017: finished. All 0 changes written to "work_done.js" for audit/review.\n[root@gbl15217 jd]# ls -ltr\ntotal 4493680\n-rw-r-----. 1 root root 4601517935 Dec  8 10:51 mongo_data.tar.gz\n-rw-r-----. 1 root root        715 Dec  8 10:51 mongo.snar\n-rwxr-xr-x. 1 root root       3742 Dec  8 10:53 remove_dups_units_ostree.sh\n-rw-r-----. 1 root root          0 Dec  8 10:53 remove_mongo.js\n-rw-r-----. 1 root root          0 Dec  8 10:53 repo_content_units.txt\n-rw-r-----. 1 root root          0 Dec  8 11:00 work_done.js\n[root@gbl15217 jd]#\n[root@gbl15217 jd]# sudo -u apache pulp-manage-db\nAttempting to connect to localhost:27017\nAttempting to connect to localhost:27017\nWrite concern for Mongo connection: {}\nLoading content types.\nLoading type descriptors []\nParsing type descriptors\nValidating type descriptor syntactic integrity\nValidating type descriptor semantic integrity\nLoading unit model: erratum = pulp_rpm.plugins.db.models:Errata\nLoading unit model: distribution = pulp_rpm.plugins.db.models:Distribution\nLoading unit model: package_group = pulp_rpm.plugins.db.models:PackageGroup\nLoading unit model: package_category = pulp_rpm.plugins.db.models:PackageCategory\nLoading unit model: iso = pulp_rpm.plugins.db.models:ISO\nLoading unit model: package_environment = pulp_rpm.plugins.db.models:PackageEnvironment\nLoading unit model: drpm = pulp_rpm.plugins.db.models:DRPM\nLoading unit model: srpm = pulp_rpm.plugins.db.models:SRPM\nLoading unit model: rpm = pulp_rpm.plugins.db.models:RPM\nLoading unit model: yum_repo_metadata_file = pulp_rpm.plugins.db.models:YumMetadataFile\nLoading unit model: puppet_module = pulp_puppet.plugins.db.models:Module\nLoading unit model: docker_blob = pulp_docker.plugins.models:Blob\nLoading unit model: docker_manifest = pulp_docker.plugins.models:Manifest\nLoading unit model: docker_image = pulp_docker.plugins.models:Image\nLoading unit model: docker_tag = pulp_docker.plugins.models:Tag\nLoading auxiliary model: erratum_pkglists = pulp_rpm.plugins.db.models:ErratumPkglist\nUpdating the database with types []\nFound the following type definitions that were not present in the update collection [puppet_module,                                                                                                             iso, package_environment, drpm, srpm, rpm, distribution, docker_image]\nUpdating the database with types [puppet_module, docker_tag, erratum, docker_blob, docker_manifest,                                                                                                             srpm, docker_image]\nContent types loaded.\nEnsuring the admin role and user are in place.\nAdmin role and user are in place.\nBeginning database migrations.\nMigration package pulp.server.db.migrations is up to date at version 24\nMigration package pulp_docker.plugins.migrations is up to date at version 2\nMigration package pulp_puppet.plugins.migrations is up to date at version 5\nMigration package pulp_rpm.plugins.migrations is up to date at version 35\nLoading unit model: erratum = pulp_rpm.plugins.db.models:Errata\nLoading unit model: distribution = pulp_rpm.plugins.db.models:Distribution\nLoading unit model: package_group = pulp_rpm.plugins.db.models:PackageGroup\nLoading unit model: package_category = pulp_rpm.plugins.db.models:PackageCategory\nLoading unit model: iso = pulp_rpm.plugins.db.models:ISO\nLoading unit model: package_environment = pulp_rpm.plugins.db.models:PackageEnvironment\nLoading unit model: drpm = pulp_rpm.plugins.db.models:DRPM\nLoading unit model: srpm = pulp_rpm.plugins.db.models:SRPM\nLoading unit model: rpm = pulp_rpm.plugins.db.models:RPM\nLoading unit model: yum_repo_metadata_file = pulp_rpm.plugins.db.models:YumMetadataFile\nLoading unit model: puppet_module = pulp_puppet.plugins.db.models:Module\nLoading unit model: docker_blob = pulp_docker.plugins.models:Blob\nLoading unit model: docker_manifest = pulp_docker.plugins.models:Manifest\nLoading unit model: docker_image = pulp_docker.plugins.models:Image\nLoading unit model: docker_tag = pulp_docker.plugins.models:Tag\nLoading auxiliary model: erratum_pkglists = pulp_rpm.plugins.db.models:ErratumPkglist\nDatabase migrations complete.\n[root@gbl15217 jd]#\n[root@gbl15217 jd]# for i in pulp_resource_manager pulp_workers pulp_celerybeat; do service $i start                                                                                                           ; done\nRedirecting to /bin/systemctl start pulp_resource_manager.service\nRedirecting to /bin/systemctl start pulp_workers.service\nRedirecting to /bin/systemctl start pulp_celerybeat.service</text>, <text>Hello,\nmy name is Pavel Moravec and I review the case as Subject Matter Expert for Satellite6.\n\nPer the qpid-stat output, the original problem does not appear further. \n\nThe "E11000 duplicate key error index" should be prevented by installing pulp hotfix per [1].\n\nFor the high number of tasks, there is a rake script to cleanup oldest ones - please follow [2].\n\n\nKind regards,\nPavel Moravec\nGSS SEG\n\n[1] https://bugzilla.redhat.com/show_bug.cgi?id=1468022#c51\n[2] https://access.redhat.com/solutions/2755731</text>, <text>Thanks for the update on this case.  The hotfix is installed, the remove_dups_units_ostree.sh script has been run, we see no old tasks present in the database, and we see no further duplicate key issues.  Suggest this case is now closed.</text>, <text>Hello,\n\nThank you for update. I am going to close this case as per your request.\n\nThank you.\n\nWith regards,\nPrashant Waghmare.</text>, <text>qa_notes:\n\n6.2.13-1:\n- been able to safely reproduce the duplicatekeyerror by creating 2 mock consumers and uploading the package profiles for them while manually adding a sleep(30) to the applicability_profile creation branch (to make more time for the tasks to end up trying to create the profile).</text>, <text>qa_notes:\n\nwaiting for https://bugzilla.redhat.com/show_bug.cgi?id=1446712 since it blocks my reproducer.\nAnyway, the fix looks sane to me and i believe the duplicatekeyerror will be properly handled now.</text>, <text>Will/can hotfix RPMs be made for 6.2.13 as well? Or will the ones made previously for 6.2.12 work for 6.2.13 as well?\n\nThanks.</text>, <text>(In reply to Matt from comment #60)\n&gt; Will/can hotfix RPMs be made for 6.2.13 as well? Or will the ones made\n&gt; previously for 6.2.12 work for 6.2.13 as well?\n&gt; \n&gt; Thanks.\n\nI dont know plans about releasing HF for 6.2.13. Applying the HF for 6.2.12 to 6.2.13 will 1) hit package version requirements problems, 2) revert some another fix with reconnection to mongo.\n\nUnofficial unsupported patch that works for me even on 6.2.13 (again, it is unsupported):\n\n\n# cat ~/bz1468022.sat6213.patch \n--- pulp-server-2.8.7.17-1/usr/lib/python2.7/site-packages/pulp/server/managers/consumer/applicability.py\t2017-12-30 09:08:52.523887747 +0100\n+++ pulp-server-2.8.7.15-2.HOTFIXRHBZ1468022/usr/lib/python2.7/site-packages/pulp/server/managers/consumer/applicability.py\t2017-12-30 09:04:09.523498007 +0100\n@@ -7,6 +7,7 @@ from logging import getLogger\n from uuid import uuid4\n \n from celery import task\n+from pymongo.errors import DuplicateKeyError\n \n from pulp.plugins.conduits.profiler import ProfilerConduit\n from pulp.plugins.config import PluginCallConfiguration\n@@ -260,16 +261,20 @@ class ApplicabilityRegenerationManager(o\n                 _logger.debug(msg)\n                 return\n \n-            if existing_applicability:\n-                # Update existing applicability object\n-                existing_applicability.applicability = applicability\n-                existing_applicability.save()\n-            else:\n+            try:\n                 # Create a new RepoProfileApplicability object and save it in the db\n                 RepoProfileApplicability.objects.create(profile_hash,\n                                                         bound_repo_id,\n-                                                        unit_profile['profile'],\n+                                                        profile,\n                                                         applicability)\n+            except DuplicateKeyError:\n+                # Update existing applicability\n+                if not existing_applicability:\n+                    applicability_dict = RepoProfileApplicability.get_collection().find_one(\n+                        {'repo_id': bound_repo_id, 'profile_hash': profile_hash})\n+                    existing_applicability = RepoProfileApplicability(**applicability_dict)\n+                existing_applicability.applicability = applicability\n+                existing_applicability.save()\n \n     @staticmethod\n     def _get_existing_repo_content_types(repo_id):\n@@ -456,6 +461,8 @@ class RepoProfileApplicabilityManager(ob\n         # Remove all RepoProfileApplicability objects that reference these profile hashes\n         if missing_profile_hashes:\n             rpa_collection.remove({'profile_hash': {'$in': missing_profile_hashes}})\n+\n+\n # Instantiate one of the managers on the object it manages for convenience\n RepoProfileApplicability.objects = RepoProfileApplicabilityManager()\n \n# cd /\n# cat ~/bz1468022.sat6213.patch | patch -p1\npatching file usr/lib/python2.7/site-packages/pulp/server/managers/consumer/applicability.py\n# \n\n\nand to apply the patch:\n\nfor i in pulp_resource_manager pulp_workers pulp_celerybeat; do service $i restart; done\n\n\n(but again, this isnt officially supported)</text>, <text>I've unset target milestone because it's 6.3.0 BZ.\nThe one with target milestone 6.2.14 is BZ#1515195.</text>, <text>*** Bug 1530687 has been marked as a duplicate of this bug. ***</text>, <text>Hi Team,\n\nCan we get Hot Fix for 6.2.13 ?\n\nCustomer is asking for Hot Fix for 6.2.13 .\n\nThank\nPrajeesh</text>, <text>=== In Red Hat Customer Portal Case 01915599 ===\n--- Comment by Kunnumbreth, Prajeesh on 1/8/2018 8:24 PM ---\n\nHi Vincent,\n\nAs of now there is no update we will keep you posted .\n\nBest Regards,\nPrajeesh \nRed Hat Technical Support</text>, <text>VERIFIED\non sat6.3.0-33\n\n- had to disable streamer log blacklisting (pulp/server/logs.py)\n- had to introduce some time.sleep() into the `regenerate_applicability` method in order to have enough time to send out package&amp;repository profile from different host.\n- had to add some _logger.info() messages to be sure my request ended up in the DuplicateKeyError branch.\n\n- was able to reliably invoke the dupekey scenario which was now correctly handled by pulp.</text>, <text>Did this fix make it into 6.2.14, and if not might there be a hotfix pending, please?</text>, <text>Hi Ben, it looks like this was cloned to bug 1515195 and should be included in 6.2.14 as part of that bugzilla.</text>, <text>Since the problem described in this bug report should be resolved in a recent advisory, it has been closed with a resolution of ERRATA.\n&gt; \n&gt; For information on the advisory, and where to find the updated files, follow the link below.\n&gt; \n&gt; If the solution does not work for you, open a new bug report.\n&gt; \n&gt; https://access.redhat.com/errata/RHSA-2018:0336</text>]
ham	[[<description>What problem/issue/behavior are you having trouble with?  What do you expect to see?\n\nCreating a host through the api with a non-admin account and inherit hostgroup/global parameters.\n\nWhere are you experiencing the behavior?  What environment?\n\nSatellite 6.2.11\n\nWhen does the behavior occur? Frequently?  Repeatedly?   At certain times?\n\nAlways</description>], <text># API Host create ADMIN islvsu0187 - Successful - Production.log:\n\n2017-11-07 08:29:21 7263bb54 [app] [I]   Parameters: {"host"=&gt;{"build"=&gt;"true", "location_id"=&gt;"2", "name"=&gt;"islvsu0187.spc.local", "interfaces_attributes"=&gt;{"0"=&gt;{"provision"=&gt;"true", "primary"=&gt;"true",\n"type"=&gt;"interface", "mac"=&gt;"00:50:56:b0:7d:4a", "domain_id"=&gt;"3", "managed"=&gt;"true", "subnet_id"=&gt;"2", "ip"=&gt;"145.119.10.187"}, "1"=&gt;{"type"=&gt;"interface", "mac"=&gt;"00:50:56:b0:03:43", "domain_id"=&gt;"2", "m\nanaged"=&gt;"true", "subnet_id"=&gt;"6", "ip"=&gt;"192.168.106.187"}}, "managed"=&gt;"true", "hostgroup_id"=&gt;"45", "organization_id"=&gt;"3"}, "apiv"=&gt;"v2"}\n2017-11-07 08:29:21 7263bb54 [app] [I] Authorized user admin(Admin User)\n2017-11-07 08:29:23 7263bb54 [app] [I] Create DHCP reservation for islvsu0187.spc.local-00:50:56:b0:7d:4a/145.119.10.187</text>, <text># API Host create NON-ADMIN islvsu0161 - Not Succesfull - api debug output:\n\n{"host": {"name": "islvsu0161.spc.local", "organization_id": "3", "build": "true", "hostgroup_id": "45", "interfaces_attributes": {"0": {"mac": "00:50:56:b0:88:3f", "subnet_id": "2", "primary": "true", "provision": "true", "type": "interface", "domain_id": "3", "ip": "145.119.10.161", "managed": "true"}, "1": {"mac": "00:50:56:b0:59:ab", "subnet_id": "6", "type": "interface", "domain_id": "2", "ip": "192.168.106.161", "managed": "true"}}, "managed": "true", "location_id": "2"}}\n\nfull message: {\n  "error": {"id":null,"errors":{"architecture_id":["can't be blank"],"root_pass":["should be 8 characters or more","should not be blank - consider setting a global or host group default"],"medium_id":["can't be blank"]},"full_messages":["Architecture can't be blank","Root password should be 8 characters or more","Root password should not be blank - consider setting a global or host group default","Medium can't be blank"]}\n}</text>, <text># API Host create NON-ADMIN islvsu0161 (with arch, rootpw and medium) - Partly Successfull - Production.log:\n\n2017-11-07 12:10:36 179bd5bb [app] [I]   Parameters: {"host"=&gt;{"build"=&gt;"true", "hostgroup_id"=&gt;"45", "architecture_id"=&gt;"1", "interfaces_attributes"=&gt;{"0"=&gt;{"primary"=&gt;"true", "type"=&gt;"interface", "domain_id"=&gt;"3", "managed"=&gt;"true", "provision"=&gt;"true", "subnet_id"=&gt;"2", "mac"=&gt;"00:50:56:b0:88:3f", "ip"=&gt;"145.119.10.161"}, "1"=&gt;{"type"=&gt;"interface", "domain_id"=&gt;"2", "managed"=&gt;"true", "subnet_id"=&gt;"6", "mac"=&gt;"00:50:56:b0:59:ab", "ip"=&gt;"192.168.106.161"}}, "name"=&gt;"islvsu0161.spc.local", "location_id"=&gt;"2", "managed"=&gt;"true", "medium_id"=&gt;"15", "organization_id"=&gt;"3", "root_pass"=&gt;"[FILTERED]"}, "apiv"=&gt;"v2"}\n2017-11-07 12:10:36 179bd5bb [app] [I] Authorized user edithost_api(edithost_api )\n2017-11-07 12:10:37 179bd5bb [app] [I] Create DHCP reservation for islvsu0161.spc.local-00:50:56:b0:88:3f/145.119.10.161\n2017-11-07 12:10:39 179bd5bb [app] [I]   Rendered api/v2/hosts/create.json.rabl (685.0ms)\n2017-11-07 12:10:39 179bd5bb [app] [I] Completed 201 Created in 3359ms (Views: 614.5ms | ActiveRecord: 168.0ms)</text>, <text>The first host: is created as admin through the api and is succesfull. It inherits parameters from hostgroup/global\n\nSecond host: The same thing with a non-admin account complains about Architecture, Root pw, Medium.\n\nThird host: The same thing with a non-admin account with Architecture, Root pw, Medium options is partly successfull. It does not inherit other parameters as Content Source host, Operating System etc.</text>, <text>Details about the non-admin host used for the create host via the api:\n\n[kleinm@islvsu1002 ~]$ hammer user info --login edithost_api\nId:                   21\nLogin:                edithost_api\nName:                 edithost_api\nEmail:\nAdmin:                no\nAuthorized by:        Internal\nLocale:               default\nTimezone:\nLast login:           2017/11/07 11:10:36\nDefault organization:\nDefault location:\nRoles:\n    Anonymous\n    Create Content Hosts\nUser groups:\n\nLocations:\n    Amsterdam\n    DenHaag\n    Rotterdam\n    TOC-A\n    TOC-B\n    Utrecht\n    Zwolle\nOrganizations:\n    Politie\nCreated at:           2017/08/31 07:38:28\nUpdated at:           2017/11/07 11:10:36\n\n\n[kleinm@islvsu1002 ~]$ hammer filter list --search "Create Content Hosts"\n----|-------------------------|--------|------------|-----------------------|-------------------------------------------------------------\nID  | RESOURCE TYPE           | SEARCH | UNLIMITED? | ROLE                  | PERMISSIONS\n----|-------------------------|--------|------------|-----------------------|-------------------------------------------------------------\n209 | Katello::System         | none   | yes        | Register Content Host | create_content_hosts\n222 | Hostgroup               | none   | yes        | Create Content Hosts  | view_hostgroups\n223 | Katello::ActivationKey  | none   | yes        | Create Content Hosts  | view_activation_keys\n224 | Katello::System         | none   | yes        | Create Content Hosts  | edit_content_hosts, create_content_hosts, view_content_hosts\n225 | Katello::ContentView    | none   | yes        | Create Content Hosts  | view_content_views\n226 | Katello::GpgKey         | none   | yes        | Create Content Hosts  | view_gpg_keys\n227 | Katello::Subscription   | none   | yes        | Create Content Hosts  | view_subscriptions, attach_subscriptions\n228 | Host                    | none   | yes        | Create Content Hosts  | view_hosts, build_hosts, edit_hosts, create_hosts\n229 | Katello::HostCollection | none   | yes        | Create Content Hosts  | view_host_collections\n230 | Organization            | none   | yes        | Create Content Hosts  | view_organizations\n231 | Katello::KTEnvironment  | none   | yes        | Create Content Hosts  | view_lifecycle_environments\n232 | Katello::Product        | none   | yes        | Create Content Hosts  | view_products\n233 | Location                | none   | yes        | Create Content Hosts  | view_locations\n234 | Domain                  | none   | yes        | Create Content Hosts  | view_domains\n235 | Architecture            | none   | yes        | Create Content Hosts  | view_architectures\n236 | Operatingsystem         | none   | yes        | Create Content Hosts  | view_operatingsystems\n237 | Subnet                  | none   | yes        | Create Content Hosts  | view_subnets\n238 | Medium                  | none   | yes        | Create Content Hosts  | view_media\n239 | Realm                   | none   | yes        | Create Content Hosts  | view_realms\n240 | CommonParameter         | none   | yes        | Create Content Hosts  | view_globals\n241 | VariableLookupKey       | none   | yes        | Create Content Hosts  | view_external_variables\n----|-------------------------|--------|------------|-----------------------|-------------------------------------------------------------</text>, <text>Greetings, Merald!\n\nThank you for contacting Red Hat Support.\n\nMy name is Denis, I have taken brief ownership and will be assisting you in transitioning this case to an engineer. We appreciate you proactively providing us with the outputs you are receiving. This information is important to our engineers for troubleshooting. \n\nOnce an engineer is allocated to your case, we request that you provide them with some time to review it and they will update you with possible troubleshooting steps within eight business hours, per your current severity SLA. More details on SLA are in the reference link below.\n\n-Production Support Terms of Service\nhttps://access.redhat.com/support/offerings/production/sla\n\nShould you run into any more issues or have any more questions, please do not hesitate to reach out to us by updating the case as it is the most efficient way of contacting an engineer allocated to the case.\n\nAgain, thank you for contacting Red Hat Support!\n\n\nDenis Canelo\nCustomer Support Specialist\nGlobal Support Services\nRed Hat, Inc.</text>, <text>Hi Merald,\n\nMy name is Rajan and I have taken the ownership of the case. \n\nAs per the case description, You are facing an issue while creating a host using non-admin user account on satellite server.\n\nThank you for proactively providing the necessary information. Please allow me some time to check and I will provide an update on the same.\n\nI would like to know if those hostgroup and medium is available in the correct organization and location.\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hi Rajan,\n\nThe information you requested is in the correct Organization and Location:\n\n$ hammer hostgroup list --organization-id 3 --location-id 2 | grep -E "^45|^-|^ID"\n---|---------------|-------------------------------------|----------------------------|--------------------------------|------\nID | NAME          | TITLE                               | OPERATING SYSTEM           | ENVIRONMENT                 | MODEL\n---|---------------|-------------------------------------|----------------------------|--------------------------------|------\n45 | rh6-legacy    | hosting-linux/develop/rh6-legacy    | os-hosting-linux-rhel 6.9  |                 |\n---|---------------|-------------------------------------|----------------------------|--------------------------------|------\n\n\n$ hammer medium list --organization-id 3 --location-id 2 | grep -E "^15|^-|^ID"\n---|----------------------------------------------------------------------------------|---------------------------------------------------------------------------------\nID | NAME                                                                             | PATH\n---|----------------------------------------------------------------------------------|---------------------------------------------------------------------------------\n15 | Politie/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_6_Server_Kickstart_x8... | http://islvsu1002.idm.toc.internal/pulp/repos/Politie/Library/content/dist/rh...\n---|----------------------------------------------------------------------------------|---------------------------------------------------------------------------------</text>, <text>Hi Merald,\n\nJust for testing purpose, Kindly try creating a new host using the non-admin user in Satellite WebUI and share the output of the error message.\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>//Collaboration Note//\n\nIssue: Creating a host through the api with a non-admin account and inherit hostgroup/global parameters.\nIt does not inherit other parameters as Content Source host, Operating System etc.\n\nVersion: Satellite 6.2\n\n- Host Creation with non-admin user fails with following server.\n\n# API Host create NON-ADMIN islvsu0161 - Not Succesfull - api debug output:\n\n{"host": {"name": "islvsu0161.spc.local", "organization_id": "3", "build": "true", "hostgroup_id": "45", "interfaces_attributes": {"0": {"mac": "00:50:56:b0:88:3f", "subnet_id": "2", "primary": "true", "provision": "true", "type": "interface", "domain_id": "3", "ip": "145.119.10.161", "managed": "true"}, "1": {"mac": "00:50:56:b0:59:ab", "subnet_id": "6", "type": "interface", "domain_id": "2", "ip": "192.168.106.161", "managed": "true"}}, "managed": "true", "location_id": "2"}}\n\nfull message: {\n  "error": {"id":null,"errors":{"architecture_id":["can't be blank"],"root_pass":["should be 8 characters or more","should not be blank - consider setting a global or host group default"],"medium_id":["can't be blank"]},"full_messages":["Architecture can't be blank","Root password should be 8 characters or more","Root password should not be blank - consider setting a global or host group default","Medium can't be blank"]}\n}\n\n--------\n# API Host create NON-ADMIN islvsu0161 (with arch, rootpw and medium) - Partly Successfull - Production.log:\n\n2017-11-07 12:10:36 179bd5bb [app] [I]   Parameters: {"host"=&gt;{"build"=&gt;"true", "hostgroup_id"=&gt;"45", "architecture_id"=&gt;"1", "interfaces_attributes"=&gt;{"0"=&gt;{"primary"=&gt;"true", "type"=&gt;"interface", "domain_id"=&gt;"3", "managed"=&gt;"true", "provision"=&gt;"true", "subnet_id"=&gt;"2", "mac"=&gt;"00:50:56:b0:88:3f", "ip"=&gt;"145.119.10.161"}, "1"=&gt;{"type"=&gt;"interface", "domain_id"=&gt;"2", "managed"=&gt;"true", "subnet_id"=&gt;"6", "mac"=&gt;"00:50:56:b0:59:ab", "ip"=&gt;"192.168.106.161"}}, "name"=&gt;"islvsu0161.spc.local", "location_id"=&gt;"2", "managed"=&gt;"true", "medium_id"=&gt;"15", "organization_id"=&gt;"3", "root_pass"=&gt;"[FILTERED]"}, "apiv"=&gt;"v2"}\n2017-11-07 12:10:36 179bd5bb [app] [I] Authorized user edithost_api(edithost_api )\n2017-11-07 12:10:37 179bd5bb [app] [I] Create DHCP reservation for islvsu0161.spc.local-00:50:56:b0:88:3f/145.119.10.161\n2017-11-07 12:10:39 179bd5bb [app] [I]   Rendered api/v2/hosts/create.json.rabl (685.0ms)\n2017-11-07 12:10:39 179bd5bb [app] [I] Completed 201 Created in 3359ms (Views: 614.5ms | ActiveRecord: 168.0ms)\n\n----------------\n\nI am fairly new in API and Need advice. \n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hello Rajan,\n\nWhile creating this host as an Admin in Satellite WebUI, as soon as I select the hostgroup, it automatically fills other parameters:\nLifecycle Environment, Content View, content source, realm, architecture, operating system, Media Selection, Synced content, partition table, global parameters.\n\nWhen I create this host as a non-admin user in Satellite WebUI, I don't get errors, it just just does not autofill some parameters. It does fill parameters:\nLifecycle Environment, Content View, Media Selection, Synced Content, Partition Table, global parameters.\n\nRegards,\nMarald Klein</text>, <text>//collab\n\nHello Rajan,\n\nCan see that CU included roles which are assigned to the non-admin user in C#5.\n\nCan you also ask him to provide the command which he is using to create the host so we can try to reproduce it internally?\n\nFrom your collab note:\n\n# API Host create NON-ADMIN islvsu0161 (with arch, rootpw and medium) - Partly Successfull - Production.log\n\nWhat does it mean partially successful?\n\nCan customer create host successfully under satellite webui when he fills up all the parameters manually? \n\nRegards\n--Michal</text>, <text>Hello Michal,\n\nThank you for contributing to this case.\n\nFollowing query was answered by Cu in the case comment #11\n\nWhat does it mean partially successful? \n================\nWhile creating this host as an Admin in Satellite WebUI, as soon as I select the hostgroup, it automatically fills other parameters:\nLifecycle Environment, Content View, content source, realm, architecture, operating system, Media Selection, Synced content, partition table, global parameters.\n\nWhen I create this host as a non-admin user in Satellite WebUI, I don't get errors, it just just does not autofill some parameters. It does fill parameters:\nLifecycle Environment, Content View, Media Selection, Synced Content, Partition Table, global parameters.\n================\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hello Marald,\n\nWe would like to know the complete command which you are using to create a host using a non-admin user and also let us know if host creation gets successful when you create a host in WebUI?\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hello Rajan,\n\n1.) I have a Python script that does a Post request to the Satellite server:\n\nDEBUG:urllib3.connectionpool:https://islvsu1002.idm.toc.internal:443 "POST /api/v2/hosts HTTP/1.1" 201 None\n\nAs input I use a dictionary, which can be found in the first 3 messages of this case.\n\n2.) A manually created host in de Satellite WebUI with the same non-admin api-user is successfull and installation is also successfull. Except when creating the host, it does not autofill (inherit) certain options from the hostgroup or in case of the root password from a global template.\n\nRegards,\nMarald</text>, <text>//collab\n\nPlease check if the non-admin user islvsu0161 has rights to work with hostgroup:\n\nCreated "Create Content Hosts" Role should have a resource:\n\nResouce         Permissions\n===========================\nHost Group \tedit_hostgroups, create_hostgroups, view_hostgroups\n\nRegards\n--Michal</text>, <text>Hello,\n\nCould you please check followings:\n\nPlease check if the non-admin user islvsu0161 has rights to work with hostgroup:\n\nCreated "Create Content Hosts" Role should have a resource:\n\nResouce         Permissions\n===========================\nHost Group \tedit_hostgroups, create_hostgroups, view_hostgroups\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hello Rajan,\n\nWe use the islvsuxxxx for server naming. The non-admin user is "edithost_api", which does have the hostgroup permissions (post #5)\n\nRegards,\nMarald</text>, <text>Hello Michal,\n\nCould you please help in moving this case forward?\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>//collab\n\nFinally manage to test it (Satellite 6.2.12) and its working ok, I've created non-admin user with the same roles customer is using, from comment #5 and when i select the host group it inherits all the parameters from it.\n\nAccording to case description customer is using Satellite 6.2.11, can we ask him to update?\n\nRegards\n--Michal</text>, <text>Hello,\n\nWe have tested the same set of permission on our test environment and it worked in Red Hat Satellite 6.2.12. I would request you to upgrade your satellite to the same version. Make sure to take prior backup/snapshot of the satellite server before the upgrade process.\n\nTo upgrade your server you need to run the following command and make sure no tasks are executing on the satellite server.\n===========================\n# yum update -y (Reboot if kernel package gets updated)\n\n# satellite-installer --scenario satellite --upgrade\n\n# hammer ping (check the status of services)\n\n# katello-service status\n===========================\nYou can refer following documentation for upgrade procedure.(Section: CHAPTER 6. UPGRADING SATELLITE SERVER AND CAPSULE SERVER)\nhttps://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/pdf/installation_guide/Red_Hat_Satellite-6.2-Installation_Guide-en-US.pdf\n\nOnce done, Try creating a host using a non-admin user.\n\nShare your observations.\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hello Rajan,\n\nA collegue of mine did upgrade the Satellite server last thursday to 6.2.12\n\nThe issue is still present, both through the api or the WebUI.\n\nRegards,\nMarald</text>, <text>[kleinm@islvsu1002 ~]$ hammer host create --name 'islvsu0161' --organization "Politie" --location "TOC-B" --hostgroup-id 45 --build true --interface mac="00:50:56:b0:88:3f",ip="145.119.10.161",subnet_id="2",domain_id="3",managed=true,primary=true,provision=true --interface mac="00:50:56:b0:59:ab",ip="192.168.106.161",subnet_id="6",domain_id="2",managed=true\n[Foreman] Password for edithost_api:\nCould not create the host:\n  Architecture can't be blank\n  Root password should be 8 characters or more\n  Root password should not be blank - consider setting a global or host group default\n  Medium can't be blank\n\n[kleinm@islvsu1002 ~]$ rpm -qa | grep ^satellite\nsatellite-6.2.12-6.0.el7sat.noarch\nsatellite-installer-6.2.0.13-1.el7sat.noarch\nsatellite-cli-6.2.12-6.0.el7sat.noarch</text>, <text>[kleinm@islvsu1002 ~]$ hammer host create --name 'islvsu0161' --organization "Politie" --location "TOC-B" --hostgroup-id 45 --build true --interface mac="00:50:56:b0:88:3f",ip="145.119.10.161",subnet_id="2",domain_id="3",managed=true,primary=true,provision=true --interface mac="00:50:56:b0:59:ab",ip="192.168.106.161",subnet_id="6",domain_id="2",managed=true\n[Foreman] Password for admin:\nHost created</text>, <text>Hi Merald,\n\nThank you for writing back. \n\nI have involved our senior members on this issue to help you further.\n\nThank you for your patience.\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hello Michal,\n\nCould you please check the recent updates from the Cu?\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hi Merald,\n\nYou will hear from our senior engineer very soon.\nThank you for your patience.\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hello Merald,\n\nMy name is Michal and I'm working on this case with Rajan.\n\nCan we take the remote session on Monday 10:00 to check this together?\n\nRegards\n--Michal</text>, <text>Hello Michal,\n\nWithin our organization, remote sessions is not possible. It's against policy.\n\nCan we work around this?\n\nWe do have a a Red Hat TAM: Ron van der Wees. I'm not sure if he could be of assistence?\n\nRegards,\nMarald Klein</text>, <text>Hello Marald,\n\nOK, will have to work without remote then.\n\n--organization "Politie" \n--location "TOC-B"\n\nCan you please check if host group with id 45 is assigned to this organization and location under user edithost_api?\n\nRegards\n--Michal</text>, <text>Hello Michal,\n\nIt is assigned, please have a look at the following comments:\n\nhttps://access.redhat.com/support/cases/#/case/01968312?commentId=a0aA000000KwfGWIAZ\nhttps://access.redhat.com/support/cases/#/case/01968312?commentId=a0aA000000KxGxgIAF\n\nRegards,\nMarald Klein</text>, <text>Hello Marald,\n\nsorry, I didn't read trough the case history completely.\n\nGoing to discus this case with Engineering on Monday to see if they will have some more ideas to try here.\n\nRegards\n--Michal</text>, <text>Hello Michal,\n\nCould you please help in providing an update on this case?\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hi Merald,\n\nPlease provide this to have a details about host group, architecture ... when executed under user edithost_api:\n\n# hammer hostgroup info --id 45\n# hammer subnet info --id 6\n# hammer domain info --id 2\n# hammer architecture info --id X, where X is an architecture id used in hostgroup id 45\n# hammer os info --id X where X is an operating system id used in a host group id 45\n# hammer partition-table info --id X where X is an operating system id used in a host group id 45\n\nRegards\n--Michal</text>, <text>Hello Michal,\n\nYour questions reveal part of the problem. In case of (for example) the architecture, this is also inherited from a parent hostgroup. The user edithost_api may read the hostgroup id 45, but not its parents!?\n\n[kleinm@islvsu1002 ~]$ hammer -u edithost_api hostgroup info --id 45\nId:                     45\nName:                   rh6-legacy\nTitle:                  hosting-linux/develop/rh6-legacy\nOperating System:       os-hosting-linux-rhel 6.9\nPartition Table:        partition_table_hosting-linux_rhel\nPuppet CA Proxy Id:\nPuppet Master Proxy Id:\nPuppetclasses:\n\nParameters:\n    kt_activation_keys =&gt; ak-hosting-linux-develop-rh6-legacy,ak-politie-packages-rh6\nLocations:\n    TOC-A\n    TOC-B\nOrganizations:\n    Politie\nParent Id:              38/39\n\n[kleinm@islvsu1002 ~]$ hammer -u edithost_api hostgroup info --id 39\nResource hostgroup not found by id '39'\n[kleinm@islvsu1002 ~]$ hammer -u edithost_api hostgroup info --id 38\nResource hostgroup not found by id '38'\n\nDoing this as the admin user shows the following:\n\n[kleinm@islvsu1002 ~]$ hammer -u admin hostgroup info --id 39\nId:                     39\nName:                   develop\nTitle:                  hosting-linux/develop\nPuppet CA Proxy Id:\nPuppet Master Proxy Id:\nPuppetclasses:\n\nParameters:\n    hosting-linux-puppet_environment =&gt; develop\n    hosting-linux-puppet_master =&gt; puppet.unix.beheer.local\n    hosting-linux-puppet_reportserver =&gt; islvsu6001.unix.beheer.local\nOrganizations:\n    Politie\nParent Id:              38\n\n[kleinm@islvsu1002 ~]$ hammer -u admin hostgroup info --id 38\nId:                     38\nName:                   hosting-linux\nTitle:                  hosting-linux\nArchitecture:           x86_64\nPuppet CA Proxy Id:\nPuppet Master Proxy Id:\nPuppetclasses:\n\nParameters:\n    hosting-linux-puppet_ca =&gt; islvsu6001.unix.beheer.local\n    idm_domain =&gt; idm.toc.internal\n    idm_server =&gt; islvsu1001.idm.toc.internal\nOrganizations:\n    Politie\nParent Id:\n\nKind regards,\nMarald</text>, <text>The parent hostgroup "hosting-linux" didn't have any locations specified. If we select all locations on the parent hostgroup, the edithost_api user may read the parent hostgroup. This solves the part for the architecture and root password, but not for the Medium!?\n\nStrange, because according to the WebUI this is specified in hostgroup (id=45). Though the hammer hostgroup info doen't show the same information (see output in previous comment.)\n\nWe were alson under the impression that the user edithost_api should be able to read the parent hostgroup (id=38), because he is member of the location "TOC-B" and this location is defined with *all* hostgroups:\n\n[kleinm@islvsu1002 ~]$ hammer -u edithost_api location info --name TOC-B\nId:                 2\nName:               TOC-B\nUsers:\n&lt;snip&gt;\nInstallation media:\n    Politie/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_6_Server_Kickstart_x86_64_6_8\n    Politie/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_6_Server_Kickstart_x86_64_6_9\n    Politie/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_7_Server_Kickstart_x86_64_7_2\n&lt;snip&gt;\nHostgroups:\n    hosting-linux\n    rh6-legacy\n    beheer\n    rh6-legacy\n    rh6-legacy\n    rh6-legacy\n    ontw\n    prod\n    prod\n    test\n    sat6\n\nRegards,\nMarald</text>, <text>Hello Marald,\n\nYou wrote\n\n"The parent hostgroup "hosting-linux" didn't have any locations specified. If we select all locations on the parent hostgroup, the edithost_api user may read the parent hostgroup. This solves the part for the architecture and root password, but not for the Medium!?\n\nStrange, because according to the WebUI this is specified in hostgroup (id=45)"\n\nDoes this mean that when you try to create host with user edithost_api in the webui its does not auto fill media but its possible to select desired media from drop down menu? \nCreating host with hammer still screams 'Medium can't be blank.'\n\nI've also try to reproduce this as hammer output from you revealed that you are using parent/child host groups or we also call it nested host groups.\n\nHave a same type of permissions in the created role "Create Content Hosts":\n\n\n[root@provisioning ~]# hammer filter list --search "Create Content Hosts" (can just ignore first three lines):\n----|-------------------------|--------|------------|----------------------|--------------------------------------------------\nID  | RESOURCE TYPE           | SEARCH | UNLIMITED? | ROLE                 | PERMISSIONS                                      \n----|-------------------------|--------|------------|----------------------|--------------------------------------------------\n155 | Katello::System         | none   | no         | 1317806              | create_content_hosts                             \n320 | Katello::System         | none   | yes        | ALL_PERMISSIONS      | create_content_hosts                             \n395 | Katello::System         | none   | yes        | rbobek-devops        | create_content_hosts                             \n398 | Katello::System         | none   | yes        | virt-who             | create_content_hosts                             \n410 | Hostgroup               | none   | yes        | Create Content Hosts | view_hostgroups                                  \n411 | Katello::System         | none   | yes        | Create Content Hosts | create_content_hosts                             \n412 | Katello::ActivationKey  | none   | yes        | Create Content Hosts | view_activation_keys                             \n413 | Katello::ContentView    | none   | yes        | Create Content Hosts | view_content_views                               \n414 | Katello::GpgKey         | none   | yes        | Create Content Hosts | view_gpg_keys                                    \n415 | (Miscellaneous)         | none   | yes        | Create Content Hosts | access_dashboard                                 \n416 | Katello::Subscription   | none   | yes        | Create Content Hosts | attach_subscriptions, view_subscriptions         \n417 | Host                    | none   | yes        | Create Content Hosts | build_hosts, view_hosts, create_hosts, edit_hosts\n418 | Katello::HostCollection | none   | yes        | Create Content Hosts | view_host_collections                            \n419 | Organization            | none   | yes        | Create Content Hosts | view_organizations                               \n420 | Katello::KTEnvironment  | none   | yes        | Create Content Hosts | view_lifecycle_environments                      \n421 | Katello::Product        | none   | yes        | Create Content Hosts | view_products                                    \n422 | Location                | none   | yes        | Create Content Hosts | view_locations                                   \n423 | Domain                  | none   | yes        | Create Content Hosts | view_domains                                     \n424 | Architecture            | none   | yes        | Create Content Hosts | view_architectures                               \n425 | Operatingsystem         | none   | yes        | Create Content Hosts | view_operatingsystems                            \n426 | Subnet                  | none   | yes        | Create Content Hosts | view_subnets                                     \n427 | Medium                  | none   | yes        | Create Content Hosts | view_media                                       \n428 | Realm                   | none   | yes        | Create Content Hosts | view_realms                                      \n429 | CommonParameter         | none   | yes        | Create Content Hosts | view_globals                                     \n430 | VariableLookupKey       | none   | yes        | Create Content Hosts | view_external_variables                          \n----|-------------------------|--------|------------|----------------------|--------------------------------------------------\n\nCreated user mdekan and assigned him role 'Create Content Hosts'\n\n\n[root@provisioning ~]# hammer user info --login mdekan\nId:                   28\nLogin:                mdekan\nName:                  \nEmail:                mdekan@test.com\nAdmin:                no\nAuthorized by:        Internal\nLocale:               default\nTimezone:             \nLast login:           2017/11/23 09:31:43\nDefault organization: \nDefault location:     \nRoles:                \n    Create Content Hosts\n    Anonymous\nUser groups:          \n\nLocations:            \n    Default Location\nOrganizations:        \n    Default Organization\n    mdekan-org1\nCreated at:           2017/04/20 08:20:57\nUpdated at:           2017/11/23 09:31:43\n\n\nCan see child hostgroup under mdekan user:\n\n[root@provisioning ~]# hammer -u mdekan -p redhat hostgroup info --id 21\nId:                     21\nName:                   mdekan-hostgroup-test-beta\nTitle:                  RHEL7/mdekan-hostgroup-test/mdekan-hostgroup-test-beta\nOperating System:       RedHat 7.3\nArchitecture:           x86_64\nPartition Table:        Kickstart default\nMedium:                 Default_Organization/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_7_Server_Kickstart_x86_64_7_3\nPuppet CA Proxy Id:     \nPuppet Master Proxy Id: \nPuppetclasses:          \n\nParameters:             \n\nLocations:              \n    Default Location\nOrganizations:          \n    Default Organization\nParent Id:              3/20\n\nCan also see parent host groups with mdekan:\n\n\n[root@provisioning ~]# hammer -u mdekan -p redhat hostgroup info --id 3\nId:                     3\nName:                   RHEL7\nTitle:                  RHEL7\nOperating System:       RedHat 7.3\nEnvironment:            production\nSubnet:                 sysmgmt.lan\nDomain:                 sysmgmt.lan\nArchitecture:           x86_64\nPartition Table:        Kickstart default\nPuppet CA Proxy Id:     1\nPuppet Master Proxy Id: 1\nComputeProfile:         1-Small\nPuppetclasses:          \n    foreman_scap_client\nParameters:             \n    kt_activation_keys =&gt; RHEL7_Library\nLocations:              \n    Default Location\n    test_location\nOrganizations:          \n    Default Organization\n    mdekan-org1\n    virt_who_test\nParent Id:\n\n[root@provisioning ~]# hammer -u mdekan -p redhat hostgroup info --id 20\nId:                     20\nName:                   mdekan-hostgroup-test\nTitle:                  RHEL7/mdekan-hostgroup-test\nOperating System:       RedHat 7.3\nArchitecture:           x86_64\nPartition Table:        Kickstart default\nPuppet CA Proxy Id:     \nPuppet Master Proxy Id: \nPuppetclasses:          \n\nParameters:             \n\nLocations:              \n    Default Location\nOrganizations:          \n    Default Organization\nParent Id:              3\n\nSo far I fail to see what's different in your setup. \nStill trying to find out what's the cause of not inheriting Installation Media from the host group and how come its not showing info about parent host groups in your case.\n\nRegards\n--Michal</text>, <text>deleted</text>, <text>Hello Marald,\n\nFrom 'hammer -u edithost_api location info --name TOC-B' I can see that Installation Media is assigned to the location.\n\nCan you also check if the Installation Media is assigned to the Organization?\n\n# hammer -u -u edithost_api medium info --id X ---&gt; where id is taken from hammer medium list\n\nRegards\n--Michal</text>, <text>Hello Michal,\n\nApparently I also had to specify all locations on the second level of the hostgroup (hosting-linux/develop) as well. This also solves the Medium error (WebUI, hammer, API).\n\nAgain, we shouldn't have to specify all locations in the parent hostgroups, because we specified the access to the hostgroups in the Location 'TOC-B'.\n\n&gt; So far I fail to see what's different in your setup.\nIn our case the first two levels of the hostgroup, we left the location entirly empty.\nThis was only specified in the child hostgroup (third level).\n\nhammer -u edithost_api medium info --id 15\nId:                15\nName:              Politie/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_6_Server_Kickstart_x86_64_6_9\nPath:              http://islvsu1002.idm.toc.internal/pulp/repos/Politie/Library/content/dist/rhel/server/6/6.9/x86_64/kickstart/\nOS Family:         Redhat\nOperating systems:\n    os-hosting-linux-rhel 6.9\n    RedHat 6.9\nLocations:\n    Amsterdam\n    DenHaag\n    Rotterdam\n    TOC-A\n    TOC-B\n    Utrecht\n    Zwolle\nOrganizations:\n    Politie\nCreated at:        2017/09/08 10:11:28\nUpdated at:        2017/09/15 08:37:52\n\nRegards,\nMarald</text>, <text>Hello Marald,\n\nI understand your setup is this:\n\nHG: hosting-linux/develop/rh6-legacy\n      |  \n      |=========&gt; location empty\n\n       develop\n          |\n          |=====&gt; location empty  \n             |\n             | \n            rh6-legacy\n             |\n             |=====&gt; location TOC-B\n\nIf its set like this it screams medium is blank.\nAccess to all the host groups was specified in location TOC-B\n\n\n[kleinm@islvsu1002 ~]$ hammer -u edithost_api location info --name TOC-B\nId:                 2\nName:               TOC-B\nUsers:\n&lt;snip&gt;\nInstallation media:\n    Politie/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_6_Server_Kickstart_x86_64_6_8\n    Politie/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_6_Server_Kickstart_x86_64_6_9\n    Politie/Library/Red_Hat_Server/Red_Hat_Enterprise_Linux_7_Server_Kickstart_x86_64_7_2\n&lt;snip&gt;\nHostgroups:\n    hosting-linux\n    rh6-legacy\n    beheer\n    rh6-legacy\n    rh6-legacy\n    rh6-legacy\n    ontw\n    prod\n    prod\n    test\n    sat6\n\nWhen the second level of the hostgroup (hosting-linux/develop) was assigned to the location even medium was finally inherited.\nHowever it is expected that the only last child host group (rh6-legacy) should be assigned to the location 'TOC-B' to be able to inherit the medium.\n\nIs my assumption correct?\n\nRegards\n--Michal</text>, <text>Hi Michal,\n\n&gt; Is my assumption correct?\nYes, 100%\n\nRegards,\nMarald</text>, <text>Hi Merald.\n\nMichal will respond to your query soon.\n\nThank you for your patience.\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>//inernal\n\n can you crate a bz for this? WoContributor is incorrect status ... ENg needs to check this, we need bz for that ....</text>, <text>Hello Michal,\n\nCould you please help me understanding the issue here in this case since I have not worked on it and I am aware?\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hello Michal,\n\nIf this BZ (https://bugzilla.redhat.com/show_bug.cgi?id=1508609) which I had created resembles with this case?\n\nRegards,\nRajan</text>, <text>It has been almost 2 months since the last update.\nAny news on this?</text>, <text>Hello Swapnil,\n\nCu is looking for an update and fix for the 6.2. \n\nAny idea?\n\nRegards,\nRajan</text>, <text>Hello Merald,\n\nAs per the recent update, It will be fixed in Satellite 6.3. However, There is no update on getting this fixed in current version 6.2.x or earlier. I have asked for the same.\n\nThank you for your patience.\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Based on comment 7, moving to ON_QA as it was most likely resolved in 6.3. I don't think this could be backported to 6.2. If QA confirms it's resolved in 6.3, this should be closed as CURRENT RELEASE.</text>, <text>Verified on 6.4 snap 5, installation media are displayed in the output of the hammer location info straight away after creation</text>, <text>Hello,\n\nI would like to inform you that this has beed fixed and it will be available in Satellite 6.4 in future.\n\nPlease confirm if this case can be closed.\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>, <text>Hello Rajan,\n\nNot sure when Satellite 6.4 will be released, would it be possible to implement this in 6.3?\n\nRegards,\nMarald Klein</text>, <text>Hello,\n\nThis issue has been fixed in the Red Hat Satellite 6.4. However, We were not able to reproduce it in 6.3 as well. It could have also been fixed in 6.3 but there is no official update on it.\n\nPlease check if it has been fixed for you in 6.3 and share your obsrevations.\n\nRegards,\nRajan Gupta\nGSS, Red Hat</text>]
